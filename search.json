[{"title":"Embedding和Word2vec的理解","url":"/Deep-Learning/2873c4549d8b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文并不是深挖Word2vec和Embedding原理公式推导的文章，网上已经有很多针对性的原理讲解文章，大家可以自行了解。本文主要是针对词嵌入的相关概念、原理及模式进行提炼陈列，扫盲和抛砖引玉。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Embedding就是用一个低维的向量表示一个物体，这个Embedding向量的性质是能使距离相近的向量对应的物体有相近的含义，正是因为Embedding能够用低维向量对物体进行编码还能保留其含义的特点，所以其非常适合用于深度学习。由于我们熟知的one hot encoding、multi hot encoding是一种稀疏向量的编码方式，所以不适合用来深度学习进行特征表示，相反Embedding概括而言，就是一种高效的低维稠密的操作。</p>\n<blockquote>\n<p>词嵌入（ word embedding）：基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。</p>\n</blockquote>\n<p>Embedding很早之前就有人研究了，相关资料文章特别的多，不过让Embedding在行内如此流行的功劳还要归功于google的Word2vec。这里需要先说说神经网络语言模型与Word2vec的关系，神经网络语言模型做词向量有以下几种方式：</p>\n<ul>\n<li>Neural Network Language Model ，NNLM</li>\n<li>Log-Bilinear Language Model， LBL</li>\n<li>Recurrent Neural Network based Language Model，RNNLM</li>\n<li>C&amp;W 模型</li>\n<li>CBOW（ Continuous Bagof-Words）和 Skip-gram 模型</li>\n</ul>\n<p>上面几种模型只是个在逻辑概念上的东西，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具就是Word2vec。</p>\n<blockquote>\n<p>在广告、推荐、搜索等领域，由于用户数据的稀疏性，几乎必然要求在构建DNN之前对user和item进行embedding后才能进行有效的训练，所以诞生一种叫做“item2vec”的方法，感兴趣的可以自行了解，这里不展开讲。</p>\n</blockquote>\n<h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><ul>\n<li>Matrix Factorization：矩阵分解<ul>\n<li>简单来讲，MF就是将一个矩阵分解成多个矩阵的乘积，典型的算法有SVD、LSA，是一种常见的技术，在推荐、NLP都有应用。后面你就会知道，在Word2vec中，最终每一个word都会有两个向量：V_word和V_context，假设存在一个矩阵W = V_word * V_context，word2vec也可以理解成是对矩阵W的矩阵分解。</li>\n</ul>\n</li>\n<li>Pointwise Mutual Information（PMI）：点互信息<ul>\n<li>指标来衡量两个事物之间的相关性，如下公式中，w和c同时出现的次数/(w出现的次数 + c出现的次数)，和MF对比，就多了个常量logk。</li>\n</ul>\n</li>\n<li>Cosine similarity：内积空间的两个非零向量之间相似度的量度（这个自己实现也比较好实现，Word2vec用这个计算相似度）<br><img src=\"https://img-blog.csdnimg.cn/20201027230949373.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>这里多说一下余弦相似度计算，下面举个例子：<br><img src=\"https://img-blog.csdnimg.cn/20201027232354991.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"keras中的Embedding和Word2vec的区别\"><a href=\"#keras中的Embedding和Word2vec的区别\" class=\"headerlink\" title=\"keras中的Embedding和Word2vec的区别\"></a>keras中的Embedding和Word2vec的区别</h1><p>其实二者的目标是一样的，都是我们为了学到词的稠密的嵌入表示。只不过学习的方式不一样。Word2vec是无监督的学习方式，利用上下文环境来学习词的嵌入表示，因此可以学到相关词，但是只能捕捉到局部分布信息。而在keras的Embedding层中，权重的更新是基于标签的信息进行学习，为了达到较高的监督学习的效果，会将Embedding作为网络的一层，根据target进行学习和调整。比如LSTM中对词向量的微调。简单来说，Word2vec一般单独提前训练好，而Embedding一般作为模型中的层随着模型一同训练。</p>\n<h1 id=\"CBOW和Skip-gram\"><a href=\"#CBOW和Skip-gram\" class=\"headerlink\" title=\"CBOW和Skip-gram\"></a>CBOW和Skip-gram</h1><p>CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。这样讲可能比较抽象，这里借用jalammar的文章里的内容和图进行说明。</p>\n<h3 id=\"CBOW\"><a href=\"#CBOW\" class=\"headerlink\" title=\"CBOW\"></a>CBOW</h3><p>我们通过找常出现在每个单词附近的词，就能获得它们的映射关系。机制如下：</p>\n<ul>\n<li>先是获取大量文本数据</li>\n<li>后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)</li>\n<li>利用这样的滑动窗就能为训练模型生成大量样本数据。</li>\n</ul>\n<p>当这个窗口沿着文本滑动时，我们就能(真实地)生成一套用于模型训练的数据集。为了明确理解这个过程，我们看下滑动窗是如何处理这个短语的，我们把前两个单词单做特征，第三个单词单做标签:<br><img src=\"https://img-blog.csdnimg.cn/20201027233856280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如上，以此产生一个样本，然后以此慢慢的滑动下去，就会产生一个很大的数据集。接下来，我们不仅要考虑目标单词的前两个单词，还要考虑其后两个单词，像下面这样。<br><img src=\"https://img-blog.csdnimg.cn/20201027234252979.png#pic_center\" alt=\"在这里插入图片描述\"><br>通过这种方式，我们实际上构建并训练的模型就如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201027234317749.png#pic_center\" alt=\"在这里插入图片描述\"><br>上述的这种架构就被我们称为连续词袋(CBOW)。</p>\n<h3 id=\"Skip-gram\"><a href=\"#Skip-gram\" class=\"headerlink\" title=\"Skip-gram\"></a>Skip-gram</h3><p>Skip-gram不同的是，它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示（绿框中的词语是输入词，粉框则是可能的输出结果）：<br><img src=\"https://img-blog.csdnimg.cn/20201027234420824.png#pic_center\" alt=\"在这里插入图片描述\"><br>这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本<br><img src=\"https://img-blog.csdnimg.cn/20201027234453936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在移动几组位置之后，我们就能得到一批样本:<br><img src=\"https://img-blog.csdnimg.cn/20201027234533739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这种方式就被我们称为Skipgram架构。我们可以像下图这样将展示滑动窗的内容。</p>\n<h1 id=\"Skipgram-with-Negative-Sampling-SGNS\"><a href=\"#Skipgram-with-Negative-Sampling-SGNS\" class=\"headerlink\" title=\"Skipgram with Negative Sampling (SGNS)\"></a>Skipgram with Negative Sampling (SGNS)</h1><p>现在我们已经从现有的文本中获得了Skipgram模型的训练数据集，接下来让我们看看如何使用它来训练一个能预测相邻词汇的自然语言模型。从数据集中的第一个样本开始。我们将特征输入到未经训练的模型，让它预测一个可能的相邻单词。<br><img src=\"https://img-blog.csdnimg.cn/20201027234849691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>该模型会执行三个步骤并输入预测向量(对应于单词表中每个单词的概率)。因为模型未经训练，该阶段的预测肯定是错误的。但是没关系，我们知道应该猜出的是哪个单词——这个词就是我训练集数据中的输出标签:<br><img src=\"https://img-blog.csdnimg.cn/20201027234902801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。模型的偏差有多少？将两个向量相减，就能得到偏差向量:<br><img src=\"https://img-blog.csdnimg.cn/20201027234957849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。<br><img src=\"https://img-blog.csdnimg.cn/20201027235012137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这其实就是训练的第一步了。我们接下来继续对数据集内下一份样本进行同样的操作，直到我们遍历所有的样本。这就是一轮（epoch）了。我们再多做几轮（epoch），得到训练过的模型，于是就可以从中提取嵌入矩阵来用于其他应用了。但是真正的Word2vec在上面的基本流程上，还有一些关键思想：</p>\n<ul>\n<li>负样本采样：我们需要在数据集中引入负样本 - 不是邻居的单词样本。我们的模型需要为这些样本返回0，这些单词是从词汇表中随机抽取的单词，这个想法的灵感来自噪声对比估计。</li>\n</ul>\n<h1 id=\"Word2vec训练流程\"><a href=\"#Word2vec训练流程\" class=\"headerlink\" title=\"Word2vec训练流程\"></a>Word2vec训练流程</h1><p>在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。</p>\n<p>在训练阶段的开始，我们创建两个矩阵——Embedding矩阵和Context矩阵。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size——300是一个常见值，但我们在前文也看过50的例子）。<br><img src=\"https://img-blog.csdnimg.cn/2020102723540043.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在训练过程开始时，我们用随机值初始化这些矩阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：<br><img src=\"https://img-blog.csdnimg.cn/20201027235443866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们有四个单词：输入单词not和输出/上下文单词: thou（实际邻居词），aaron和taco（负面例子）。我们继续查找它们的嵌入——对于输入词，我们查看Embedding矩阵。对于上下文单词，我们查看Context矩阵（即使两个矩阵都在我们的词汇表中嵌入了每个单词）。<br><img src=\"https://img-blog.csdnimg.cn/20201027235452260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后，我们计算输入嵌入与每个上下文嵌入的点积。在每种情况下，结果都将是表示输入和上下文嵌入的相似性的数字。<br><img src=\"https://img-blog.csdnimg.cn/20201027235501133.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们需要一种方法将这些分数转化为看起来像概率的东西——我们需要它们都是正值，并且 处于0到1之间。sigmoid这一逻辑函数转换正适合用来做这样的事情啦。<br><img src=\"https://img-blog.csdnimg.cn/20201027235511889.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。</p>\n<p>既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们计算模型预测中的误差吧。为此我们只需从目标标签中减去sigmoid分数。<br><img src=\"https://img-blog.csdnimg.cn/20201027235528795.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在，我们可以利用这个错误分数来调整not、thou、aaron和taco的嵌入，使我们下一次做出这一计算时，结果会更接近目标分数。训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入。</p>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>word2vec训练过程中的两个关键超参数是窗口大小和负样本的数量。<br><img src=\"https://img-blog.csdnimg.cn/20201027235711617.png#pic_center\" alt=\"在这里插入图片描述\"><br>不同的任务适合不同的窗口大小。一种启发式方法是，使用较小的窗口大小（2-15）会得到这样的嵌入：两个嵌入之间的高相似性得分表明这些单词是可互换的（注意，如果我们只查看附近距离很近的单词，反义词通常可以互换——例如，好的和坏的经常出现在类似的语境中）。使用较大的窗口大小（15-50，甚至更多）会得到相似性更能指示单词相关性的嵌入。在实际操作中，你通常需要对嵌入过程提供指导以帮助读者得到相似的”语感“。Gensim默认窗口大小为5（除了输入字本身以外还包括输入字之前与之后的两个字）。<br><img src=\"https://img-blog.csdnimg.cn/20201027235722687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本的数量是训练训练过程的另一个因素。原始论文认为5-20个负样本是比较理想的数量。它还指出，当你拥有足够大的数据集时，2-5个似乎就已经足够了。Gensim默认为5个负样本。</p>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"http://jalammar.github.io/illustrated-word2vec/\">The Illustrated Word2vec</a></li>\n<li><a href=\"https://arxiv.org/abs/1411.2738\">word2vec Parameter Learning Explained</a></li>\n<li><a href=\"https://arxiv.org/pdf/1301.3781.pdf\">Efficient Estimation of Word Representations in Vector Space</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">Cosine similarity</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/26306795\">[NLP] 秒懂词向量Word2vec的本质</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["Embedding","Word2vec","词袋","深度学习"]},{"title":"关于RNN理论和实践的一些总结","url":"/Deep-Learning/f43bcaebe5e9/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本篇文章主要总结我在学习过程中遇到的RNN、其相关变种，并对相关结构进行说明和结构图展示。内容包括RNN、RecNN、多层、双向、RNNCell等等，同时包括在计算框架（TensorFlow及PyTorch）API层面的一些理解记录。本篇文章不进行深入推导和底层原理介绍，仅做总结记录，感兴趣者可自行根据内容详细查阅资料。</p>\n<blockquote>\n<p>RNN（递归神经网络）包括Recurrent Neural Network和Recursive Neural Network两种，分别为时间递归神经网络和结构递归神经网络。</p>\n</blockquote>\n<p>计算框架版本：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1>在进行后面内容的陈述之前，先来简单结合计算框架说明一下vanilla RNN、LSTM、GRU之间的区别。虽然将vanilla RNN、LSTM、GRU这个三个分开讲进行对比，但是不要忘记它们都是RNN，所以在宏观角度都是如下结构：<br><img src=\"https://img-blog.csdnimg.cn/20201208151624631.png#pic_center\" alt=\"在这里插入图片描述\"><br>而它们区别在于中间的那个隐藏状态计算单元，这里贴出它们的计算单元的细节，从左到右分别是vanilla RNN、LSTM、GRU。<br><img src=\"https://img-blog.csdnimg.cn/20201208152622203.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>看了隐藏单元之后，你有没有发现LSTM和其他两个的输入多了一个cell state，LSTM的门道就在这，cell state 就是实现LSTM的关键（ps：GRU其实也有分hidden state和cell state，不过在GRU中它们两个是相同的）。细节我不去深究，感兴趣的自行查看论文：</li>\n<li><a href=\"https://arxiv.org/pdf/1409.2329.pdf\">RNN</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf\">LSTM</a></li>\n<li><a href=\"https://arxiv.org/pdf/1406.1078v3.pdf\">GRU</a></li>\n</ul>\n<p>我这里就简单的结合<a href=\"https://www.tensorflow.org/api/stable\">TensorFlow</a>和<a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch</a>说明一下cell state和hidden state，首先看下面两个计算框架的调用（详细参数自行查阅文档，这里只是为了说明state）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># TensorFlow中的LSTM调用</span></span><br><span class=\"line\">whole_seq_output, final_memory_state, final_carry_state =</span><br><span class=\"line\">\t\t\ttf.keras.layers.LSTM(<span class=\"number\">4</span>, return_sequences=<span class=\"literal\">True</span>, return_state=<span class=\"literal\">True</span>)(inputs)</span><br><span class=\"line\"><span class=\"comment\"># Pytorch中的LSTM调用</span></span><br><span class=\"line\">output, (hn, cn) = torch.nn.LSTM(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)(<span class=\"built_in\">input</span>, (h0, c0))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># TensorFlow中的GRU调用</span></span><br><span class=\"line\">whole_sequence_output, final_state =</span><br><span class=\"line\">\t\t\ttf.keras.layers.GRU(<span class=\"number\">4</span>, return_sequences=<span class=\"literal\">True</span>, return_state=<span class=\"literal\">True</span>)(inputs)</span><br><span class=\"line\"><span class=\"comment\"># Pytorch中的GRU调用</span></span><br><span class=\"line\">output, hn = torch.nn.GRU(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)(<span class=\"built_in\">input</span>, h0)</span><br></pre></td></tr></table></figure>\n<p>以TensorFlow举例（PyTorch默认都返回），当return_state参数设置为True时，将会返回隐藏层状态，即cell_state。在LSTM 的网络结构中，直接根据当前input 数据，得到的输出称为 hidden state，还有一种数据是不仅仅依赖于当前输入数据，而是一种伴随整个网络过程中用来记忆，遗忘，选择并最终影响hidden state结果的东西，称为 cell state。cell state默认是不输出的，它仅对输出 hidden state 产生影响。通常情况，我们不需要访问cell state，但当需要对 cell state 的初始值进行设定时，就需要将其返回。所以在上面的TensorFlow对LSTM的调用中，final_memory_state是最后一个timestep的状态，final_carry_state是最后一个timestep的cell state。既然见到LSTM和GRU，那下面就贴一张它们的状态更新公式图以作记录：<br><img src=\"https://img-blog.csdnimg.cn/20201208161136131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>后面简要阐述的所有RNN及其变种，都是代指vanilla RNN、LSTM、GRU三个，只不过为了方便描述，以RNN作为总称进行说明。</p>\n<blockquote>\n<p>TensorFlow中，RNN类是作为如第一张结构图那些的宏观结构，所以它有一个cell参数，你可以根据实际需要传入SimpleRNNCell、LSTMCell和GRUCell（这三个你就可以理解成上面讲的计算单元），它们三个可以单独使用，在一些地方特别管用。</p>\n</blockquote>\n<blockquote>\n<p>PyTorch中大致是一样的，不过RNN类则是标准的RNN实现的，而不是像Tensorflow那样的架构，PyTorch同样有RNNCell、LSTMCell和GRUCell</p>\n</blockquote>\n<h1 id=\"标准RNN\"><a href=\"#标准RNN\" class=\"headerlink\" title=\"标准RNN\"></a>标准RNN</h1><p>RNN忽略单元细节的具体结构图如下。从图中就能够很清楚的看到，上一时刻的隐藏层是如何影响当前时刻的隐藏层的（注意这里Output的数量画少了，看起来不够形象，应该是 $X=[x_1,x_2,…,x_m]$和 $O=[o_1,o_2,…,o_m]$）。这里的Output是对应时间步的状态，而 $s$ 是隐藏状态，一般在实践中用它来初始化RNN。<br><img src=\"https://img-blog.csdnimg.cn/20201208112157265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然，可以换一种方式画结构图，如下图所示，按照RNN时间线展开。注意了，隐藏层 $s_t$ 不仅取决于 $x_t$ 还取决与 $s_{t-1}$。<br><img src=\"https://img-blog.csdnimg.cn/2020120811351449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从上面总结公式如下：<br>$$o_t=g(V_{s_t}) \\quad\\quad (1)$$    $$s_t=f(U_{x_t}+W_{s_{t-1}}) \\quad\\quad (2)$$<br>式（1）是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。$V$是输出层的权重矩阵，$g$是激活函数。式（2）是隐藏层的计算公式，它是循环层。$U$ 是输入 $x$ 的权重矩阵，$W$ 是上一次的值作为这一次的输入的权重矩阵，$f$ 是激活函数。从宏观意义上来说，循环层和全连接层的区别就是循环层多了一个权重矩阵 $W$。通过循环带入得下式：<br>$$o_t=Vf(U_{x_t}+Wf(U_{x_{t-1}}+Wf(U_{x_{t-2}}+Wf(U_{x_{t-3}}+…))))$$<br>从上面可以看出，循环神经网络的输出值 $o_t$，是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、$x_{t-3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p>\n<h1 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h1><p>论文：<a href=\"http://deeplearning.cs.cmu.edu/F20/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf\">Link</a><br><img src=\"https://img-blog.csdnimg.cn/20201208144157106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从上图可以看出，双向RNN的隐藏层要保存两个值，一个 $A$ 参与正向计算，另一个值 $A’$ 参与反向计算（注意了，正向计算和反向计算不共享权重），最终的输出值取决于 $A$ 和 $A’$ 的计算方式。其计算方法有很多种，这里结合TensorFlow和PyTorch说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># TensorFlow中，需要使用Bidirectional来实现双向RNN，如下所示</span></span><br><span class=\"line\"><span class=\"comment\"># 其中merge_mode就是A和A&#x27;两者的计算方式：&#123;&#x27;sum&#x27;, &#x27;mul&#x27;, &#x27;concat&#x27;, &#x27;ave&#x27;, None&#125;</span></span><br><span class=\"line\">tf.keras.layers.Bidirectional(</span><br><span class=\"line\">    layer, merge_mode=<span class=\"string\">&#x27;concat&#x27;</span>, weights=<span class=\"literal\">None</span>, backward_layer=<span class=\"literal\">None</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># PyTorch则不同，在各RNN的具体实现中，都有一个bidirectional参</span></span><br><span class=\"line\"><span class=\"comment\"># 数来控制是否是双向的，可自行查看PyTorch的API文档，特别说明的是</span></span><br><span class=\"line\"><span class=\"comment\"># PyTorch没有merge_mode，所以双向RNN直接会返回正向和反向的状态，</span></span><br><span class=\"line\"><span class=\"comment\"># 需要你自行进行合并操作</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"Multi-layer-stacked-RNN\"><a href=\"#Multi-layer-stacked-RNN\" class=\"headerlink\" title=\"Multi-layer(stacked) RNN\"></a>Multi-layer(stacked) RNN</h1><p>将多个RNN堆叠成多层RNN，每层RNN的输入为上一层RNN的输出，如下图所示。多层 (Multi-layer) RNN 效果很好，但可能会常用到 skip connections 的方式<br><img src=\"https://img-blog.csdnimg.cn/20201208164736875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"深度循环神经网络\"><a href=\"#深度循环神经网络\" class=\"headerlink\" title=\"深度循环神经网络\"></a>深度循环神经网络</h1><p>前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/2020120816540656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们把第 $i$ 个隐藏层的值表示为 $s_t^{(i)}$、$s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为：<br>$$o_t=g(V^{(i)}s_t^{(i)}+V^{‘(i)}s_t^{‘(i)})$$    $$s_t^{(i)}=f(U^{(i)}s_t^{(i-1)}+W^{(i)}s_{t-1})$$     $$s_t^{‘(i)}=f(U^{‘(i)}s_t^{‘(i-1)}+W^{‘(i)}s_{t+1}^{‘})$$    $$s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1})$$    $$s_t^{‘(1)}=f(U^{‘(1)}x_t+W^{‘(1)}s_{t+1}^{‘})$$</p>\n<h1 id=\"Recursive-Neural-Network\"><a href=\"#Recursive-Neural-Network\" class=\"headerlink\" title=\"Recursive Neural Network\"></a>Recursive Neural Network</h1><p>RNN适用于序列建模，而许多NLP问题需要处理树状结构，因此提出了RecNN的概念。与RNN将前序句子编码成状态向量类似，RecNN将每个树节点编码成状态向量。RecNN中的每棵子树都由一个向量表示，其值由其子节点的向量表示递归确定。<br><img src=\"https://img-blog.csdnimg.cn/20201208170829765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>RecNN接受的输入为一个有n个单词的句子的语法分析树，每个单词都表示为一个向量，语法分析树表示为一系列的生成式规则。举个例子，The boy saw her duck的分析树如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201208170959474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>对应的生成式规则（无标签+有标签）如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201208171104105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>RecNN的输出为句子的内部状态向量（inside state vectors），每一个状态向量都对应一个树节点。具体RecNN细节自行详细查阅资料。</p>\n<h1 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h1><p>普遍来看, 神经网络都会有梯度消失和梯度爆炸的问题，其根源在于现在的神经网络在训练的时候，大多都是基于BP算法，这种误差向后传递的方式，即多元函数求偏导中，链式法则会产生 vanishing，而 RNN 产生梯度消失的根源是权值矩阵复用。</p>\n<h3 id=\"循环神经网络的训练算法：BPTT\"><a href=\"#循环神经网络的训练算法：BPTT\" class=\"headerlink\" title=\"循环神经网络的训练算法：BPTT\"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>\n<ul>\n<li>前向计算每个神经元的输出值</li>\n<li>反向计算每个神经元的误差项 $\\delta_j$ 值，它是误差函数 $E$ 对神经元 $j$ 的加权输入 $net_j$ 的偏导数</li>\n<li>计算每个权重的梯度</li>\n<li>最后再用随机梯度下降算法更新权重。</li>\n</ul>\n<h3 id=\"RNN的梯度爆炸和消失问题\"><a href=\"#RNN的梯度爆炸和消失问题\" class=\"headerlink\" title=\"RNN的梯度爆炸和消失问题\"></a>RNN的梯度爆炸和消失问题</h3><p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>\n<ul>\n<li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li>\n<li>使用 $relu$ 代替 $sigmoid$ 和 $tanh$ 作为激活函数。</li>\n<li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/\">How to use return_state or return_sequences in Keras</a></li>\n<li><a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a></li>\n<li><a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a></li>\n<li><a href=\"https://www.jianshu.com/p/5f39b91bff11\">RNN summarize</a></li>\n<li><a href=\"https://www.jianshu.com/p/f08eb58cf16b\">从动图中理解 RNN，LSTM 和 GRU</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/135320350\">RNN、lstm、gru详解</a></li>\n<li><a href=\"https://www.jianshu.com/p/403665b55cd4\">用 Recursive Neural Networks 得到分析树</a></li>\n<li><a href=\"https://zybuluo.com/hanbingtao/note/541458\">循环神经网络</a></li>\n<li><a href=\"https://www.cnblogs.com/chenjieyouge/p/12556237.html\">双向 和 多重 RNN</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","RNN","LSTM","GRU"]},{"title":"利器：TTS-Frontend中英Text-to-Phoneme-Converter，附代码","url":"/Deep-Learning/43c556e88dfe/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>NLP的语音合成中，有一种关键技术是将文字拆解成音素，再去语音库里匹配相同音素的语音片段，来实现文字转换语音。音素是给定语言的语音，如果与另一个音素交换，则会改变单词的含义，同时，音素是绝对的，并不是特定于任何语言，但只能参考特定语言讨论音素。由于音素的特性，非常适合用于语音合成领域。</p>\n<blockquote>\n<p>音素（phone），是语音中的最小的单位，依据音节里的发音动作来分析，一个动作构成一个音素。音素分为元音、辅音两大类。</p>\n</blockquote>\n<p>说白了，音素其实就是人在说话时，能发出最最最最短小、简洁的不能再分割的发音，不同的音素就是不同的短发音，可以组成不同的长发音，再组成词句形成语言。<br><img src=\"https://img-blog.csdnimg.cn/20201214120124692.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>本篇文章就来讲讲中文和英文中，如何将文本转换为音素序列。</p>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>众所周知，语音合成系统通常包含前端和后端两个模块。前端模块主要是对输入文本进行分析，提取后端模块所需要的语言学信息。前端模块一般包含文本正则化、分词、词性预测、多音字消歧、韵律预测等子模块。后端模块根据前端分析结果，通过一定的方法生成语音波形。后端模块一般分为基于统计参数建模的语音合成（Statistical Parameter Speech Synthesis，SPSS，以下简称参数合成），以及基于单元挑选和波形拼接的语音合成（以下简称拼接合成）两条技术主线。</p>\n<p>而“端到端”架构的语音合成系统的出现，能够直接从字符文本合成语音，打破了各个传统组件之间的壁垒，使得我们可以从<code>&lt;文本，声谱&gt;</code>配对的数据集上，完全随机从头开始训练。最具代表性的端到端语音合成系统，就是2017年初，Google 提出的端到端的语音合成系统——Tacotron</p>\n<p>从通俗一点的角度来讲，语音合成过程，需要处理两部分内容，分别是文本(Text)处理和音频(speech)处理，如下图是端到端的语音合成系统整体技术架构选型。我们文章要讲的就是属于TTS Frontend范畴的Text-to-Phoneme。<br><img src=\"https://img-blog.csdnimg.cn/20201214143030869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"文本规范化\"><a href=\"#文本规范化\" class=\"headerlink\" title=\"文本规范化\"></a>文本规范化</h1><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等，对于中文而言，有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音，比如：</p>\n<ul>\n<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”</li>\n<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>\n</ul>\n<p>对于英文而言，也需要将年份、货币、数字、字母等文本信息，转换为完整单词，比如：</p>\n<ul>\n<li>类别为年份（NYER）： 2011 → twenty eleven</li>\n<li>类别为货币(MONEY): £100 → one hundred pounds</li>\n<li>类别为非单词，需要拟音(ASWD): IKEA → apply letter-to-sound</li>\n<li>类别为数字(NUM) : 100 NUM → one hundred</li>\n<li>类别为字母(LSEQ) : DVD → dee vee dee</li>\n</ul>\n<p>这些文本规范化预处理不放在深度学习模型去学习，而是通过各种规则的正则表达式进行转换，所以涉及到的代码工作量还是比较大的，所以我将写好的代码更新至GitHub项目中，方便需要者使用（TensorFlow和PyTorch版本同步更新）：<a href=\"https://github.com/DengBoCong/nlp-paper/tree/master/paper-code/data\">NLP相关Paper笔记和代码复现</a>，这里粘贴一个作为举例，方便大家知道是啥。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_clean_number</span>(<span class=\"params\">text: <span class=\"built_in\">str</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    对句子中的数字相关进行统一单词转换</span></span><br><span class=\"line\"><span class=\"string\">    :param text: 单个句子文本</span></span><br><span class=\"line\"><span class=\"string\">    :return: 转换后的句子文本</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    comma_number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;([0-9][0-9\\,]+[0-9])&quot;</span>)</span><br><span class=\"line\">    decimal_number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;([0-9]+\\.[0-9]+)&quot;</span>)</span><br><span class=\"line\">    pounds_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;£([0-9\\,]*[0-9]+)&quot;</span>)</span><br><span class=\"line\">    dollars_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;\\$([0-9\\.\\,]*[0-9]+)&quot;</span>)</span><br><span class=\"line\">    ordinal_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;[0-9]+(st|nd|rd|th)&quot;</span>)</span><br><span class=\"line\">    number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;[0-9]+&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    text = re.sub(comma_number_re, <span class=\"keyword\">lambda</span> m: m.group(<span class=\"number\">1</span>).replace(<span class=\"string\">&#x27;,&#x27;</span>, <span class=\"string\">&#x27;&#x27;</span>), text)</span><br><span class=\"line\">    text = re.sub(pounds_re, <span class=\"string\">r&quot;\\1 pounds&quot;</span>, text)</span><br><span class=\"line\">    text = re.sub(dollars_re, _dollars_to_word, text)</span><br><span class=\"line\">    text = re.sub(decimal_number_re, <span class=\"keyword\">lambda</span> m: m.group(<span class=\"number\">1</span>).replace(<span class=\"string\">&#x27;.&#x27;</span>, <span class=\"string\">&#x27; point &#x27;</span>), text)</span><br><span class=\"line\">    text = re.sub(ordinal_re, <span class=\"keyword\">lambda</span> m: inflect.engine().number_to_words(m.group(<span class=\"number\">0</span>)), text)</span><br><span class=\"line\">    text = re.sub(number_re, _number_to_word, text)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> text</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"英文Text-to-Phoneme\"><a href=\"#英文Text-to-Phoneme\" class=\"headerlink\" title=\"英文Text-to-Phoneme\"></a>英文Text-to-Phoneme</h1><p>我们先来讲讲英文的Text-to-Phoneme，想要将英文单词转换成音素，想必需要了解Arpabet，下面是WiKi上的解释：</p>\n<blockquote>\n<p>ARPABET（也称为ARPAbet）是高级研究计划局（ARPA）在1970年代语音理解研究项目中开发的一组语音转录代码。 它代表具有不同ASCII字符序列的通用美国英语的音素和同音素。 </p>\n</blockquote>\n<p>可以理解为通过字母组合定义发音规则，英文发音是由元音辅音等组成的，同时，在元音后紧接着用数字表示压力， 辅助符号在1和2个字母的代码中相同， 在2个字母的符号中，段之间用空格隔开，大概如下这样：<br><img src=\"https://img-blog.csdnimg.cn/20201214160312984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>我们将文本转换为ARPABET并不需要我们了解很多语言学的东西，我们只需要选择现有合适的发音字典就可以了，使用比较多的还是CMU的发音词典，<a href=\"http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=off\">官方网站</a>，想了解原理的可以看<a href=\"http://www.cs.cmu.edu/~awb/papers/ESCA98_lts.pdf\">这篇论文</a>。可以先在官网体验一下是啥效果，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201214161122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>自行从官网下载发音字典（也可以去我的github下载，链接在文章顶部），音素集，39个音素，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">Phoneme Example Translation</span><br><span class=\"line\">      ------- ------- -----------</span><br><span class=\"line\">      AA\todd     AA D</span><br><span class=\"line\">      AE\tat\tAE T</span><br><span class=\"line\">      AH\thut\tHH AH T</span><br><span class=\"line\">      AO\tought\tAO T</span><br><span class=\"line\">      AW\tcow\tK AW</span><br><span class=\"line\">      AY\thide\tHH AY D</span><br><span class=\"line\">      B \tbe\tB IY</span><br><span class=\"line\">      CH\tcheese\tCH IY Z</span><br><span class=\"line\">      D \tdee\tD IY</span><br><span class=\"line\">      DH\tthee\tDH IY</span><br><span class=\"line\">      EH\tEd\tEH D</span><br><span class=\"line\">      ER\thurt\tHH ER T</span><br><span class=\"line\">      EY\tate\tEY T</span><br><span class=\"line\">      F \tfee\tF IY</span><br><span class=\"line\">      G \tgreen\tG R IY N</span><br><span class=\"line\">      HH\the\tHH IY</span><br><span class=\"line\">      IH\tit\tIH T</span><br><span class=\"line\">      IY\teat\tIY T</span><br><span class=\"line\">      JH\tgee\tJH IY</span><br><span class=\"line\">      K \tkey\tK IY</span><br><span class=\"line\">      L \tlee\tL IY</span><br><span class=\"line\">      M \tme\tM IY</span><br><span class=\"line\">      N \tknee\tN IY</span><br><span class=\"line\">      NG\tping\tP IH NG</span><br><span class=\"line\">      OW\toat\tOW T</span><br><span class=\"line\">      OY\ttoy\tT OY</span><br><span class=\"line\">      P \tpee\tP IY</span><br><span class=\"line\">      R \tread\tR IY D</span><br><span class=\"line\">      S \tsea\tS IY</span><br><span class=\"line\">      SH\tshe\tSH IY</span><br><span class=\"line\">      T \ttea\tT IY</span><br><span class=\"line\">      TH\ttheta\tTH EY T AH</span><br><span class=\"line\">      UH\thood\tHH UH D</span><br><span class=\"line\">      UW\ttwo\tT UW</span><br><span class=\"line\">      V \tvee\tV IY</span><br><span class=\"line\">      W \twe\tW IY</span><br><span class=\"line\">      Y \t<span class=\"keyword\">yield</span>\tY IY L D</span><br><span class=\"line\">      Z \tzee\tZ IY</span><br><span class=\"line\">      ZH\tseizure\tS IY ZH ER</span><br></pre></td></tr></table></figure>\n<p>有了音素集之后就可以使用脚本将文本转换为音素了，我的转换脚本如下:（完整代码同GitHub可找到）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">text_to_phonemes_converter</span>(<span class=\"params\">text: <span class=\"built_in\">str</span>, cmu_dict_path: <span class=\"built_in\">str</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    将句子按照CMU音素字典进行分词切分</span></span><br><span class=\"line\"><span class=\"string\">    :param text: 单个句子文本</span></span><br><span class=\"line\"><span class=\"string\">    :param cmu_dict_path: cmu音素字典路径</span></span><br><span class=\"line\"><span class=\"string\">    :return: 按照音素分词好的数组</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    _, symbols_set = get_phoneme_dict_symbols()</span><br><span class=\"line\"></span><br><span class=\"line\">    alt_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&#x27;\\([0-9]+\\)&#x27;</span>)</span><br><span class=\"line\">    cmu_dict = &#123;&#125;</span><br><span class=\"line\">    text = _clean_text(text)</span><br><span class=\"line\">    text = re.sub(<span class=\"string\">r&quot;([?.!,])&quot;</span>, <span class=\"string\">r&quot; \\1&quot;</span>, text)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 文件是从官网下载的，所以文件编码格式要用latin-1</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(cmu_dict_path, <span class=\"string\">&#x27;r&#x27;</span>, encoding=<span class=\"string\">&#x27;latin-1&#x27;</span>) <span class=\"keyword\">as</span> cmu_file:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> cmu_file:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) <span class=\"keyword\">and</span> (line[<span class=\"number\">0</span>] &gt;= <span class=\"string\">&quot;A&quot;</span> <span class=\"keyword\">and</span> line[<span class=\"number\">0</span>] &lt;= <span class=\"string\">&quot;Z&quot;</span> <span class=\"keyword\">or</span> line[<span class=\"number\">0</span>] == <span class=\"string\">&quot;&#x27;&quot;</span>):</span><br><span class=\"line\">                parts = line.split(<span class=\"string\">&#x27;  &#x27;</span>)</span><br><span class=\"line\">                word = re.sub(alt_re, <span class=\"string\">&#x27;&#x27;</span>, parts[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># 这里要将非cmu音素的干扰排除</span></span><br><span class=\"line\">                pronunciation = <span class=\"string\">&quot; &quot;</span></span><br><span class=\"line\">                temps = parts[<span class=\"number\">1</span>].strip().split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">for</span> temp <span class=\"keyword\">in</span> temps:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> temp <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> symbols_set:</span><br><span class=\"line\">                        pronunciation = <span class=\"literal\">None</span></span><br><span class=\"line\">                        <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> pronunciation:</span><br><span class=\"line\">                    pronunciation = <span class=\"string\">&#x27; &#x27;</span>.join(temps)</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> word <span class=\"keyword\">in</span> cmu_dict:</span><br><span class=\"line\">                        cmu_dict[word].append(pronunciation)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        cmu_dict[word] = [pronunciation]</span><br><span class=\"line\"></span><br><span class=\"line\">    cmu_result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> text.split(<span class=\"string\">&#x27; &#x27;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 因为同一个单词，它的发音音素可能不一样，所以存在多个</span></span><br><span class=\"line\">        <span class=\"comment\"># 音素分词，我这里就单纯的取第一个，后面再改进和优化</span></span><br><span class=\"line\">        cmu_word = cmu_dict.get(word.upper(), [word])[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cmu_word != word:</span><br><span class=\"line\">            cmu_result.append(<span class=\"string\">&quot;&#123;&quot;</span> + cmu_word + <span class=\"string\">&quot;&#125;&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            cmu_result.append(cmu_word)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot; &quot;</span>.join(cmu_result)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"中文Text-to-Phoneme\"><a href=\"#中文Text-to-Phoneme\" class=\"headerlink\" title=\"中文Text-to-Phoneme\"></a>中文Text-to-Phoneme</h1><p>对于中文，其实我们再熟悉不过了，中文的音素其实就是汉语拼音的最小单元，包括声母，韵母，但是其中还会有一些整体认读音节，更详细的拼音标注文本分析，可以参考<a href=\"https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html\">MTTS文本分析</a>。同一个字在不同分词情况下的发音不同，所以导致数量级比较大，比较典型的可以参见清华大学的<a href=\"https://www.openslr.org/resources.php\">thchs30</a>中文数据集，里面提供了分词好的音素字典，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">SIL sil</span><br><span class=\"line\">&lt;SPOKEN_NOISE&gt; sil</span><br><span class=\"line\">啊 aa a1</span><br><span class=\"line\">啊 aa a2</span><br><span class=\"line\">啊 aa a4</span><br><span class=\"line\">啊 aa a5</span><br><span class=\"line\">啊啊啊 aa a2 aa a2 aa a2</span><br><span class=\"line\">啊啊啊 aa a5 aa a5 aa a5</span><br><span class=\"line\">阿 aa a1</span><br><span class=\"line\">阿 ee e1</span><br><span class=\"line\">阿尔 aa a1 ee er3</span><br><span class=\"line\">阿根廷 aa a1 g en1 t ing2</span><br><span class=\"line\">阿九 aa a1 j iu3</span><br><span class=\"line\">阿克 aa a1 k e4</span><br><span class=\"line\">阿拉伯数字 aa a1 l a1 b o2 sh u4 z iy4</span><br><span class=\"line\">阿拉法特 aa a1 l a1 f a3 t e4</span><br></pre></td></tr></table></figure>\n<p>对应脚本在<a href=\"https://github.com/X-CCS/mandarin_tacotron_GL/blob/master/datasets/thchs30.py\">这里</a>，不过其实汉子转拼音还有更方便的方式，就是使用Python 的拼音库 PyPinyin，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pypinyin <span class=\"keyword\">import</span> pinyin</span><br><span class=\"line\">print(pinyin(<span class=\"string\">&#x27;朝阳&#x27;</span>))</span><br><span class=\"line\"><span class=\"comment\"># [[&#x27;zhāo&#x27;], [&#x27;yáng&#x27;]]</span></span><br></pre></td></tr></table></figure>\n<p> PyPinyin可以用于汉字注音、排序、检索等等场合，是基于 hotto/pinyin 这个库开发的，一些站点链接如下：</p>\n<ul>\n<li><a href=\"https://github.com/mozillazg/python-pinyin\">GitHub</a> </li>\n<li><a href=\"https://pypinyin.readthedocs.io/zh_CN/master/\">文档</a></li>\n<li><a href=\"https://pypi.org/project/pypinyin/\">PyPi</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","TensorFlow","Pytorch","语音合成","NLP","音素"]},{"title":"SpringBoot的流行实践解读","url":"/Spring-Boot/fafc44587d95/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得点个关注和点个赞哦，非常感谢支持</strong><br>看完这篇文章，感觉受益匪浅，特别是文中提到的一些最佳实践文章以及技术，非常值得一看，来源：<a href=\"http://t.cn/EJWZNra\">http://t.cn/EJWZNra</a><br>Spring Boot是最流行的用于开发微服务的Java框架。在本文中，我将与你分享自2016年以来我在专业开发中使用Spring Boot所采用的最佳实践。这些内容是基于我的个人经验和一些熟知的Spring Boot专家的文章。</p>\n<p>在本文中，我将重点介绍Spring Boot特有的实践（大多数时候，也适用于Spring项目）。以下依次列出了最佳实践，排名不分先后。</p>\n<h2 id=\"使用自定义BOM来维护第三方依赖\"><a href=\"#使用自定义BOM来维护第三方依赖\" class=\"headerlink\" title=\"使用自定义BOM来维护第三方依赖\"></a>使用自定义BOM来维护第三方依赖</h2><p>这条实践是我根据实际项目中的经历总结出的。</p>\n<p>Spring Boot项目本身使用和集成了大量的开源项目，它帮助我们维护了这些第三方依赖。但是也有一部分在实际项目使用中并没有包括进来，这就需要我们在项目中自己维护版本。如果在一个大型的项目中，包括了很多未开发模块，那么维护起来就非常的繁琐。</p>\n<p>怎么办呢？事实上，Spring IO Platform就是做的这个事情，它本身就是Spring Boot的子项目，同时维护了其他第三方开源库。我们可以借鉴Spring IO Platform来编写自己的基础项目platform-bom，所有的业务模块项目应该以BOM的方式引入。这样在升级第三方依赖时，就只需要升级这一个依赖的版本而已。</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependencyManagement&gt;</span><br><span class=\"line\">   &lt;dependencies&gt;</span><br><span class=\"line\">       &lt;dependency&gt;</span><br><span class=\"line\">           &lt;groupId&gt;io.spring.platform&lt;/groupId&gt;</span><br><span class=\"line\">           &lt;artifactId&gt;platform-bom&lt;/artifactId&gt;</span><br><span class=\"line\">           &lt;version&gt;Cairo-SR3&lt;/version&gt;</span><br><span class=\"line\">           &lt;type&gt;pom&lt;/type&gt;</span><br><span class=\"line\">           &lt;scope&gt;import&lt;/scope&gt;</span><br><span class=\"line\">       &lt;/dependency&gt;</span><br><span class=\"line\">   &lt;/dependencies&gt;</span><br><span class=\"line\">&lt;/dependencyManagement&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用自动配置\"><a href=\"#使用自动配置\" class=\"headerlink\" title=\"使用自动配置\"></a>使用自动配置</h2><p>Spring Boot的一个主要特性是使用自动配置。这是Spring Boot的一部分，它可以简化你的代码并使之工作。当在类路径上检测到特定的jar文件时，自动配置就会被激活。</p>\n<p>使用它的最简单方法是依赖Spring Boot Starters。因此，如果你想与Redis进行集成，你可以首先包括：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">   &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>如果你想与MongoDB进行集成，需要这样：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">   &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>借助于这些starters，这些繁琐的配置就可以很好地集成起来并协同工作，而且它们都是经过测试和验证的。这非常有助于避免可怕的Jar地狱。</p>\n<blockquote>\n<p><a href=\"https://dzone.com/articles/what-is-jar-hell\">https://dzone.com/articles/what-is-jar-hell</a></p>\n</blockquote>\n<p>通过使用以下注解属性，可以从自动配置中排除某些配置类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration(exclude =&#123;ClassNotToAutoconfigure.class&#125;)</span></span><br></pre></td></tr></table></figure>\n<p>但只有在绝对必要时才应该这样做。</p>\n<p>有关自动配置的官方文档可在此处找到：</p>\n<blockquote>\n<p><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-auto-configuration.html\">https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-auto-configuration.html</a></p>\n</blockquote>\n<h2 id=\"使用Spring-Initializr来开始一个新的Spring-Boot项目\"><a href=\"#使用Spring-Initializr来开始一个新的Spring-Boot项目\" class=\"headerlink\" title=\"使用Spring Initializr来开始一个新的Spring Boot项目\"></a>使用Spring Initializr来开始一个新的Spring Boot项目</h2><blockquote>\n<p>这一条最佳实践来自Josh Long （Spring Advocate，@starbuxman）。</p>\n</blockquote>\n<p>Spring Initializr 提供了一个超级简单的方法来创建一个新的Spring Boot项目，并根据你的需要来加载可能使用到的依赖。</p>\n<blockquote>\n<p><a href=\"https://start.spring.io/\">https://start.spring.io/</a></p>\n</blockquote>\n<p>使用Initializr创建应用程序可确保你获得经过测试和验证的依赖项，这些依赖项适用于Spring自动配置。你甚至可能会发现一些新的集成，但你可能并没有意识到这些。</p>\n<h2 id=\"考虑为常见的组织问题创建自己的自动配置\"><a href=\"#考虑为常见的组织问题创建自己的自动配置\" class=\"headerlink\" title=\"考虑为常见的组织问题创建自己的自动配置\"></a>考虑为常见的组织问题创建自己的自动配置</h2><p>这一条也来自Josh Long（Spring Advocate，@starbuxman）——这个实践是针对高级用户的。</p>\n<p>如果你在一个严重依赖Spring Boot的公司或团队中工作，并且有共同的问题需要解决，那么你可以创建自己的自动配置。</p>\n<p>这项任务涉及较多工作，因此你需要考虑何时获益是值得投入的。与多个略有不同的定制配置相比，维护单个自动配置更容易。</p>\n<p>如果将这个提供Spring Boot配置以开源库的形式发布出去，那么将极大地简化数千个用户的配置工作。</p>\n<h2 id=\"正确设计代码目录结构\"><a href=\"#正确设计代码目录结构\" class=\"headerlink\" title=\"正确设计代码目录结构\"></a>正确设计代码目录结构</h2><p>尽管允许你有很大的自由，但是有一些基本规则值得遵守来设计你的源代码结构。</p>\n<p>避免使用默认包。确保所有内容（包括你的入口点）都位于一个名称很好的包中，这样就可以避免与装配和组件扫描相关的意外情况；</p>\n<p>将Application.java（应用的入口类）保留在顶级源代码目录中；</p>\n<p>我建议将控制器和服务放在以功能为导向的模块中，但这是可选的。一些非常好的开发人员建议将所有控制器放在一起。不论怎样，坚持一种风格！</p>\n<h2 id=\"保持-Controller的简洁和专注\"><a href=\"#保持-Controller的简洁和专注\" class=\"headerlink\" title=\"保持@Controller的简洁和专注\"></a>保持@Controller的简洁和专注</h2><p>Controller应该非常简单。你可以在此处阅读有关GRASP中有关控制器模式部分的说明。你希望控制器作为协调和委派的角色，而不是执行实际的业务逻辑。以下是主要做法：</p>\n<blockquote>\n<p><a href=\"https://en.wikipedia.org/wiki/GRASP(object-orienteddesign)#Controller\">https://en.wikipedia.org/wiki/GRASP(object-orienteddesign)#Controller</a></p>\n</blockquote>\n<ul>\n<li>控制器应该是无状态的！默认情况下，控制器是单例，并且任何状态都可能导致大量问题；</li>\n<li>控制器不应该执行业务逻辑，而是依赖委托；</li>\n<li>控制器应该处理应用程序的HTTP层，这不应该传递给服务；</li>\n<li>控制器应该围绕用例/业务能力来设计。</li>\n</ul>\n<p>要深入这个内容，需要进一步地了解设计REST API的最佳实践。无论你是否想要使用Spring Boot，都是值得学习的。</p>\n<h2 id=\"围绕业务功能构建-Service\"><a href=\"#围绕业务功能构建-Service\" class=\"headerlink\" title=\"围绕业务功能构建@Service\"></a>围绕业务功能构建@Service</h2><p>Service是Spring Boot的另一个核心概念。我发现最好围绕业务功能/领域/用例（无论你怎么称呼都行）来构建服务。</p>\n<p>在应用中设计名称类似 AccountService, UserService, PaymentService这样的服务，比起像 DatabaseService、 ValidationService、 CalculationService这样的会更合适一些。</p>\n<p>你可以决定使用Controler和Service之间的一对一映射，那将是理想的情况。但这并不意味着，Service之间不能互相调用！</p>\n<h2 id=\"使数据库独立于核心业务逻辑之外\"><a href=\"#使数据库独立于核心业务逻辑之外\" class=\"headerlink\" title=\"使数据库独立于核心业务逻辑之外\"></a>使数据库独立于核心业务逻辑之外</h2><p>我之前还不确定如何在Spring Boot中最好地处理数据库交互。在阅读了罗伯特·C·马丁的“Clear Architecture”之后，对我来说就清晰多了。</p>\n<p>你希望你的数据库逻辑与服务分离出来。理想情况下，你不希望服务知道它正在与哪个数据库通信，这需要一些抽象来封装对象的持久性。</p>\n<blockquote>\n<p>罗伯特C.马丁强烈地说明，你的数据库是一个“细节”，这意味着不将你的应用程序与特定数据库耦合。过去很少有人会切换数据库，我注意到，使用Spring<br>Boot和现代微服务开发会让事情变得更快。</p>\n</blockquote>\n<h2 id=\"保持业务逻辑不受Spring-Boot代码的影响\"><a href=\"#保持业务逻辑不受Spring-Boot代码的影响\" class=\"headerlink\" title=\"保持业务逻辑不受Spring Boot代码的影响\"></a>保持业务逻辑不受Spring Boot代码的影响</h2><p>考虑到“Clear Architecture”的教训，你还应该保护你的业务逻辑。将各种Spring Boot代码混合在一起是非常诱人的……不要这样做。如果你能抵制诱惑，你将保持你的业务逻辑可重用。</p>\n<p>部分服务通常成为库。如果不从代码中删除大量Spring注解，则更容易创建。</p>\n<h2 id=\"推荐使用构造函数注入\"><a href=\"#推荐使用构造函数注入\" class=\"headerlink\" title=\"推荐使用构造函数注入\"></a>推荐使用构造函数注入</h2><p>这一条实践来自Phil Webb（Spring Boot的项目负责人, @phillip_webb）。</p>\n<p>保持业务逻辑免受Spring Boot代码侵入的一种方法是使用构造函数注入。不仅是因为 @Autowired注解在构造函数上是可选的，而且还可以在没有Spring的情况下轻松实例化bean。</p>\n<h2 id=\"熟悉并发模型\"><a href=\"#熟悉并发模型\" class=\"headerlink\" title=\"熟悉并发模型\"></a>熟悉并发模型</h2><p>我写过的最受欢迎的文章之一是“介绍Spring Boot中的并发”。我认为这样做的原因是这个领域经常被误解和忽视。如果使用不当，就会出现问题。</p>\n<blockquote>\n<p><a href=\"https://www.e4developer.com/2018/03/30/introduction-to-concurrency-in-spring-boot/\">https://www.e4developer.com/2018/03/30/introduction-to-concurrency-in-spring-boot/</a></p>\n</blockquote>\n<p>在Spring Boot中，Controller和Service是默认是单例。如果你不小心，这会引入可能的并发问题。你通常也在处理有限的线程池。请熟悉这些概念。</p>\n<p>如果你正在使用新的WebFlux风格的Spring Boot应用程序，我已经解释了它在“Spring’s WebFlux/Reactor Parallelism and Backpressure”中是如何工作的。</p>\n<h2 id=\"加强配置管理的外部化\"><a href=\"#加强配置管理的外部化\" class=\"headerlink\" title=\"加强配置管理的外部化\"></a>加强配置管理的外部化</h2><p>这一点超出了Spring Boot，虽然这是人们开始创建多个类似服务时常见的问题……</p>\n<p>你可以手动处理Spring应用程序的配置。如果你正在处理多个Spring Boot应用程序，则需要使配置管理能力更加强大。</p>\n<p>我推荐两种主要方法：</p>\n<ul>\n<li>使用配置服务器，例如Spring Cloud Config；</li>\n<li>将所有配置存储在环境变量中（可以基于git仓库进行配置）。</li>\n</ul>\n<p>这些选项中的任何一个（第二个选项多一些）都要求你在DevOps更少工作量，但这在微服务领域是很常见的。</p>\n<h2 id=\"提供全局异常处理\"><a href=\"#提供全局异常处理\" class=\"headerlink\" title=\"提供全局异常处理\"></a>提供全局异常处理</h2><p>你真的需要一种处理异常的一致方法。Spring Boot提供了两种主要方法：</p>\n<ul>\n<li>你应该使用HandlerExceptionResolver定义全局异常处理策略；</li>\n<li>你也可以在控制器上添加@ExceptionHandler注解，这在某些特定场景下使用可能会很有用。</li>\n</ul>\n<p>这与Spring中的几乎相同，并且Baeldung有一篇关于REST与Spring的错误处理的详细文章，非常值得一读。</p>\n<blockquote>\n<p><a href=\"https://www.baeldung.com/exception-handling-for-rest-with-spring\">https://www.baeldung.com/exception-handling-for-rest-with-spring</a></p>\n</blockquote>\n<h2 id=\"使用日志框架\"><a href=\"#使用日志框架\" class=\"headerlink\" title=\"使用日志框架\"></a>使用日志框架</h2><p>你可能已经意识到这一点，但你应该使用Logger进行日志记录，而不是使用System.out.println()手动执行。这很容易在Spring Boot中完成，几乎没有配置。只需获取该类的记录器实例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Logger logger = LoggerFactory.getLogger(MyClass.class);</span><br></pre></td></tr></table></figure>\n<p>这很重要，因为它可以让你根据需要设置不同的日志记录级别。</p>\n<h2 id=\"测试你的代码\"><a href=\"#测试你的代码\" class=\"headerlink\" title=\"测试你的代码\"></a>测试你的代码</h2><p>这不是Spring Boot特有的，但它需要提醒——测试你的代码！如果你没有编写测试，那么你将从一开始就编写遗留代码。</p>\n<p>如果有其他人使用你的代码库，那边改变任何东西将会变得危险。当你有多个服务相互依赖时，这甚至可能更具风险。</p>\n<p>由于存在Spring Boot最佳实践，因此你应该考虑将Spring Cloud Contract用于你的消费者驱动契约，它将使你与其他服务的集成更容易使用。</p>\n<h2 id=\"使用测试切片让测试更容易，并且更专注\"><a href=\"#使用测试切片让测试更容易，并且更专注\" class=\"headerlink\" title=\"使用测试切片让测试更容易，并且更专注\"></a>使用测试切片让测试更容易，并且更专注</h2><p>这一条实践来自Madhura Bhave（Spring 开发者, @madhurabhave23）。</p>\n<p>使用Spring Boot测试代码可能很棘手——你需要初始化数据层，连接大量服务，模拟事物……实际上并不是那么难！答案是使用测试切片。</p>\n<p>使用测试切片，你可以根据需要仅连接部分应用程序。这可以为你节省大量时间，并确保你的测试不会与未使用的内容相关联。来自spring.io的一篇名为Custom test slice with Spring test 1.4的博客文章解释了这种技术。</p>\n<blockquote>\n<p><a href=\"https://spring.io/blog/2016/08/30/custom-test-slice-with-spring-boot-1-4\">https://spring.io/blog/2016/08/30/custom-test-slice-with-spring-boot-1-4</a></p>\n</blockquote>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>感谢Spring Boot，编写基于Spring的微服务正变得前所未有的简单。我希望通过这些最佳实践，你的实施过程不仅会变得很快，而且从长远来看也会更加强大和成功。祝你好运！</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","自动配置","BOM"]},{"title":"多种方式轻松搞定SpringBoot部署Docker","url":"/Spring-Boot/f90f7de884de/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得点个关注和点个赞哦，非常感谢支持</strong><br>在部署应用程序时，Spring Boot的灵活打包选项提供了很多选择。可以将Spring Boot应用程序部署到各种云平台，容器映像（例如Docker）或虚拟机/真实机上。这里我们就来探讨SpringBoot如果部署到Docker中。</p>\n<p>我们都知道，在对 Kubernetes 微服务实践过程中，接触最多的肯定莫过于 Docker 镜像。Kubernetes是啥，我这里简单说明一下，Kubernetes 微服务简单说就是一群镜像间的排列组合与相互间调的关系，故而如何编译镜像会使服务性能更优，使镜像构建、推送、拉取速度更快，使其占用网络资源更少。更详细的可以自行查阅，这里就不做更详细的解释。</p>\n<p>@[TOC]</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>这一篇文章，想着是主要把探讨的内容放在SpringBoot和Docker的结合上，所以不想用过多的篇幅讲解Docker安装以及SpringBoot的普通应用构建上，不过这些都是我们对本篇文章进行讲解之前的准备工作，所以我在这里贴出我之前写过的文章，如果没有安装Docker以及不知道怎么构建SpringBoot的普通应用的朋友，可以先跳转过去看，个人认为写的挺详细的。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/103864834\">Docker安装教程</a>：这篇文章把Ubuntu、CentOS、Windows主流系统的安装方法详细的讲解了一遍，包括如何配置镜像加速等内容。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/103915632\">Docker新手宝典（必备）</a>：因为后面要将的内容要构建DockerFile，所以如果还不知道DockerFile是啥的朋友，可以看一下这篇文章。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a>：这篇文章看完，就可以构建一个简单的hello world应用了。</p>\n<ul>\n<li>一个简单的SpringBoot2.x程序，里面就单纯的创建一个Controller控制器，可以访问<a href=\"http://localhost/index%EF%BC%8C%E5%A6%82%E4%B8%8B\">http://localhost/index，如下</a><br><img src=\"https://img-blog.csdnimg.cn/20200327105143301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>Docker我使用的是19版本</li>\n<li>我的服务器用的是CentOS7系统<h2 id=\"手动使用DockerFile构建\"><a href=\"#手动使用DockerFile构建\" class=\"headerlink\" title=\"手动使用DockerFile构建\"></a>手动使用DockerFile构建</h2>这里解释一下Dockfile，Dockfile是一种被Docker程序解释的脚本，Dockerfile由一条一条的指令组成，每条指令对应Linux下面的一条命令。Docker程序将这些Dockerfile指令翻译真正的Linux命令。Dockerfile有自己书写格式和支持的命令，Docker程序解决这些命令间的依赖关系，类似于Makefile。Docker程序将读取Dockerfile，根据指令生成定制的image。相比image这种黑盒子，Dockerfile这种显而易见的脚本更容易被使用者接受，它明确的表明image是怎么产生的。有了Dockerfile，当我们需要定制自己额外的需求时，只需在Dockerfile上添加或者修改指令，重新生成image即可，省去了敲命令的麻烦。</li>\n</ul>\n<p>首先，我们把上面的SpringBoot应用打包，使用Maven指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">mvn <span class=\"keyword\">package</span></span><br></pre></td></tr></table></figure>\n<p>当然，你如果使用的是Idea作为dev的话，可以通过Maven工具打包，不需要输入指令，如下<br><img src=\"https://img-blog.csdnimg.cn/20200327105533786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这样就会将项目打包好了jar包，我这里打包的jar包改名为<code>example.jar</code>，更直观一点，然后接着我们到目标服务器上（注意了，服务器上要已经安装好了Docker），然后随便找个目录创建DockerFile文件，因为我用的是CentOS7，习惯在<code>/var/tmp</code>下创建，创建Dockerfile文件指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">touch Dockerfile</span><br></pre></td></tr></table></figure>\n<p>注意力，Dockerfile必须和Jar包在同一路径下，所以你在上传jar包的时候，要注意了。下面贴出DockerFile的内容，只是用最基本的构建指令，更复杂的可以熟悉之后，自行查阅DockerFile指令进行构建</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">FROM java:<span class=\"number\">8</span></span><br><span class=\"line\">EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\">VOLUME /slm</span><br><span class=\"line\">ADD example.jar boot-docker.jar</span><br><span class=\"line\">RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\">ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br></pre></td></tr></table></figure>\n<p>这里稍微解释一下基本语法</p>\n<ul>\n<li>FROM 基础镜像必要，代表你的项目将构建在这个基础上面</li>\n<li>EXPOSE 允许指定端口转发</li>\n<li>VOLUME 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。</li>\n<li>ADD 将文件从路径 复制添加到容器内部路径 支持远程url 如果是远程url权限将会是600，我这里因为直接上传了，所以就服务器本机就可以了</li>\n<li>ENV 可以用于为docker容器设置环境变量</li>\n<li>ENTRYPOINT 指定 Docker image 运行成 instance (也就是 Docker container) 时，要执行的命令或者文件。</li>\n<li>CMD 和 ENTRYPOINT 都能用来指定开始运行的程序，而且这两个命令都有两种不用的语法：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">CMD [<span class=\"string\">&quot;ls&quot;</span>,<span class=\"string\">&#x27;&#x27;</span>-l<span class=\"string\">&quot;]</span></span><br><span class=\"line\"><span class=\"string\">CMD ls -l</span></span><br></pre></td></tr></table></figure>\n<p>编写好了DockerFile之后，我们开始构建镜像，指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker build -t boot-docker .</span><br></pre></td></tr></table></figure>\n<p><code>-t boot-docker</code> 代表你要构建的名字</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Sending build context to Docker daemon  <span class=\"number\">16.</span>81MB</span><br><span class=\"line\">Step <span class=\"number\">1</span>/<span class=\"number\">7</span> : FROM java:<span class=\"number\">8</span></span><br><span class=\"line\"> ---&gt; d23bdf5b1b1b</span><br><span class=\"line\">Step <span class=\"number\">2</span>/<span class=\"number\">7</span> : EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b2445bf62da8</span><br><span class=\"line\">Step <span class=\"number\">3</span>/<span class=\"number\">7</span> : VOLUME /slm</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b73d0b73b868</span><br><span class=\"line\">Step <span class=\"number\">4</span>/<span class=\"number\">7</span> : ADD example.jar boot-docker.jar</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 2b4868aafca9</span><br><span class=\"line\">Step <span class=\"number\">5</span>/<span class=\"number\">7</span> : RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 816b59f199af</span><br><span class=\"line\">Step <span class=\"number\">6</span>/<span class=\"number\">7</span> : ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 784f033b9dd6</span><br><span class=\"line\">Step <span class=\"number\">7</span>/<span class=\"number\">7</span> : ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 92a0da91ea19</span><br><span class=\"line\">Successfully built 92a0da91ea19</span><br><span class=\"line\">Successfully tagged bootdocker:latest</span><br></pre></td></tr></table></figure>\n<p>我们可以看到已经构建完成，<code>Successfully built 92a0da91ea19</code>这句话指明了刚刚构建的镜像ID现在我们可以根据这个ID来进行操作。输入run命令来启动。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker run -itd -p <span class=\"number\">8080</span>:<span class=\"number\">8080</span> --name example 92a0da91ea19</span><br></pre></td></tr></table></figure>\n<ul>\n<li>-d 表示后台运行</li>\n<li>-p映射端口</li>\n<li>–name 容器名称</li>\n</ul>\n<p>已经运行成功访问接口。注意这里因为映射到了宿主机的端口所以访问的是宿主机的IP加端口，比如<a href=\"http://ip:8080/index\">http://ip:8080/index</a></p>\n<h2 id=\"使用Maven构建\"><a href=\"#使用Maven构建\" class=\"headerlink\" title=\"使用Maven构建\"></a>使用Maven构建</h2><p>上面说了使用Dockerfile构建，现在使用Maven来构建，我们还是使用上面的DockerFile内容，我们在项目的目录下创建DockerFile，把上面DockerFIle内容复制过去，结构如下<br><img src=\"https://img-blog.csdnimg.cn/20200327123355916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在pom中加入docker构建依赖</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t&lt;!--新增的docker maven插件--&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;com.spotify&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.4.13&lt;/version&gt;</span><br><span class=\"line\">\t&lt;!--execution 节点中配置当执行 mvn <span class=\"keyword\">package</span> 的时候，顺便也执行一下 docker:build--&gt;</span><br><span class=\"line\">\t&lt;executions&gt;</span><br><span class=\"line\">        &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;build-image&lt;/id&gt;</span><br><span class=\"line\">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">                &lt;goal&gt;build&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">        &lt;/execution&gt;</span><br><span class=\"line\">    &lt;/executions&gt;</span><br><span class=\"line\">\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t&lt;!--Docker 的主机地址--&gt;</span><br><span class=\"line\">\t\t&lt;dockerHost&gt;http:<span class=\"comment\">//192.168.66.131:2375&lt;/dockerHost&gt;</span></span><br><span class=\"line\">\t\t&lt;!--镜像名字--&gt;</span><br><span class=\"line\">\t\t&lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt;</span><br><span class=\"line\">\t\t&lt;!--DokcerFile文件地址--&gt;</span><br><span class=\"line\">\t\t&lt;dockerDirectory&gt;$&#123;project.basedir&#125;&lt;/dockerDirectory&gt;</span><br><span class=\"line\">\t\t&lt;!--镜像的 tags--&gt;</span><br><span class=\"line\">\t\t&lt;imageTags&gt;</span><br><span class=\"line\">            &lt;imageTag&gt;$&#123;project.version&#125;&lt;/imageTag&gt;</span><br><span class=\"line\">        &lt;/imageTags&gt;</span><br><span class=\"line\">\t\t&lt;resources&gt;</span><br><span class=\"line\">\t\t\t&lt;resource&gt;</span><br><span class=\"line\">\t\t\t\t&lt;targetPath&gt;/&lt;/targetPath&gt;</span><br><span class=\"line\">\t\t\t\t&lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt;</span><br><span class=\"line\">\t\t\t\t&lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt;</span><br><span class=\"line\">\t\t\t&lt;/resource&gt;</span><br><span class=\"line\">\t\t&lt;/resources&gt;</span><br><span class=\"line\">\t&lt;/configuration&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure>\n<p>注意dockerDirectory还是要设置Dockerfile文件的路径，然后如果你不配置<code>dockerHost</code>的话，要把项目复制到Linux主机中，执行解压命令</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">unzip boot-docker.zip</span><br><span class=\"line\">cd boot-docker</span><br></pre></td></tr></table></figure>\n<p>执行命令：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">mvn <span class=\"keyword\">package</span> docker:build</span><br></pre></td></tr></table></figure>\n<p>而如果配置了<code>dockerHost</code>的话<br><img src=\"https://img-blog.csdnimg.cn/20200327105533786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>上面两种方式都开始构建build</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">[INFO] Building image boot-docker/boot-docker</span><br><span class=\"line\">Step <span class=\"number\">1</span>/<span class=\"number\">7</span> : FROM java:<span class=\"number\">8</span></span><br><span class=\"line\"> ---&gt; d23bdf5b1b1b</span><br><span class=\"line\">Step <span class=\"number\">2</span>/<span class=\"number\">7</span> : EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b2445bf62da8</span><br><span class=\"line\">Step <span class=\"number\">3</span>/<span class=\"number\">7</span> : VOLUME /slm</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b73d0b73b868</span><br><span class=\"line\">Step <span class=\"number\">4</span>/<span class=\"number\">7</span> : ADD boot-docker-<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT.jar boot-docker.jar</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 2b4868aafca9</span><br><span class=\"line\">Step <span class=\"number\">5</span>/<span class=\"number\">7</span> : RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 816b59f199af</span><br><span class=\"line\">Step <span class=\"number\">6</span>/<span class=\"number\">7</span> : ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 784f033b9dd6</span><br><span class=\"line\">Step <span class=\"number\">7</span>/<span class=\"number\">7</span> : ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 92a0da91ea19</span><br><span class=\"line\">ProgressMessage&#123;id=<span class=\"keyword\">null</span>, status=<span class=\"keyword\">null</span>, stream=<span class=\"keyword\">null</span>, error=<span class=\"keyword\">null</span>, progress=<span class=\"keyword\">null</span>, progressDetail=<span class=\"keyword\">null</span>&#125;</span><br><span class=\"line\">Successfully built 92a0da91ea19</span><br><span class=\"line\">Successfully tagged boot-docker/boot-docker:latest</span><br><span class=\"line\">[INFO] Built boot-docker/boot-docker</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time: <span class=\"number\">19.</span>908s</span><br><span class=\"line\">[INFO] Finished at: Wed Jul <span class=\"number\">10</span> <span class=\"number\">16</span>:<span class=\"number\">00</span>:<span class=\"number\">21</span> CST <span class=\"number\">2019</span></span><br><span class=\"line\">[INFO] Final Memory: 35M/86M</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n<p>接着就可以启动容器了</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker run -itd -p <span class=\"number\">8080</span>:<span class=\"number\">8080</span> --name example 92a0da91ea19</span><br></pre></td></tr></table></figure>\n<p>已经运行成功访问接口。注意这里因为映射到了宿主机的端口所以访问的是宿主机的IP加端口，比如<a href=\"http://ip:8080/index\">http://ip:8080/index</a></p>\n<h2 id=\"使用Idea部署\"><a href=\"#使用Idea部署\" class=\"headerlink\" title=\"使用Idea部署\"></a>使用Idea部署</h2><p>沿用在Maven构建中的pom中的配置，此时我们的 IDEA 中多了一个选项，就是 docker，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020032712400069.png#pic_center\" alt=\"在这里插入图片描述\"><br>点击左边的绿色启动按钮，连接上 Docker 容器，连接成功之后，我们就可以看到目前 Docker 中的所有容器和镜像了，当然也包括我们刚刚创建的 Docker 镜像，如下：<br><img src=\"https://img-blog.csdnimg.cn/20200327124116922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>此时，我们选中这个镜像，右键单击，即可基于此镜像创建出一个容器，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200327124154996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们选择 Create container，然后填入容器的一些必要信息，配置一下容器名称，镜像 ID 会自动填上，暴露的端口使用 Specify 即可，然后写上端口的映射关系：<br><img src=\"https://img-blog.csdnimg.cn/20200327124229736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>配置完成后，点击下方的 run 按钮，就可以开始运行了。</p>\n<p>应用容器化是近年来的热点。而且容器技术层出不穷，掌握应用的容器化技术还是很有必要的。今天我们一步一步从零利用 Docker 构建了一个 Spring Boot 容器 。希望对你有所帮助。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Docker","Maven"]},{"title":"如何理解TensorFlow计算图？","url":"/Deep-Learning/ad4aecf69cbb/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于TensorFlow或者Pytorch，可能很多小伙伴已经使用它执行了很多模型任务了，但是回过头来仔细想想，对于这些计算框架中的计算图可能还一知半解，没有好好理解研究过，这篇文章就来捋一捋计算图，这毕竟是Tensorflow和Pytorch这样的深度学习计算框架非常重要的概念。</p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>无论是机器学习也好，还是深度学习也好，都是围绕着数学建模以及数学运算进行的，在这样的背景之下，诞生了许多的计算框架，我们所熟知的TensorFlow和Pytorch就是其中的主流，这些计算框架以计算服务为根本，自然需要一个计算模型。如果接触过开发的小伙伴就能体会到，这些计算框架的编程方式有着很大的差异。无论是编译类型的语言还是脚本语言，都是一步一步的计算变量， 从而得到结果，比如<code>result = input1 + input2</code>，当执行完语句后，就会得到<code>result</code>的值。</p>\n<p>而TensorFlow和Pytorch则不一样，首先需要通过编程构建一个计算图，然后将数据作为输入，通过这个计算图规定的计算操作进行计算，最后得到计算结果。这种符号式编程有着较多的嵌入和优化，性能也随之提升。同时计算图非常适合用来思考数学表达式，举个例子，比如计算 $e=(a+b)*(b+1)$，在这个式子中存在两个加法和一个乘法的运算，为了更加方便我们讨论，我们引入中间变量来给每个运算的输出表示为一个变量，如下：<br>$$c=a+b$$   $$d=b+1$$   $$e=c∗d$$<br>接下来，我们来构建计算图，我们将所有这些操作放入节点中，并同时计算出计算结果，如下：<br><img src=\"https://pic4.zhimg.com/80/v2-afe67feb6df30d1fe6e7a18caa288ee7_720w.jpg\" alt=\"在这里插入图片描述\"><br>我们可以清晰的看到运算表达式中，各个运算操作以及变量间的依赖和调用关系。 接着我们来求边的偏导数，如下：<br><img src=\"https://pic3.zhimg.com/80/v2-dd6e59e2939393d595fb0d145014f9be_720w.jpg\" alt=\"在这里插入图片描述\"><br>通过链式法则，我们逐节点的计算偏导数，在网络backward时候，需要用链式求导法则求出网络最后输出的梯度，然后再对网络进行优化。类似上图的表达形式就是TensorFlow以及Pytorch的基本计算模型。<strong>总结而言，计算图模型由节点(nodes)和线(edges)组成，节点表示操作符Operator，或者称之为算子，线表示计算间的依赖，实线表示有数据传递依赖，传递的数据即张量，虚线通常可以表示控制依赖，即执行先后顺序。</strong></p>\n<p>计算图从本质上来说，是TensorFlow在内存中构建的程序逻辑图，计算图可以被分割成多个块，并且可以并行地运行在多个不同的cpu或gpu上，这被称为并行计算。因此，计算图可以支持大规模的神经网络，如下：<br><img src=\"https://pic4.zhimg.com/80/v2-da6151da56abe898e72115915e76f603_720w.jpg\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Tensorflow中的计算图\"><a href=\"#Tensorflow中的计算图\" class=\"headerlink\" title=\"Tensorflow中的计算图\"></a>Tensorflow中的计算图</h1><p>TensorFlow中的计算图有三种，分别是静态计算图，动态计算图，以及Autograph，目前TensorFlow2默认采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果（在TensorFlow1中，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图）。对于动态图的好处显而易见，它方便调试程序，让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的，当然，这相对于静态图来讲牺牲了些效率，因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信，而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>\n<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码，使用tf.function构建静态图的方式叫做 Autograph。</p>\n<ul>\n<li>静态计算图：一种比较早先使用静态计算图的方法分两步，第一步定义计算图，第二步在会话中执行计算图，如下展示了TensorFlow1.0和TensorFlow2.0的写法（可以调用tf.global_variables_initializer去初始化变量或者通过tf.control_dependencies去执行计算图中没有包含的节点）：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># TensorFlow1.0</span></span><br><span class=\"line\"><span class=\"comment\">#定义计算图</span></span><br><span class=\"line\">g = tf.Graph()</span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    <span class=\"comment\">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class=\"line\">    x = tf.placeholder(name=<span class=\"string\">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)  </span><br><span class=\"line\">    y = tf.placeholder(name=<span class=\"string\">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    z = tf.string_join([x,y],name = <span class=\"string\">&#x27;join&#x27;</span>,separator=<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#执行计算图</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session(graph = g) <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class=\"string\">&quot;hello&quot;</span>,y:<span class=\"string\">&quot;world&quot;</span>&#125;))</span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"comment\"># TensorFlow2.0</span></span><br><span class=\"line\">g = tf.compat.v1.Graph()</span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    x = tf.compat.v1.placeholder(name=<span class=\"string\">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    y = tf.compat.v1.placeholder(name=<span class=\"string\">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    z = tf.strings.join([x,y],name = <span class=\"string\">&quot;join&quot;</span>,separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.compat.v1.Session(graph = g) <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    <span class=\"comment\"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class=\"line\">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class=\"string\">&quot;hello&quot;</span>,y:<span class=\"string\">&quot;world&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>动态计算图：动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行，因此称之为 Eager Excution。对于上面的操作，我们可以直接如下面代码的第一部分那样直接使用，也可以将使用动态计算图代码的输入和输出关系封装成函数，如下：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 第一部分</span></span><br><span class=\"line\"><span class=\"comment\"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class=\"line\">x = tf.constant(<span class=\"string\">&quot;hello&quot;</span>)</span><br><span class=\"line\">y = tf.constant(<span class=\"string\">&quot;world&quot;</span>)</span><br><span class=\"line\">z = tf.strings.join([x,y],separator=<span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第二部分</span></span><br><span class=\"line\"><span class=\"comment\"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strjoin</span>(<span class=\"params\">x,y</span>):</span></span><br><span class=\"line\">    z =  tf.strings.join([x,y],separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">    tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> z</span><br><span class=\"line\">result = strjoin(tf.constant(<span class=\"string\">&quot;hello&quot;</span>),tf.constant(<span class=\"string\">&quot;world&quot;</span>))</span><br><span class=\"line\">print(result) <span class=\"comment\"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Autograph：动态计算图运行效率相对较低，可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。不需要使用会话了，一切都像原始的Python语法一样自然。<strong>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率</strong>，如下（这就是为什么我们上面第二部分封装成函数的原因）：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用autograph构建静态图</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@tf.function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strjoin</span>(<span class=\"params\">x,y</span>):</span></span><br><span class=\"line\">    z =  tf.strings.join([x,y],separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">    tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> z</span><br><span class=\"line\">result = strjoin(tf.constant(<span class=\"string\">&quot;hello&quot;</span>),tf.constant(<span class=\"string\">&quot;world&quot;</span>))</span><br><span class=\"line\">print(result) <span class=\"comment\"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"tf-function\"><a href=\"#tf-function\" class=\"headerlink\" title=\"@tf.function\"></a>@tf.function</h2><p>需要注意的是不是所有的函数都可以通过tf.function进行加速的，有的任务并不值得将函数转化为计算图形式，比如简单的矩阵乘法，然而，对于大量的计算，如对深度神经网络的优化，这一图转换能给性能带来巨大的提升。我们也把这样的图转化叫作tf.AutoGraph，在Tensorflow 2.0中，会自动的对被@tf.function装饰的函数进行AutoGraph优化。下面我们来看看被tf.function装饰的函数第一次执行时都做了什么：</p>\n<ul>\n<li>函数被执行并且被跟踪(tracing)，Eager execution处于关闭状态，所有的Tensorflow函数被当做tf.Operation进行图的创建。</li>\n<li>AutoGraph被唤醒，去检测Python代码可以转为Tensorflow的逻辑，比如while &gt; tf.while, for &gt; tf.while, if &gt; tf.cond, assert &gt; tf.assert。</li>\n<li>通过以上两步，对函数进行建图，为了保证Python代码中每一行的执行顺序，tf.control_dependencies被自动加入到代码中，保证第i行执行完后我们会执行第i+1行。</li>\n<li>返回tf.Graph，根据函数名和输入参数，将这个graph存到一个cache中。</li>\n<li>对于任何一个该函数的调用，我们会重复利用cache中的计算图进行计算。</li>\n</ul>\n<p>我们来看一下Tensorflow 2.0中Eager Execution的代码如何转为tf.function的代码，首先来看一段简单的Tensorflow 2.0代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span>():</span></span><br><span class=\"line\">    a = tf.constant([[<span class=\"number\">10</span>,<span class=\"number\">10</span>],[<span class=\"number\">11.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    x = tf.constant([[<span class=\"number\">1.</span>,<span class=\"number\">0.</span>],[<span class=\"number\">0.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    b = tf.Variable(<span class=\"number\">12.</span>)</span><br><span class=\"line\">    y = tf.matmul(a, x) + b</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\">print(f().numpy())</span><br><span class=\"line\"><span class=\"comment\">#执行结果</span></span><br><span class=\"line\">[[<span class=\"number\">22.</span> <span class=\"number\">22.</span>]</span><br><span class=\"line\"> [<span class=\"number\">23.</span> <span class=\"number\">13.</span>]]</span><br></pre></td></tr></table></figure>\n<p>因为Tensorflow 2.0默认是Eager execution，代码的阅读和执行就和普通的Python代码一样，简单易读。首先我们简单的加上@tf.function装饰一下，为了方便调试，我们加入一个print和一个tf.print，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@tf.function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span>():</span></span><br><span class=\"line\">    a = tf.constant([[<span class=\"number\">10</span>,<span class=\"number\">10</span>],[<span class=\"number\">11.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    x = tf.constant([[<span class=\"number\">1.</span>,<span class=\"number\">0.</span>],[<span class=\"number\">0.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    b = tf.Variable(<span class=\"number\">12.</span>)</span><br><span class=\"line\">    y = tf.matmul(a, x) + b</span><br><span class=\"line\">    print(<span class=\"string\">&quot;PRINT: &quot;</span>, y)</span><br><span class=\"line\">    tf.print(<span class=\"string\">&quot;TF-PRINT: &quot;</span>, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\">f()</span><br><span class=\"line\"><span class=\"comment\">#执行结果</span></span><br><span class=\"line\">PRINT:  Tensor(<span class=\"string\">&quot;add:0&quot;</span>, shape=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), dtype=float32)</span><br><span class=\"line\">ValueError: tf.function-decorated function tried to create variables on non-first call.</span><br></pre></td></tr></table></figure>\n<p>这里有个异常，为什么？因为tf.function可能会对一段Python函数进行多次执行来构图，在多次执行的过程中，同样的Variable被创建了多次，产生错误。这其实也是一个很容易混乱的概念，在eager mode下一个Variable是一个Python object，所以会在执行范围外被销毁，但是在tf.function的装饰下，Variable变成了tf.Variable，是在Graph中持续存在的。所以，把一个在eager mode下正常执行的函数转换到Tensorflow图形式，需要一边思考着计算图一边构建程序。</p>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"http://colah.github.io/posts/2015-08-Backprop/\">Calculus on Computational Graphs: Backpropagation</a></li>\n<li><a href=\"https://medium.com/@yaoyaowd/tensorflow-2-0%E4%B8%8A%E6%89%8B6-%E8%A7%A3%E5%89%96tf-function%E7%9A%84%E4%BD%BF%E7%94%A8-b48cef249ca4\">Tensorflow 2.0上手6: 解剖tf.function的使用</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/33378444\">pytorch的计算图</a></li>\n<li><a href=\"http://www.likuli.com/archives/705/\">TensorFlow计算模型——计算图</a></li>\n<li><a href=\"https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/2-2,%E4%B8%89%E7%A7%8D%E8%AE%A1%E7%AE%97%E5%9B%BE.md\">eat_tensorflow2_in_30_days</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","TensorFlow","AutoGraph","计算图"]},{"title":"有必要了解的Subword算法模型","url":"/Deep-Learning/38fa61dd1a7b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>Subword算法在预训练语言模型中有着不小的地位，它在分词和字典方面的优化改进带来的影响，间接或直接影响着任务最终的性能效果，因此，作为NLP研究者或开发者，有必要了解下Subword算法的原理。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>在NLP领域，对语料进行预处理的过程中，我们需要进行分词和生成词典。很多时候用多了框架的API，觉得分词和生成字典就是调用的事情，不过事情并没有那么简单，比如其中涉及到的未登录词的问题，就对任务性能影响很大。一种很朴素的做法就是将未见过的词编码成#UNK ，有时为了不让字典太大，只会把出现频次大于某个阈值的词丢到字典里边，剩下所有的词都统一编码成#UNK 。</p>\n<blockquote>\n<p>未登录词：简单来讲就是在验证集或测试集出现了训练集从来没见到过的单词。</p>\n</blockquote>\n<p>总结而言就是，任务（如对话、机翻）的词表是定长的，但是需要实际输入的词汇是开放的(out of vocabulary)。以前的做法是新词汇添加到词典中，但是过大的词典会带来两个问题：</p>\n<ul>\n<li>稀疏问题：某些词汇出现的频率很低，得不到充分的训练</li>\n<li>计算量问题：词典过大，也就意味着embedding过程的计算量会变大</li>\n</ul>\n<p>对于分词和生成字典方面，常见的方法有：</p>\n<ul>\n<li>给低频次再设置一个back-off 表，当出现低频次的时候就去查表</li>\n<li>不做word-level转而使用char-level，既然以词为对象进行建模会有未登录词的问题，那么以单个字母或单个汉字为对象建模不就可以解决了嘛？因为不管是什么词它肯定是由若干个字母组成的。</li>\n</ul>\n<p>第一种方法，简单直接，若干back-off做的很好的话，对低频词的效果会有很大的提升，但是这种方法依赖于back-off表的质量，而且也没法处理非登录词问题。第二种方法，的确可以从源头解决未登录词的问题，但是这种模型粒度太细。</p>\n<p>下面举例word-level和subword-level的一种直观感受：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">训练集的词汇: old older oldest smart smarter smartest</span><br><span class=\"line\">word-level 词典: old older oldest smart smarter smartest 长度为 <span class=\"number\">6</span></span><br><span class=\"line\">subword-level 词典: old smart er est 长度为 <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"Subword算法\"><a href=\"#Subword算法\" class=\"headerlink\" title=\"Subword算法\"></a>Subword算法</h1><p>如何将词分解成subword，依据不同的策略，产生了几种主流的方法: Byte Pair Encoding (BPE)、wordpiece 和 Unigram Language Model等。值得一提的是，这几种算法的处理流程跟语言学没有太大的关系，单纯是统计学的解决思路，Subword模型的主要趋势：</p>\n<ul>\n<li>与单词级别的模型架构相同，但使用的是字符级别的输入</li>\n<li>采用混合架构，输入主要是字符，但是会混入其他信息</li>\n</ul>\n<h3 id=\"Byte-Pair-Encoding（BPE）\"><a href=\"#Byte-Pair-Encoding（BPE）\" class=\"headerlink\" title=\"Byte Pair Encoding（BPE）\"></a>Byte Pair Encoding（BPE）</h3><p>BPE算法流程可参考<a href=\"https://arxiv.org/abs/1508.07909\">论文</a>。Byte Pair Encoding最初是一种压缩算法，其主要是使用一些出现频率高的byte pair来组成新的byte。但它也可以作为一种分词算法(尽管其本质是自下而上的聚类方法)，它以数据中所有（Unicode）字符的单字组词汇开头并且使用最常见的n-gram对来组成一个新的n-gram。例如”loved”,”loving”,”loves”这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>\n<p><strong>这个算法有一些需要注意的地方</strong>：</p>\n<ul>\n<li>有一个目标词汇量大小并在到达时停止训练</li>\n<li>需要确定单词的最长分割片段</li>\n<li>分词过程仅在由某些先前的标记器（通常为MT的Moses标记器）标识的单词内进行。</li>\n<li>自动决定系统的词汇，不再以常规方式过度使用“单词”</li>\n</ul>\n<p><strong>获取subword词表的流程（learn-bpe）</strong>：</p>\n<ul>\n<li>准备足够大的训练语料</li>\n<li>确定期望的subword词表大小</li>\n<li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>\n<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>\n<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>\n</ul>\n<p><strong>示例</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">第一步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第二步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w es t &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d es t &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第三步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w est &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d est &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第四步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w est&lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d est&lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>实现（论文给出示例，如下）</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201204144643865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p><strong>编码和解码（apply-bpe及其逆过程）</strong>：</p>\n<ul>\n<li>在之前的算法中，我们已经得到了subword的词表，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。</li>\n<li>我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</li>\n</ul>\n<blockquote>\n<p>停止符”</w>“的意义在于表示subword是词后缀。举例来说：”st”字词不加”</w>“可以出现在词首如”st ar”，加了”</w>“表明改字词位于词尾，如”wide st</w>“，二者意义截然不同。</p>\n</blockquote>\n<p><strong>示例</strong>：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">编码过程：</span><br><span class=\"line\"><span class=\"comment\"># 给定单词序列</span></span><br><span class=\"line\">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 假设已有排好序的subword词表</span></span><br><span class=\"line\">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 迭代结果</span></span><br><span class=\"line\">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class=\"line\">&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]</span><br><span class=\"line\">&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">解码过程：</span><br><span class=\"line\"><span class=\"comment\"># 编码序列</span></span><br><span class=\"line\">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 解码序列</span></span><br><span class=\"line\">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure>\n<p>更直观的以单词“where”为例，首先按照字符拆分开，然后查找词表文件，逐对合并，优先合并频率靠前的字符对。85 319 9 15 表示在该字符对在词表文件中的评率排名。<br><img src=\"https://img-blog.csdnimg.cn/20201204143517536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>最终where</w>可以在词表文件中被找到，因此where的bpe分词结果为where</w>，对于其他并不能像where一样能在词表文件中找到整个词的词来说，bpe分词结果以最终查询结束时的分词结果为准。</p>\n<h3 id=\"Wordpiece-Sentecepiece-model\"><a href=\"#Wordpiece-Sentecepiece-model\" class=\"headerlink\" title=\"Wordpiece\\Sentecepiece model\"></a>Wordpiece\\Sentecepiece model</h3><p>Google NMT使用了借鉴了上面的方法，其V1使用的是<a href=\"https://arxiv.org/pdf/1609.08144.pdf\">wordpiece mode</a>，V2使用的是sentecepiece model。它并没有采取字符集别的n-gram计数方法，而是使用贪心近似来最大化语言模型日志可能性来选择片段。添加n-gram信息，是为了最大限度地减少perplexity，Wordpiece模型标记化内部的单词，Sentencepiece模型则对原始文本进行处理。</p>\n<p>wordpiece作为BERT使用的分词方式，其生成词表的方式和BPE非常相近，都是用合并token的方式来生成新的token，最大的区别在于选择合并哪两个token。BPE选择频率最高的相邻字符对进行合并，而wordpiece是基于概率生成的。</p>\n<p>BERT模型使用的是wordpiece的变体，对于一些常见词如1910s、at、fairfax等词直接使用，对于其他词则根据wordpieces来构建。所以，需要注意方在其他任务中使用bert时，必须处理这个问题。</p>\n<blockquote>\n<p>Choose the new word unit out of all possible ones that increase the likelihood on the training data the most when added to the mode.</p>\n</blockquote>\n<p>从字面上可能有些难以理解，列一下公式就比较清楚了。在做分词的时候假设词和词之间是独立的，所以句子的likelihood等于句子中每个词概率的乘积：<br>$$logP(Sentence)=\\sum_{i=1}^nlogP(t_i) \\quad if\\ Sentence\\ has\\ token\\ t_1…t_n$$<br>如果把相邻的 $x$ 和 $y$ 两个token合并生成一个新的token叫做 $z$，那么整个句子likelihood的变化可以用下面的式子来表达：<br>$$logP(t_z)-(logP(t_x) + logP(t_y))=log\\frac{P(t_z)}{P(t_x\\cdot P(t_y))}$$<br>这不就是两个token之间的互信息嘛！所以wordpiece和BPE的核心区别就在于wordpiece是按token间的互信息来进行合并而BPE是按照token一同出现的频率来合并的。</p>\n<p>wordpiece算法中subword词表的学习跟BPE也差不多：</p>\n<ul>\n<li>准备语料，分解成最小单元，比如英文中26个字母加上各种符号，作为原始词表</li>\n<li>利用上述语料训练语言模型</li>\n<li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li>\n<li>重复上步骤，直至词表大小达到设定值或概率增量低于某一阈值</li>\n</ul>\n<h3 id=\"unigram-language-model\"><a href=\"#unigram-language-model\" class=\"headerlink\" title=\"unigram language model\"></a>unigram language model</h3><p>语言模型作为NLP的大厦根基，也是unigram分词的基础。在wordpiece算法中，其实已经用到了language modeling，在选择token进行合并的时候目标就是能提高句子的likelihood。而<a href=\"https://arxiv.org/pdf/1804.10959.pdf\">unigram</a>分词则更进一步，直接以最大化句子的likelihood为目标来直接构建整个词表。</p>\n<p>首先，了解一下怎么样在给定词表的条件下最大化句子的likelihood。 给定词表及对应概率值: {“你”:0.18, “们”:0.16, “好”:0.18, “你们”:0.15}，对句子”你们好“进行分词:</p>\n<ul>\n<li>划分为”你” “们” “好” 的概率为 0.18<em>0.16</em>0.18=0.005184</li>\n<li>划分为”你们” “好” 的概率为 0.15*0.18=0.027</li>\n</ul>\n<p>明显看出后一种分词方式要比前一种好，当然在真实的案例下词表可能有几万个token，直接罗列各种组合的概率显然不可能，所以需要用到Viterbi算法。因此在给定词表的情况下，可以 </p>\n<ul>\n<li>计算每个token对应的概率</li>\n<li>找到一个句子最好的分词方式</li>\n</ul>\n<p>但是在词表没有确定的情况下，同时要优化词表和词表里每个token的概率很难做到。unigram分词使用逐步迭代的方式来求解，具体步骤如下：</p>\n<ul>\n<li>首先初始化一个很大的词表</li>\n<li>重复以下步骤直到词表数量减少到预先设定的阈值：<ul>\n<li>保持词表不变，用EM算法来求解每个token的概率</li>\n<li>对于每一个token，计算如果把这个token从词表中移除而导致的likelihood减少值，作为这个token的loss</li>\n<li>按loss从大到小排序，保留前n%（原文中为80%）的token。</li>\n</ul>\n</li>\n</ul>\n<p>初始化词表可以用不同的方法，一个比较直接的办法就是用所有长度为1的token加上高频出现的ngram来作为起始词表。</p>\n<h1 id=\"其他Subword模型\"><a href=\"#其他Subword模型\" class=\"headerlink\" title=\"其他Subword模型\"></a>其他Subword模型</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1610.03017.pdf\">Fully Character-Level Neural Machine Translation without Explicit Segmentation</a><br><img src=\"https://img-blog.csdnimg.cn/2020120416231516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"http://proceedings.mlr.press/v32/santos14.pdf\">Character-level to build word-level</a>：该网络结构主要是对字符进行卷积以生成单词嵌入，同时使用固定窗口对PoS标记的字嵌入进行操作。<br><img src=\"https://img-blog.csdnimg.cn/20201204162518375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1508.06615.pdf\">Character-Aware Neural Language Models</a>：这是一个更加复杂的方法，其主要动机在于：提供一种功能强大，功能强大的语言模型，可以在各种语言中有效。其可编码子词相关性：eventful, eventfully, uneventful。同时解决先前模型的罕见字问题，使用更少的参数获得可比较的表现力。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201204163241225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Hybrid NMT(<a href=\"https://arxiv.org/pdf/1604.00788.pdf)%EF%BC%9A%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E9%9D%9E%E5%B8%B8%E5%87%BA%E8%89%B2%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9C%A8%E5%8D%95%E8%AF%8D%E7%BA%A7%E5%88%AB%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%EF%BC%8C%E4%BD%86%E6%98%AF%E5%9C%A8%E6%9C%89%E9%9C%80%E8%A6%81%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%AF%E4%BB%A5%E5%BE%88%E6%96%B9%E4%BE%BF%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7%E5%88%AB%E7%9A%84%E8%BE%93%E5%85%A5%E3%80%82\">https://arxiv.org/pdf/1604.00788.pdf)：这是一个非常出色的框架，主要是在单词级别进行翻译，但是在有需要的时候可以很方便的使用字符级别的输入。</a><br><img src=\"https://img-blog.csdnimg.cn/20201204163135592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1606.02601.pdf\">chars for word embeddings</a>：该模型的目标与word2vec相同，但是使用的时字符集别的输入。它使用了双向的LSTM结构尝试捕获形态并且能够推断出词根。<br><img src=\"https://img-blog.csdnimg.cn/20201204163452177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1607.04606.pdf\">Enriching Word Vectors with Subword Information</a>：它是word2vec的升级版，对于具有大量形态学的稀有词和语言有更好的表征，它也可以说是带有字符n-gram的w2v skip-gram模型的扩展。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/86965595\">深入理解NLP Subword算法：BPE、WordPiece、ULM</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/112444056\">NLP中的subword算法及实现</a></li>\n<li><a href=\"https://www.jianshu.com/p/4b5e84c48628\">CS224N(12)-子词模型</a></li>\n<li><a href=\"https://github.com/rsennrich/subword-nmt\">subword-nmt</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/69414965\">CS224N笔记(十二):Subword模型</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\">Byte pair encoding</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["词袋","深度学习","NLP","Subword","BPE"]},{"title":"深度学习序列数据处理利器-tokenizer，结合TensorFlow和PyTorch","url":"/Deep-Learning/14c91d9dedcf/","content":"<blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>这里我们来好好探讨一下深度学习中，专门用于序列数据处理的Tokenizer，它可以帮助我们快速的建立词汇表字典，并提供了各种方法，针对文本和序列之间的转换，极大的方便的使用。TensorFlow中有keras实现的Tokenizer，而PyTorch本身是没有Tokenizer，但是我们可以通过引入torchtext或torchnlp库达到同样的效果，本文将对这几种工具的分词器部分的使用进行说明讲解。使用到的计算框架版本如下：</p>\n<ul>\n<li>TensorFlow：2.3.1</li>\n<li>PyTorch：1.7.0</li>\n<li>torchtext：0.8.0</li>\n<li>torchnlp：0.5.0</li>\n</ul>\n<p>这里我想多提一点，可能有助于后面内容的理解。我研究了这些分词器的源码，其实内部实现并不是很复杂，除了torchtext特殊一点（所以它也是使用起来最复杂的一个），torchtext不仅仅是分词器的作用，它还同时做了词嵌入，所以你使用它的时候可以选择用何种预训练词嵌入模型，如果词汇数量够多，你甚至可以直接使用内置的词向量嵌入数据，复杂但是功能齐全.</p>\n<p>而torchnlp和Tokenizer则是通过维护一个内部词汇表，不过区别在于，torchnlp内部表有两个，分别是index_to_token和token_to_index，一个是list，一个是dict，词汇的分布则是按照token出现的顺序进行编码的。而Tokenizer同样也有两个，一个是word_index和index_word，都是dict，不过它们维护的词汇分布则是按照词汇的频率由高到低，相同频率则按照出现顺序进行编码的。</p>\n<p>本文主要讲解使用方法，不讲解内部原理，有兴趣的可以去看看它们的源码实现，逻辑性还是很清晰的，看着很舒服。</p>\n<h1 id=\"TensorFlow中的Tokenizer\"><a href=\"#TensorFlow中的Tokenizer\" class=\"headerlink\" title=\"TensorFlow中的Tokenizer\"></a>TensorFlow中的Tokenizer</h1><p>其实相对而言，使用Keras的Tokenizer比较顺畅，一种丝滑的感觉（封装的比较完整），使用它我们可以对文本进行预处理，序列化，向量化等。Tokenizer基于矢量化语料库、单词数、TF-IDF等，将每个文本转换为整数序列（每个整数是字典中标记的索引）或转换成矢量（其中每个标记的系数可以是二进制的）。</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">Tokenizer</a><br>​</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">www.tensorflow.org</span><br><span class=\"line\">tf.keras.preprocessing.text.Tokenizer(</span><br><span class=\"line\">    num_words=<span class=\"literal\">None</span>, filters=<span class=\"string\">&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`&#123;|&#125;~\\t\\n&#x27;</span>, lower=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    split=<span class=\"string\">&#x27; &#x27;</span>, char_level=<span class=\"literal\">False</span>, oov_token=<span class=\"literal\">None</span>, document_count=<span class=\"number\">0</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tnum_words：根据单词频率，保留的最大单词数。仅保留最常见的num_words-<span class=\"number\">1</span>个单词，也就是保留前num_words-<span class=\"number\">1</span>个频率高的单词，不会影响内部词汇表的大小，但是会限制text和sequence转换的词汇量大小。</span><br><span class=\"line\">\tfilters：一个字符串（注意，不是正则表达式字符串哦），其中每个元素都是将从文本中过滤掉的字符。默认值为所有标点符号，加上制表符和换行符，再减去<span class=\"string\">&#x27;字符。</span></span><br><span class=\"line\"><span class=\"string\">\tlower：bool类型，是否将文本转换为小写。</span></span><br><span class=\"line\"><span class=\"string\">\tsplit：字符串，分隔词的分隔符，用于split()方法。</span></span><br><span class=\"line\"><span class=\"string\">\tchar_level：如果为True，则每个字符都将被视为token。</span></span><br><span class=\"line\"><span class=\"string\">\toov_token：如果给定的话，它将被添加到word_index中，并在text_to_sequence调用期间用于替换词汇外的单词，字典中的编码一直都是第一个。</span></span><br></pre></td></tr></table></figure>\n<p>默认情况下，Tokenizer将删除所有标点符号，从而将文本转换为以空格分隔的单词序列（单词可能包含<code>&#39;</code>字符）。 然后将这些序列分为token列表，然后将它们编码索引或向量化。注意了，<code>0</code>是一个保留索引，不会分配给任何单词。下面通过调用代码以及输出效果直观的展示用法，也能体会深刻，具体方法的参数和含义，自行查看官方文档：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">text = [</span><br><span class=\"line\">  <span class=\"string\">&quot;你 去 那儿 竟然 不喊 我 生气 了&quot;</span>,</span><br><span class=\"line\">  <span class=\"string\">&quot;道歉 ！ ！ 再有 时间 找 你 去&quot;</span></span><br><span class=\"line\"> ]</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=<span class=\"string\">&#x27;&lt;UNK&gt;&#x27;</span>, num_words=<span class=\"literal\">None</span>)</span><br><span class=\"line\">tokenizer.fit_on_texts(text) <span class=\"comment\">#text可以是字符串列表，字符串生成器（以提高内存效率）或字符串的列表的列表。</span></span><br><span class=\"line\">print(tokenizer.word_counts)</span><br><span class=\"line\"><span class=\"comment\">#OrderedDict([(&#x27;你&#x27;, 2), (&#x27;去&#x27;, 2), (&#x27;那儿&#x27;, 1), (&#x27;竟然&#x27;, 1), (&#x27;不喊&#x27;, 1), (&#x27;我&#x27;, 1), (&#x27;生气&#x27;, 1), (&#x27;了&#x27;, 1), (&#x27;道歉&#x27;, 1), (&#x27;！&#x27;, 2), (&#x27;再有&#x27;, 1), (&#x27;时间&#x27;, 1), (&#x27;找&#x27;, 1)])</span></span><br><span class=\"line\"><span class=\"comment\"># 单词计数，按次数排序</span></span><br><span class=\"line\">print(tokenizer.word_index)</span><br><span class=\"line\"><span class=\"comment\">#&#123;&#x27;&lt;UNK&gt;&#x27;: 1, &#x27;你&#x27;: 2, &#x27;去&#x27;: 3, &#x27;！&#x27;: 4, &#x27;那儿&#x27;: 5, &#x27;竟然&#x27;: 6, &#x27;不喊&#x27;: 7, &#x27;我&#x27;: 8, &#x27;生气&#x27;: 9, &#x27;了&#x27;: 10, &#x27;道歉&#x27;: 11, &#x27;再有&#x27;: 12, &#x27;时间&#x27;: 13, &#x27;找&#x27;: 14&#125;</span></span><br><span class=\"line\">print(tokenizer.index_word)</span><br><span class=\"line\"><span class=\"comment\">#&#123;1: &#x27;&lt;UNK&gt;&#x27;, 2: &#x27;你&#x27;, 3: &#x27;去&#x27;, 4: &#x27;！&#x27;, 5: &#x27;那儿&#x27;, 6: &#x27;竟然&#x27;, 7: &#x27;不喊&#x27;, 8: &#x27;我&#x27;, 9: &#x27;生气&#x27;, 10: &#x27;了&#x27;, 11: &#x27;道歉&#x27;, 12: &#x27;再有&#x27;, 13: &#x27;时间&#x27;, 14: &#x27;找&#x27;&#125;</span></span><br><span class=\"line\">print(tokenizer.num_words)</span><br><span class=\"line\"><span class=\"comment\">#None，它不会影响内部词汇表的大小，但是会限制text和sequence转换的词汇量大小</span></span><br><span class=\"line\">print(tokenizer.document_count)</span><br><span class=\"line\"><span class=\"comment\">#2，输出的是传入序列的数量</span></span><br><span class=\"line\">print(tokenizer.index_docs)</span><br><span class=\"line\"><span class=\"comment\">#defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;8: 1, 7: 1, 2: 2, 9: 1, 10: 1, 6: 1, 5: 1, 3: 2, 13: 1, 12: 1, 11: 1, 4: 1, 14: 1&#125;)</span></span><br><span class=\"line\"><span class=\"comment\">#每个词汇的次数，以索引为key</span></span><br><span class=\"line\">print(tokenizer.word_docs)</span><br><span class=\"line\"><span class=\"comment\"># defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;我&#x27;: 1, &#x27;不喊&#x27;: 1, &#x27;你&#x27;: 2, &#x27;生气&#x27;: 1, &#x27;了&#x27;: 1, &#x27;竟然&#x27;: 1, &#x27;那儿&#x27;: 1, &#x27;去&#x27;: 2, &#x27;时间&#x27;: 1, &#x27;再有&#x27;: 1, &#x27;道歉&#x27;: 1, &#x27;！&#x27;: 1, &#x27;找&#x27;: 1&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 每个词汇的次数，以单词为key</span></span><br><span class=\"line\">print(tokenizer.texts_to_sequences(text))</span><br><span class=\"line\"><span class=\"comment\"># [[2, 3, 5, 6, 7, 8, 9, 10], [11, 4, 4, 12, 13, 14, 2, 3]]</span></span><br><span class=\"line\"><span class=\"comment\"># 如果设置了num_words为5，则只能转换前4个频率最高的单词，其余为&lt;UNK&gt;，输出如下：</span></span><br><span class=\"line\"><span class=\"comment\">#[[2, 3, 1, 1, 1, 1, 1, 1], [1, 4, 4, 1, 1, 1, 2, 3]]</span></span><br><span class=\"line\">print(tokenizer.texts_to_matrix(text, <span class=\"string\">&quot;binary&quot;</span>))</span><br><span class=\"line\"><span class=\"comment\"># &quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># [[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]</span></span><br><span class=\"line\"><span class=\"comment\"># [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]</span></span><br><span class=\"line\"><span class=\"comment\"># binary：one-hot编码，count:计数编码，tfidf：词频-逆文档频率编码，freq：频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">sequence = [[<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>], [<span class=\"number\">11</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">print(tokenizer.sequences_to_matrix(sequence))</span><br><span class=\"line\"><span class=\"comment\">#[[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]</span></span><br><span class=\"line\"><span class=\"comment\"># [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]</span></span><br><span class=\"line\"><span class=\"comment\">#&quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;</span></span><br><span class=\"line\">print(tokenizer.sequences_to_texts(sequence))</span><br><span class=\"line\"><span class=\"comment\">#[&#x27;你 去 那儿 竟然 不喊 我 生气 了&#x27;, &#x27;道歉 ！ ！ 再有 时间 找 你 去&#x27;]</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"torchnlp\"><a href=\"#torchnlp\" class=\"headerlink\" title=\"torchnlp\"></a>torchnlp</h1><p>PyTorch-NLP是Python中的自然语言处理（NLP）库。 它是根据最新的研究成果而构建的，从一开始就旨在支持快速原型设计。 PyTorch-NLP带有预训练的嵌入，采样器，数据集加载器，度量，神经网络模块和文本编码器。编码方法里面很多，这里使用比较典型的<code>StaticTokenizerEncoder</code>进行说明。</p>\n<p><a href=\"https://pytorchnlp.readthedocs.io/en/latest/index.html\">Welcome to Pytorch-NLP’s documentation!</a><br>​</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">pytorchnlp.readthedocs.io</span><br><span class=\"line\">torchnlp.encoders.text.StaticTokenizerEncoder(</span><br><span class=\"line\">\tsample, min_occurrences=<span class=\"number\">1</span>, append_sos=<span class=\"literal\">False</span>, append_eos=<span class=\"literal\">False</span>, tokenize=&lt;function _tokenize&gt;, </span><br><span class=\"line\">\tdetokenize=&lt;function _detokenize&gt;, reserved_tokens=[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;/s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;copy&gt;&#x27;</span>], </span><br><span class=\"line\">\tsos_index=<span class=\"number\">3</span>, eos_index=<span class=\"number\">2</span>, unknown_index=<span class=\"number\">1</span>, padding_index=<span class=\"number\">0</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsample: 用于构建编码字典的数据样本</span><br><span class=\"line\">\tmin_occurrences (<span class=\"built_in\">int</span>, optional): 要添加到编码字典中的token的最小出现次数。</span><br><span class=\"line\">tokenize (<span class=\"built_in\">callable</span>): 序列的分词方法</span><br><span class=\"line\">detokenize (<span class=\"built_in\">callable</span>): 序列的token合并方法</span><br><span class=\"line\">append_sos (<span class=\"built_in\">bool</span>, optional): 如果为<span class=\"literal\">True</span>，则在编码向量中插入SOS token。</span><br><span class=\"line\">append_eos (<span class=\"built_in\">bool</span>, optional): 如果为<span class=\"literal\">True</span>，则在编码向量中插入EOS token。</span><br><span class=\"line\">reserved_tokens (<span class=\"built_in\">list</span> of <span class=\"built_in\">str</span>, optional): 将保留标记列表插入字典开头。</span><br><span class=\"line\">sos_index (<span class=\"built_in\">int</span>, optional): sos token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">eos_index (<span class=\"built_in\">int</span>, optional): eos token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">unknown_index (<span class=\"built_in\">int</span>, optional): unk token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">padding_index (<span class=\"built_in\">int</span>, optional): pad token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">batch (<span class=\"built_in\">list</span> of torch.Tensor): 编码序列的batch大小</span><br><span class=\"line\">lengths (torch.Tensor): 编码序列中，序列的长度列表</span><br><span class=\"line\">dim (<span class=\"built_in\">int</span>, optional): 指定分隔的序列维度</span><br></pre></td></tr></table></figure>\n<p>传给<code>StaticTokenizerEncoder</code>的sample是一个序列列表，这个和在Tokenizer中的是差不多的，<code>tokenize</code>和Tokenizer中的<code>split</code>是类似的功能，只不过<code>tokenize</code>传入的是方法，<code>StaticTokenizerEncoder</code>内部有一个初始化的token列表，长这样：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;/s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;copy&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>然后添加进来的序列就在其末尾进行顺序的补入，还有要说的就是，如果上面关于sos、eos等等的参数没有特别指定，就直接使用这个初始化列表里面的token。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">texts = [</span><br><span class=\"line\">  <span class=\"string\">&quot;你 去 那儿 竟然 不喊 我 生气 了&quot;</span>,</span><br><span class=\"line\">  <span class=\"string\">&quot;道歉 ！ ！ 再有 时间 找 你 去&quot;</span></span><br><span class=\"line\"> ]</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = StaticTokenizerEncoder(sample=texts, tokenize=<span class=\"keyword\">lambda</span> x: x.split())</span><br><span class=\"line\">print(tokenizer.index_to_token)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;copy&gt;&#x27;, &#x27;你&#x27;, &#x27;去&#x27;, &#x27;那儿&#x27;, &#x27;竟然&#x27;, &#x27;不喊&#x27;, &#x27;我&#x27;, &#x27;生气&#x27;, &#x27;了&#x27;, &#x27;道歉&#x27;, &#x27;！&#x27;, &#x27;再有&#x27;, &#x27;时间&#x27;, &#x27;找&#x27;]</span></span><br><span class=\"line\"><span class=\"comment\"># 编码就是按照它的list索引进行的</span></span><br><span class=\"line\">print(tokenizer.token_to_index)</span><br><span class=\"line\"><span class=\"comment\"># &#123;&#x27;&lt;pad&gt;&#x27;: 0, &#x27;&lt;unk&gt;&#x27;: 1, &#x27;&lt;/s&gt;&#x27;: 2, &#x27;&lt;s&gt;&#x27;: 3, &#x27;&lt;copy&gt;&#x27;: 4, &#x27;你&#x27;: 5, &#x27;去&#x27;: 6, &#x27;那儿&#x27;: 7, &#x27;竟然&#x27;: 8, &#x27;不喊&#x27;: 9, &#x27;我&#x27;: 10, &#x27;生气&#x27;: 11, &#x27;了&#x27;: 12, &#x27;道歉&#x27;: 13, &#x27;！&#x27;: 14, &#x27;再有&#x27;: 15, &#x27;时间&#x27;: 16, &#x27;找&#x27;: 17&#125;</span></span><br><span class=\"line\">print(tokenizer.vocab)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;copy&gt;&#x27;, &#x27;你&#x27;, &#x27;去&#x27;, &#x27;那儿&#x27;, &#x27;竟然&#x27;, &#x27;不喊&#x27;, &#x27;我&#x27;, &#x27;生气&#x27;, &#x27;了&#x27;, &#x27;道歉&#x27;, &#x27;！&#x27;, &#x27;再有&#x27;, &#x27;时间&#x27;, &#x27;找&#x27;]</span></span><br><span class=\"line\">print(tokenizer.vocab_size)</span><br><span class=\"line\"><span class=\"comment\"># 18</span></span><br><span class=\"line\">print([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts])</span><br><span class=\"line\"><span class=\"comment\"># [tensor([ 5,  6,  7,  8,  9, 10, 11, 12]), tensor([13, 14, 14, 15, 16, 17,  5,  6])]</span></span><br><span class=\"line\"><span class=\"comment\"># 你会发现返回的是一个torch.tensor的列表，你如果想要整理成一个array的Tensor，使用如下方法</span></span><br><span class=\"line\">print(stack_and_pad_tensors([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts]))</span><br><span class=\"line\"><span class=\"comment\"># BatchedSequences(tensor=tensor([[ 5,  6,  7,  8,  9, 10, 11, 12],</span></span><br><span class=\"line\"><span class=\"comment\">#         [13, 14, 14, 15, 16, 17,  5,  6]]), lengths=tensor([8, 8]))</span></span><br><span class=\"line\">print(stack_and_pad_tensors([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts])[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 5,  6,  7,  8,  9, 10, 11, 12],</span></span><br><span class=\"line\"><span class=\"comment\">#         [13, 14, 14, 15, 16, 17,  5,  6]])</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"torchtext\"><a href=\"#torchtext\" class=\"headerlink\" title=\"torchtext\"></a>torchtext</h1><p>torchtext库是PyTorch项目的一部分，和torchvision等一样，和torch核心库分离开，从torchtext这个名字我们也能大概猜到该库是pytorch圈中用来预处理文本数据集的库，torchtext在文本数据预处理方面特别强大。使用它来对文本数据进行预处理简直不要太方便，提供了非常多的API使用。当然本文主要是说明文本序列转换方面的，具体torchtext的完整使用，就不做赘述，想要深入了解的可以参考其官方文档。</p>\n<p><a href=\"https://pytorch.org/text/stable/index.html\">TorchText</a><br>​<br>我这里简单的概括一下它在数据预处理方面的简单流程，让你有个简单的了解。<br><img src=\"https://pic4.zhimg.com/80/v2-327ae85e5c64443a7faa8a721b283307_720w.jpg\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Train/Validation/Test数据集分割</li>\n<li>文件数据导入（File Loading）</li>\n<li>分词（Tokenization） 文本字符串切分为词语列表</li>\n<li>构建词典(Vocab) 根据训练的预料数据集构建词典</li>\n<li>数字映射(Numericalize/Indexify) 根据词典，将数据从词语映射成数字，方便机器学习</li>\n<li>导入预训练好的词向量(word vector)</li>\n<li>分批(Batch) 数据集太大的话，不能一次性让机器读取，否则机器会内存崩溃。解决办法就是将大的数据集分成更小份的数据集，分批处理</li>\n<li>向量映射（Embedding Lookup） 根据预处理好的词向量数据集，将5的结果中每个词语对应的索引值变成 词语向量</li>\n</ul>\n<p>除了第一步和最后一步需要我们使用其他库或者自己编写方法外，其他所有的步骤，torchtext都提供了API。言归正传，说会文本数据转换。如果你使用上面的预处理流程，就可以得到关于贴合原数据的向量序列。这里我就不说这种方法，我这里说一下，使用现有字典或预训练模型进行转换的方法。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">torchtext.data.functional.sentencepiece_numericalizer(sp_model)</span><br><span class=\"line\">sp_id_generator = sentencepiece_numericalizer(sp_model)</span><br><span class=\"line\">list_a = [<span class=\"string\">&quot;sentencepiece encode as pieces&quot;</span>, <span class=\"string\">&quot;examples to   try!&quot;</span>]</span><br><span class=\"line\"><span class=\"built_in\">list</span>(sp_id_generator(list_a))</span><br><span class=\"line\"><span class=\"comment\">#[[9858, 9249, 1629, 1305, 1809, 53, 842], [2347, 13, 9, 150, 37]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">torchtext.data.functional.numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=<span class=\"literal\">None</span>)</span><br><span class=\"line\">vocab = &#123;<span class=\"string\">&#x27;Sentencepiece&#x27;</span> : <span class=\"number\">0</span>, <span class=\"string\">&#x27;encode&#x27;</span> : <span class=\"number\">1</span>, <span class=\"string\">&#x27;as&#x27;</span> : <span class=\"number\">2</span>, <span class=\"string\">&#x27;pieces&#x27;</span> : <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">ids_iter = numericalize_tokens_from_iterator(vocab,</span><br><span class=\"line\">                             simple_space_split([<span class=\"string\">&quot;Sentencepiece as pieces&quot;</span>,</span><br><span class=\"line\">                                              <span class=\"string\">&quot;as pieces&quot;</span>]))</span><br><span class=\"line\"><span class=\"keyword\">for</span> ids <span class=\"keyword\">in</span> ids_iter:</span><br><span class=\"line\">    print([num <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> ids])</span><br><span class=\"line\"><span class=\"comment\">#[0, 2, 3]</span></span><br><span class=\"line\"><span class=\"comment\">#[2, 3]</span></span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["词袋","深度学习","TensorFlow","PyTorch","Tokenizer","分词器"]},{"title":"聊一聊Spring-Boot中跨域场景","url":"/Spring-Boot/c3c6ba065761/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>跨域问题我相信大多数人都遇见过，这里我做一个简单的总结，大体上将跨域问题进行一个简单的介绍，以及针对SpringBoot进行跨域解决方案的说明。如果觉得写得好有所收获，记得点个赞及点个关注哦。</p>\n<h2 id=\"介绍跨域\"><a href=\"#介绍跨域\" class=\"headerlink\" title=\"介绍跨域\"></a>介绍跨域</h2><p>跨域有个的英文简称，叫做CORS，其全称叫做跨域资源共享(CORS) ，是一种机制。跨域的基本原理就是使用额外的 HTTP 头来告诉浏览器，让运行在一个 origin (domain) 上的 Web 应用被准许访问来自不同源服务器上的指定的资源。当一个资源从与该资源本身所在的服务器「不同的域、协议或端口」请求一个资源时，资源会发起一个「跨域 HTTP 请求」。</p>\n<p>这里要强调一下的是，很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。同源策略是由 Netscape 提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持 JavaScript 的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。换而言之，如果协议、域名或者端口不相同，那么应用与服务之间的请求就是跨域请求，这个时候就需要进行特殊的处理，才能通过浏览器的安全策略。</p>\n<p>同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是 JSONP，JSONP 虽然能解决跨域但是有一个很大的局限性，那就是只支持 GET 请求，不支持其他类型的请求，在 RESTful 时代这几乎就没什么用。而这里说的 CORS（跨域源资源共享）是一个 W3C 标准，它是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是 JSONP 模式的现代版。在 Spring 框架中，对于 CORS 也提供了相应的解决方案，在 Spring Boot 中，这一方案得倒了简化，无论是单纯的跨域，还是结合 Spring Security 之后的跨域，都变得非常容易了。</p>\n<h2 id=\"事前准备\"><a href=\"#事前准备\" class=\"headerlink\" title=\"事前准备\"></a>事前准备</h2><p>首先创建两个普通的 Spring Boot 项目，这个就不用我多说，第一个命名为 provider 提供服务，第二个命名为 consumer 消费服务，第一个配置端口为 8080，第二个配置配置为 8081，然后在 provider 上提供两个 hello 接口，一个 get，一个 post，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloController</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@GetMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@PostMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;post hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 consumer 的 <code>resources/static</code> 目录下创建一个 html 文件，发送一个简单的 ajax 请求，如下：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div id=<span class=\"string\">&quot;app&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;input type=<span class=\"string\">&quot;button&quot;</span> onclick=<span class=\"string\">&quot;btnClick()&quot;</span> value=<span class=\"string\">&quot;get_button&quot;</span>&gt;</span><br><span class=\"line\">&lt;input type=<span class=\"string\">&quot;button&quot;</span> onclick=<span class=\"string\">&quot;btnClick2()&quot;</span> value=<span class=\"string\">&quot;post_button&quot;</span>&gt;</span><br><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">btnClick</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">        $.get(<span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span>, <span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">msg</span>) </span>&#123;</span><br><span class=\"line\">            $(<span class=\"string\">&quot;#app&quot;</span>).html(msg);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">btnClick2</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">        $.post(<span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span>, <span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">msg</span>) </span>&#123;</span><br><span class=\"line\">            $(<span class=\"string\">&quot;#app&quot;</span>).html(msg);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/script&gt;</span><br></pre></td></tr></table></figure>\n<p>然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Access to XMLHttpRequest at <span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span> from origin <span class=\"string\">&#x27;http://localhost:8081&#x27;</span> has been blocked by CORS policy: No <span class=\"string\">&#x27;Access-Control-Allow-Origin&#x27;</span> header is present on the requested resource.</span><br></pre></td></tr></table></figure>\n<p>可以看到，由于同源策略的限制，请求无法发送成功。使用 CORS 可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在 provider 中如何配置。首先可以通过 <code>@CrossOrigin</code> 注解配置某一个方法接受某一个域的请求，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloController</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@CrossOrigin(value = &quot;http://localhost:8081&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@GetMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@CrossOrigin(value = &quot;http://localhost:8081&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@PostMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;post hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个注解表示这两个接口接受来自 <code>http://localhost:8081</code> 地址的请求，配置完成后，重启 provider ，再次发送请求，浏览器控制台就不会报错了，consumer 也能拿到数据了。此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200620000112985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>这个表示服务端愿意接收来自 <code>http://localhost:8081</code> 的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。provider 上，每一个方法上都去加注解未免太麻烦了，有的小伙伴想到可以讲注解直接加在 Controller 上，不过每个 Controller 都要加还是麻烦，在 Spring Boot 中，还可以通过全局配置一次性解决这个问题，全局配置只需要在 SpringMVC 的配置类中重写 <code>addCorsMappings</code> 方法即可，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebMvcConfig</span> <span class=\"keyword\">implements</span> <span class=\"title\">WebMvcConfigurer</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addCorsMappings</span><span class=\"params\">(CorsRegistry registry)</span> </span>&#123;</span><br><span class=\"line\">        registry.addMapping(<span class=\"string\">&quot;/**&quot;</span>)</span><br><span class=\"line\">        .allowedOrigins(<span class=\"string\">&quot;http://localhost:8081&quot;</span>)</span><br><span class=\"line\">        .allowedMethods(<span class=\"string\">&quot;*&quot;</span>)</span><br><span class=\"line\">        .allowedHeaders(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>/**</code> 表示本应用的所有方法都会去处理跨域请求，<code>allowedMethods</code> 表示允许通过的请求数，<code>allowedHeaders</code> 则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。</p>\n<h2 id=\"SpringSecurity中解决跨域问题\"><a href=\"#SpringSecurity中解决跨域问题\" class=\"headerlink\" title=\"SpringSecurity中解决跨域问题\"></a>SpringSecurity中解决跨域问题</h2><p>如果使用了 Spring Security，上面的跨域配置会失效，因为请求被 Spring Security 拦截了。当引入了 Spring Security 的时候，我们有两种办法开启 Spring Security 对跨域的支持。</p>\n<h4 id=\"方式一\"><a href=\"#方式一\" class=\"headerlink\" title=\"方式一\"></a>方式一</h4><p>方式一就是在前面的基础上，添加 Spring Security 对于 CORS 的支持，只需要添加如下配置即可：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .authorizeRequests()</span><br><span class=\"line\">                .anyRequest().authenticated()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .formLogin()</span><br><span class=\"line\">                .permitAll()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .httpBasic()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf()</span><br><span class=\"line\">                .disable();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>一个 <code>.cors</code> 就开启了 Spring Security 对 CORS 的支持。</p>\n<h4 id=\"方式二\"><a href=\"#方式二\" class=\"headerlink\" title=\"方式二\"></a>方式二</h4><p>方式二则是去除前面的跨域配置，直接在 Spring Security 中做全局配置，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .authorizeRequests()</span><br><span class=\"line\">                .anyRequest().authenticated()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .formLogin()</span><br><span class=\"line\">                .permitAll()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .httpBasic()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors()</span><br><span class=\"line\">                .configurationSource(corsConfigurationSource())</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf()</span><br><span class=\"line\">                .disable();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\">CorsConfigurationSource <span class=\"title\">corsConfigurationSource</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        UrlBasedCorsConfigurationSource source = <span class=\"keyword\">new</span> UrlBasedCorsConfigurationSource();</span><br><span class=\"line\">        CorsConfiguration configuration = <span class=\"keyword\">new</span> CorsConfiguration();</span><br><span class=\"line\">        configuration.setAllowCredentials(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        configuration.setAllowedOrigins(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setAllowedMethods(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setAllowedHeaders(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setMaxAge(Duration.ofHours(<span class=\"number\">1</span>));</span><br><span class=\"line\">        source.registerCorsConfiguration(<span class=\"string\">&quot;/**&quot;</span>,configuration);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> source;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过 CorsConfigurationSource 实例对跨域信息作出详细配置，例如允许的请求来源、允许的请求方法、允许通过的请求头、探测请求的有效期、需要处理的路径等等。使用这种方式就可以去掉前面的跨域配置了。</p>\n<h2 id=\"OAuth2跨域处理\"><a href=\"#OAuth2跨域处理\" class=\"headerlink\" title=\"OAuth2跨域处理\"></a>OAuth2跨域处理</h2><p>还有一种情况就是 OAuth2 允许跨域，如果用户要访问 OAuth2 端点，例如<code>/oauth/token</code> ，出现了跨域该怎么配置呢？这个其实很简单，我们只要在请求中携带一个Token就可以了，主要是配置一个 CorsFilter，我这里就把核心配置类列出来：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GlobalCorsConfiguration</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> CorsFilter <span class=\"title\">corsFilter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        CorsConfiguration corsConfiguration = <span class=\"keyword\">new</span> CorsConfiguration();</span><br><span class=\"line\">        corsConfiguration.setAllowCredentials(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedOrigin(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedHeader(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedMethod(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = <span class=\"keyword\">new</span> UrlBasedCorsConfigurationSource();</span><br><span class=\"line\">        urlBasedCorsConfigurationSource.registerCorsConfiguration(<span class=\"string\">&quot;/**&quot;</span>, corsConfiguration);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> CorsFilter(urlBasedCorsConfigurationSource);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后在 SecurityConfig 中开启跨域支持：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@Order(Ordered.HIGHEST_PRECEDENCE)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .requestMatchers().antMatchers(HttpMethod.OPTIONS, <span class=\"string\">&quot;/oauth/**&quot;</span>)</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf().disable().formLogin()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","categories":["Spring-Boot"],"tags":["Sprint Boot","跨域","SCORS"]},{"title":"论文阅读笔记：An-End-to-End-Trainable-Neural-Network-Model-with...","url":"/Paper-Reading/66c85cb16080/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog<br>原文链接：<a href=\"https://arxiv.org/pdf/1708.05956.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>我们提出了面向任务的对话系统的新型端到端可训练神经网络模型，该模型能够跟踪对话状态，基于知识（KB）的API调用，并将结构化的KB查询结果合并到系统响应中，从而成功完成面向任务的对话。通过在对话历史上的进行belief tracking和KB结果处理，进而模型产生结构良好的系统响应。我们使用从第二个Dialog State Tracking Challenge（DSTC2）语料库转换而来的数据集在饭店搜索域中评估模型。实验结果表明，在给定对话历史记录的情况下，该模型可以很好地跟踪对话状态。此外，我们的模型在产生适当的系统响应方面表现出令人鼓舞的结果，其使用基于每个响应的准确性评估指标优于以前的端到端可训练神经网络模型。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>端到端可训练神经网络模型可以直接针对最终系统目标函数（例如任务成功率）进行优化，从而缓解了可信分配和在线适应的挑战。在这项工作中，我们提出了面向任务的对话的端到端可训练神经网络模型，该模型将统一网络应用于belief tracking，基于知识（KB）操作和响应创建。该模型能够跟踪对话状态，与KB交互以及将结构化KB查询结果合并到系统响应中，从而成功完成面向任务的对话框。我们表明，在给出对话历史记录的情况下，我们提出的模型可以有效地跟踪状态。与先前的端到端可训练神经网络模型相比，我们的模型还证明了在提供适当的系统响应和进行面向任务的对话方面的有更好的性能。</p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><ul>\n<li>Dialog State Tracking<ul>\n<li>在口语对话系统中，对话状态跟踪或belief tracking是指在可能的对话状态上保持分布的任务，这些状态直接确定系统的动作。</li>\n<li>使用诸如CRF或RNN之类的序列模型进行判别的方法可以灵活地探索任意特征并实现最新的DST性能，从而解决了生成模型的局限性。</li>\n</ul>\n</li>\n<li>End-to-End Task-Oriented Dialog Models<ul>\n<li>我们的模型使用统一的网络进行信念跟踪，知识操作和响应生成，以充分探索可以在不同任务之间共享的知识</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Proposed-Method\"><a href=\"#Proposed-Method\" class=\"headerlink\" title=\"Proposed Method\"></a>Proposed Method</h1><p>我们将面向任务的对话建模为一个多任务序列学习问题，其组件用于编码用户输入，跟踪信念状态，发出API调用，处理KB结果以及生成系统响应。模型架构如下图所示，对话框中的多轮序列使用LSTM递归神经网络进行编码， 根据对话历史记录，会话状态保持在LSTM状态。LSTM状态向量用于生成：（1）通过从非词法化系统响应候选列表中选择句结构；（2）信念跟踪器中每个插槽值的概率分布；（3）指向检索到的KB结果中与用户查询匹配的实体。通过用预测的插槽值和实体属性值替换去词化的token来生成最终的系统响应。<br><img src=\"https://img-blog.csdnimg.cn/20200929090815736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Utterance Encoding：这里的话语编码是指将单词序列编码为连续的密集向量。我们使用双向LSTM将用户输入编码为句向量，其中用户输入第 $k$ 轮对话共 $T_k$ 个 单词表示为$U_k=(w_1,w_2,…,w_{T_k})$。用户的句向量 $U_k$ 表示为 $U_k=[\\overrightarrow{h_{T_k}^{U_k}},\\overleftarrow{h_{1}^{U_k}}]$，$\\overrightarrow{h_{T_k}^{U_k}}$和 $\\overleftarrow{h_{1}^{U_k}}$ 是第 $k$ 轮最后的前向和反向的句级LSTM状态。</li>\n<li> Belief Tracking：信念跟踪（或对话状态跟踪）通过沿对话顺序积累信息来维持和调整对话状态（例如用户的目标）。在第 $k$ 轮从用户输入中收集新信息后，神经对话模型会更新每种插槽类型 $m∈M$ 的候选值的概率分布 $P(S_{k}^{m})$。在回合 $k$，对话级LSTM（LSTMD）更新其隐藏状态 $s_k$，并在接收到用户输入编码 $U_k$ 和 K_B$ 指示器 $I_k$（将在下面的部分中进行说明）之后，使用它来推断用户目标的任何更新。<br>$$s_k=LSTM_D(s_{k-1}, [U_k, I_k])$$$$P(S_{k}^{m}|U_{\\leq k}, I_{\\leq k}) = SlotDist_m(s_k)$$<br>其中，$SlotDist_m(s_k)$是在插槽类型 $m∈M$ 上具有softmax激活功能的多层感知器（MLP）</li>\n<li>Issuing API Calls：基于对话状态，模型可以基于信念跟踪输出发出API调用以查询KB。该模型首先生成一个简单的API调用命令模板。 通过使用信念追踪器针对每个目标插槽的最佳假设替换命令模板中的插槽类型token，产生最终的API调用命令。</li>\n<li>KB Results Processing：一旦神经对话模型接收到KB查询结果，它将通过从返回的列表中选择实体来向用户建议选项。KB搜索或数据库查询的输出通常具有定义明确的结构，并且实体属性与实体索引相关联。在对话的第 $k$ 轮，将二进制KB指示器 $I_k$ 传递给神经对话模型。该指标由上一次API调用中检索到的实体数和当前实体指针决定。当系统处于向用户建议实体的状态时，如果接收到零值 $I_k$，则该模型很可能会通知用户与当前查询匹配的实体不可用，否则，如果 $I_k$ 有值，该模型可能会根据实体指针 $P(E_k)$ 的更新概率分布从检索结果中选择一个实体：<br>$$P(E_k|U_{\\leq k}, I_{\\leq k}) = EntityPointerDist(s_k)$$<br>其中，其中 $EntityPointerDist$ 是具有softmax激活的MLP。</li>\n<li>System Response Generation：在对话的第 $k$ 轮，从非词化响应候选列表中选择句结构 $R_k$。最终的系统响应是通过用预测的插槽值和实体属性值替换非词性化token来产生的。</li>\n<li>Model Training：我们通过找到参数集 $θ$ 来训练神经对话模型，该参数集 $θ$ 最小化了目标标签，实体指针和去词化系统响应的预测分布和真实分布的交叉熵：<br>$$\\underset{\\Theta}{min}\\sum_{k=1}^{K}-[\\sum_{m=1}^{M}\\lambda_{S^m}logP(S_{k}^{m*}|U_{\\leq k},I_{\\leq k};\\Theta )$$$$+\\lambda_ElogP(E_{k}^{<em>}|U_{\\leq k},I_{\\leq k};\\Theta )$$$$+\\lambda_RlogP(R_{k}^{</em>}|U_{\\leq k}, I_{\\leq k};\\Theta )]$$<br>其中，其中 $λ<em>s$ 是每个系统输出成本的线性插值权重。$S</em>{k}^{m∗}$，$E_K^*$和$R_k^*$ 是第 $k$ 轮每个任务的真实标签。</li>\n<li>Alternative Model Designs：直观地，如果模型被明确告知目标值估计并且知道其先前对用户做出的响应，则该模型可能会提供更好的响应。因此，我们设计并评估了一些替代模型架构，以验证这种假设：<ul>\n<li>具有先前发出的非词化系统响应的模型连接回对话级LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,R_{k-1}])$</li>\n<li>具有先前发出的插槽标签的模型连接回对话级LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,S_{k-1}^{1},…,R_{k-1}^{M}])$</li>\n<li>具有先前发出的响应和插槽标签的模型都已连接回对话框LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,R_{k-1},S_{k-1}^{1},…,R_{k-1}^{M}])$</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>我们使用DSTC2中的数据进行模型评估。 在这项研究中，我们通过保留对话状态注释并添加系统命令（API调用）来结合原始的DSTC2语料库和此转换版本。下表汇总了该数据集的统计信息。<br><img src=\"https://img-blog.csdnimg.cn/20200929093437880.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用Adam优化方法进行批量为32的小批量模型训练，在模型训练期间，具有dropout的正则化应用于非循环连接[26]，dropout率为0.5。我们将梯度的最大范数设置为5，以防止梯度爆炸。对话层LSTM和话语层LSTM的隐藏层大小分别设置为200和150，大小为300的单词嵌入是随机初始化的，我们还尝试使用在Google新闻数据集上受过训练的预训练词向量来初始化词嵌入。</p>\n<p>下表显示了使用不同用户话语编码方法和不同单词嵌入初始化的模型的评估结果：<br><img src=\"https://img-blog.csdnimg.cn/20200929104233676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了不同的递归模型架构的评估结果：<br><img src=\"https://img-blog.csdnimg.cn/20200929104439793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表中的结果所示，我们的系统可实现与最新系统相当的信念跟踪性能：<br><img src=\"https://img-blog.csdnimg.cn/20200929104545966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>即使使用相同的评估度量，我们的模型在设计上的设置也与下表中的其他已发布模型相比略有不同<br><img src=\"https://img-blog.csdnimg.cn/20200929104800976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><p>在这项工作中，我们为面向任务的对话系统提出了一种新颖的端到端可训练神经网络模型。该模型能够跟踪对话的状态，通过发出API调用与知识交互，并将结构化的查询结果合并到系统响应中，从而成功完成面向任务的对话框。在餐厅搜索域的评估中，使用来自第二个“Dialog State Tracking Challenge”语料库的转换数据集，我们提出的模型显示了在对话轮次序列上跟踪对话状态的鲁棒性能。该模型还展示了在生成适当的系统响应方面的有不错的性能，优于以前的端到端可训练神经网络模型。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","End-to-End","Belief-Tracking","对话系统","神经网络"]},{"title":"论文阅读笔记：Attention-Is-All-You-Need","url":"/Paper-Reading/14883e8ec3b5/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Attention Is All You Need<br>原文链接：<a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>序列转导模型基于复杂的递归或卷积神经网络，包括编码器和解码器，表现最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即Transformer，它完全基于注意力机制，完全消除了重复和卷积。在两个机器翻译任务上进行的实验表明，这些模型在质量上具有优势，同时具有更高的可并行性，并且所需的训练时间大大减少。我们的模型在WMT 2014英语到德语的翻译任务上达到了28.4 BLEU，比包括集成学习在内的现有最佳结果提高了2 BLEU。在2014年WMT英语到法语翻译任务中，我们的模型在八个GPU上进行了3.5天的训练后，创造了新的单模型最新BLEU分数41.8，比文献中最好的模型的训练成本更小。我们展示了Transformer通过将其成功应用于具有大量训练数据和有限训练数据的英语解析，将其很好地概括了其他任务。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>在Transformer出现之前，RNN、LSTM、GRU等在序列模型和转导问题的方法中占据了稳固的地位，比如语言模型、机器翻译等，人们一直在努力扩大循环语言模型和编码器-解码器体系结构的界限。递归模型通常沿输入和输出序列的符号位置考虑计算。将位置与计算时间中的步骤对齐，它们根据先前的隐藏状态ht-1和位置t的输入生成一系列隐藏状态ht。这种固有的顺序性导致了没办法并行化进行训练，这在较长的序列长度上变得至关重要。最近的工作通过分解技巧和条件计算大大提高了计算效率，同时在后者的情况下还提高了模型性能，但是，顺序计算的基本约束仍然存在。注意力机制已成为各种任务中引人注目的序列建模和转导模型不可或缺的一部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。在这项工作中，我们提出了一种Transformer，一种避免重复的模型体系结构，而是完全依赖于注意力机制来绘制输入和输出之间的全局依存关系。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>减少顺序计算的目标也构成了扩展神经GPU，ByteNet和ConvS2S的基础，它们全部使用卷积神经网络作为基本构建块，并行计算所有输入和输出的隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数在位置之间的距离中增加，对于ConvS2S线性增长，而对于ByteNet则对数增长，这使得学习远处位置之间的依存关系变得更加困难。在Transformer中，此操作被减少为恒定的操作次数，尽管以平均注意力加权位置为代价，导致有效分辨率降低，但是我们用多头注意力抵消了这种代价。</p>\n<p>Self-attention（有时称为d intra-attention）是一种与单个序列的不同位置相关的注意力机制，目的是计算序列的表示形式。Self-attention已成功用于各种任务中，包括阅读理解，抽象摘要和学习与任务无关的句子表示。Transformer是第一个完全依靠Self-attention来计算其输入和输出表示的转导模型，而无需使用序列对齐的RNN或卷积。</p>\n<h1 id=\"Model-Architecture\"><a href=\"#Model-Architecture\" class=\"headerlink\" title=\"Model Architecture\"></a>Model Architecture</h1><p>Transformer依旧是遵循encoder-decoder结构，其模型的每一步都是自回归的，在生成下一个模型时，会将先前生成的符号用作附加输入。在此基础上，使用堆叠式Self-attention和point-wise，并在encoder和decoder中使用全连接层，结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20200917160031494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Encoder-and-Decoder-Stacks\"><a href=\"#Encoder-and-Decoder-Stacks\" class=\"headerlink\" title=\"Encoder and Decoder Stacks\"></a>Encoder and Decoder Stacks</h2><ul>\n<li><p>Encoder</p>\n<ul>\n<li>编码器由$N = 6$个相同层的堆栈组成，每层有两个子层，分别是Self-attention机制和位置完全连接的前馈网络</li>\n<li>每个子层周围都使用残差连接并进行归一化，也就是说每个子层的输出为$LayerNorm(x+Sublayer(x))$</li>\n<li>为了促进这些残差连连接，模型中的所有子层以及嵌入层均产生尺寸为dmodel = 512的输出</li>\n</ul>\n</li>\n<li><p>Decoder</p>\n<ul>\n<li>解码器还由N = 6个相同层的堆栈组成</li>\n<li>除了每个编码器层中的两个子层之外，解码器还插入一个第三子层，该子层对编码器堆栈的输出执行多头注意力</li>\n<li>对编码器堆栈的输出执行多头注意力时，要注意使用mask，保证预测只能依赖于小于当前位置的已知输出。</li>\n<li>每个子层周围都使用残差连接并进行归一化</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Attention\"><a href=\"#Attention\" class=\"headerlink\" title=\"Attention\"></a>Attention</h2><p>注意力方法可以描述为将query和一组key-value映射到输出，其中query，key，value和输出都是向量。输出是计算value的加权总和，其中分配给每个value的权重是通过query与相应key的方法来计算的。<br><img src=\"https://img-blog.csdnimg.cn/20200917162420181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Scaled-Dot-Product-Attention\"><a href=\"#Scaled-Dot-Product-Attention\" class=\"headerlink\" title=\"Scaled Dot-Product Attention\"></a>Scaled Dot-Product Attention</h3><p>它的输入是$d_k$维的queries和keys组成，使用所有key和query做点积，并除以$\\sqrt{d_k}$，然后应用softmax函数获得value的权重，公式如下：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$$</p>\n<ul>\n<li>常用注意力方法<ul>\n<li>相加（在更大的$d_k$下，效果更好）</li>\n<li>点积（更快一些）</li>\n<li>所以为了在较大的$d_k$下，点积也能工作的好，在公式中才使用了$\\frac{1}{\\sqrt{d_k}}$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Multi-Head-Attention\"><a href=\"#Multi-Head-Attention\" class=\"headerlink\" title=\"Multi-Head Attention\"></a>Multi-Head Attention</h3><p>多头注意力使模型可以共同关注来自不同位置的不同表示子空间的信息，最后取平均：<br>$$<br>MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^{O}\\<br>head_1=Attention(QW_{i}^{Q},K_{i}^{K},V_{i}^{V})<br>$$<br>论文中使用$h=8$注意力层，其中$d_k=d_v=\\frac{d_{model}}{h}=64$</p>\n<h3 id=\"Applications-of-Attention-in-our-Model\"><a href=\"#Applications-of-Attention-in-our-Model\" class=\"headerlink\" title=\"Applications of Attention in our Model\"></a>Applications of Attention in our Model</h3><p>Transformer以三种不同方式使用多头注意力:</p>\n<ul>\n<li>在“encoder-decoder注意”层中，queries来自先前的decoder层，而keys和values来自encoder的输出，这允许解码器中的每个位置都参与输入序列中的所有位置。</li>\n<li>encoder包含self-attention层。 在 self-attention层中，所有key，value和query都来自同一位置，在这种情况下，是编码器中前一层的输出。</li>\n<li>类似地，decoder中的self-attention层允许decoder中的每个位置都参与decoder中直至并包括该位置的所有位置。我们需要阻止decoder中的向左信息流，以保留自回归属性。</li>\n</ul>\n<h2 id=\"Position-wise-Feed-Forward-Networks\"><a href=\"#Position-wise-Feed-Forward-Networks\" class=\"headerlink\" title=\"Position-wise Feed-Forward Networks\"></a>Position-wise Feed-Forward Networks</h2><p>除了关注子层之外，我们的encoder和decoder中的每个层还包含一个完全连接的前馈网络，该网络分别应用于每个位置。 这由两个线性变换组成，两个线性变换之间有ReLU激活。<br>$$<br>FNN(x)=max(0,xW_1+b_1)W_2+b_2<br>$$<br>虽然线性变换在不同位置上相同，但是它们使用不同的参数</p>\n<h2 id=\"Embeddings-and-Softmax\"><a href=\"#Embeddings-and-Softmax\" class=\"headerlink\" title=\"Embeddings and Softmax\"></a>Embeddings and Softmax</h2><p>与其他序列转导模型类似，使用学习的嵌入将输入标记和输出标记转换为维dmodel的向量。我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一个token概率</p>\n<h2 id=\"Positional-Encoding\"><a href=\"#Positional-Encoding\" class=\"headerlink\" title=\"Positional Encoding\"></a>Positional Encoding</h2><p>位置编码的维数dmodel与嵌入的维数相同，因此可以将两者相加，位置编码有很多选择，可以学习和固定。在这项工作中，我们使用不同频率的正弦和余弦函数，其中pos是位置，i是维度。<br>$$<br>PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})<br>PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>$$<br>也就是说，位置编码的每个维度对应于一个正弦曲线，波长形成从2π到10000·2π的几何级数。当然还有其他的方法，不过选择正弦曲线版本是因为它可以使模型外推到比训练期间遇到的序列长的序列长度</p>\n<h1 id=\"Why-Self-Attention\"><a href=\"#Why-Self-Attention\" class=\"headerlink\" title=\"Why Self-Attention\"></a>Why Self-Attention</h1><p>考虑一下三点：</p>\n<ul>\n<li>每层的总计算复杂度</li>\n<li>可以并行化的计算量，以所需的最少顺序操作数衡量</li>\n<li>网络中远程依赖关系之间的路径长度，在许多序列转导任务中，学习远程依赖性是一项关键挑战。影响学习这种依赖性的能力的一个关键因素是网络中前向和后向信号必须经过的路径长度。输入和输出序列中位置的任意组合之间的这些路径越短，学习远程依赖关系就越容易</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200917175632580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>作为附带的好处，自我关注可以产生更多可解释的模型</p>\n<h1 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h1><h2 id=\"Training-Data-and-Batching\"><a href=\"#Training-Data-and-Batching\" class=\"headerlink\" title=\"Training Data and Batching\"></a>Training Data and Batching</h2><p>我们对标准WMT 2014英语-德语数据集进行了培训，该数据集包含约450万个句子对。句子是使用字节对编码的，字节对编码具有大约37000个token的共享源目标词汇。</p>\n<h2 id=\"Hardware-and-Schedule\"><a href=\"#Hardware-and-Schedule\" class=\"headerlink\" title=\"Hardware and Schedule\"></a>Hardware and Schedule</h2><p>大型模型接受了300,000步（3.5天）的训练。</p>\n<h2 id=\"Optimizer\"><a href=\"#Optimizer\" class=\"headerlink\" title=\"Optimizer\"></a>Optimizer</h2><p>我们使用Adam优化器，其中β1= 0.9，β2= 0.98和$\\xi $= 10-9。 根据公式，我们在训练过程中改变了学习率：<br>$$lrate=d_{model}^{-0.5}\\cdot min(step_num^{-0.5},step_num\\cdot warmup_steps^{-1.5})$$<br>这对应于第一个warmup_steps训练步骤的线性增加学习率，此后与步骤数的平方根的平方成反比地降低学习率，我们使用的warmup_steps=4000。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><ul>\n<li>Residual Dropout</li>\n<li>Label Smoothing</li>\n</ul>\n<h1 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h1><p><img src=\"https://img-blog.csdnimg.cn/20200917203338187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在这项工作中，我们介绍了Transformer，这是完全基于注意力的第一个序列转导模型，用多头自注意力代替了编码器-解码器体系结构中最常用的循环层。对于翻译任务，与基于循环层或卷积层的体系结构相比，可以比在体系结构上更快地训练Transformer。 在WMT 2014英语到德语和WMT 2014英语到法语的翻译任务中，我们都达到了最新水平。 在前一项任务中，我们最好的模型甚至胜过所有先前报告。我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。 我们计划将Transformer扩展到涉及文本以外的涉及输入和输出形式的问题，并研究局部受限的注意机制，以有效处理大型输入和输出，例如图像，音频和视频。 使生成减少连续性是我们的另一个研究目标。</p>\n","categories":["Paper-Reading"],"tags":["Attention","注意力机制","Transformer","Paper"]},{"title":"论文阅读笔记：Covariate-Shift：基于机器学习分类器的回顾和分析","url":"/Paper-Reading/12f6d260a557/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Covariate Shift: A Review and Analysis on Classifiers<br>原文链接：<a href=\"https://ieeexplore.ieee.org/abstract/document/8978471\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>我们都知道在机器学习模型中，训练数据和测试数据是不同的阶段，并且，通常是是假定训练数据和测试数据点遵循相同的分布。但是实际上，模型的输入和输出的联合分布在训练数据和测试数据之间是不同的，这称为dataset shift。dataset shift的一种简单情况就是covariate shift，covariate shift仅输入分布发生变化，而在给定输入的输出条件分布保持不变。本文主要概述了现有covariate shift检测和自适应方法及其应用，同时基于包含合成数据和真实数据的四种数据集，提供了各种covariate shift自适应技术在分类算法上的实验效果分析。实验结果标明，使用Importance Reweighting（重要性重加权）方法和feature-dropping方法能够让机器学习模型在covariate shift问题的表现上有明显提高。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>熟悉机器学习的小伙伴应该都知道，Supervised learning中涉及的步骤包括从真实来源收集数据，数据整合，数据转换，对已知数据进行训练和验证算法，最后将其应用于未知测试数据。所以为了提高这些Supervised learning算法的性能，数据质量起着重要作用。 可以基于各种角度来分析数据质量，例如数据复杂性，缺失值，噪声，数据不平衡，离群值，缩放值等。而在决定机器学习模型的性能中，起着重要作用的一种数据质量度量是dataset shift。它是下面提到的三种shifts的总称：</p>\n<ul>\n<li>Covariate Shift：Change in the independent variables</li>\n<li>Prior probability shift: Change in the target variable</li>\n<li>Concept Shift: Change in the relationship between the independent and the target variable</li>\n</ul>\n<p>在假设测试和训练数据中存在的点或实例属于相同的特征空间和相同的分布的假设下，常用的机器学习模型可以很好地工作，但是，当分布发生变化时，需要使用新的训练数据从头开始重建基础统计模型。</p>\n<p>Covariate Shift一词描述为学习阶段和泛化阶段之间（训练数据和测试数据）输入变量“ X”的分布变化。虽然Covariate Shift是dataset shifts中研究最多的shifts类型，但没有很合适的确切定义。比如在机器学习的角度来看，这种预测性建模通常称为transfer learning，也有一些相似的名称，但是概念上的差异很小，例如population drift，concept drift， dataset shift。以下是文献中存在的几种Covariate Shift定义：</p>\n<ul>\n<li>令 $x$ 为解释变量或协变量，$q_1(x)$是评估预测时的概率密度，$q_0(x)$ 表示观察数据中的概率密度，则$q_0(x)\\neq q_1(x)$ 的情况称为分布的Covariate Shift。</li>\n<li>产生特征向量 $x$ 及其相关类别标签 $y$ 的数据分布由于潜在变量 $t$ 而变化，因此当 $P(y|x,t_1)\\neq P(y|x,t_2)$ 时，可以说发生了Covariate Shift。</li>\n</ul>\n<h1 id=\"Covariate-Shift检测和自适应算法\"><a href=\"#Covariate-Shift检测和自适应算法\" class=\"headerlink\" title=\"Covariate Shift检测和自适应算法\"></a>Covariate Shift检测和自适应算法</h1><p>可以通过使用以下公式给出的重要性权重来消除因Covariate Shift而导致的预测误差：<br>$$W(X)=\\frac{p_{test}(X)}{p_{train}(X)} \\tag{1}$$<br>其中 $p_{test}(X)$ 和 $p_{train}(X)$ 分别是在测试和训练数据集中找到输入 $X$ 的概率。公式（1）来自这样的直觉，即如果特定训练实例出现在测试集中的概率很高，则它必须获得更高的权重。$W(X)$ 给出每个训练输入点的重要性值，将其与这些点相乘将得出更准确的预测。但是，此值是先验未知的，因此需要从数据样本中估算其值，因此，接下来分别列出一些在该领域中引入的最重要的重要性估计方法。</p>\n<h2 id=\"Kernel-Density-Estimation-KDE\"><a href=\"#Kernel-Density-Estimation-KDE\" class=\"headerlink\" title=\"Kernel Density Estimation (KDE)\"></a>Kernel Density Estimation (KDE)</h2><p>KDE是一种非参数方法，用于获得随机变量的概率密度函数的近似值，公式（2）是高斯核方程，公式（3）是KDE的方程<br>$$K(x,x^{‘})=exp(\\frac{-||x-x^{‘}||^2}{2\\sigma^2})\\tag{2}$$    $$\\hat{p}(x)=\\frac{1}{n(2\\pi \\sigma^2)^{\\frac{d}{2}}}\\sum_{i=1}^nK_{\\sigma}(x-x_i)\\tag{3}$$<br>其中，$x$ 和 $x^{‘}$ 是两个内核样本，$\\sigma$ 是内核宽度。KDE给出的近似值的精度完全由上式中选定的 $\\sigma$ 值确定。$\\sigma$ 的最佳值可以通过交叉验证获得，因此，训练和测试数据点可用于通过等式（2）分别获得 $\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ ，并且重要性可以估计为：<br>$$W(X)=\\frac{\\hat{p}<em>{test}(X)}{\\hat{p}</em>{train}(X)}$$<br>但是，上面讨论的方法受到维数的限制，而且支持可靠逼近所需的数据量通常随维数呈指数增长，这在数据样本数量有限的情况下非常复杂。因此，KDE无法用于高维数据，一种解决方法是直接找到 $W(X)$ 而无需计算 $p_{test}(X)$ 和 $p_{train}(X)$ 。</p>\n<h2 id=\"Discriminative-Learning\"><a href=\"#Discriminative-Learning\" class=\"headerlink\" title=\"Discriminative Learning\"></a>Discriminative Learning</h2><p>概率分类器也可以用来直接估计重要性，从训练集中提取的样本标记为 $\\mu = 0$，从测试集中提取的样本标记为 $\\mu = 1$。则概率密度可以表示为如下：<br>$$p_{tr}(X)=p(X|\\mu=0) \\ and \\  p_{te}(X)=p(X|\\mu=1)$$<br>使用贝叶斯定理，重要性权重 $W(X)$ 可写为：<br>$$W(X)=\\frac{p_{tr}}{p_{te}}=\\frac{p(\\mu=0)p(\\mu=1|X) }{p(\\mu=1)p(\\mu=0|X) }$$<br>其中 $\\frac{p(\\mu=0)}{p(\\mu=1)}\\approx \\frac{n_{tr}}{n_{te}}$ 可以容易得到。可以通过使用Logistic回归，随机森林，SVM等分类器区分 ${x_i}<em>{i=1}^{n</em>{tr}}$ 和 ${x_j}<em>{j=1}^{n</em>{te}}$ 来近似估计概率 $p(\\mu|X)$。在此还需要注意的是，可以将训练样本与测试样本分离的概率用作检测数据集中是否存在Covariate Shift的度量，在本文中称为<em>判别测试</em>。但是，训练这些模型有时会很耗时，因此，已经引入了有效的概率分类方法，例如LSPC （最小二乘概率分类器）和IWLSPC（结合了重要性加权LSPC和重要性重加权LSPC）。</p>\n<h2 id=\"Kernel-Mean-Matching\"><a href=\"#Kernel-Mean-Matching\" class=\"headerlink\" title=\"Kernel Mean Matching\"></a>Kernel Mean Matching</h2><p>KMM直接能获得 $W(X)$ 而无需计算 $p_{test}(X)$ 和 $p_{train}(X)$ ，KMM的基本思想是找到ܹ $W(X)$ ，从而使再现核Hilbert（RKHS）空间中的训练点和测试点的方法接近。等式（2）中的高斯核是计算通用RKHS核的示例，并且已证明下式给出的优化问题的解给得出真实的重要性值：<br>$$min_{w_i}[\\frac{1}{2}\\sum_{i,i^{‘}=1}^{n_{tr}}w_iw_{i^{‘}}K_{\\sigma(x_i^{tr},x_{i^{‘}}^{tr})}-sum_{i=1}^{n_{tr}}w_iK_i]\\tag{4}$$<br>其中，$(\\frac{1}{n_{tr}})|\\sum_{i=1}^{n_{tr}}w_i-n_{tr}\\leq \\epsilon| \\ and\\  0\\leq w_1,w_2,w_3,…,w_{n_{tr}}\\leq B$，且$K_i=\\frac{n_{tr}}{n_{te}}\\sum_{j=1}^{n_{te}}K_\\sigma(x_i^{tr},x_j^{te})$</p>\n<p>KMM的性能完全取决于调整参数$B$，$\\epsilon$ 和 $\\sigma$ 的值，因此，诸如交叉验证之类的常规模型选择方法无法找到最佳值。KMM的一种变体解决方案是，$\\sigma$ 选择一个样本间的中值距离。实验证明KMM优于natural plug-in估算器</p>\n<h2 id=\"Kullback-Leblier-Importance-Estimation-Procedure-KLIEP\"><a href=\"#Kullback-Leblier-Importance-Estimation-Procedure-KLIEP\" class=\"headerlink\" title=\"Kullback Leblier Importance Estimation Procedure(KLIEP)\"></a>Kullback Leblier Importance Estimation Procedure(KLIEP)</h2><p>通过交叉验证完成的算法（例如KMM）可能会因Covariate Shift下的偏差导致模型选择失败，因此在CV上使用了重要性加权版本IWCV（重要性加权交叉验证）。但是，在IWCV中，模型选择需要通过重要性估计步骤内的无监督学习来完成，这是一个主要缺点。KLEIP找到重要性估计$\\hat{w}(x)$，以使以使真实测试输入密度 $p_{te}(x)$ 和 $\\hat{p}<em>{te(x)}$ 之间的Kullback-Leibler方差最小，其中$\\hat{p}</em>{te(x)}=\\hat{w}(x)p_{tr}(x)$，这样无需显式建模 $p_{te}(x)$ 和 $p_{tr}(x)$ 即可完成此操作。</p>\n<h2 id=\"Least-Squares-Importance-Fitting-LSIF-Unconstrained-Least-Squares-Importance-Fitting-uLSIF\"><a href=\"#Least-Squares-Importance-Fitting-LSIF-Unconstrained-Least-Squares-Importance-Fitting-uLSIF\" class=\"headerlink\" title=\"Least Squares Importance Fitting (LSIF), Unconstrained Least Squares Importance Fitting (uLSIF)\"></a>Least Squares Importance Fitting (LSIF), Unconstrained Least Squares Importance Fitting (uLSIF)</h2><p>KLIEP使用Kullback-Leibler散度找出两个函数之间的密度差异，LSIF使用平方损失代替$\\hat{w}(x)$建模为KLIEP。交叉验证用于查找诸如正则化参数和内核宽度 $\\sigma$ 之类的调整参数的最佳值。但是，由于数字误差的累积，LSIF有时会给出错误的结果，为了解决这个问题，已经提出了一种近似形式的LSIF，称为uLSIF，它可以通过简单地求解线性方程组来进行解的计算，因此，uLSIF在数值上是稳定的。</p>\n<h1 id=\"Covariate-Shift自适应实际应用\"><a href=\"#Covariate-Shift自适应实际应用\" class=\"headerlink\" title=\"Covariate Shift自适应实际应用\"></a>Covariate Shift自适应实际应用</h1><ul>\n<li>半监督speaker识别：与会话有关的变化，录音场景中的变化以及身体情感变化等</li>\n<li>人脸识别判断年龄：由于环境中光照条件的变化，训练和测试数据倾向于具有不同的分布。</li>\n<li>基于脑电图的脑机接口：脑信号的非平稳性质</li>\n<li>…</li>\n</ul>\n<h1 id=\"实验结果：分类算法的性能分析\"><a href=\"#实验结果：分类算法的性能分析\" class=\"headerlink\" title=\"实验结果：分类算法的性能分析\"></a>实验结果：分类算法的性能分析</h1><p>实验中使用的数据集是一个数据集来自Kaggle仓库包含三个合成数据集，实验的分类算法如下：</p>\n<ul>\n<li>线性判别分析</li>\n<li>K邻近算法</li>\n<li>决策树分类器</li>\n<li>朴素贝叶斯分类器</li>\n</ul>\n<h2 id=\"训练和测试数据为不同分布\"><a href=\"#训练和测试数据为不同分布\" class=\"headerlink\" title=\"训练和测试数据为不同分布\"></a>训练和测试数据为不同分布</h2><ul>\n<li>处理技术：Discriminative Learning</li>\n</ul>\n<p>实验用数据集（称为数据集-I）具有1000个训练样本和1000个测试样本，其中训练样本的分布是正态的，而测试样本的分布是二项式的，训练样本包含特征变量 $X$，$Y$。训练集是通过选择1000个遵循均匀分布且方差= 1和均值= 25（在下面的等式中表示为“data”）的随机样本而形成的：<br>$$X=11\\times data-6\\tag{5a}$$    $$Y=X^2+10\\times X -5\\tag{5b}$$<br>类似地，测试集由遵循二项式分布的1000个随机数组成，被选择的概率为 $p = 0.8$，值的范围为1到20。特征$X$，$Y$使用上式计算。 $\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ 的分布如下：<br><img src=\"https://img-blog.csdnimg.cn/20201226121106135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实验中，LDA，KNN和决策树分类器的准确性得分有所提高，而朴素贝叶斯分类器的准确性得分下降。</p>\n<h2 id=\"训练和测试数据不同的均指和方差\"><a href=\"#训练和测试数据不同的均指和方差\" class=\"headerlink\" title=\"训练和测试数据不同的均指和方差\"></a>训练和测试数据不同的均指和方差</h2><ul>\n<li>处理技术：KDE</li>\n</ul>\n<p>第二个实验数据集（称为数据集-II）具有相同分布但均值和方差不同，训练和测试集分别有1000个样本，且训练和测试样本的分布都均匀。训练集包含两个特征 $X$，$Y$。将生成1000个均值为25和方差为1的随机值。类似地，创建具有正态分布的测试数据，并由$X$，$Y$两列组成。选择了均值为80，方差为1的1000个随机数。$\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ 的分布如下：<br><img src=\"https://img-blog.csdnimg.cn/20201226125001315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表实验结果显示，所有分类算法的性能都有提高：<br><img src=\"https://img-blog.csdnimg.cn/20201226125124775.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"训练和测试数据具有相同的分布，且属性数量增加\"><a href=\"#训练和测试数据具有相同的分布，且属性数量增加\" class=\"headerlink\" title=\"训练和测试数据具有相同的分布，且属性数量增加\"></a>训练和测试数据具有相同的分布，且属性数量增加</h2><ul>\n<li>处理技术：KLIEP</li>\n</ul>\n<p>对于第三个实验，生成了两个具有正态分布的数据集。训练和测试集大约有500个样本和3个属性$X$，$Y$和$Z$，其中$X$，$Y$是输入属性，$Z$是预测标签，$Z$的计算公式如下：<br>$$Z=sin(Y\\times \\pi)+X\\tag{6}$$<br><img src=\"https://img-blog.csdnimg.cn/20201226125627102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实验中，决策树分类器的性能提高约30%，其他三个分类器减少近20%。</p>\n<h2 id=\"真实数据\"><a href=\"#真实数据\" class=\"headerlink\" title=\"真实数据\"></a>真实数据</h2><p>此实验的数据集是从Kaggle仓库中提取的“俄罗斯住房市场”数据集（Dataset-IV）进行的。<br><img src=\"https://img-blog.csdnimg.cn/20201226130159555.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>机器学习算法的性能是将其用于现实世界场景中要考虑的重要因素， 它在很大程度上取决于数据集和数据的分布。当将诸如决策树或神经网络之类的机器学习模型在一个场景下训练并利用其来提高另一种场景下的泛化时，则发生的域自适应称为转移学习。但是在监督学习算法中，要确保模型在训练和测试场景中都能正常工作，重要的是要确保训练样本和测试样本的分布相同。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","AutoGraph","计算图","Covariate Shift"]},{"title":"论文阅读笔记：CrossWOZ：A-Large-Scale-Chinese-Cross-Domain-Task...","url":"/Paper-Reading/0e5d72d373dc/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset<br>原文链接：<a href=\"https://arxiv.org/pdf/2002.11893.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>最近在搜集一些对话数据集，众所周知，生成对话数据集是一件费钱又费时的工作，所以一般只有大机构才能做出高质量且庞大的数据集，所以看到好的数据集，那还不赶紧收藏一波。<br><a href=\"https://github.com/thu-coai/CrossWOZ\">CrossWOZ代码和数据</a></p>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>为了推进多域（跨域）对话建模并缓解中文面向任务的数据集的不足的问题，我们提出了CrossWOZ，这是第一个大规模的中文跨域“人机交互”任务导向的数据集。CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。大约60％的对话具有跨域用户目标，这些目标有利于域间依赖性，并有助于对话中跨域自然过渡。我们还为pipeline的面向任务的对话系统提供了一个用户模拟器和一些基准模型，这将有助于研究人员在该语料库上比较和评估他们的模型。CrossWOZ的规模庞大和丰富注释使它适合研究跨域对话建模中的各种任务，例如对话状态跟踪，策略学习，用户模拟等。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>许多语料库已经推进了面向任务的对话系统的研究，不过大部分都是单领域对话，比如ATIS、DSTC 2、Frames、KVRET、WOZ 2.0和M2M等。这些数据集的大小，语言变化或任务复杂性仍然受到限制。在现实生活中的对话中，人们自然会在不同领域或场景之间进行转换，同时仍保持连贯的上下文，因此，现实对话比仅在单个域中模拟的对话要复杂得多。提及到多领域对话语料，最多被提及的就是MultiWOZ，但是，这个数据集状态注释有很多噪音，并且缺少用户端对话行为。<strong>跨域的依赖关系可以简单地体现为对不同的域施加相同的预先规定的约束，例如要求将旅馆和景点都定位在城镇中心。</strong>，如下是一个对话示例：<br><img src=\"https://img-blog.csdnimg.cn/20200927172302406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>跨领域对话的数据样例</strong><br><img src=\"https://img-blog.csdnimg.cn/20200927172857102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本数据集的特点：</p>\n<ul>\n<li> 用户在某个领域的选择可能会影响到与之相关的领域的选择，在跨领域上下文理解更有挑战。</li>\n<li> 第一个大规模中文跨领域任务导向数据集。</li>\n<li> 在用户端和系统端都有详细的对话状态记录，标注信息全面。</li>\n</ul>\n<p><strong>与其他数据集的对比</strong><br><img src=\"https://img-blog.csdnimg.cn/20200927173339629.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>对话数据的收集方式有三类：</p>\n<ul>\n<li>human-to-human</li>\n<li>human-to-machine</li>\n<li>machine-to-machine</li>\n</ul>\n<h1 id=\"Data-Collection\"><a href=\"#Data-Collection\" class=\"headerlink\" title=\"Data Collection\"></a>Data Collection</h1><p>语料库是模拟旅行者寻找旅游信息并计划其在北京旅行的场景，domains包括酒店，景点，餐厅，地铁和出租车。 数据收集过程总结如下：</p>\n<ol>\n<li>基础数据库的构建：通过爬虫从网络上获取了北京市的酒店/旅游景点/饭店以及地铁和出租车信息。</li>\n<li>目标构建：论文通过算法自动生成标注人员的对话目标。</li>\n<li>对话收集：要求工人进行少量对话，并向他们提供有关对话质量的反馈。</li>\n<li>数据标注：每个对话都包含结构化的目标，任务描述，用户状态，系统状态，对话行为和对话。</li>\n</ol>\n<p>基础数据库信息示例如下：<br><img src=\"https://img-blog.csdnimg.cn/20200927180441535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>用户目标示例如下：<br><img src=\"https://img-blog.csdnimg.cn/20200927180519346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Statistics\"><a href=\"#Statistics\" class=\"headerlink\" title=\"Statistics\"></a>Statistics</h1><p><img src=\"https://img-blog.csdnimg.cn/20200927182041653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200927182214247.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200927182241872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Benchmark-and-Analysis\"><a href=\"#Benchmark-and-Analysis\" class=\"headerlink\" title=\"Benchmark and Analysis\"></a>Benchmark and Analysis</h1><p><img src=\"https://img-blog.csdnimg.cn/20200927182351883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了第一个大规模的中文跨域任务导向对话数据集，CrossWOZ。CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域。此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。大约60％的对话具有跨域用户目标，这鼓励了相关域之间的自然过渡。得益于丰富的对话状态注释以及用户端和系统端的对话行为，该语料库为研究跨域对话建模的各种任务（例如对话状态跟踪，策略学习等）提供了新的测试平台。我们的实验表明，跨域约束对于所有这些任务都是具有挑战性的。相关领域之间的转换对建模尤其具有挑战性。 除了基于语料库的组件级评估外，我们还使用用户模拟器执行了系统级评估，这需要针对pipeline跨域对话系统的所有组件提供更强大的模型。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","面向任务","CrossWOZ"]},{"title":"论文阅读笔记：Layer-Normalization","url":"/Paper-Reading/18004f87a32e/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Layer Normalization<br>原文链接：<a href=\"https://arxiv.org/pdf/1607.06450.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>训练最新的深度神经网络在计算上是昂贵的，减少训练时间的一种方法是归一化神经元，最近引入的一种称为<strong>批归一化</strong>的技术使用训练案例的小批量上神经元的总输入分布来计算均值和方差，然后使用均值和方差对每个训练案例中该神经元的总输入进行归一化，这大大减少了前馈神经网络的训练时间。但是，批归一化的效果取决于小批量的大小，如何将其应用于递归神经网络尚不明显。在本文中，我们通过在单个训练案例上计算从层的所有总输入到神经元的归一化的均值和方差，将批归一化转换为层归一化。像批归一化一样，我们还为每个神经元提供了自己的自适应偏差和增益，这些偏差和增益在归一化之后且在非线性之前应用。与批归一化不同，层归一化在训练和测试期间执行完全相同的计算。通过在每个时间步分别计算归一化统计量，将其应用于递归神经网络也很简单。层归一化在动态稳定循环网络中的隐藏状态方面非常有效。从经验上讲，我们表明与以前发布的技术相比，层归一化可以大大减少训练时间。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>已经证明，使用某种形式的随机梯度下降训练的深度神经网络在计算机视觉和语音处理的各种监督学习任务上的性能大大优于以前的方法。但是最先进的深度神经网络通常需要几天的训练，可以通过在不同机器上为训练案例的不同子集计算梯度或在许多机器上拆分神经网络本身来加快学习速度，但这可能需要大量的交互且复杂 软件，随着并行度的增加，这将导致收益迅速减少。正交方法是修改在神经网络的前向传递中执行的计算，以使学习更轻松。最近，提出了批归一化，以通过在深度神经网络中包括其他标准化阶段来减少训练时间。<strong>归一化使用训练数据中的平均值和标准差对每个求和的输入进行标准化</strong>，即使使用简单的SGD，使用批归一化训练的前馈神经网络也能更快收敛，除了改善训练时间外，批统计中的随机性还可以作为训练期间的正则化器。</p>\n<p><strong>尽管简单，但批归一化仍需要对求和的输入统计信息求平均值，在深度固定的前馈网络中，直接为每个隐藏层分别存储统计信息很简单，但是，递归神经网络（RNN）中递归神经元的总输入通常随序列长度而变化，因此对RNN应用批归一化似乎需要针对不同时间步长进行不同统计。此外，批归一化不能应用于在线学习任务或批必须很小的超大型分布式模型</strong>。</p>\n<p>本文介绍了层归一化，这是一种提高各种神经网络模型训练速度的简单归一化方法，与批量归一化不同，该方法从隐藏层内神经元的总输入直接估算归一化统计数据，因此归一化不会在训练案例之间引入任何新的依存关系，我们表明，层归一化对RNN效果很好，并改善了几种现有RNN模型的训练时间和泛化性能。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>前馈神经网络是从输入模式$x$到输出向量$y$的非线性映射，考虑深度前馈神经网络中的第$l$个隐藏层，并将$l$表示为该层中神经元的总输入的向量表示，通过权重矩阵$W^l$和自下而上的输入$h^l$的线性投影来计算总和输入，如下所示：<br>$$a_{i}^{l}={w_{i}^{l}}^{T}h^l $$ $$h_{i}^{l+1}=f(a_{i}^{l}+b_{i}^{l})$$<br>其中f(.)是逐个元素的非线性函数，而$w_{i}^{l}$是第$i$个隐藏单元的传入权重，$b_{i}^{l}$是标量偏差参数，使用基于梯度的优化算法学习神经网络中的参数，并通过反向传播计算梯度。</p>\n<p>深度学习的挑战之一是，相对于一层中权重的梯度高度依赖于上一层中神经元的输出，特别是如果这些输出以高度相关的方式变化时。因此提出了批归一化，以减少这种不希望的“Internal Covariate Shift”。该方法对训练案例中每个隐藏单元的求和输入进行归一化。 具体来说，对于第$l$层中的第$i$个求和输入，批归一化方法根据求和输入在数据分布下的方差重新缩放<br>$$\\bar{a}<em>{i}^{l}=\\frac{g</em>{i}^{l}}{\\sigma_{i}^{l}}(a_{i}^{l}-\\mu_{i}^{l})$$ $$\\mu_{i}^{l}=\\underset{X\\sim P(x)}{\\mathbb{E}}[a_{i}^{l}]$$  $$\\sigma_{i}^{l}=\\sqrt{\\underset{X\\sim P(x)}{\\mathbb{E}}[(a_{i}^{l}-\\mu_{i}^{l})^2]}$$<br>其中$\\bar{a}_{i}^{l}$是第$l$层中第$i$个隐藏单元的归一化总和输入，$g_i$是在非线性激活函数之前缩放归一化激活的增益参数，注意期望是在整个训练数据分布下。 在上面的等式中精准计算期望值通常是不切实际的，因为这将需要使用当前的权重集前向传递至整个训练数据集。 取而代之的是，使用当前小批量的经验样本估算出$µ$和$σ$，这限制了小批量生产的规模，并且很难应用于递归神经网络。</p>\n<p>Internal Covariate Shift一个较规范的定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p>\n<p>关于Batch Normalization的相关介绍，可以看这边<a href=\"https://zhuanlan.zhihu.com/p/34879333\">知乎文章</a>，里面写的蛮清晰的。</p>\n<h1 id=\"Layer-normalization\"><a href=\"#Layer-normalization\" class=\"headerlink\" title=\"Layer normalization\"></a>Layer normalization</h1><p>请注意，一层输出的变化将趋向于导致对下一层求和的输入发生高度相关的变化，尤其是对于ReLU单元，其输出可以变化$l$。这表明可以通过固定每一层内求和输入的均值和方差来减少“covariate shift”问题。因此，我们计算与以下相同层中所有隐藏单元的层归一化统计量：<br>$$\\mu^{l}=\\frac{1}{H}\\sum_{i=1}^{H}a_{i}^{l}$$ $$\\sigma^{l}=\\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_{i}^{l}-\\mu^l)^2}$$<br>其中H表示层中隐藏单元的数量，和batch Normalization不同的是，这是在层归一化下，层中所有隐藏单元共享相同的归一化项$μ$和$σ$，但是不同的训练案例具有不同的归一化项。</p>\n<p> 与批归一化不同，层归一化对小批处理的大小没有任何限制，它可以在批大小为1的在线方式中使用。</p>\n<h2 id=\"Layer-normalized-recurrent-neural-networks\"><a href=\"#Layer-normalized-recurrent-neural-networks\" class=\"headerlink\" title=\"Layer normalized recurrent neural networks\"></a>Layer normalized recurrent neural networks</h2><p>在NLP任务中，对于不同的训练案例，通常有不同的句子长度。这在RNN中很容易处理，因为每个时间步使用相同的权重。但是，当我们以明显的方式将批归一化应用于RNN时，我们需要为序列中的每个时间步计算并存储单独的统计信息。如果测试序列比任何训练序列都要长，这是有问题的。层归一化不存在此类问题，因为其归一化项仅取决于当前时间步对层的求和输入。 在所有时间步中，它也只有一组增益和偏置参数共享。</p>\n<p>在标准RNN中，根据当前输入$x^t$和隐藏状态$h^{t-1}$的计算递归层中的总输入，计算得出$a^t=W_{hh}h^{t-1}+W_{xh}x^t$<br>$$h^t=f[\\frac{g}{\\sigma^{t}}\\bigodot (a^t-\\mu^t)+b]$$ $$\\mu^t=\\frac{1}{H}\\sum_{i=1}^{H}a_{i}^{t}$$  $$\\sigma^t=\\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_{i}^{t}-\\mu^t)^2}$$</p>\n<p>$W_{hh}$是递归隐藏层的隐藏权重，$W_{xh}$是自底向上的输入的隐藏权重，$\\bigodot$是两向量之间逐元素点积，$b$和$g$定义为与$h^t$维度相同的偏置和增益参数。</p>\n<p>在标准RNN中，递归单元的总输入的平均幅度影响了每个时间步长增长或收缩，从而导致梯度爆炸或消失。在层归一化的RNN中，归一化项使将所有求和的输入重新缩放为层不变，这将导致更稳定的隐藏到隐藏的变化。</p>\n<h1 id=\"Related-work\"><a href=\"#Related-work\" class=\"headerlink\" title=\"Related work\"></a>Related work</h1><p>在权重归一化中，使用输入权重的L2范数代替方差来归一化对神经的求和输入，使用预期统计量应用权重归一化或批归一化等效于对原始前馈神经网络进行不同的参数化，但是，我们提出的层归一化方法不是对原始神经网络的重新参数化。因此，层归一化模型具有与其他方法不同的不变性，我们将在以下部分中研究该方法。</p>\n<h1 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h1><ul>\n<li>权重不变和数据转换</li>\n<li>训练期间参数的几何空间：paper证明归一化标量$σ$可以隐式降低学习率并使学习更稳定</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200924224142904.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Experimental-results\"><a href=\"#Experimental-results\" class=\"headerlink\" title=\"Experimental results\"></a>Experimental results</h1><p><img src=\"https://img-blog.csdnimg.cn/20200924224048429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224237130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224246129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224300892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们介绍了层归一化以加快神经网络的训练。我们提供了理论分析，将层归一化与批归一化和权重归一化的不变性进行了比较。我们显示了层归一化对于每个训练案例特征平移和缩放都是不变的。根据经验，我们证明了递归神经网络从拟议的方法中受益最大，特别是对于长序列和小型小批处理。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Paper","Layer-Normalization","归一化","白化"]},{"title":"论文阅读笔记：Latent-Intention-Dialogue-Models","url":"/Paper-Reading/f52747614156/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Latent Intention Dialogue Models<br>原文链接：<a href=\"https://arxiv.org/pdf/1705.10229.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>开发能够做出自主决策并通过自然语言进行交流的对话代理是机器学习研究的长期目标之一。传统方法要么依靠手工制作一个小的状态动作集来应用不可扩展的强化学习，要么构建确定性模型来学习无法捕获自然对话可变性的对话语句。论文提出了一种隐意图对话模型（Latent Intention Dialogue Model, LIDM），通过离散的隐变量来学习对话意图，这些隐变量可以看作引导对话生成的动作决策，进而运用强化学习可以提升性能。实际上在任务型对话中，这个隐含的意图可以理解为是action。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>本文的贡献有两个方面：首先，我们表明神经变分推理框架能够从数据中发现离散的，可解释的意图，从而形成对话主体的决策基础。其次，代理能够基于相同框架内的外部奖励来修改其对话策略，这很重要，因为它为构建自主对话代理程序提供了垫脚石，该对话代理程序可以通过与用户交互来不断提高自身水平。实验结果证明了我们的潜在意图模型的有效性，该模型在基于语料库的自动评估和人工评估方面均达到了最新水平。</p>\n<h1 id=\"Latent-Intention-Dialogue-Model-for-Goal-oriented-Dialogue\"><a href=\"#Latent-Intention-Dialogue-Model-for-Goal-oriented-Dialogue\" class=\"headerlink\" title=\"Latent Intention Dialogue Model for Goal-oriented Dialogue\"></a>Latent Intention Dialogue Model for Goal-oriented Dialogue</h1><blockquote>\n<p>Knowledge graph（KG）和Knowledge base（KB）几乎可以看做同义词，只不过Knowledge base是知识库，而Knowledge graph则是基于知识库的图结构。</p>\n</blockquote>\n<p>LIDM基于(Wen et al,2017，这篇文章也有<a href=\"https://dengbocong.blog.csdn.net/article/details/108912048\">笔记</a>)中描述的端到端系统架构。目标导向型对话通过人机对话交互，帮助用户完成特定的任务。给定一个用户输入 $u_t$ 和知识库(KB)，模型需要将输入解析为可执行的命令Q在知识库搜索。基于返回的搜索结果，模型需要以自然语言的形式给出回复 ，整体模型结构如下<br><img src=\"https://img-blog.csdnimg.cn/2020100809581517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>将分为三个模块：Representation Construction，Policy Network和Generator。</p>\n<h2 id=\"Representation-Construction\"><a href=\"#Representation-Construction\" class=\"headerlink\" title=\"Representation Construction\"></a>Representation Construction</h2><p>表征构建部分主要是为了捕捉用户的意图和获取知识库查询结果。$S_t$ 是对话状态向量，$u_t$ 是用户输入经过BiLSTM的最后隐层状态；belief vector 是特定槽-值队的概率分布拼接，通过预训练的RNN-CNN belief tracker抽取。$m_{t-1}$ 是上一轮的机器回复，$b_{t-1}$ 是上一轮的belief vector。基于belief vector，查询Q与知识库交互，并返回表示匹配程度的one-hot vector $x_t$。</p>\n<ul>\n<li>将utterance通过BiLSTM取两个方向最后一个时刻的隐层状态concat后得到表征 $u$<br>$$u_t=biLSTM_\\Theta(u_t)$$</li>\n<li>分别对utterance句子 $u_t$ 和系统上一时刻的回复 $m_{t-1}$ 先用CNN，尔后通过一个RNN做轮与轮之间的belief tracker<br>$$b_t=RNN-CNN(u_t,m_{t-1},b_{t-1})$$</li>\n<li>通过当前轮得到的 $b$ 生成一个查询语句Q(<strong>基于belief向量，通过取每个槽位的最大值的并集来形成查询Q</strong>)，查询KB后能够得到一个结果表征 $x$（<strong>表示KB中的匹配程度</strong>）</li>\n<li>将$u,b,x$进行concatenation后得到state表征$s$<br>$$s_t=u_t⊕b_t⊕x_t$$</li>\n</ul>\n<h2 id=\"Policy-Network-and-Generator\"><a href=\"#Policy-Network-and-Generator\" class=\"headerlink\" title=\"Policy Network and Generator\"></a>Policy Network and Generator</h2><p>基于之前构建的对话状态，策略网络通过一个MLP输出latent intention（实际上就是一个action） $z_t$。<br>$$\\pi_\\Theta (z_t|s_t)=softmax(W_2^T\\cdot tanh(W_1^Ts_t+b_1)+b_2)$$  $$z_t^{(n)}\\sim \\pi_\\Theta(z_t|s_t)$$<br>和以往的隐变量是一个多维值连续的向量不同，这里的 $z_t$ 是离散的多个类别，类别数是人工设定的，$\\pi_t$ 输出的是 $z_t$ 的softmax概率。从强化学习的角度来看，一个隐意图 $z_t^{(n)}$ 可以看作是一个动作。从策略网络采样得到一个隐意图 $z_t^{(n)}$ 加上之前的状态向量$S_t$，计算出控制向量 $d_t$，作为条件控制LSTM 语言模型生成回复。<br>$$d_t=W_4^Tz_t⊕[sigmoid(W_3^Tz_t+b_3)\\cdot W_5^Ts_t]$$  $$p_\\Theta(m_t|s_t,z_t)=\\coprod_{j}p(w_{j+1}^t|w_j^t,h_{j-1}^t,d_t)$$<br>上面两式子中的 $z_t$ 是 $z_t^{(n)}$的one-hot形式，$w_j^t$是生成的第 $t$ 轮对话的最后一个词，$h_{j-1}^t$ 论文说是解码器的最后一个隐层状态（下标是 $j-1$有点奇怪）。从而，LIDM模型的公式化表示就是：<br>$$p_\\Theta(m_t|s_t)=\\sum_{z_t}p_\\Theta(m_z|z_t,s_t)\\pi_\\Theta(z_t|s_t)$$</p>\n<h2 id=\"Inference\"><a href=\"#Inference\" class=\"headerlink\" title=\"Inference\"></a>Inference</h2><p>LIDM模型的公式化式很难直接求解，因为有隐变量的存在，套路是将其转化为优化下界。变分推断部分，LIDM通过识别网络 $q_{\\phi}(z_t|s_t,m_t)$ 来逼近隐变量的后验分布 $p(z_t|s_t,m_t)$, 训练时优化变分下界。<br>$$L =\\mathbb{E}_{q\\phi (z_t)}[logp_\\Theta (m_t|z_t,s_t)]-\\lambda D_{KL}(q_\\phi (z_t)||\\pi_\\Theta (z_t|s_t))$$  $$\\leq log\\sum_{z_t}p_\\Theta (m_t|z_t,s_t)\\pi_\\Theta (z_t|s_t)$$  $$=logp_\\Theta (m_t|s_t)$$<br>其中识别网络 $q_{\\phi}(z_t|s_t,m_t)$<br>$$q_{\\phi}(z_t|s_t,m_t)=Multi(o_t)=softmax(W_6o_t)$$   $$o_t=MLP_\\phi(b_t,x_t,u_t,m_t)$$   $$u_t=biLSTM_\\phi(u_t),m_t=biLSTM_\\phi(m_t)$$<br>而值得注意的是，目标函数中包含三个不同的模型，因此各自的参数也各不相同，需要分开进行优化</p>\n<ul>\n<li>对于生成回复模型（即p(m|z,s)部分）的参数优化<br><img src=\"https://img-blog.csdnimg.cn/20201008121203569.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>对于policy network的参数优化<br><img src=\"https://img-blog.csdnimg.cn/20201008121241452.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>对于inference network的参数优化（这里我觉得文中定义的r(m,z,s)似乎少了一项）<br><img src=\"https://img-blog.csdnimg.cn/20201008121319185.png#pic_center\" alt=\"在这里插入图片描述\"><h2 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h2>变分推断的隐变量 $z$ 是从一个分布采样得到，存在high variance的缺点，并且容易陷入LSTM语言模型的disconnection 现象中。论文通过半监督学习提升训练的稳定性和阻止disconnction。针对这些问题，论文提出用Semi-Supervision和Reinforcement learning缓解这一问题。<h3 id=\"Semi-Supervision\"><a href=\"#Semi-Supervision\" class=\"headerlink\" title=\"Semi-Supervision\"></a>Semi-Supervision</h3>论文将隐意图的推断看作是无监督的聚类任务，因此运用标准的聚类算法处理一部分语料，并生成标签 $\\hat{z}_t$，并把它当作一个观测变量。训练的数据分成两部分：未标注的样本集 $(m_t,s_t)\\in\\mathbb{U}$ 和聚类生成标注样本集 $(m_t,s_t,\\hat{z})\\in\\mathbb{L}$ ，分别用不同的目标函数：</li>\n<li>将未能够聚类的样例U，继续利用上面的方法进行训练；<br>$$L_1 =\\sum_{(m_t,s_t)\\in U}\\mathbb{E}_{q\\phi (z_t|s_t,m_t)}[logp_\\Theta (m_t|z_t,s_t)]-\\lambda D_{KL}(q_\\phi (z_t|s_t,m_t)||\\pi_\\Theta (z_t|s_t))$$</li>\n<li>将能够聚类得到了标签的样例则利用下式进行优化<br>$$L_2 =\\sum_{(m_t,\\hat{z}_t,s_t)\\in L}log[p_\\Theta(m_t|\\hat{z}_t,s_t)\\pi_\\Theta(\\hat{z}_t|s_t)q_\\phi (\\hat{z}_t|s_t,m_t)]$$</li>\n<li>二者形成一个joint objective，$\\alpha$是监督样本和无监督样本之间的权衡因子。<br>$$L=\\alpha L_1+L_2$$</li>\n</ul>\n<h3 id=\"Reinforcement-Learning\"><a href=\"#Reinforcement-Learning\" class=\"headerlink\" title=\"Reinforcement Learning\"></a>Reinforcement Learning</h3><p>策略网络 $\\pi_\\Theta(z_t|s_t)$ 是从潜在的数据分布学习到的，但不一定是最优的决策。论文提出用强化学习对策略网络进行微调参数，对未标注的数据集 $\\mathbb{U}$ 训练过程如下： 从策略网络采样得到动作 $z_t^{(n)}$，得到反馈 $r_t^{(n)}$，进行梯度更新：<br>$$\\frac{\\partial \\jmath}{\\partial \\theta }\\approx \\frac{1}{N}\\sum_{n}r_t^{(n)}\\frac{\\partial log\\pi_\\theta(z_t^{(n)}|s_t)}{\\partial \\theta’}$$<br><img src=\"https://img-blog.csdnimg.cn/20201008155355324.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验用的数据集是<strong>CamRest676 corpus</strong>，是一个预订餐馆的对话语料。用户可以用三个信息槽（food, pricerange, area）来限制搜索的范围，一旦完成预订，系统可以返回另外三个信息（address, phone, postcode）。语料共有676个对话，将近2750轮。对话隐意图的数量I设置为50，70和100。实验结果用task success rate和BLEU作为指标。<br><img src=\"https://img-blog.csdnimg.cn/20201008155700274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Ground Truth: human-authored response</li>\n<li>NDM：the vanilla neural dialogue model</li>\n<li>NDM+Att: NDM plus an attention mechanism on the belief tracker</li>\n<li>NDM+Att+SS: the attentive NDM with self-supervised sub-task neuron</li>\n</ul>\n<p>可以看到，LIDM模型在BLEU指标上有良好的结果，但在任务成功率上表现不佳。将LIDM训练的策略网络作为initial policy，经过Reinforcement Learning微调参数后，任务成功率得到大幅度的提升，并且BLEU指标没有下降太多。</p>\n<p>论文还在Succss、Compression、Naturalness三个指标上进行人工打分，得到的结果如下：<br><img src=\"https://img-blog.csdnimg.cn/202010081559391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Comprehension和Naturalness的最高分是5分。可以看到，其实三个系统的任务成功率其实相差不是很大，不过在Comprehension和Naturalness上LIDM均超过了NDM。</p>\n<p>从下面的示例对话可以感受下隐意图变量控制生成的回复。不同的隐意图会生成不同的回复，但也可以看到一些隐意图（如intention 0）可以生成风格非常不同的回复，作者认为这是变分推断的variance性质造成的。</p>\n<p>另外一个有趣的现象是，经过RL训练后的LIDM，会采取更“贪婪”的追求任务成功的行为，具体表现在Table 5，一旦在数据库搜索到相关结果，会抢先用户提问之前将该餐馆的所有信息都返回给用户，这也造成对话的轮数更简短。这也说明增强学习训练效果的一个直观体现。<br><img src=\"https://img-blog.csdnimg.cn/20201008160057779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201008160114977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了一个通过离散潜在变量模型学习对话意图的框架，并介绍了用于目标导向对话模型的潜在意图对话模型（LIDM）。我们已经表明，LIDM可以从基础数据分布中发现有效的初始策略，并且能够使用强化学习基于外部奖励来修改其策略。我们认为，这是构建自主对话代理的有希望的一步，因为学习到的离散潜在变量接口使代理能够使用几种不同的范例进行学习。实验表明，所提出的LIDM能够与人类受试者进行交流，并且优于以前发表的结果。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","对话系统","Paper","Latent-Intention"]},{"title":"论文阅读笔记：MultiWOZ-2.2","url":"/Paper-Reading/9f6b46ae549c/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines<br>原文链接：<a href=\"https://arxiv.org/pdf/2007.12720.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>最近在搜集一些对话数据集，众所周知，生成对话数据集是一件费钱又费时的工作，所以一般只有大机构才能做出高质量且庞大的数据集，所以看到好的数据集，那还不赶紧收藏一波。</p>\n<ul>\n<li><a href=\"https://github.com/budzianowski/multiwoz\">代码链接</a></li>\n<li><a href=\"http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\">数据集链接</a></li>\n<li><a href=\"https://arxiv.org/pdf/1810.00278.pdf\">MultiWOZ论文地址</a></li>\n<li><a href=\"https://arxiv.org/pdf/1907.01669.pdf\">MultiWOZ 2.1论文地址</a></li>\n</ul>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>MultiWOZ是一个著名的面向任务的对话数据集，其中包含10,000多个跨越8个域的带注释对话，而被广泛用作对话状态跟踪的基准。但是，最近的工作报告说，对话状态注释中存在大量噪音。MultiWOZ 2.1中识别并修复了许多错误的注释和用户话语，从而改进了该数据集的版本。本篇论文工作介绍了MultiWOZ 2.2，它是该数据集的又一个改进版本。首先，我们在MultiWOZ 2.1之上的17.3％话语中识别并修复对话状态注释错误。其次，我们通过不允许带有大量可能值的槽（例如，餐厅名称，预订时间）来重新定义数据集。此外，我们为这些插槽引入了插槽跨域注释，以在最近的模型中将它们标准化，该模型以前使用自定义字符串匹配启发法来生成它们。我们还会在更正后的数据集上对一些最新的对话状态跟踪模型进行基准测试，以方便将来进行比较。最后，我们讨论了对话数据收集的最佳做法，可以帮助避免注释错误。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>最近，数据驱动技术已针对不同的对话系统模块实现了最先进的性能，但是，由于训练上述模块需要广泛的注释，因此收集高质量的注释对话数据集仍然是研究人员的一个挑战。很多公共数据集，如DSTC2、WOZ、SimulatedDialogue、MultiWOZ、TaskMaster、SGD等等，对促进这一领域的研究非常有用。在这些数据集中，MultiWOZ是用于对话状态跟踪的最广泛使用的基准。它包含超过10,000个对话，涉及8个领域，分别是：餐厅，酒店，景点，出租车，火车，医院，公共汽车和警察。MultiWOZ 2.2做出了如下三点贡献：</p>\n<ul>\n<li>我们修复MultiWOZ 2.1中的注释错误，不一致和本体问题，并发布其改进版本。</li>\n<li>我们为用户和系统话语添加了插槽跨度注释，以在未来的模型中对其进行标准化。我们还为每个用户的语句注解活跃用户的意图和请求槽。</li>\n<li>我们在更正后的数据集中对一些最新的对话状态跟踪模型进行基准测试，以便于将来的工作进行比较。</li>\n</ul>\n<h1 id=\"Annotation-Errors\"><a href=\"#Annotation-Errors\" class=\"headerlink\" title=\"Annotation Errors\"></a>Annotation Errors</h1><p>Wizard-of-Oz（人机交互）：在此设置中，两个人群工作人员配对在一起，一个充当用户，另一个充当对话代理。每个对话由一组指定用户目标的唯一指令驱动，这些指令与扮演用户角色的人群共享。在每个用户转过身后，扮演对话代理（向导）角色的人群工人会注释更新后的对话状态。更新状态后，该工具会显示将对话状态与向导匹配的一组实体，然后向导使用该实体来生成响应并将其发送给用户。</p>\n<p>以下两类主要错误，但在MultiWOZ 2.1中未进行纠正：</p>\n<ul>\n<li>Hallucinated Values：幻觉值以对话状态存在，而未在对话历史记录中指定，我们观察到了四种不同类型的此类错误，它们在下图中显示并在下面进行了描述。<ul>\n<li>过早的加入，代理在以后的讲话中已经提到了这些值。</li>\n<li>对话中甚至在未来的讲话中都根本没有提到这些值。</li>\n<li>由于印刷错误，在对话历史记录中找不到这些值。</li>\n<li>隐式时间处理，这特别涉及以时间为值的插槽</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927233324153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Inconsistent State Updates：我们还遇到了MultiWOZ 2.1中的注释，这些注释在语义上是正确的，但是没有遵循一致的注释准则。对话状态中出现不一致的原因有三个：<ul>\n<li>多种来源，可以通过各种来源在对话状态中引入插槽值。</li>\n<li>值的歧义，比如“18:00”和“6 pm”</li>\n<li>跟踪策略不一致</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927234238301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Ontology-Issues\"><a href=\"#Ontology-Issues\" class=\"headerlink\" title=\"Ontology Issues\"></a>Ontology Issues</h1><p>为了解决不完整的问题，MultiWOZ 2.1通过列出整个数据集中对话状态中存在的所有值来重建了本体，但是仍然存在一些未解决的问题。首先，对于某些插槽，列出了共享相同语义的多个值。其次，我们观察到本体中有多个插槽值，这些值无法与数据库中的任何实体相关联。</p>\n<h1 id=\"Correction-Procedure\"><a href=\"#Correction-Procedure\" class=\"headerlink\" title=\"Correction Procedure\"></a>Correction Procedure</h1><p>为了避免上述问题，我们主张在数据收集之前定义本体，这不仅可以作为注释者的指南，而且还可以防止数据集中的注释不一致以及印刷和注释错误导致的本体破坏。本节描述了我们对新本体的定义，我们将其称为策略，然后是对状态和动作注释的更正。最后，我们还显示了修改的统计信息。</p>\n<ul>\n<li>策略定义：策略将不同的插槽分为两类-非分类和分类。具有大量可能值的动态插槽被归为“非分类”，与本体不同，架构不提供此类插槽的预定义值列表，他们的值是从对话历史中提取的。相反有限的插槽被归为“分类”，与本体类似，策略列出了此类插槽的所有可能值。此外，在注释期间，必须从模式中定义的预定义候选列表中选择对话状态和用户或系统操作中这些插槽的值，这有助于实现注释的完整性和一致性。如下图：</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927235925309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>分类插槽：分类插槽所有可能值的列表是从MultiWOZ 2.1随附的相应数据库中构建的。</li>\n<li>非分类插槽：从对话历史记录中提取非分类插槽的值，我们使用一种自定义的字符串匹配方法，该方法考虑了可能的拼写错误和替代表达式来定位语义上与注释相似的所有值。如果有多个匹配项，我们选择最近提及的值并注释其跨度。下图是2.1和2.2的区别</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200928000339675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>用户和系统动作：用户和系统动作注释提供了相应话语的语义表示。为了保持对话行为和domain之间的联系，这里用到了本文二作之前的一个做法，将同一domain下的对话对话行为组合起来放到了frames里。</li>\n<li>数据：<br><img src=\"https://img-blog.csdnimg.cn/20200928145728211.png#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"Additional-annotations\"><a href=\"#Additional-annotations\" class=\"headerlink\" title=\"Additional annotations\"></a>Additional annotations</h1>除了跨域注释外，我们还为每个用户回合添加了活动的用户意图和请求槽。预测活动用户的意图和请求槽是两个新的子任务，可用于评估模型性能并促进对话状态跟踪。主动意图或API的预测对于支持数百个API的大规模对话系统的效率也至关重要。</li>\n<li>Active：active intent指定了用户话语中表达的所有意图。</li>\n<li>Requested Slots：Requested slots指定了用户想系统请求的槽位，也就是所谓的提问槽。</li>\n</ul>\n<h1 id=\"Dialogue-State-Tracking-Benchmarks\"><a href=\"#Dialogue-State-Tracking-Benchmarks\" class=\"headerlink\" title=\"Dialogue State Tracking Benchmarks\"></a>Dialogue State Tracking Benchmarks</h1><p>最新数据驱动对话系统状态跟踪模型主要采用两种方法：span-based和candidate-based，本文baseline包括：SGDbaseline、TRADE、DS-DST。</p>\n<ul>\n<li><p>各个模型在这三个数据及上的joint acc<br><img src=\"https://img-blog.csdnimg.cn/20200928150723774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p>在categorical和non-categorical两类slot上单独计算joint acc<br><img src=\"https://img-blog.csdnimg.cn/2020092815080387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h1><p>Wizard-of-Oz范式虽然是个强有力的收集自然语言对话的技术，但是他也存在很大的噪音。作者在这一节就提出了几点，来最小化标注错误。</p>\n</li>\n<li><p>在标注数据前，应该先定义一个本体或者schema。对于categorical的槽位，这个schema需要定义好明确的slot，每个slot都有一个固定的可能的value的集合。然后标注接口，需要强制性的保证这些槽值的正确性。对于non-categorical的槽位，标注接口要限制标注的value必须是出现在对话历史中的值。</p>\n</li>\n<li><p>在标注任务之后，可以进行简单的检查，以识别错误的标注。</p>\n</li>\n</ul>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>MultiWOZ 2.1是MultiWOZ 2.0数据集的改进版本，被广泛用作对话状态跟踪的基准。我们找出在MultiWOZ 2.1中未解决的注释错误，不一致和与本体相关的问题，并发布更正的版本– MultiWOZ 2.2。我们添加了新的策略，标准化的插槽值，更正的注释错误和标准化的跨域注释。此外，我们为每个用户回合注释了活动意图和请求槽位，除了修复现有操作外，并添加了丢失的用户和系统操作。我们在新数据集上对一些最新模型进行了基准测试：实验结果表明，MultiWOZ 2.1和MultiWOZ 2.2的模型性能相似。我们希望清理后的数据集有助于在模型之间进行更公平的比较，并促进该领域的研究。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","MultiWOZ"]},{"title":"论文阅读笔记：Neural-Belief-Tracker--数据驱动的对话状态跟踪","url":"/Paper-Reading/e6ce0f001fde/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Neural Belief Tracker: Data-Driven Dialogue State Tracking<br>原文链接：<a href=\"https://arxiv.org/pdf/1606.03777.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>belief tracker是现代口语对话系统的核心组件之一，它可以在对话的每个步骤中估算用户的目标，但是，大多数当前方法很难扩展到更大，更复杂的对话域。这是由于它们对以下方面的依赖：a）需要大量带注释的训练数据的口语理解模型； 或b）手工制作的词典，用于捕获用户语言的某些语言变化。我们提出了一种新颖的Neural Belief Tracking (NBT) 框架，该框架通过基于表示学习的最新进展来克服这些问题。NBT通过推理对预先训练的单词向量进行建模，学习将其组合为用户话语和对话上下文的分布表示形式。我们对两个数据集的评估表明，该方法超越了过去的局限性，与依赖于手工制作的语义词典的最新模型的性能相匹配，并且在不提供此类词典的情况下其性能优于后者。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>下图中的示例显示了三轮对话中每个用户语句后的真实状态，从该示例可以看出，DST模型依赖于标识用户话语中的本体。<br><img src=\"https://img-blog.csdnimg.cn/20201006114135584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图给出了一个针对三个槽值对的字典的示例（传统的做法是建语义词典），我们称其为<strong>非词化</strong>(delexicalisation)的这种方法显然无法扩展到更大，更复杂的对话域。如意大利语和德语这种词汇和形态丰富的语言。<br><img src=\"https://img-blog.csdnimg.cn/20201006114443735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在本文中，我们介绍了两个新模型，统称为 Neural Belief Tracker (NBT)系列，这些模型将SLU和DST结合在一起，可以有效地学习处理变化，而无需任何手工资源。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><ul>\n<li>Separate SLU</li>\n<li>Joint SLU/DST</li>\n</ul>\n<p>本文提出的工作的主要动机是克服影响以前的信念跟踪模型的限制。NBT模型通过以下方式有效地从可用数据中学习</p>\n<ul>\n<li>利用预训练词向量中的语义信息来解决词汇/形态上的歧义</li>\n<li>最大化本体值之间共享的参数数量</li>\n<li>具有学习领域特定释义和其他变体的灵活性，这使得依靠精确匹配和去词缀化作为一种可靠的策略是不可行的</li>\n</ul>\n<h1 id=\"Neural-Belief-Tracker\"><a href=\"#Neural-Belief-Tracker\" class=\"headerlink\" title=\"Neural Belief Tracker\"></a>Neural Belief Tracker</h1><p>下图展示了该模式下的信息流<br><img src=\"https://img-blog.csdnimg.cn/20201006152514188.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>给定三个模型输入，NBT层次结构的第一层执行表示学习，从而为用户话语生成矢量表示$(r)$，当前的候选插槽值对表示$(c)$，系统对话动作表示为$(t_q, t_s, t_v)$。随后，学习到的向量表示通过上下文建模和语义解码子模块进行交互，以获得中间交互向量$d_r,d_c,d$，这些用作最终决策模块的输入，该模块决定用户是否表达了由候选插槽值对表示的意图。</p>\n<p>目的是根据用户的输入（User Utterance，由ASR得到的结果，当然也可以直接是用户的文本输入）和系统上一轮的回复（System Output），遍历Domain Ontology（说白了就是某个领域内slot-value对可能取值）中每一个(slot,value)对，以判断用户真实意图中包含该slot-value对的概率大小。例如上图中的Domain Ontology存在三个可能的slot-value对，分别是(food, Indian), (food, Persian), (food, Czech)。而本论文的目的便是需要分别遍历这三个可能取值，假设当前遍历到了(food, Persian)这个取值，通过表征模型可以得到它的表征c，再通过图中所示的流程，最后可以得到一个结果y，这个结果便表明了(food, Persian)这个slot-value对属于用户真实意图的可能性大小。</p>\n<h3 id=\"Representation-Learning\"><a href=\"#Representation-Learning\" class=\"headerlink\" title=\"Representation Learning\"></a>Representation Learning</h3><p>这里分别使用了两个模型来得到文本的表征：NBT-DNN和NBT-CNN，所有的表征学习都是建立在词向量上，论文说用专注于语义的词向量，效果会比普通的词向量好，可以看作是《同义词林》的“词向量版”。</p>\n<p>模型的输入包括系统的前一个对话动作，用户的输入 $u$ 和一个候选的slot-value对。输入 $u$ 的词向量分别是 $u_1,…,u_k$。 $V_i^n$ 是n个词向量的拼接。<br>$$V_i^n=u_i⊕…⊕u_{i+n-1}$$</p>\n<p>先看看NBT-DNN，结构如下图所示。计算累积n-gram特征向量 $r_1, r_2, r_3$，分别对应unigrams(1-gram)，bigrams(2-gram)和trigrams(3-gram)；再经过全连接层和非线性映射得到 $r_n^{‘}$，s代表不同的slot；最后求和得到用户输入的一个表征向量 $r$。<img src=\"https://img-blog.csdnimg.cn/20201006162453533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>$$r_n=\\sum_{i=1}^{k_u-n+1}V_i^n$$  $$r_n^{‘}=\\sigma (W_n^sr_n+b_n^s)$$ $$r=r_1^{‘}+r_2^{‘}+r_3^{‘}$$</p>\n<p>实际上，模型应该能学到哪些utterance是更重要的，如更侧重于形容词、名词的检测。因此，论文利用了NLU上得到成功应用的CNN架构实现第二个版本NBT-CNN。CNN结构也很熟悉，词向量的输入，过卷积层，抽n-gram特征，然后是非线性激活函数，max-pooling，求和。$F_n^s \\in R^{L\\times nD}$代表卷积过滤器，$m_n=[V_1^n;V_2^n,…;V_{k-n+1}^n]$ 是n-grams的各个拼接词向量。<br>$$R_n=F_n^sm_n$$  $$r_n^{‘}=maxpool(ReLU(R_n+b_n^s))$$</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201006223101275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实际上就是一个简单的CNN模型，分别取了filter-size为1,2,3这三种，output size都是L=300。</p>\n<h3 id=\"Semantic-Decoding\"><a href=\"#Semantic-Decoding\" class=\"headerlink\" title=\"Semantic Decoding\"></a>Semantic Decoding</h3><p>这个模块对表征 $r$ 检测是否包含候选slot-value对 $c$，处理方法也比较简单。$(c_s, c_v)$ 分别是slot和value的词向量表示，投影映射成与 $r$ 相同维度的向量，点积求相似度 $d$。<br>$$c=\\sigma(W_c^s(c_s+c_v)+b_c^s)$$   $$d=r⊕c$$<br>这个模块主要是计算slot-value对和用户句子的关系，简单而言的话，slot的词向量（如果有多个词则简单相加）和value的词向量（如果有多个词则简单相加），通过一个全连接层和非线性映射后得到表征c（该表征将slot和value的信息融合成一个向量），与句子表征r进行element-wise的乘积，得到d（依然是一个向量）。</p>\n<h3 id=\"Context-Modelling\"><a href=\"#Context-Modelling\" class=\"headerlink\" title=\"Context Modelling\"></a>Context Modelling</h3><p>当用户询问时，仅从当前用户的输入还不足以抽取意图，<em>belief tracker</em>应该考虑对话的上下文，特别是上一句系统的动作。论文提出了两种动作：<em>System Request</em>和<em>System Confirm</em>。</p>\n<ul>\n<li>系统请求（System Request）：系统上一轮在向用户请求一个具体的信息，比如”what price range would you like?”，此时用户需要给出一个具体的信息，此时用t(q)表示”price range”这个slot；</li>\n<li>系统确认（System Confirm）：系统上一轮在让用户在确认一个具体的信息，比如”‘how about Turkish food?’”，此时用户一般只需要回答是与不是即可，此时用(t(s),t(v))表示（food, Turkish）这个slot-value对。</li>\n</ul>\n<p>第一种情景是，系统对一个特定的slot发出提问，用户一般会给出具体的value。第二种是系统询问用户，某个slot-value是否正确，用户一般只会回答对或错。这两个场景应分别计算。$t_q$ 是request的参数，$(t_s, t_v)$是confirm的参数。$t_q,t_s,t_v$ 都是slot/value的词向量，多个词时直接求和得到。通过系统动作，候选对 $(c_s,c_v)$ 作为一个门，控制输入表征$r$的信息输出（般情况下系统要么是请求，要么是确认，那么此时t(q)为0向量或者(t(s),t(v))是零向量。）：<br>$$m_r=(c_s \\cdot t_q)r$$  $$m_c=(c_s \\cdot t_s)(c_v \\cdot t_v)r$$</p>\n<p>该机制有点类似于将候选槽值与系统请求某个槽的信息或确认某个槽值对，计算一个相似度（上面公式都是点乘），然后通过这个相似度对用户的句子表征进行一个类似于门的控制（主要是scale作用）。</p>\n<p>Binary Decision Maker：最后的二分类决策层。$\\phi_{dim}(x)=\\sigma(W_x+b)$ 将输入 $x$ 映射到维度为size的向量，softmax二分类，完成slot-value对的存在检测：<br>$$y=\\phi_2(\\phi_{100}(d)+\\phi_{100}(m_r)+\\phi_{100}(m_c))$$</p>\n<h1 id=\"Belief-State-Update-Mechanism\"><a href=\"#Belief-State-Update-Mechanism\" class=\"headerlink\" title=\"Belief State Update Mechanism\"></a>Belief State Update Mechanism</h1><p>论文提出了一种简单的belief state的更新机制：先估计当前轮对话的slot-value，再更新历史记录。在嘈杂的环境中，取ASR输出的前N个最佳结果（N-best list）进行分析。对于第 $t$ 轮对话，$sys^{t-1}$ 表示前一个系统动作，$h^t$ 是ASR输出的结果假设，$h_i^t$ 是N-best list中的第 $i$ 个，$s$ 是slot，$v$ 是value，NBT模型需要估计 $(s,v)$ 在用户的口语输入中的概率：<br>$$\\mathbb{P}(s,v|h^t, sys^{t-1})=\\sum_{i=1}^{N}p_i^t\\mathbb{P}(s,v|h_i^t,sys^t)$$<br>对于当前和历史对话的belief state更新，引入一个权重系数 $\\lambda$：<br>$$\\mathbb{P}(s,v|h^{1:t},sys^{1:t-1})=\\lambda\\mathbb{P}(s,v|h^t,sys^{t-1})+(1-\\lambda)\\mathbb{P}(s,v|h^{1:t-1},sys^{1:t-2})$$<br>然后对于slot $s$检测到的values，取概率最大的作为当前的goal value。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验仍然是task-oriented的对话任务，数据集有两个：DSTC2和WOZ 2.0。<br><img src=\"https://img-blog.csdnimg.cn/20201006232123896.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到，NBT-DNN和NBT-CNN都能超过基于语义词典的模型，当然NBT-CNN多了不同n-grams特征的权重学习，会更好一点。论文还做了不同词向量对结果影响的实验。<br><img src=\"https://img-blog.csdnimg.cn/20201006232145637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看出，专门针对语义任务的词向量Paragram-SL999对实验效果提升明显，这也很显然，先验知识更丰富，对下游的任务当然效果更佳。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了一种新颖的神经信念跟踪（NBT）框架，旨在克服当前在现实世界中的对话域中部署对话系统的障碍。NBT模型提供了将口语理解和对话状态跟踪相结合的已知优势，而无需依赖手工制作的语义词典来实现最新的性能。我们的评估证明了这些好处：NBT模型与使用此类词典的模型的性能相匹配，并且在这些词典不可用时性能大大优于它们。最后，我们证明了NBT模型的性能随着底层单词向量的语义质量而提高。据我们所知，我们第一个超越内在评估并证明语义专业化可以提高下游任务性能。在未来的工作中，我们打算探索NBT在多域对话系统中的应用，以及在英语以外要求复杂形态变化处理的语言中的应用。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","对话系统","对话状态跟踪","Neural-Belief-Tracker"]},{"title":"论文阅读笔记：Recent-Advances-and-Challenges-in-Task...","url":"/Paper-Reading/a78371b14017/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Recent Advances and Challenges in Task-oriented Dialog Systems<br>原文链接：<a href=\"https://arxiv.org/pdf/2003.07490.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>由于在人机交互和自然语言处理中的重要性和价值，面向任务的对话系统在学术界和工业界都受到越来越多的关注。在本文中，我们调查了面向任务的对话系统的最新进展和挑战。我们还讨论了面向任务的对话系统的三个关键主题：（1）提高数据效率以促进在资源匮乏的环境中进行对话建模；（2）为对话策略学习建模多回合模型以实现更好的任务完成性能；（3）将领域本体知识整合到对话模型中。此外，我们回顾了对话评估和一些常用语料库的最新进展。我们认为，尽管这项调查不完整，但可以为面向任务的对话系统的未来研究提供启发。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>通常，面向任务的对话系统是建立在结构化本体之上的，该本体定义了任务的领域知识。有关面向任务的对话系统的现有研究可以大致分为两类：pipeline和end-to-end。建立pipeline系统通常需要大规模的标记对话数据来训练每个组件，模块化的结构使系统比端到端的系统更具解释性和稳定性，因此，大多数现实世界的商业系统都是以这种方式构建的。而端到端的结构像是黑匣子，这更加不可控。如下图所示，对于pipeline和end-to-end方法中的每个单独组件，我们列出了一些关键问题，在这些问题中提出了典型的作品。<br><img src=\"https://img-blog.csdnimg.cn/20200928160614477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在pipeline方法中，最近的研究更多地集中在对话框状态跟踪和对话框策略组件上，这也称为“对话框管理”。基于域本体，通过预测每个槽的值，DST任务可以视为分类任务（受限制与训练数据，OOV问题），对话策略学习任务通常被认为是强化学习任务。然而，与其他众所周知的RL任务不同，对话策略的训练需要真实的人作为环境，这是非常昂贵的。面向任务的对话系统中的三个关键问题：</p>\n<ul>\n<li>数据效率：资源匮乏的问题是主要的挑战之一。</li>\n<li>多回合策略：提出了许多解决方案以解决多轮交互式训练中的这些问题，以更好地进行策略学习，包括基于模型的计划，奖励估计和端到端策略学习。</li>\n<li>本体整合：面向任务的对话系统必须查询知识库（KB）以检索一些实体以生成响应，由于没有显式的状态表示形式，因此这种简化使构造查询变得困难。</li>\n</ul>\n<h1 id=\"Modules-and-Approaches\"><a href=\"#Modules-and-Approaches\" class=\"headerlink\" title=\"Modules and Approaches\"></a>Modules and Approaches</h1><p>有关面向任务的对话系统的现有研究可以大致分为两类：pipeline和end-to-end。在pipeline方法中，该模型通常由几个组件组成，包括自然语言理解（NLU），对话状态跟踪（DST），对话策略和自然语言生成（NLG），如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/2020092820241314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>值得注意的是，尽管NLU-DST-Policy-NLG框架是pipeline系统的典型配置，但还有其他一些配置。有一些研究合并了一些典型的组件，例如单词级DST和单词级策略。在端到端方法中，对话系统在端到端方式，无需指定每个单独的组件。</p>\n<ul>\n<li>NLU：主要是识别对话动作，其由意图和插槽值组成，即由意图识别和槽值提取组成，示例如下。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/2020092820284111.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>DST：对话状态跟踪器通过将整个对话上下文作为输入来估算每个时间步的用户目标。在时间 $t$ 的对话状态可以看作是直到 $t$ 之前的对话回合的抽象表示。</li>\n<li>对话策略：以对话状态为条件，对话策略会产生下一个系统动作。如下图所示，在特定的时间步 $t$ 处，用户在 $a_t$ 处执行操作，收到奖励 $R_t$，状态更新为 $S_t$。<br><img src=\"https://img-blog.csdnimg.cn/20200928204116814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>NLG：该任务将对话用作输入并生成自然语言响应。为了改善用户体验，所产生的话语应该（1）充分传达对话行为的语义以完成任务，并且（2）与人类语言类似，是自然的，特定的，信息丰富的。</li>\n<li>End-to-end方法：面向任务的对话系统的端到端方法受到开放域对话系统研究的启发，如下图。<br><img src=\"https://img-blog.csdnimg.cn/20200928205223851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1>大多数评估研究都遵循PARADISE框架，一种是对话成本，它衡量对话中产生的成本，例如对话回合数。另一个是任务成功，评估系统是否成功解决了用户问题。评估面向任务的对话系统的方法可以大致分为以下三种：</li>\n<li>Automatic Evaluation</li>\n<li>Simulated Evaluation</li>\n<li>Human Evaluation</li>\n</ul>\n<h1 id=\"Corpora\"><a href=\"#Corpora\" class=\"headerlink\" title=\"Corpora\"></a>Corpora</h1><p>收集了具有不同域和注释粒度的大量语料库，以促进对面向任务的对话系统的研究。如下图所示：</p>\n<ul>\n<li>informable slot 一般是由用户告知系统的，用来约束对话的一些条件，系统为了完成任务必须满足这些约束。</li>\n<li>requestable slot 一般是用户向系统咨询的，可以来做选择的一些slot。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200928210217414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h1><ul>\n<li>数据效率：资源匮乏的问题是主要的挑战之一。回顾了为缓解此问题而提出的一些最新方法。我们首先回顾一下迁移学习方法，这些方法可以从大规模数据中获取先验知识，或者从其他任务中采用经过训练的模型。然后，我们介绍了一些无监督的方法，这些方法可以通过启发式规则在资源很少的情况下直接学习而几乎没有注释。此外，我们还回顾了最近在构建数据驱动的用户模拟器方面的工作。</li>\n<li>多回合策略：提出了许多解决方案以解决多轮交互式训练中的这些问题，以更好地进行策略学习，包括基于模型的计划，奖励估计和端到端策略学习。面向任务的对话系统的对话管理的最新研究主要集中在以下主题上：（1）带有带有用于自由槽位的值解码器的DST；（2）进行对话计划以提高策略学习中的样本效率（3）用户目标估计，以预测任务成功和用户满意度。</li>\n<li>本体整合：面向任务的对话系统必须查询知识库（KB）以检索一些实体以生成响应，由于没有显式的状态表示形式，因此这种简化使构造查询变得困难。我们介绍有关（1）对话任务模式集成（2）面向任务的对话模型中的知识库集成的一些最新进展。</li>\n</ul>\n<h1 id=\"Discussion-and-Future-Trends\"><a href=\"#Discussion-and-Future-Trends\" class=\"headerlink\" title=\"Discussion and Future Trends\"></a>Discussion and Future Trends</h1><p>在本文中，我们回顾了面向任务的对话系统的最新进展，并讨论了三个关键主题：数据效率、多回合策略、本体知识整合。最后，我们讨论面向任务的对话系统的一些未来趋势：</p>\n<ul>\n<li>对话系统的预训练方法</li>\n<li>领域适应，跨领域应用</li>\n<li>鲁棒性</li>\n<li>End-to-end模型</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","NLP","对话系统","面向任务","综述"]},{"title":"论文阅读笔记：SMN检索式多轮对话系统","url":"/Paper-Reading/7820bcc68547/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots<br>原文链接：<a href=\"https://arxiv.org/pdf/1612.01627v2.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>本文的SMN模型结构可以说影响了很多后续相关的论文，所解决的是基于检索的聊天机器人中多回合对话的回复选择。在之前的工作，基于检索的聊天机器人的做法是将context里所有的utterances都连接在一起，将这个长长的context做处理然后和response作匹配，这样做可能会丢失语句间的关系或重要的上下文信息。Sequential Matching Network（SMN）模型就是为了解决这些问题而来的。</p>\n<p><strong>SMN首先在多个粒度级别上将上下文中每个utterance都和response做匹配，然后通过卷积和池化操作从每对中提取重要的匹配信息作为向量，接着通过递归神经网络（RNN）按时间顺序累积矢量，该神经网络对utterance之间的关系进行建模，最后使用RNN的隐藏状态计算最终匹配分数。</strong></p>\n<p>构建对话机器人的现有方法中，可以分为 generation-based（生成式）和retrieval-based（检索式），相对于生成式而言，检索式拥有的信息更加丰富，且运行流畅的特点。选择response的关键在于输入response匹配。与单回合对话不同，多回合对话需要在响应和对话上下文之间进行匹配，在该上下文中，不仅需要考虑response和输入信息之间的匹配，而且还需要考虑前一回合中response和utterances之间的匹配。总结而言该领域的任务存在如下的challenges：</p>\n<ul>\n<li>如何根据上下文识别重要信息（单词，短语和句子），这对于选择正确的response并利用相关信息进行匹配至关重要</li>\n<li>如何对上下文中的utterances之间的关系进行建模。</li>\n</ul>\n<p>下图展示了challenges的例子：第二句的“hold a drum class”和第三句的“drum”相关性很强，所以回复是高度依赖于上下文以及语句之间的关系的。<br><img src=\"https://img-blog.csdnimg.cn/20201030203732642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在上面粗体中描述了SMN的结构流程，具体来说，对于每个utterance-response对而言，模型通过word embeddings和带有门控递归单元（GRU）的递归神经网络的隐藏状态来构造word-word相似度矩阵和sequence-sequence相似度矩阵。这两个矩阵分别在word级别和segment（单词子序列）级别上捕获成对的重要匹配信息，并且通过交替对矩阵进行卷积和池化操作，将信息提取和融合为匹配向量。</p>\n<p>通过这种方式，可以在response的充分监督下识别上下文中多个粒度级别的重要信息，并以最小的损失进行匹配，然后将匹配向量喂给另一个GRU，以形成context和response的匹配分数，这个GRU会根据上下文中utterances的时间顺序在其隐藏状态下累积匹配。它以匹配的方式对utterances之间的关系和依存关系进行建模，并以utterances的顺序来监督配对匹配的累积。context和response的匹配程度是由具有GRU隐藏状态的logit模型计算的。</p>\n<h1 id=\"具体结构及实现\"><a href=\"#具体结构及实现\" class=\"headerlink\" title=\"具体结构及实现\"></a>具体结构及实现</h1><h3 id=\"问题符号化\"><a href=\"#问题符号化\" class=\"headerlink\" title=\"问题符号化\"></a>问题符号化</h3><p>数据集表示为 $D={(y_i,s_i,r_i)}<em>{i=1}^N$，其中$s_i={u_{i,1},…,u</em>{i,n_i}}$表示对话上下文，${u_{i,k}}_{k=1}^{n_i}$表示utterances，$r_i$表示response的候选，$y_i\\in{0,1}$表示标签（当$y_i=1$ 时意味着 $r_i$ 是 $s_i$ 合适的回复，否则 $y_i=0$），目标是学习 $D$ 的匹配模型 $g(.,.)$，对于任何context-response pair $(s,r)$，$g(s,r)$ 衡量 $s$ 和 $r$ 之间的匹配程度。</p>\n<h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><p><img src=\"https://img-blog.csdnimg.cn/20201030215924927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>按照上面的模型结构图，SMN首先将context-response分解为几个utterance-response pair匹配，然后通过递归神经网络将所有pairs匹配累积为基于上下文的匹配。模型分为三层：</p>\n<ul>\n<li>在word级别和segment级别将候选response与上下文中的每个utterance匹配，然后通过卷积提取两个层上的重要匹配信息，池化并编码为匹配向量。</li>\n<li>将上一层得到的匹配向量喂入第二层，在该层中，它们按照上下文中utterances的时间顺序在具有GRU的递归神经网络的隐藏状态下进行累积。</li>\n<li>将上一层的隐藏状态用于计算最终匹配分数</li>\n</ul>\n<p><strong>这样的模型结构有以下的优势：</strong></p>\n<ul>\n<li>一个候选response可以在一开始就匹配上下文中的每个utterances，因此可以充分提取每个utterance-response pair中的匹配信息，并以最小的损失将其携带到最终匹配分数</li>\n<li>由于从每个utterance中提取信息是在不同的粒度级别上进行的，并且在response的充分监督下进行，因此可以很好地识别和提取对每个utterance中的response选择有用的语义结构。</li>\n<li>匹配和utterance关系是耦合而不是分开建模的，因此，作为一种knowledge，utterance关系（例如顺序）可以监督匹配分数的形成。</li>\n</ul>\n<p>接下来针对三层的细节展开描述。</p>\n<h3 id=\"Utterance-Response匹配\"><a href=\"#Utterance-Response匹配\" class=\"headerlink\" title=\"Utterance-Response匹配\"></a>Utterance-Response匹配</h3><p>给定上下文 $s$ 中的语句 $u$ 和候选响应 $r$，然后对$u$ 和 $r$ 进行Embedding得到对应的表示 $U=[e_{u,1},…,e_{u,n_u}]$ 和 $R=[e_{r,1},…,e_{r,n_r}]$ ，其中 $e_{u,i},e_{r,i}\\in \\mathbb{R}^d$ 分别是$u$ 和 $r$ 的第 $i$ 个单词的嵌入。然后使用 $U\\in \\mathbb{R}^{d\\times n_u}$ 和 $R\\in \\mathbb{R}^{d\\times n_r}$ 构造word-word相似度矩阵 $M_1\\in \\mathbb{R}^{n_u\\times n_r}$，以及sequence-sequence相似度矩阵 $M_2\\in \\mathbb{R}^{n_u\\times n_r}$，这两个矩阵作为卷积神经网络（CNN）的两个input channels，CNN从矩阵中提取重要的匹配信息，并将该信息编码为匹配向量 $v$。具体来说，$\\forall i,j$，$M_1$ 第 $(i,j)$ 个元素被定义为（公式1）：<br>$$e_{1,i,j}=e_{u,i}^T\\cdot e_{r,j}$$<br>$M_1$ 在单词级别上模拟 $u$ 和 $r$ 之间的匹配。为了构造$M_2$ ，我们首先使用GRU将 $U$ 和 $R$ 转换为隐藏向量。假设 $H_u=[h_{u,1},…,h_{u,n_u}]$ 是 $U$ 的隐藏向量，则 $\\forall i,h_{u,i}\\in \\mathbb{R}^m$ 定义为（公式2）：<br>$$z_i=\\sigma (W_{ze_{u,i}}+U_zh_{u,i-1})$$  $$r_i=\\sigma (W_{re_{u,i}}+U_rh_{u,i-1})$$  $$\\tilde{h}<em>{u,i}=tanh(W</em>{he_{u,i}}+U_{h}(r_i\\odot h_{u,i-1}))$$  $$h_{u,i}=z_i\\odot\\tilde{h}<em>{u,i}+(1-z_i)\\odot h_{u,i-1}$$<br>其中 $h</em>{u,0}=0$，$z_i$ 和 $r_i$ 分别是update gate和reset gate，$\\sigma(\\cdot)$是一个sigmoid函数，$W_z,W_h,W_r,U_z,U_r,U_h$ 都是参数，同样，我们有 $H_r=[h_{r,1},…,h_{r,n_r}]$ 作为 $R$ 的隐藏向量，然后 $\\forall i,j$，$M_2$ 第 $(i,j)$ 个元素被定义为（公式3）：<br>$$e_{2,i,j}=e_{u,i}^TAh_{r,j}$$<br>其中 $A\\in \\mathbb{R}^{m\\times m}$ 是一个线性变换， $\\forall i$，GRU对直到位置 $i$ 的单词之间的顺序关系和依赖关系进行建模，并对text segment进行编码，直到第 $i$ 个单词为隐藏矢量为止，$M_2$ 在segment级别上建模$u$ 和 $r$ 之间的匹配。</p>\n<p>然后，CNN将 $M_1$ 和 $M_2$ 处理为  $v$。$\\forall f=1,2$，CNN视 $M_f$ 为输入通道，交替进行卷积和最大池化操作。假设$\\forall f=1,2$， $z^{(l,f)}=[z_{i,j}^{(l,f)}]<em>{I^{(l,f)}\\times J^{(l,f)}}$ 表示 $l$ 层上类型为 $f$ 的特征图的输出，其中 $z^{(0,f)}=M_f$。在卷积层上，我们使用窗口大小为 $r_w^{(l,f)}\\times r_h^{(l,f)}$ 的2D卷积运算，并将 $z_{i,j}^{(l,f)}$ 定义为（公式4）：<br>$$z</em>{i,j}^{(l,f)}=\\sigma(\\sum_{f^{‘}=0}^{F_{l-1}}\\sum_{s=0}^{r_{w}^{(l,f)}}\\sum_{t=0}^{r_{h}^{(l,f)}}W_{s,t}^{(l,f)}\\cdot z_{i+s,j+t}^{(l-1,f^{‘})}+b^{l,k})$$<br>其中，$\\sigma(\\cdot)$是一个ReLU，$W^{(l,f)}\\in \\mathbb{R}^{r_w^{(l,f)}\\times r_h^{(l,f)}}$ 和 $b^{l,k}$ 是参数，$F_{l-1}$ 是第 $(l-1)$ 层上的特征图的数量，最大池化操作基于卷积操作，可以表示为（公式5）：<br>$$z_{i,j}^{(l,f)}=\\underset{p_w^{(l,f)}&gt;s\\geq0}{max} \\underset{p_h^{(l,f)}&gt;t\\geq0}{max}z_{i+s,j+t}$$<br>其中 $p_w^{(l,f)}$ 和 $p_h^{(l,f)}$ 分别是2D池的宽度和高度，最终特征图的输出被串联并映射到低维空间，并通过线性变换作为匹配向量 $v\\in \\mathbb{R^q}$</p>\n<h3 id=\"Accumulation匹配\"><a href=\"#Accumulation匹配\" class=\"headerlink\" title=\"Accumulation匹配\"></a>Accumulation匹配</h3><p>假设 $[v_1,…,v_n]$ 是第一层的输出（对应 $n$ pairs），在第二层，GRU取 $[v_1,…,v_n]$ 作为输入，并将匹配序列编码为其隐藏状态 $H_m=[h_1^{‘},…,h_n^{‘}]\\in  \\mathbb{R^{q\\times n}}$。 其详细参数设置与公式（2）相似，这个层有两个函数：</p>\n<ul>\n<li>它在上下文中建模utterances的依存关系和时间关系</li>\n<li>它利用时间关系来监督对accumulation的累积，作为基于上下文的匹配</li>\n</ul>\n<h3 id=\"Prediction和Learning匹配\"><a href=\"#Prediction和Learning匹配\" class=\"headerlink\" title=\"Prediction和Learning匹配\"></a>Prediction和Learning匹配</h3><p>存在 $[h_1^{‘},…,h_n^{‘}]$，我们将 $g(s,r)$ 定义为（公式6）：<br>$$g(s,r)=softmax(W_2L[h_1^{‘},…,h_n^{‘}]+b_2)$$<br>其中，$W_2$ 和 $b_2$ 是参数，我们考虑  $L[h_1^{‘},…,h_n^{‘}]$ 的三个参数化：</p>\n<ul>\n<li>仅使用最后一个隐藏状态，  $L[h_1^{‘},…,h_n^{‘}]=h_n^{‘}$。</li>\n<li>隐藏状态线性组合，  $L[h_1^{‘},…,h_n^{‘}]=\\sum_{i=1}^{n}w_ih_i^{‘}$，其中 $w_i\\in  \\mathbb{R}$</li>\n<li>我们遵循并运用注意力机制来组合隐藏状态，则 $L[h_1^{‘},…,h_n^{‘}]$ 被定义为（公式7）：<br>$$t_i=tanh(W_{1,1}h_{u_i,n_u}+W_{1,2}h_i^{‘}+b_1)$$  $$a_i=\\frac{exp(t_i^Tt_s)}{\\sum_i(exp(t_i^Tt_s))}$$  $$L[h_1^{‘},…,h_n^{‘}]=\\sum_{i=1}^{n}a_ih_i^{‘}$$<br>其中， $W_{1,1} \\in \\mathbb{R^{q\\times m}}$ ，$W_{1,2}\\in  \\mathbb{R^{q\\times q}}$和 $b_1\\in  \\mathbb{R^{q}}$ 是参数， $h_i^{‘}$ 和 $h_{u_i,n_u}$ 分别是第 $i$ 个匹配向量和第 $i$ 个utterance的最终隐藏状态， $t_s\\in \\mathbb{R^{q}}$ 是一个虚拟上下文向量，它是随机初始化的，并在训练中共同学习。我们用 $L[h_1^{‘},…,h_n^{‘}]$ 的三个参数化来表示我们的模型，分别是 $SMN_{last}$，$SMN_{static}$ 和 $SMN_{dynamic}$，并在实验中进行比较。</li>\n</ul>\n<p>我们通过用 $D$ 最小化交叉熵来学习 $g(\\cdot,\\cdot)$。令 $\\theta$ 表示 $SMN$ 的参数，则学习的目标函数 $L(D,\\theta)$ 可表示为（公式8）：<br>$$-\\sum_{i=1}^{N}[y_ilog(g(s_i,r_i))+(1-y_i)log(1-g(s_i,r_i))]$$</p>\n<h3 id=\"检索候选Response\"><a href=\"#检索候选Response\" class=\"headerlink\" title=\"检索候选Response\"></a>检索候选Response</h3><p>作者利用启发式方法从索引中获取候选response，将前一轮的utterances ${u_1,…,u_{n-1}}$ 和 $u_n$ 进行计算，根据他们的<strong>tf-idf</strong>得分，从 ${u_1,…,u_{n-1}}$ 中提取前 $5$ 个关键字，然后，我们将扩展后的message用于索引，并使用索引的内联检索算法来检索候选response。最后，我们使用 $g(s,r)$ 对候选进行排名，并返回第一个作为对上下文的response。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><ul>\n<li>Ubuntu语料</li>\n<li>豆瓣多轮语料：下图给出三组的统计数据。<br><img src=\"https://img-blog.csdnimg.cn/20201031102216402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p><strong>下图显示两个数据集的评估结果：</strong><br><img src=\"https://img-blog.csdnimg.cn/20201031103318145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用Ubuntu语料库中的示例对第二层中的相似性矩阵和GRU的gates进行可视化，以进一步阐明我们的模型如何在上下文中识别重要信息，以及如何使用前文所述的GRU的门机制选择重要的匹配向量，示例及可视化图如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\tu1: how can unzip many rar (_number_ for example ) files at once; </span><br><span class=\"line\">\tu2: sure you can do that in bash; </span><br><span class=\"line\">\tu3: okay how? </span><br><span class=\"line\">\tu4: are the files all in the same directory? </span><br><span class=\"line\">\tu5: yes they all are; </span><br><span class=\"line\">\tr: then the command glebihan should extract them all from&#x2F;to that directory</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/2020103110442392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>下面是消融（Ablation）实验的结果：</strong><br><img src=\"https://img-blog.csdnimg.cn/20201031104859530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>跨context长度的效果：</strong>显示了豆瓣语料库上不同长度间隔的MAP的比较<br><img src=\"https://img-blog.csdnimg.cn/20201031104816446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>最大context长度：</strong>展示了在最大context长度方面，SMN在Ubuntu Corpus和Douban Corpus上的性能<br><img src=\"https://img-blog.csdnimg.cn/20201031105902650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>错误分析：</strong></p>\n<ul>\n<li>逻辑一致性。 SMN在语义级别上对context和response进行建模，但很少关注逻辑一致性</li>\n<li>检索后没有正确的候选</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文提出了一个新的基于上下文的模型，用于基于检索的聊天机器人中的多轮响应选择，论文还详尽的介绍了豆瓣对话语料库，并且做了实验去研究应该在context中取多少个utterance，即取多少轮对话，实验证明轮数取10的时候效果最好，很值得学习的论文。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","SMN","多轮对话"]},{"title":"论文阅读笔记：Scheduled-Sampling-for-Transformers","url":"/Paper-Reading/8fd93ce36699/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Scheduled Sampling for Transformers<br>原文链接：<a href=\"https://arxiv.org/pdf/1906.07651.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Scheduled sampling(计划采样)是一种避免Exposure Bias的技术，它包括向模型喂入Teacher-Forcing的embeddings和训练时间上一步中的模型预测的混合，该技术已用于通过递归神经网络（RNN）改善模型性能。在Transformer模型中，与RNN不同，新单词的生成会涉及到到目前为止生成的完整句子，而不仅是最后一个单词，致使应用Scheduled sampling技术并非易事。文中提出了一些结构上的更改，以允许通过两次遍历解码策略将Scheduled sampling应用于Transformer架构。</p>\n<blockquote>\n<p>由于训练和预测的时候decode行为的不一致， 导致预测单词（predict words）在训练和预测的时候是从不同的分布中推断出来的。而这种不一致导致训练模型和预测模型直接的Gap，就叫做 Exposure Bias。</p>\n</blockquote>\n<p>本文的创新和贡献：</p>\n<ul>\n<li>提出了一种在Transformer模型中使用Scheduled sampling的新策略，即在训练阶段内经过decoder两次</li>\n<li>比较了使用模型代替标准目标时以模型预测为条件的几种方法</li>\n<li>在两个语言对的机器翻译任务中使用Transformer测试了Scheduled sampling，并获得了接近Teacher-Forcing基线的结果（某些模型的改进幅度高达1个BLEU点）。</li>\n<li>线性衰减，指数衰减和反sigmoid衰减</li>\n</ul>\n<h1 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h1><p>众所周知，Transformer是一个 autoregressive模型，其中，每个单词的生成都取决于序列中所有先前的单词，而不仅是最后生成的单词。单词的顺序是通过将位置嵌入与相应的单词嵌入相加来实现的，而在解码器中使用位置屏蔽可确保每个单词的生成仅取决于序列中的前一个单词，而不取决于随后的单词。由于这些原因，想要将Scheduled sampling应用在Transformer中比较困难，所以需要对Transformer的结构进行一定的修改，更改后的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020202136686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如果你熟悉Transformer你会发现，其实encoder和decoder的结构都没有变，只是多了一个decoder</p>\n<h2 id=\"Transformer的Two-decoder\"><a href=\"#Transformer的Two-decoder\" class=\"headerlink\" title=\"Transformer的Two-decoder\"></a>Transformer的Two-decoder</h2><p>流程分为以下几步</p>\n<ul>\n<li>首次通过decoder，获取模型预测</li>\n<li>将标准序列与预测序列混合，对于序列中的每个位置，我们以给定的概率选择是使用标准token还是使用来自模型的预测</li>\n<li>将标准序列与预测序列的混合再次输入decoder，产生最终预测</li>\n</ul>\n<p>注意：重要的是要提到两个decoder是相同的，并且共享相同的参数</p>\n<h2 id=\"Embedding-Mix\"><a href=\"#Embedding-Mix\" class=\"headerlink\" title=\"Embedding Mix\"></a>Embedding Mix</h2><p>对于序列中的每个位置，第一遍解码器都会给出每个词汇词的得分。以下是使用模型预测时使用这些分数的几种方法</p>\n<ul>\n<li>完全不混合嵌入并通过模型预测传递argmax，即使用来自解码器的得分最高的词汇词嵌入。</li>\n<li>混合top-k嵌入，即使用得分最高的5个词汇词嵌入的加权平均值。</li>\n<li>通过将嵌入结合softmax与temperature进行传递，使用较高的temperature参数可使argmax更好地近似，公式如下：其中 $\\bar{e}<em>{i-1}$ 是在当前位置使用的向量，通过所有词汇词的嵌入量之和，以及分数 $s</em>{i-1}$ 的softmax加权获得。<br>$$\\bar{e}<em>{i-1}=\\sum_ye(y)\\frac{exp(as_{i-1}(y))}{\\sum</em>{y^{‘}}exp(as_{i-1}(y^{‘}))}$$</li>\n<li>使用argmax的另一种方法是从softmax分布中采样嵌入，公式如下：其中，$U ∼ Uniform(0, 1)$，$G=-log(-logU)$<br>$$\\bar{e}<em>{i-1}=\\sum_ye(y)\\frac{exp(a(s_{i-1}(y))+G_y)}{\\sum_{y^{‘}}exp(a(s</em>{i-1}(y^{‘})+G_{y^{‘}}))}$$</li>\n<li>通过嵌入的sparsemax</li>\n</ul>\n<h2 id=\"权重更新\"><a href=\"#权重更新\" class=\"headerlink\" title=\"权重更新\"></a>权重更新</h2><p>基于第二个解码器遍历的输出来计算交叉熵损失。对于将所有词汇单词相加的情况（Softmax，Gumbel softmax，Sparsemax），尝试两种更新模型权重的方法，如下：</p>\n<ul>\n<li>仅根据目标与模型预测之间的混合，通过最终做出预测的解码器进行反向传播。</li>\n<li>通过第二遍以及第一遍解码器进行反向传播，从而预测模型输出</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验在如下两个数据集中进行：</p>\n<ul>\n<li>IWSLT 2017 German−English </li>\n<li>KFTT Japanese−English</li>\n</ul>\n<p>使用字节对编码（BPE）进行联合分割，超参数如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020211057568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本论文的数据使用线性衰减最适合：$t(i)=max{\\epsilon, k-ci}$，其中，$0 \\leq \\epsilon &lt;1$ 是模型中要使用的最小Teacher-Forcing概率，$k$ 和 $c$ 提供衰减的偏移量和斜率。此函数确定训练步骤 $i$ 的Teacher-Forcing比 $t$，即在序列中每个位置进行Teacher-Forcing的概率，实验结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020211943651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>仅使用模型预测的得分最高的单词的Scheduled sampling效果不佳，使用混合嵌入（top-k，softmax，Gumbel softmax或sparsemax）并且仅使用第二个解码器通过的反向传播模型，在验证集上的性能略好于基准。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>这篇论文阐述了在Transformer上使用Scheduled Sampling的思路，对于几种Scheduled策略也进行了实验，说明了效果，值得借鉴。总体来说，实现思路不是很复杂，不过中间的可控性不高，并且可能需要找到符合数据集的一种更佳方式，可能泛化上不是很好。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Transformer","Paper","Scheduled-Sampling"]},{"title":"论文阅读笔记：Self-Attention-with-Relative-Position-Representation","url":"/Paper-Reading/5ad967c3667a/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Self-Attention with Relative Position Representations<br>原文链接：<a href=\"https://arxiv.org/pdf/1803.02155.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>了解Transformer的都知道，与递归和卷积神经网络相反，它没有在其结构中显式地建模相对或绝对位置信息，而是它需要在其输入中添加绝对位置的表示，这是一种完全依赖于注意力机制的方法。在本篇论文中，提出了一种替代方法，扩展了自注意机制，可以有效地考虑相对位置或序列元素之间距离的表示。本文描述了该方法的有效实现，并将其转换为可感知到任意图标记输入的相对位置感知自注意力机制的实例，即提出了一种将相对位置表示形式并入Transformer自注意机制的有效方法，残差连接有助于将位置信息传播到更高的层。</p>\n<p>循环神经网络（RNN）通常根据时间 $t$ 的输入和先前的隐藏状态 $h_{t-1}$ 计算隐藏状态 $h_t$，直接通过其顺序结构沿时间维度捕获相对位置和绝对位置。非循环模型不必一定要顺序考虑输入元素，因此可能需要显式编码位置信息才能使用序列顺序。</p>\n<p>一种常见的方法是使用与输入元素结合的位置编码，以将位置信息公开给模型。这些位置编码可以是位置的确定性函数或学习的表示形式。比如，卷积神经网络捕获每个卷积内核大小内的相对位置，已被证明仍然受益于位置编码。</p>\n<h3 id=\"Relation-aware自注意力\"><a href=\"#Relation-aware自注意力\" class=\"headerlink\" title=\"Relation-aware自注意力\"></a>Relation-aware自注意力</h3><h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文提出了自注意力的扩展，可用于合并序列的相对位置信息，从而提高了机器翻译的性能。论文中的这个思路可以借鉴参考，通过对自注意力的改造，就不需要进行硬位置编码了，但是论文中貌似没有比较硬位置编码和该方法的效果。</p>\n","categories":["Paper-Reading"],"tags":["Transformer","Paper","Self-Attention","Relative-Position-Representation","RPR","相对位置编码"]},{"title":"论文阅读笔记：Tacotron和Tacotron2","url":"/Paper-Reading/cea2357273fe/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Tacotron和Tacotron2<br>Tacotron：<a href=\"http://bengio.abracadoudou.com/cv/publications/pdf/wang_2017_arxiv.pdf\">Tacotron: A Fully End-To-End Text-To-Speech Synthesis Model</a><br>Tacotron2：<a href=\"https://arxiv.org/pdf/1712.05884.pdf\">Natural TTS Synthesis By Conditioning Wavenet On Mel Spectrogram Predictions</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文主要是对Tacotron和Tacotron2论文中的关键部分进行阐述和总结，之所以两篇论文放在一起，是因为方便比较模型结构上的不同点，更清晰的了解Tacotron2因为改进了哪些部分，在性能上表现的比Tacotron更好。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>语音合成系统通常包含多个阶段，例如TTS Frontend，Acoustic model和Vocoder，如下图更直观清晰一点：<br><img src=\"https://img-blog.csdnimg.cn/2020121421532563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>构建这些组件通常需要广泛的领域专业知识，并且可能包含脆弱的设计选择。在很多人困扰于繁杂的特征处理的时候，Google推出了Tacotron，一种从文字直接合成语音的端到端的语音合成模型，虽然在效果上相较于传统方法要好，但是相比Wavenet并没有明显的提升（甚至不如Wavenet），不过它更重要的意义在于end-to-end（Wavenet是啥将在后面对比vocoder的时候讲解，顺便提一下Tacotron使用的是Griffin-Lim算法，而Tacotron2使用的是修改版Wavenet）。此外，相较于其他样本级自回归方法合成语音，Tacotron和Tacotron2是在帧级生成语音，因此要快得多。</p>\n<p>在传统的Pipeline的统计参数TTS，通常有一个文本前端提取各种语言特征，持续时间模型，声学特征预测模型和基于复杂信号处理的声码器。而端到端的语音合成模型，只需要对文本语音进行简单的处理，就能喂给模型进行学习，极大的减少的人工干预，对文本的处理只需要进行文本规范化以及分词token转换（论文中使用character，不过就语音合成而言，使用Phoneme字典更佳），关于文本规范化（数字、货币、时间、日期转完整单词序列）以及text-to-phoneme可以参见我的另一篇<a href=\"https://zhuanlan.zhihu.com/p/336872753\">利器：TTS Frontend 中英Text-to-Phoneme Converter，附代码</a>。端到端语音合成系统的优点如下：</p>\n<ul>\n<li>减少对特征工程的需求</li>\n<li>更容易适应新数据（不同语言、说话者等）</li>\n<li>单个模型可能比组合模型更健壮，在组合模型中，每个组件的错误都可能叠加而变得更加复杂</li>\n</ul>\n<p><strong>端到端语音合成模型的困难所在：</strong><br>不同Speaker styles以及不同pronunciations导致的对于给定的输入，模型必须对不同的信号有着更大的健壮性，除此之外Tacotron原本下描述：</p>\n<blockquote>\n<p>TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into audio</p>\n</blockquote>\n<p>上面这句是Tacotron原文中说的，简单来说就是TTS输出是连续的，并且输出序列（音频）通常比输入序列（文本）长得多，导致预测误差迅速累积。想要了解更多关于语音合成的背景知识，可以参考文章<a href=\"https://www.jianshu.com/p/46888767dcef\">Text-to-speech</a></p>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><h2 id=\"Tacotron\"><a href=\"#Tacotron\" class=\"headerlink\" title=\"Tacotron\"></a>Tacotron</h2><p>Tacotron的基础架构是Seq2Seq模型，下图是模型的总体架构，该模型包括编码器，基于注意力的解码器和post-processing net，从高层次上讲，模型将字符作为输入，并生成频谱图，然后将其转换为波形。<br><img src=\"https://img-blog.csdnimg.cn/20201214230823985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>要特别说明的是架构中，raw text经过pre-net后，将会把输出喂给一个叫CBHG的模块以映射为hidden representation，再之后decoder会生成mel-spectrogram frame。所谓CBHG就是作者使用的一种用来从序列中提取高层次特征的模块，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201214231254861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"CBHG内部结构说明\"><a href=\"#CBHG内部结构说明\" class=\"headerlink\" title=\"CBHG内部结构说明\"></a>CBHG内部结构说明</h3><p>CBHG使用了1D卷积、highway、残差链接和双向GRU的组合，输入序列，输出同样也是序列，因此，它从序列中提取表示非常强大。CBHG架构流程如下</p>\n<ul>\n<li>首先使用 $K$ 组1D卷积对输入序列进行卷积，其中第 $k$ 组表示为 $C_k$ ，卷积的宽度为 $k$（即 $k=1,2,…,K$）。 这些卷积层显式地对本地和上下文信息进行建模（类似于对unigram，bigrams以及K-gram的建模）</li>\n<li>然后将卷积输出堆叠在一起，并进行最大化池，以增加局部不变性。注意了，最大化池使用stride为1来保留原始时间分辨率</li>\n<li>接着将处理后的序列传递给一些固定宽度的1D卷积，其输出通过残差连接与原始输入序列相加，同时批量归一化用于所有卷积层</li>\n<li>然后将输出喂到多层highway网络中以提取高级特征。</li>\n<li>最后，在顶部堆叠双向GRU RNN，以从前向和后向上下文中提取顺序特征。</li>\n</ul>\n<p>在Encoder中，输入被CBHG处理之前还需要经过pre-net进行预处理，作者设计pre-net（pre-net是由全连接层+dropout组成的模块）的意图是让它成为一个bottleneck layer来提升模型的泛化能力，以及加快收敛速度。</p>\n<h3 id=\"Decoder结构说明\"><a href=\"#Decoder结构说明\" class=\"headerlink\" title=\"Decoder结构说明\"></a>Decoder结构说明</h3><p>随后就是Decoder了，论文中使用两个decoder</p>\n<ul>\n<li>attention decoder：attention decoder用来生成query vector作为attention的输入，交由注意力模块生成context vector</li>\n<li>output decoder：output decoder则将query vector和context vector组合在一起作为输入。</li>\n</ul>\n<p>作者并没有选择直接用output decoder来生成spectrogram，而是生成了80-band mel-scale spectrogram，也就是我们之前提到的mel-spectrogram，熟悉信号处理的同学应该知道，spectrogram的size通常是很大的，因此直接生成会非常耗时，而mel-spectrogram虽然损失了信息，但是相比spectrogram就小了很多，且由于它是针对人耳来设计的，因此对最终生成的波形的质量不会有很多影响。</p>\n<p>随后使用post-processing network（下面会讲）将seq2seq目标转换为波形，然后使用一个全连接层来预测decoder输出。Decoder中有一个<strong>trick就是在每个decoder step预测多个(r个)frame，这样做可以缩减计算量，且作者发现这样做还可以加速模型的收敛</strong>。</p>\n<blockquote>\n<p>论文提到scheduled sampling在这里使用会损失音频质量</p>\n</blockquote>\n<h3 id=\"post-processing-net和waveform-synthesis\"><a href=\"#post-processing-net和waveform-synthesis\" class=\"headerlink\" title=\"post-processing net和waveform synthesis\"></a>post-processing net和waveform synthesis</h3><p>作者使用比较简单的Griffin-Lim 算法来生成最终的波形，由于decoder生成的是mel-spectrogram，因此需要转换成linear-scale spectrogram才能使用Griffin-Lim算法，这里作者同样使用CBHG来完成这个任务。实际上这里post-processing net中的CBHG是可以被替换成其它模块用来生成其它东西的，比如直接生成waveform，在Tacotron2中，CBHG就被替换为Wavenet来直接生成波形。</p>\n<h3 id=\"模型详细的配置\"><a href=\"#模型详细的配置\" class=\"headerlink\" title=\"模型详细的配置\"></a>模型详细的配置</h3><p><img src=\"https://img-blog.csdnimg.cn/20201214233931169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>对Decoder和post-processing net使用L1损失，并取平均。作者使用32batch，并将序列padding到最大长度。关于padding的说明，Tacotron原文如下：</p>\n<blockquote>\n<p>It’s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames. However, we found that models trained this way don’t know when to stop emitting outputs, causing repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct the zero-padded frames.</p>\n</blockquote>\n<h2 id=\"Tacotron2\"><a href=\"#Tacotron2\" class=\"headerlink\" title=\"Tacotron2\"></a>Tacotron2</h2><p>Tacotron比较明显的缺点就是生成最终波形的Griffin-Lim算法，Tacotron中作者也提到了，这个算法只是一个简单、临时的neural vocoder的替代，因此要改进Tacotron就需要有一个更好更强大的vocoder。</p>\n<p>接下来我们来看看Tacotron2，它的模型大体上分为两个部分：</p>\n<ul>\n<li>具有注意力的循环序列到序列特征预测网络，该网络根据输入字符序列预测梅尔谱帧的序列</li>\n<li>WaveNet的修改版，可生成以预测的梅尔谱帧为条件的time-domain waveform样本</li>\n</ul>\n<p>结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201215095332283.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Tacotron2选择预测a low-level acoustic表示，即mel-frequency spectrograms（Tacotron使用 linear-frequency scale spectrograms），Tacotron2原文描述如下：</p>\n<blockquote>\n<p>This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame.</p>\n</blockquote>\n<p>mel-frequency spectrogram与linear-frequency spectrograms有关，即短时傅立叶变换（STFT）幅度。mel-frequency是通过对STFT的频率轴进行非线性变换而获得的，同时受到人类听觉系统的启发，用较少的维度表示频率内容，原因很好理解，低频中的细节对于音频质量至关重要，而高频中往往包含摩擦音等噪音，因此通常不需要对高频细节建模。</p>\n<p>虽然linear spectrograms会丢弃相位信息（因此是有损的），但是诸如Griffin-Lim之类的算法能够估算此丢弃的信息，从而可以通过短时傅立叶逆变换进行时域转换。而mel spectrogram会丢弃更多信息，因此它的逆问题更具有挑战性，这个时候作者想到了WaveNet。</p>\n<p>除了Wavenet，Tacotron2和Tacotron的主要不同在于：</p>\n<ul>\n<li>不使用CBHG，而是使用普通的LSTM和Convolution layer</li>\n<li>decoder每一步只生成一个frame</li>\n<li>增加post-net，即一个5层CNN来精调mel-spectrogram</li>\n</ul>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h2 id=\"Tacotron-1\"><a href=\"#Tacotron-1\" class=\"headerlink\" title=\"Tacotron\"></a>Tacotron</h2><p>下图展示Decoder step中，使用不同组件学习到attention alignment的效果：<br><img src=\"https://img-blog.csdnimg.cn/20201215001842202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示了post-processing net的实验效果，可以看到有post-processing net的网络效果更好：<br><img src=\"https://img-blog.csdnimg.cn/20201215090803459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>MOS分数对比如下表：<br><img src=\"https://img-blog.csdnimg.cn/20201215105516463.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Tacotron2-1\"><a href=\"#Tacotron2-1\" class=\"headerlink\" title=\"Tacotron2\"></a>Tacotron2</h2><p>下表展示了Tacotron2与各种现有系统的MOS分数比较。Tacotron2的分数已经和人类不相上下了，这在很大程度上要归功于Wavenet。<br><img src=\"https://img-blog.csdnimg.cn/20201215105745330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是对合成的音频的评价：<br><img src=\"https://img-blog.csdnimg.cn/20201215111449854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>文中提到，Wavenet在这个模型中是和剩下的模型分开训练的，Wavenet的输入是mel-spectrogram，输出是waveform，这个时候就需要考虑输入的mel-spectrogram是选择ground truth，还是选用prediction，作者做了相关实验，结果如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201215112349842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到使用模型生成的mel-spectrogram来训练的Wavenet取得了最好的结果，作者认为这是因为这种做法保证了数据的一致性。下表是生成mel-spectrogram和linear spectrogram的区别（结果证明mel-spectrogram是最好的，同时还能够减少计算，加快inference的时间）：<br><img src=\"https://img-blog.csdnimg.cn/20201215112431948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是对WaveNet简化之后的MOS分数情况：<br><img src=\"https://img-blog.csdnimg.cn/20201215112759691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"关于vocoder\"><a href=\"#关于vocoder\" class=\"headerlink\" title=\"关于vocoder\"></a>关于vocoder</h1><p>Tacotron使用的是Griffin-Lim算法，Griffin-Lim是一种声码器，常用于语音合成，用于将语音合成系统生成的声学参数转换成语音波形，这种声码器不需要训练，不需要预知相位谱，而是通过帧与帧之间的关系估计相位信息，从而重建语音波形。更正式一点的解释是Griffin-Lim算法是一种已知幅度谱，未知相位谱，通过迭代生成相位谱，并用已知的幅度谱和计算得出的相位谱，重建语音波形的方法，具体可参考这篇<a href=\"https://zhuanlan.zhihu.com/p/66809424\">Griffin-Lim 声码器介绍</a></p>\n<p>而Tacotron2使用的WaveNet采用了扩大卷积和因果卷积的方法，让信息随着网络深度增加而成倍增加，可以对原始语音数据进行建模。WaveNet是强大的音频生成模型。 它适用于TTS，但由于其样本级自回归特性而速度较慢。不过要注意的是，WaveNet还需要对现有TTS前端的语言功能进行调整，因此不是端对端的：它仅替代声码器和声学模型。具体可参见如下两篇文章，或参见原论文：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/24317897\">谷歌WaveNet如何通过深度学习方法来生成声音？</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/24568596\">谷歌WaveNet 源码详解</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","TensorFlow","语音合成","NLP","Paper","Tacotron"]},{"title":"论文阅读笔记：使用Transformer进行语音合成","url":"/Paper-Reading/5dff8ecd5ba4/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Neural Speech Synthesis with Transformer Network<br>原文链接：<a href=\"https://arxiv.org/pdf/1809.08895.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>虽然像Tacotron2这样的TTS模型实现了最新的性能，但它们仍然存在两个问题：</p>\n<ul>\n<li>在训练和推理过程中效率低下（巨慢）</li>\n<li>难以使用当前的递归神经网络（RNN）对长期依赖性进行建模</li>\n</ul>\n<p>本文受Transformer启发，使用多头自注意力机制取代Tacotron2中的RNN结构和原始注意力机制。借助多头自注意力机制，可以并行构造编码器和解码器中的隐藏状态，从而提高训练效率，同时，不同时间步的任意两个输入通过自注意力机制直接连接，有效解决了远程依赖问题。使用phoneme（音素）序列作为输入，Transformer TTS网络生成梅尔频谱图，然后通过WaveNet声码器以输出最终的音频结果。</p>\n<blockquote>\n<p>phoneme音素：能区分意义的最小声音单位，比如dog和fog中，d和f只要改变一个就改变了意义。</p>\n</blockquote>\n<ul>\n<li>Tacotron2模型结构<br><img src=\"https://img-blog.csdnimg.cn/20201205235515714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>Transformer模型结构<br><img src=\"https://img-blog.csdnimg.cn/20201206000024694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>这里对一些语音方面的相关概念进行说明，如果不感兴趣想要直接看论文内容，可以直接跳过这一节。</p>\n<ul>\n<li>speech wave：语音波是一种compound wave，即包含各种频率的波。因此在频域上表示语音更为合适。</li>\n<li>pitch音高：声音的尖锐程度，在频域中表现为频率的高低。</li>\n<li>基础频率F0：浊音中存在基础频率，而清音中不存在，F0决定了声音的音高。</li>\n<li>formants共振峰：是一种元音特有的在频域中的现象，因为只有元音有基础频率。每个元音都有两个共振峰，可以用来区分元音，记为F1和F2。F1,F2取决于基础频率，如果基础频率太高，共振峰可能会消失，这种情况下就区分不出来元音，这种现象在各种女高音身上比较常见。</li>\n<li>timbre音色：音色在广义上是指声音不同于其它的特点，在语音中不同的音节都有不同的特点，这可以通过频域观察出来，另外，特别地，对于元音我们可以通过共振峰来分辨音色。</li>\n<li>noise：噪音、辅音(摩擦音)都会有broad spectrum，也就是说我们无法通过共振峰来识别它们。</li>\n<li>envelope包络：在波的时域和频域图中，用来形容图形的整体形状的叫做包络。</li>\n<li>frontend：主要是文字处理，使用NLP技术，从离散到离散，包括基本的分词、text normalization、POS以及特有的Pronunciation标注。</li>\n<li>backend：根据前端结果生成语音，从离散到连续</li>\n<li>segmentation &amp; normalization：去噪、分句、分词以及把缩写、日期、时间、数字还有符号都换成可发音的词，这一步叫spell out。基本都基于规则。</li>\n</ul>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><p>与基于RNN的模型相比，在神经TTS中使用Transformer具有两个优点：</p>\n<ul>\n<li>因为可以并行提供解码器输入序列的帧，它可以通过取代循环连接来进行并行训练。</li>\n<li>自注意力将整个序列的全局上下文注入到每个输入帧中，直接建立了长距离依赖关系</li>\n</ul>\n<p>在本节中，将介绍Transformer TTS模型的体系结构，并分析每个部分的功能，整个模型结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/2020120609460844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Text-to-Phoneme转换器：由于训练数据不足的情况下很难学习语言的所有规律性，并且某些例外情况很少出现，而无法通过神经网络学习。因此，作者建立了一个规则系统并将其实现为文本到音素的转换器，它可以覆盖绝大多数情况。</li>\n<li>Scaled Positional编码：采用的位置编码是Transformer的正弦位置编码：<br>$$PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$    $$PE(pos,2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$<br>其中 $pos$ 是时间步长索引，$2i$ 和 $2i+1$ 是通道索引，$d_{model}$ 是每个帧的维数。不过有一点要注意的是，不像文本训练那样，source和target都是一个语言空间，embedding的编码是相似的。TTS中使用固定的位置嵌入可能会对编码器和解码器的pre-nets都施加严格的约束（将在后面描述），因此作者使用具有可训练权重的位置编码，以便这些位置编码可以自适应地匹配编码器和解码器pre-nets输出的比例，公式如下：<br>$$x_i=prenet(phoneme_i)+\\alpha PE(i)$$<br>其中$\\alpha$是可训练权重。</li>\n<li>Encoder Pre-net：在Tacotron2中，将三层CNN应用于输入文本嵌入，它可以对输入字符序列中的上下文进行建模。在这里的Transformer TTS模型中，将 phoneme序列输入到同一网络中，这称为“encoder pre-net”。每个phoneme具有512维的可训练嵌入，每个卷积层的输出具有512个通道，然后进行batch normalization、ReLU激活以及dropout。此外，由于ReLU的输出范围是 $[0,\\infty]$，而这些位置编码的每个维数都在 $[-1,1]$ 中，所以作者在最终ReLU激活后添加了线性层。在实验中证明，在Nonnegative Embeddings中添加以0为中心的位置信息将影响模型的性能。</li>\n<li>Decoder Pre-net：梅尔频谱图首先被具有ReLU激活的，由两个全连接层（每个层都有256个隐藏单元）组成的神经网络处理，称为“decoder pre-net”，它在TTS系统中起着重要作用。phonemes具有可训练的嵌入，因此其子空间是自适应的，而梅尔频谱图的是固定的。<strong>作者推断decoder pre-net负责将梅尔频谱图投影到与phonemes嵌入相同的子空间中，从而可以计算 $\\left \\langle phoneme, mel\\ frame \\right \\rangle$ 对的相似性，从而使注意力机制发挥作用</strong>。此外，还尝试了2个没有非线性激活的全连接层，但是无法生成合理的注意力矩阵来对齐编码器和解码器的隐藏状态。此外，<strong>作者推测梅尔谱图具有一个紧凑且低维的子空间，其中256个隐藏单元足以映射</strong>。同encoder pre-net一样，还添加了一个附加的线性层，不仅用于中心一致性，而且还获得与位置编码相同的尺寸。</li>\n<li>Encoder：在Tacotron2中，编码器是双向RNN，而这里使用Transformer编码器代替它。与原始的双向RNN相比，多头注意力将注意力分散到几个子空间中，从而可以在多个不同方面建模帧关系，并直接建立任意两个帧之间的长依赖关系，因此每个帧都被视为整个序列的全局上下文。这对于合成音频韵律至关重要，尤其是在句子较长的情况下。</li>\n<li>Decoder：在Tacotron2中，解码器是一个结合location sensitive attention的2层RNN，而这里使用Transformer解码器代替它。</li>\n<li> Mel Linear、Stop Linear和Post-net：与Tacotron2相同，我们分别使用两个不同的线性层来预测梅尔频谱图和停止标记，并使用5层CNN产生残差来完善mel频谱图的重建。值得一提的是，对于停止标记的线性层而言，每个序列的末尾只有一个正样本，表示“停止”，而其他帧则是负样本，这种不平衡可能导致无法停止的推断。在计算二元交叉熵损失时，作者在停止标记的正样本上施加正权重 $(5.0\\sim 8.0)$，从而有效地解决了这个问题。</li>\n</ul>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>实验使用25小时的专业语音对测试Transformer TTS模型，并通过人工测试在MOS和CMOS中评估音频质量。<strong>由于训练样本的长度相差很大，因此，如果以长样本为准扩大batch尺寸将占用很大内存，而如果以短样本为准缩小batch尺寸则会浪费并行计算能力，因此，作者使用动态batch大小，其中最大总的Mel光谱图帧数是固定的，并且一个batch应包含尽可能多的样本。</strong></p>\n<p>Tacotron2使用字符序列作为输入，而本文的模型是根据pre-normalized phoneme序列训练的。自回归WaveNet包含2个QRNN层和20个扩张层，所有残差通道和扩张通道的大小均为 $256$。QRNN最终输出的每一帧均被复制200次，以具有与音频样本相同的空间分辨率且条件为20扩张层。</p>\n<p>下表是MOS和CMOS指标的对比结果：<br><img src=\"https://img-blog.csdnimg.cn/20201206111801454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图是模型生成的梅尔频谱的结果对比：如我们所见，论文模型在重建以红色矩形标记的细节方面做得更好，而Tacotron2在高频区域则省略了细节纹理。<br><img src=\"https://img-blog.csdnimg.cn/20201206112001432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了中心一致的位置编码效果稍好：<br><img src=\"https://img-blog.csdnimg.cn/2020120611221863.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下图表明编码器和解码器的最终位置编码比例不同的对比：<br><img src=\"https://img-blog.csdnimg.cn/20201206114754487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下表显示了具有可缩放比例的模型，其性能略有提高：<br><img src=\"https://img-blog.csdnimg.cn/20201206112337437.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下面3张表是比较具有不同层数和头注意力数的性能和训练速度。发现减少层数和头注意力数均可以提高训练速度，但另一方面，会在不同程度上损害模型性能。<br><img src=\"https://img-blog.csdnimg.cn/20201206113930677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201206113940804.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>值得再次提及的是batch大小对于训练稳定性至关重要，并且更多的层可以完善生成的mel频谱图的细节，尤其是在高频区域，从而提高模型性能。论文作者对这一模型做了很多的实验，总的来说，训练时期的速度大大提高，加快了2到3倍，生成语音的质量也好于传统RNN结构模型（存疑，复现版本仅仅能做到效果相接近，可能是作者的调参技艺比较高超）。基于Transformer的TTS模型已是现在主流的End-to-End TTS系统的baseline，它的实现必不可少，而且因为Transformer本身优异的结构，也能大大加快实验的速度。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Pytorch","语音合成","NLP","Transformer","Paper"]},{"title":"论文阅读笔记：全局-局部自注意力的对话状态跟踪","url":"/Paper-Reading/9c4b9a5af67b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Global-Locally Self-Attentive Dialogue State Tracker<br>原文链接：<a href=\"https://arxiv.org/pdf/1805.09655.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>对话状态跟踪（在对话上下文中估计用户目标和请求）是面向任务的对话系统的重要组成部分。在本文中，提出了“全局-局部自注意力对话状态追踪”（GLAD），该学习器使用全局本地模块来学习用户话语的表示和以前的系统动作。模型使用全局模块在不同类型（称为插槽）的对话状态的估计量之间共享参数，并使用本地模块学习特定于插槽的特征。DST中的状态（state）通常由一个请求（request）和联合目标（joint goals）的集合组成。请求即请求系统返回所需信息（例如：request(address)），目标即用户想要完成的动作（例如：inform(food=french)），请求和目标可以用槽位-值对（slot-value pair）来表示（(food,french)， (request, address)）。<br><img src=\"https://img-blog.csdnimg.cn/20201019151853610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>作者认为传统的DST极其依赖Spoken Language Understanding（SLU），而依赖SLU极易导致错误的积累，且通常的SLU对比较少见的slot-value对容易分错，为了解决这些问题，本文提出了一种全局-局部自注意力对话状态追踪方法（Global-Locally Self-Attentive Dialogue State Tracker， GLAD），使用全局模块在预测每个slot的预测器之间共享参数，而使用局部模块来学习每个slot的特征表示。通过这种设计，能够使GLAD在更小的训练数据上泛化到更少见的slot-value对。</p>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><p>模型整体框架如下，主要包含两个模块：encoder模块和scoring模块。在encoder模块中，可以看到分别针对system action，user utterance和候选slot-value对这三者进行encoder。注意，后面讨论的都是针对某个具体的候选slot-value对。scoring模块包含两个打分器，分别给当前话语和之前轮次系统行为对当前状态预测的贡献打分。<br><img src=\"https://img-blog.csdnimg.cn/20201019232904710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"全局-局部自注意力编码器\"><a href=\"#全局-局部自注意力编码器\" class=\"headerlink\" title=\"全局-局部自注意力编码器\"></a>全局-局部自注意力编码器</h2><p>在针对三部分输入进行encoding的过程中，使用的都是Global-Locally Self-Attentive Encoder，这个encoder的框架如下：<br><img src=\"https://img-blog.csdnimg.cn/20201019233304219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>基本可以分为如下几个步骤：</p>\n<ul>\n<li>将输入分别通过global和local的BiLSTM得到各自的表征，全局与局部双向LSTM类似，输入embedding后的序列，得到编码输出，区别在于全局双向LSTM的参数在不同slot间共享，而每个slot都有各自的局部双向LSTM参数。<br>$$H^g=biLSTM^g(X)\\in \\mathbb{R}^{n \\times d_{rnn}}$$  $$H^s=biLSTM^s(X)\\in \\mathbb{R}^{n \\times d_{rnn}}$$</li>\n<li>将global和local的进行加权融合，两个LSTM的输出通过一个混合层相结合，形成全局-局部编码的最终输出，其中 $\\beta^s$ 是学习到的一个0-1之间的权重值（每个slot的 $\\beta^s$ 不同）。<br>$$H = \\beta^sH^s+(1-\\beta^s)H^g\\in \\mathbb{R}^{n \\times d_{rnn}}$$</li>\n<li>类似的，针对global和local的分别使用attention机制，并最后做融合。其中，对 $H$ 的每个元素 $H_i$ 计算一个标量attention分值，通过softmax标准化，再对 $H_i$ 做加权求和，得到最终的输出。<br>$$a_{i}^{g}=W^gH_i+b^g\\in \\mathbb{R}$$  $$p^g=softmax(a^g)\\in \\mathbb{R}^n$$  $$c^g=\\sum_ip_{i}^gH_i\\in  \\mathbb{R}^{d_{rnn}}$$</li>\n</ul>\n<p>$$a_{i}^{s}=W^sH_i+b^s\\in \\mathbb{R}$$  $$p^s=softmax(a^s)\\in \\mathbb{R}^n$$  $$c^s=\\sum_ip_{i}^sH_i\\in  \\mathbb{R}^{d_{rnn}}$$<br>最终的全局-局部自注意力表示为：<br>$$c=\\beta^sc^s+(1-\\beta^s)c^g\\in\\mathbb{R}^{n \\times d_{rnn}}$$</p>\n<ul>\n<li>注意，encoder会输出两个东西，$H$ 和 $c$ 都会在下面的scoreing module中使用到：</li>\n</ul>\n<h2 id=\"Encoding-module\"><a href=\"#Encoding-module\" class=\"headerlink\" title=\"Encoding module\"></a>Encoding module</h2><p>分别针对三个输入利用上面的encoder进行encoding，其中，用 $U$ 表示当前话语的embedding序列， $A_j$ 表示之前的第 $j$ 个系统行为， $V$ 表示当前考虑的slot-value对，以上述全局-局部自注意力编码器为基础的编码模块的输出为：<br>$$H^{utt},c^{utt}=encode(U)$$  $$H_j^{act},c_j^{act}=encode(A_j)$$  $$H^{val},c^{val}=encode(V)$$</p>\n<h2 id=\"Scoring-Module\"><a href=\"#Scoring-Module\" class=\"headerlink\" title=\"Scoring Module\"></a>Scoring Module</h2><p>分为三个步骤：</p>\n<ul>\n<li>通过类似attention机制进行utterance与slot-value的匹配和打分，具体而言，当前用户话语对于当前slot-value对是否在当前轮次中的贡献是用户直接表述出来的（比如：how about a French restaurant in the centre of town?）。针对这种情况，使用当前slot-value对表示 $c^{val}$ 对 $H^{utt}$ 进行attention并加权求和，用所得结果为该slot-value对打分。<br>$$a_i^{utt}=(H_i^{utt})^Tc^{val}\\in\\mathbb{R}$$  $$p^{utt}=softmax(a^{utt})\\in\\mathbb{R}^m$$  $$q^{utt}=\\sum_ip_i^{utt}H_i^{utt}\\in\\mathbb{R}^{d_{rnn}}$$  $$y^{utt}=Wq^{utt}+b\\in\\mathbb{R}$$</li>\n<li>同样地，先进行system action与utterance之间的融合，尔后再与slot-value进行匹配。具体而言，而当当前用户话语没有呈现足够信息时，对当前轮次状态的推断则需要考虑之前轮次的系统行为（例如用户只回答了“yes”）。针对这种情况，采取与上述attention过程类似的思路对过往轮次系统行为对当前状态推断的贡献打分。<br>$$a_j^{act}=(C_j^{act})^Tc^{utt}\\in\\mathbb{R}$$  $$p^{act}=softmax(a^{act})\\in\\mathbb{R}^{l+1}$$  $$q^{act}=\\sum_ip_j^{act}C_j^{act}\\in\\mathbb{R}^{d_{rnn}}$$  $$y^{act}=(q^{act})^Tc^{val}\\in\\mathbb{R}$$</li>\n<li>最后打分由两部分分数加权求和，并经过sigmoid标准化。<br>$$y=\\sigma(y^{utt}+wy^{act})\\in\\mathbb{R}$$</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><ul>\n<li>展示了GLAD与以前的最新模型相比的性能<br><img src=\"https://img-blog.csdnimg.cn/2020102011100272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>我们在开发套件上进行了分解实验，以分析GLAD不同组件的有效性。<ul>\n<li>时间顺序对于状态跟踪很重要</li>\n<li>Self-attention可实现特定于插槽的强大功能学习</li>\n<li>Global-local共享可改善目标跟踪<br><img src=\"https://img-blog.csdnimg.cn/20201020111233472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这是一个示例，其中self-attention模块专注于话语的相关部分。<br><img src=\"https://img-blog.csdnimg.cn/20201020111654205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示了，在训练数据中显示了GLAD的性能以及这两种共享变体在不同出现次数上的表现。对于具有大量训练数据的槽值对，模型之间没有明显的性能差异，因为有足够的数据可以概括<br><img src=\"https://img-blog.csdnimg.cn/20201020112030255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示GLAD的预测示例<br><img src=\"https://img-blog.csdnimg.cn/20201020112258567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1>GLAD的核心是Global-Locally自注意编码器，其全局模块允许在插槽之间共享参数，而本地模块则允许学习特定于插槽的特征。这使GLAD可以在很少的训练数据的情况下对稀有的插槽值对进行概括。global和local的思想值得一些借鉴。</li>\n</ul>\n</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["对话系统","Attention","注意力机制","Paper","状态跟踪"]},{"title":"论文阅读笔记：四种用于学习对话上下文表示的无监督预训练方法的对比和分析","url":"/Paper-Reading/7f6535cceaa6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Pretraining Methods for Dialog Context Representation Learning<br>原文链接：<a href=\"https://arxiv.org/pdf/1906.00414.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>本文考察了各种用于学习对话上下文表示的无监督预训练目标， 提出了两种新颖的对话上下文编码器预训练方法，并研究了四种方法。使用MultiWoz数据集对每个预训练目标进行了微调，并在一组下游对话任务上进行了评估，并观察到了出色的性能改进。 进一步的评估表明，我们的预训练目标不仅可以带来更好的性能，而且可以带来更好的收敛性，并且模型需要的数据更少，并且具有更好的领域通用性。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>目前预训练方法仍处在起步阶段，我们仍然不能完全了解他们的性质。大多数方法都是基于语言模型的，给一个句子，预测当前词，下一个词或者被mask的词。如Word2Vec，Glove，ELMO等。这些方法将自然语言看作是word token的流，需要复杂的模型利用大规模的语料库和庞杂的计算来发现更高级别的依赖关系。BERT模型也是基于语言模型，但是加入了句子对级别的信息，预测两句话是否是连续的。这种方法在预训练时利用了语句之间的关系。但是，在对话上下文建模这种存在多轮的依赖关系的任务上还并没有行之有效的预训练方法，于是本文在这个方面做了一些尝试。本文目的就是研究几个预训练话语级语言表示的方法，本文迈出了建立对话系统预训练方法系统分析框架的第一步。</p>\n<p>评估预训练方法的四个假设：</p>\n<ul>\n<li>预训练能够在整个可用数据集上进行微调，且提升下游任务</li>\n<li>预训练结果需要更好的收敛</li>\n<li>预训练将在有限的数据下表现出色</li>\n<li>预训练有助于领域通用化</li>\n</ul>\n<p>对话与其他文本的区别：</p>\n<ul>\n<li>对话必须是语句之间连贯的，并在多轮上达到一个交际的目的。</li>\n<li>对话在本质上是互动的，说话者之间有反馈，而且说话者轮流进行发言。</li>\n</ul>\n<p>本文的主要贡献：</p>\n<ul>\n<li>针对对话上下文表示研究四个不同的预训练方法，包括两个新的方法</li>\n<li>在四个下游任务上，综合分析预训练对对话上下文表示的影响</li>\n</ul>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>这项工作与NLP系统的辅助多任务学习和带预训练的迁移学习的研究紧密相关。</p>\n<h2 id=\"Training-with-Auxiliary-Tasks\"><a href=\"#Training-with-Auxiliary-Tasks\" class=\"headerlink\" title=\"Training with Auxiliary Tasks\"></a>Training with Auxiliary Tasks</h2><p>结合有用的辅助损失函数来补充主要目标已被证明可以改善深度神经网络模型的性能。一些辅助损失函数专门设计来提高特殊任务的性能。在一些案例中，辅助函数被用来提升模型的泛化能力。经过适当的辅助任务预训练后，模型可以捕获更长的依赖关系。</p>\n<h2 id=\"Transfer-Learning-with-Pretraining\"><a href=\"#Transfer-Learning-with-Pretraining\" class=\"headerlink\" title=\"Transfer Learning with Pretraining\"></a>Transfer Learning with Pretraining</h2><p>基本过程通常是首先在无监督目标的海量文本数据上预训练功能强大的神经编码器。 第二步是使用更小的域内数据集对特定的下游任务微调此预训练模型。ELMo使用BiLSTM网络来训练双向语言模型来同时预测前一个词和后一个词。OpenAI的GPT使用Transformer网络和BERT进行了两个目标的同时训练：掩蔽语言模型和下一句预测。每个模型均已在GLUE基准上展示了最新的结果。这些利用大规模预训练的模型优于仅使用域内数据的系统。用于学习从输入文本中提取话语级别信息的预训练方法的工作很少。BERT中的下一句话预测损失是朝着这个方向迈出的一步。尽管这些预训练方法擅长于对顺序文本进行建模，但它们并未明确考虑对话的独特话语级功能。因此，我们在研究预训练目标时采取了第一步，以提取对话上下文的更好的话语级表示形式。</p>\n<h1 id=\"Pretraining-Objectives\"><a href=\"#Pretraining-Objectives\" class=\"headerlink\" title=\"Pretraining Objectives\"></a>Pretraining Objectives</h1><p>本文定义了一种强有力的表示形式，它可以捕获整个对话历史中的话语级信息以及构成该历史的话语中的话语级信息，在本文的定义下，当表示允许模型在各种下游任务上表现更好时，表示就足够通用了。</p>\n<ul>\n<li>一个任意T轮对话（对话历史）的表示符号：$c = [u_1,…,u_t]$，$u_i$是一个话语。</li>\n<li>对话回复$R = {r_1,…,r_M}$</li>\n</ul>\n<h2 id=\"Next-Utterance-Retrieval（NUR-检索下一句话）\"><a href=\"#Next-Utterance-Retrieval（NUR-检索下一句话）\" class=\"headerlink\" title=\"Next-Utterance Retrieval（NUR-检索下一句话）\"></a>Next-Utterance Retrieval（NUR-检索下一句话）</h2><p>NUR的目的就是在$k$个候选回复中选择正确的下一句话。对于此任务，本文使用分层编码器来生成对话上下文的表示，方法是首先通过双向长期短期记忆网络（biLSTM）独立运行每个话语，然后使用所得的话语表示来生成整个对话上下文的表示。给定$[u_1，… u_{T-1}]$，NUR的任务是从R中选择正确的下一个话语$u_T$。损失运算公式如下：<br>$$<br>\\hat{u_i}=f_u(u_i), i\\in [1,T-1]$$<br>$$r_{gt} = f_r(u_T)$$<br>$$r_{j} = f_r(r_j),r_j\\sim p_n(r)$$<br>$$a_{gt} = (h_{T-1})^{T}r_{gt}$$<br>$$a_{j} = (h_{T-1})^{T}r_{j}$$<br>总损失如下：<br>$$<br>L = -log_p(u_T|u_1,…u_{T-1})=-log(\\frac{exp(a_{gt})}{exp(a_{gt}+\\sum_{j=1}^{K}exp(a_j)})<br>$$</p>\n<h2 id=\"Next-Utterance-Generation（NUG-生成下一句话）\"><a href=\"#Next-Utterance-Generation（NUG-生成下一句话）\" class=\"headerlink\" title=\"Next-Utterance Generation（NUG-生成下一句话）\"></a>Next-Utterance Generation（NUG-生成下一句话）</h2><p>给定对话历史，根据对话历史生成下一句话。预训练时使用分层Encoder-Decoder结构，在进行下游任务时，仅使用Encoder。对话上下文和下一个话语被编码为式8，最小化损失为式9：<br>$$<br>L = -log_p(u_T|u_1,…u_{T-1})\\<br>=-\\sum_{k}^{N}log_p(w_k|w&lt;k,h_{T-1})<br>$$</p>\n<h2 id=\"Masked-Utterance-Retrieval（MUR-根据mask的对话历史检索下一句话）\"><a href=\"#Masked-Utterance-Retrieval（MUR-根据mask的对话历史检索下一句话）\" class=\"headerlink\" title=\"Masked-Utterance Retrieval（MUR-根据mask的对话历史检索下一句话）\"></a>Masked-Utterance Retrieval（MUR-根据mask的对话历史检索下一句话）</h2><p>与NUR相同，给定对话历史，从$K$个候选回复中选择正确的下一句话，区别</p>\n<ul>\n<li>对话历史中的一句话被随机选择的另一句话替换。</li>\n<li>用替换掉的句子的表示作为对话历史的表示。</li>\n</ul>\n<p>替换的语句索引为$t$，且是在对话部分中随机采样的<br>$$<br>t \\sim Uniform[1,T]<br>$$<br>其中<br>$$<br>\\hat{u_i}=f_u(u_i), i\\in [1,T]$$<br>$$r_{gt} = f_r(u_T)$$<br>$$r_{j} = f_r(r_j),r_j\\sim p_n(r)$$<br>$$a_{gt} = (h_T)^{T}r_{gt}$$<br>$$a_{j} = (h_T)^{T}r_{j}<br>$$</p>\n<p>总损失：<br>$$<br>L = -log_p(u_T|u_1,…,q,…u_{T-1})\\<br>=-log(\\frac{exp(a_{gt})}{exp(a_{gt}+\\sum_{j=1}^{K}exp(a_j)})<br>$$</p>\n<h2 id=\"Inconsistency-Identification（InI-识别不一致语句）\"><a href=\"#Inconsistency-Identification（InI-识别不一致语句）\" class=\"headerlink\" title=\"Inconsistency Identification（InI-识别不一致语句）\"></a>Inconsistency Identification（InI-识别不一致语句）</h2><p>识别一段对话历史中不一致的句子。输入是一段对话历史，其中的一句被随机替换掉，模型需要找到被替换的是哪一句。<br>$$<br>L = -log_p(t|u_1,…,q,…u_T)\\<br>=-log(\\frac{exp(a_t)}{\\sum_{j=1}^{T}exp(a_i)})<br>$$<br>其中<br>$$<br>\\hat{u_i}=f_u(u_i)),i\\in [1,T]\\<br>[h_1…h_T]=f_c(\\hat{u_1},…\\hat{u_T})\\<br>a_i=(h_T)^{T}h_i,i\\in[1,T]<br>$$<br>这个任务的目标是建模单个语句的表示和对话上下文的全局表示的一致性。</p>\n<h1 id=\"Downstream-Tasks\"><a href=\"#Downstream-Tasks\" class=\"headerlink\" title=\"Downstream Tasks\"></a>Downstream Tasks</h1><p>本文选择了以下四个下游任务来测试预训练表示的通用性和有效性。实验数据用的是MultiWoz，其中8422个对话用于训练，1000个用于验证，另外1000个用于测试。</p>\n<ul>\n<li>预测对话状态：这是一个多分类任务，给定对话历史，预测当前的对话状态。对话状态由27种实体的1784个值的one-hot向量拼接而成。这个任务度量了系统维护完整且准确的对话上下文状态表示的能力。由于输出有1784维，这就要求预训练的对话历史表示模型必须有足够强的概括性，才能对对话状态进行准确的预测。</li>\n<li>预测对话行为：与上个任务类似，是一种多分类任务。给定对话历史，预测系统下一步可能采取的动作，输出是一个32维的对话行为向量。</li>\n<li>生成下一句话</li>\n<li>检索下一句话</li>\n</ul>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>每个模型都训练了15个epoch，选择在验证集上表现最好的模型用于测试。实验中所用参数如下：<br><img src=\"https://img-blog.csdnimg.cn/20200913193940733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为了更直接的评估预训练过程中目标设置不同的差异，这里的预训练和fine-tune都是在同一数据集上进行的。在完整数据集上的表现：<br><img src=\"https://img-blog.csdnimg.cn/20200913194136216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>该实验是为了测试预训练是否对下游任务有用。表中的第一行对每个任务的模型进行随机初始化。如表一所示，预训练表示展示出了它的有效性和通用性。通过非监督的预训练，模型产生的对话表示提升了很多下游任务的性能。通用性体现在这些下游任务都受益于相同的预训练模型。</p>\n<p>在对话行为预测（DAP）和下一句话生成（NUG）任务上，以识别不一致语句（InI）为目标的预训练模型效果最好。这可能是因为在序列生成模型中全局上下文表示和局部话语表示同样重要。</p>\n<p>在对话行为预测（DAP）任务上，以识别不一致语句（InI）和根据mask的对话历史检索下一句话（MUR）的得分都远远高于基线和其他方法，这可能是因为这两种方法都是训练来学习每个话语的表示，而不仅仅是一个整体的上下文表示。</p>\n<p>在检索下一句话（NUR）任务上，以生成下一句话（NUG）为目标进行预训练时效果最好，这可能是因为生成下一个话语必须捕获的信息与检索下一个话语所需的信息类似。</p>\n<ul>\n<li>本文设置了实验观察预训练表示对下游任务在收敛性上的影响。实验证明，预训练过的模型能更快地收敛到更好的效果。</li>\n<li>一个好的预训练模型应该在下游任务中仅有少量数据的微调的情况下，也能达到很好地效果。本文做了实验验证在微调数据仅有(1%, 2%, 5%, 10% and 50%)时，在下游任务上的表现。</li>\n<li>该实验模拟了在下游任务中添加新域时的场景，假设在所有领域都存在大量的无监督的未标记数据，而对于下游任务仅有50个（0.1%）域内的标记数据和1000个（2%）新域的标注数据。在域内数据上做测试，实验证明预训练模型产生了更通用的表示，并促进了域的泛化。</li>\n</ul>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><p>在这篇文章中，提到了4种无监督的预训练目标来学习对话的上下文的表示，并在有限的微调数据和域外数据的条件下，证明了预训练模型对于提高下游任务性能方面的有效性。其中根据mask的对话历史检索下一句话和不一致语句识别是本文提出的两种新的目标。</p>\n<p>在本文中，无监督预训练被证明能够有效地学习对话上下文的表示。也就是说在有大量未标记的对话数据时，可以采取本文中的几种方法进行预训练。尤其是在标注数据量比较少的情况下。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","无监督学习","预训练"]},{"title":"论文阅读笔记：对NMT架构的超参数首次进行大规模消融实验分析","url":"/Paper-Reading/19ca8a5a5db8/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Massive Exploration of Neural Machine Translation Architectures<br>原文链接：<a href=\"https://arxiv.org/pdf/1703.03906.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>在计算机视觉中通常会在大型超参数空间中进行扫描，但对于NMT模型而言，这样的探索成本过高，从而限制了研究人员完善的架构和超参数选择。更改超参数成本很大，在这篇论文中，展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。本文工作探索NMT架构的常见变体，并了解哪些架构选择最重要，同时展示所有实验的BLEU分数，perplexities，模型大小和收敛时间，包括每个实验多次运行中计算出的方差数。论文主要贡献如下：</p>\n<ul>\n<li>展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。例如，深层编码器比解码器更难优化，密度残差连接比常规的残差连接具有更好的性能，LSTM优于GRU，并且调整好的BeamSearch对于获得最新的结果至关重要。</li>\n<li>确定了随机初始化和细微的超参数变化对指标（例如BLEU）的影响程度，有助于研究人员从随机噪声中区分出具有统计学意义的结果。</li>\n<li>发布了基于TensorFlow的<a href=\"https://github.com/google/seq2seq/\">开源软件包</a>，该软件包专为实现可再现的先进sequence-to-sequence模型而设计。</li>\n</ul>\n<p>本篇论文使用的sequence-to-sequence结构如下，对应的编号是论文中章节阐述部分。<br><img src=\"https://img-blog.csdnimg.cn/20201203111623761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>编码器方法 $f_{enc}$ 将 $x=(x_1,…,x_m)$ 的源tokens序列作为输入，并产生状态序列 $h=(h_1,…,h_m)$。在base model中， $f_{enc}$ 是双向RNN，状态 $h_i$ 对应于由后向RNN和前向RNN的状态的concatenation，$h_i = [\\overrightarrow{h_i};\\overleftarrow{h_i}]$。解码器 $f_{dec}$ 是RNN，可根据 $h$ 预测目标序列 $y =(y_1,…,y_k)$ 的概率。根据解码器RNN中的循环状态 $s_i$、前一个单词 $y_{&lt;i}$ 和上下文向量 $c_i$ 预测每个目标token $y_i\\in 1,…V$ 的概率。上下文向量 $c_i$ 也称为attention向量，并计算为源状态的加权平均值。<br>$$c_i=\\sum_{j}a_{ij}h_j$$   $$a_{ij}=\\frac{\\hat{a}<em>{ij}}{\\sum_j\\hat{a}_{ij}}$$   $$\\hat{a}</em>{ij}=att(s_i,h_j)$$<br>这里的 $att(s_i,h_j)$用于计算编码器状态 $h_j$ 和解码器状态 $s_i$ 之间的unnormalized alignment score。在base model中，使用 $att(s_i,h_j)= \\left \\langle  W_hh_j,W_ss_i\\right \\rangle$，其中矩阵 $W$ 用于将源状态和目标状态转换为相同大小的表示形式。解码器输出固定大小为 $V$ 的词汇表上的分布。<br>$$P(y_i|y_1,…,y_{i-1},x)=softmax(W[s_i,c_i]+b)$$<br>通过使用随机梯度下降来最小化目标词的负对数似然性来对整个模型进行end-to-end训练。</p>\n<p>在以下每个实验中，除了要研究的一个超参数外，基线模型的超参数都保持恒定。以此避免各种超参数更改的影响。当然，此过程并未考虑到超参数之间的相互作用，所以当作者认为这种相互作用很可能发生时，进行了额外的实验。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>论文为了简洁起见，在下面的表中仅报告均值BLEU，标准差，括号中的最高BLEU和模型大小。</p>\n<h3 id=\"Embedding维数\"><a href=\"#Embedding维数\" class=\"headerlink\" title=\"Embedding维数\"></a>Embedding维数</h3><p>我们期望更大的嵌入维数得到更好的BLEU得分，或者至少降低perplexities，但是作者发现情况并非总是如此，下表显示了2048维嵌入效果总体上最佳，但是提升的幅度很小。<br><img src=\"https://img-blog.csdnimg.cn/20201203194252399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>表中发现甚至很小的128维嵌入效果也出乎意料地出色，而收敛速度几乎快一倍。<br><strong>结论</strong></p>\n<ul>\n<li>嵌入维数的大小对梯度更新没有显着差异，并且无论大小如何，对嵌入矩阵的梯度更新的范数在整个训练过程中大致保持恒定。</li>\n<li>没有观察到较大嵌入维数的过度拟合，并且在整个实验中训练的log<br>perplexity大致相等，这表明该模型没有有效利用额外的参数，可能需要更好的优化技术。</li>\n</ul>\n<h3 id=\"RNN-Cell-Variant\"><a href=\"#RNN-Cell-Variant\" class=\"headerlink\" title=\"RNN Cell Variant\"></a>RNN Cell Variant</h3><p>我们使用诸如GRU和LSTM之类的门控单元的动机是梯度消失问题，而使用vanilla RNN（序列分析）单元，深度网络无法通过多层和时间步长有效地传播信息和梯度。对于基于注意力的模型，作者认为解码器应该几乎能够完全基于当前输入和注意力上下文做出决策。我们始终将解码器状态初始化为零而不是传递编码器状态，这一事实支持了这一假设，这意味着解码器状态不包含有关编码源的信息。<br><img src=\"https://img-blog.csdnimg.cn/20201203195750132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>实验中，LSTM单元始终优于GRU单元，但由于softmax的原因，没有观察到两者之间在训练速度上存在较大差异。</li>\n<li>Vanilla-Dec无法像门控变体单元一样学习。这表明解码器确实在多个时间步长中通过其自己的状态传递信息，而不是仅依赖于注意力机制和当前输入。</li>\n</ul>\n<h3 id=\"Encoder-和-Decoder-的深度\"><a href=\"#Encoder-和-Decoder-的深度\" class=\"headerlink\" title=\"Encoder 和 Decoder 的深度\"></a>Encoder 和 Decoder 的深度</h3><p>一般而言，我们希望更深层的网络能比更浅层的网络表现更好。但先前的研究结果没有支撑这一结论，所以作者探讨了多达8层编码器和解码器在深度方面的影响。对于更深的网络，作者还尝试了两种残差连接的形式，以刺激梯度传播。作者在连续的层之间插入残差连接，当 $h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})$ 是 $l$ 层RNN在 $t$ 步的输入，则：<br>$$x_t^{(l+1)}=h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})+x_t^{(l)}$$<br>其中，$x_t^{(0)}$ 输入token的词嵌入。除此之外，还尝试另一种残差连接的变体，在此变体中，添加了从每一层到所有其他层的skip connections（跳跃连接，作用是在比较深的网络中，解决在训练的过程中 梯度爆炸 和 梯度消失 问题）<br>$$x_t^{(l+1)}=h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})+\\sum_{j=0}^lx_t^{(j)}$$<br>下表显示了在有和没有残差连接的情况下改变编码器和解码器深度的结果：<br><img src=\"https://img-blog.csdnimg.cn/20201203201626714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>没有明显的证据表明编码器深度必须超过两层，但发现具有残余连接的更深的模型在训练过程中发散的可能性更大。</li>\n<li>对于解码器而言，如果没有残差连接，不可能训练8层及以上的网络</li>\n<li>在整个深度解码器实验中，密度残差连接始终优于常规残差连接，并且在步数方面收敛得更快，如下图所示：</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201203202337587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"单向与双向编码器\"><a href=\"#单向与双向编码器\" class=\"headerlink\" title=\"单向与双向编码器\"></a>单向与双向编码器</h3><p>在这组实验中，探索了有无源输入的反向序列的单向编码器，因为这是一种常用技巧，它使编码器可以为较早的单词创建更丰富的表示形式。下表显示了双向编码器通常优于单向编码器，但差距不大：<br><img src=\"https://img-blog.csdnimg.cn/20201203202750532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>具有反向源输入的编码器始终优于其非反向源输入的编码器，但比较浅的双向编码器效果差。</li>\n</ul>\n<h3 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h3><p>两种最常用的注意力机制分别是加法式的Bahdanau注意力和乘法式的Luong注意力，公式分别如下：<br>$$score(h_j,s_i)=\\left \\langle v,tanh(W_1h_j+W_2s_i) \\right \\rangle$$    $$score(h_j,s_i)=\\left \\langle W_1h_j,W_2s_i \\right \\rangle$$<br>我们将 $W_1h_j$ 和 $W_2s_i$ 的维数称为“注意维数”，并通过更改层大小将其从128更改为1024。作者还通过使用最后一个编码器状态初始化解码器状态（None-State），或将最后一个解码器状态与每个解码器输入连接来使用无注意机制（None-Input）进行实验，结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20201203203732691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>加法式注意机制略微但始终优于乘法式注意力机制，而注意维数的影响很小。</li>\n<li>“Non-Input”表现很差，因为它们在每个时间步都可以访问编码器信息。</li>\n<li>发现在整个训练过程中，基于注意力的模型对解码器状态表现出明显更大的梯度更新。</li>\n</ul>\n<h3 id=\"Beam-Search策略\"><a href=\"#Beam-Search策略\" class=\"headerlink\" title=\"Beam Search策略\"></a>Beam Search策略</h3><p>下表显示了改变Beam Search大小和增加长度归一化惩罚的影响：<br><img src=\"https://img-blog.csdnimg.cn/20201203204652203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p><strong>结论</strong></p>\n<ul>\n<li>即使有长度损失，很大的Beam Search性能也比较小的Beam Search差，所以选择正确的Beam Search大小对于获得最佳结果至关重要。</li>\n<li>发现非常大的beam大小会产生较差的结果，并且存在最佳beam大小。</li>\n</ul>\n<h3 id=\"系统比较\"><a href=\"#系统比较\" class=\"headerlink\" title=\"系统比较\"></a>系统比较</h3><p>最后，作者将在newstest2013验证集上选择的所有实验中性能最佳的模型（具有512维加法式注意力的base model）与下表中其他模型的历史结果进行了比较。<br><img src=\"https://img-blog.csdnimg.cn/20201203205123391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>下表是综合了所有hyperparameter的最佳方案配置：<br><img src=\"https://img-blog.csdnimg.cn/20201203205301354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>在这篇论文中，展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。总结以上的重要结论如下：</p>\n<ul>\n<li>2048维的嵌入维度效果最佳，但提升幅度很小，即使嵌入维度为128也似乎有足够的能力来捕获大多数必要的语义信息。</li>\n<li>LSTM单元始终优于GRU单元</li>\n<li>具有2至4层的双向编码器效果最佳，对于更深的编码器训练起来更加不稳定，但是如果可以很好地优化它们，则有着很好的潜力。</li>\n<li>深度为4层的解码器略胜于浅层解码器。残差连接对于训练具有8层解码器是必需的，而密集的残差连接则提供了额外的鲁棒性。</li>\n<li>加法式注意力机制产生了总体最佳结果。</li>\n<li>表现良好的Beam Search使用长度惩罚是至关重要的，beam大小为5到10，长度惩罚为1.0似乎效果很好。</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","TensorFlow","NMT","seq2seq"]},{"title":"论文阅读笔记：应用了Transformer的Encoder的DAM检索式多轮对话系统","url":"/Paper-Reading/6ed7dab2ef88/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network<br>原文链接：<a href=\"https://www.aclweb.org/anthology/P18-1103.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>聊天机器人中的一项重要任务是响应选择，其目的是在给定对话上下文的情况下，从一组候选响应中选择最匹配的响应。除了在基于检索的聊天机器人中发挥关键作用外，响应选择模型还可以用于对话生成的自动评估以及基于GAN（生成对抗网络）神经对话生成的判别器。之所以使用更丰富的上下文信息，是因为人为产生的响应在很大程度上取决于语义和场景上的不同粒度（单词，词组，句子等）的先前对话段。下图展示跨上下文和响应的段对之间的语义连接。<br><img src=\"https://img-blog.csdnimg.cn/20201122202203122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如图所示，在上下文和响应中，通常存在两种匹配的段对，它们的粒度不同：</p>\n<ul>\n<li>表面文本的相关性，例如单词“packages”-“package”和短语“debian package manager”-“debian package manager”的词法重叠。</li>\n<li>这些片段在语义/功能上有着彼此相关的潜在依赖关系。例如，响应中的“it”一词在上下文中指“dpkg”，以及响应中的短语“its just reassurance”，它潜在地指向“what packages are installed on my system”。</li>\n</ul>\n<p>先前的研究表明，在上下文和响应中以不同的粒度捕获那些匹配的片段对是多回合响应选择的关键，如Multi-view和SMN。然而这些对话系统只考虑了表面的文本关联性（surface text relevance），且多采用RNN结构，这会极大的增加模型的推理代价，因此本文提出了基于注意力机制的结构。本文使用完全基于注意力的依赖项信息来研究将响应与多回合上下文匹配的过程，受到Transformer的影响，本文通过两种方式扩展注意力机制来获取表示和匹配信息：</p>\n<ul>\n<li><strong>self-attention</strong>：只使用堆叠的自注意力构造不同粒度的文本段表示，这样我们可以捕获其词级内的依存关系。self-attention其实就是Transformer的Encoder层（没有用Multi-Head），其中 $Q$ 、 $K$ 、$V$ 都是一样的，要么都是response、要么都是utterance。从word embedding开始堆叠self-attention层，每一层self-attention层抽取一种粒度的表示，不同层就抽取了不一样的表示，从而获得句子多粒度的表示。</li>\n<li><strong>cross-attention</strong>：尝试整个上下文和响应的注意力提取真正匹配的句段对。结构上还是Transformer的Encoder层，只不过输入不一样了。其中 $Q$ 是response（utterance），而 $K$ 、 $V$ 是utterance（response）。这样做的话，其实就像用response去表示utterance，用utterance去表示response，从而为response和utterance做匹配提供更多的信息。</li>\n</ul>\n<p>本文在一个统一的神经网络中介绍了这两种注意力，网络命名为Deep Attention Matching Network（DAM），用于多回合响应选择。在实践中，DAM将上下文或响应中的语句的每个词作为抽象语义段的中心含义，并通过堆叠式的自注意力丰富其表示，从而逐渐围绕中心词生成越来越复杂的段表示 。考虑到文本相关性和依存性信息，上下文和响应中的每个语句都基于不同粒度的句段对进行匹配。这样，DAM通常会捕获上下文之间的匹配信息以及从单词级到句子级的响应，然后使用卷积和最大池化操作提取重要的匹配特征，最后通过单层感知器将其融合为一个匹配分数。更重要的是，由于大多数注意力计算可以完全并行化，因此DAM有望在实践中方便部署。</p>\n<h1 id=\"模型细节\"><a href=\"#模型细节\" class=\"headerlink\" title=\"模型细节\"></a>模型细节</h1><h2 id=\"问题符号化\"><a href=\"#问题符号化\" class=\"headerlink\" title=\"问题符号化\"></a>问题符号化</h2><p>给定一个对话数据集 $D={(c,r,y)z}_{Z=1}^N$，其中 $c={u_0,…u_{n-1}}$ 表示一个对话上下文，其中 ${u_i}_{i=0}^{n-1}$ 作为语句和 $r$ 作为候选响应。$y\\in{0,1}$ 是一个二进制标签，指示 $r$ 是否是对 $c$ 的合适响应。我们的目标是学习与 $D$ 匹配的模型$g(c,r)$，该模型可以测量上下文 $c$ 和候选响应 $r$ 之间的相关性。</p>\n<h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h2><p>下图展示了DAM的模型结构，该模型通过representation-matching-aggregation框架来将响应与多回合上下文进行匹配。对于上下文中的每个语句 $u_i=[w_{u_i,k}]<em>{k=0}^{n</em>{u_i}-1}$ 和它的候选响应 $r=[w_{r,t}]<em>{t=0}^{n_r-1}$（其中，$n_{u_i}$ 和 $n_r$ 代表单词数），DAM首先查找共享的单词嵌入表，并将 $u_i$ 和 $r$ 表示为单词嵌入序列，即 $U_i^0=[e</em>{u_i,0}^0,…,e_{u_i,n_{u_i}-1}^0]$ 和 $R^0=[e_{r,0}^0,…,e_{r,n_r-1}^0]$，其中 $e\\in\\mathbb{R}^d$ 表示 $d$ 维词嵌入。<br><img src=\"https://img-blog.csdnimg.cn/20201122220222537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>接下来，通过表示模块为 $u_i$ 和 $r$ 在不同的粒度上构造语义表示。实现方式是通过 $L$ 个相同的自注意层堆叠，每个第 $l$ 个自注意层都将第 $l-1$ 层的输出作为其输入，从而将输入语义向量合成为更复杂的表示形式。通过这种方式，逐渐构建了 $u_i$ 和 $r$ 的多粒度表示，分别表示为 $[U_i^l]<em>{l=0}^L$ 和 $[R^l]</em>{l=0}^L$。</p>\n<p>给定 $[U_i^0,…,U_i^L]$ 和 $[R^0,…,R^L]$，语句 $u_i$ 和响应 $r$ 随后以segment-segment相似矩阵的方式相互匹配。实际上，对于每个粒度 $l\\in[0…L]$，构造两种匹配矩阵，即self-attention匹配 $M_{self}^{u_i,r,l}$ 和cross-attention匹配 $M_{corss}^{u_i,r,l}$，这样便分别使用文字信息和依存关系信息测量话语和回应之间的相关性。这些匹配分数最终被合并为 $3D$ 的匹配 $Q^1$。$Q$ 的每个维度表示<strong>each utterance in context, each word in utterance and each word in response</strong>。然后，通过使用最大池操作进行卷积，提取跨多回合上下文的段对之间的重要匹配信息和候选响应，然后通过单层感知器将其进一步融合为一个匹配分数，代表候选响应与整体上下文之间的匹配程度。</p>\n<p>注意力，我们使用一个共享组件，即“注意力模块”来实现表示中的self-attention和匹配中的cross-attention。在以下各节中，我们将详细讨论注意力模块的实现以及如何使用它来实现self-attention和cross-attention</p>\n<h2 id=\"注意力模块-Attentive-Module\"><a href=\"#注意力模块-Attentive-Module\" class=\"headerlink\" title=\"注意力模块-Attentive Module\"></a>注意力模块-Attentive Module</h2><p>下图显示了Attentive模块的结构，与Transformer中使用的模块相似。<br><img src=\"https://img-blog.csdnimg.cn/20201122221757924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>注意力模块具有三个输入语句：$Q$，$K$，$V$，即$Q=[e_{i=0}^{n_Q-1}]$，$K=[e_i]<em>{i=0}^{n_K-1}$，$V=[e_i]</em>{i=0}^{n_V-1}$，其中，其中 $n_Q$，$n_K$ 和 $n_V$ 表示每个句子中的单词数，而 $e_i$ 表示维数嵌入，$n_K$ 等于 $n_V$。注意力模块首先通过Scaled Dot-Product Attention将查询语句中的每个单词与关键语句中的单词关联，然后将结果应用于值语句，定义为：<br>$$Att(Q,K)=[softmax(\\frac{Q[i]\\cdot K^T}{\\sqrt{d}})]<em>{i=0}^{n_Q-1}$$  $$V_{att}=Att(Q,K)\\cdot V\\in \\mathbb{R}^{n_Q\\times d}$$<br>其中 $Q[i]$是查询语句 $Q$ 中的第 $i$ 个嵌入，$V_{att}$ 的每一行（表示为 $V</em>{att}[i]$）存储值语句中可能与查询语句中的第 $i$ 个单词相关的单词的语义信息。对于每个 $i$，将 $V_{att}[i]$ 和 $Q[i]$ 加在一起，将它们组合成一个包含其联合含义的新表示形式。然后应用层归一化操作，可防止梯度消失或爆炸。接着将具有RELU的前馈网络FFN应用于标准化结果，以便进一步处理融合嵌入，定义为：<br>$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$<br>其中，$x$ 是与查询语句 $Q$ 形状相同的 $2D$ 张量，$W_1$，$b_1$，$W_2$，$b_2$ 是学习参数。$FFN(x)$ 的结果是一个具有与 $x$ 相同形状的 $2D$ 张量，然后将 $FFN(x)$ 残留连接添加到 $x$，最后将融合结果标准化为最终输出。将整个注意力模块表示为：<br>$$AttentiveModule(Q,K,V)$$</p>\n<p>如上所述，注意力模块可以捕获查询语句和关键语句之间的依存关系，并进一步使用依存关系信息将查询语句和值语句中的元素合成为组成表示形式。我们利用Attentive Module的此属性来构造多粒度语义表示以及与依赖项信息的匹配。</p>\n<h2 id=\"整理表示\"><a href=\"#整理表示\" class=\"headerlink\" title=\"整理表示\"></a>整理表示</h2><p>给定 $U_i^0$ 或 $R^0$ （语句 $u_i$ 或响应 $r$ 的单词级嵌入表示），DAM将 $U_i^0$ 或 $R^0$ 用作输入，并分层堆叠Attentive模块以构造 $u_i$ 和 $r$ 的多粒度表示，即表示为：<br>$$U_i^{l+1}=AttentiveModule(U_i^l,U_i^l,U_i^l)$$  $$R^{l+1}=AttentiveModule(R^l,R^l,R^l)$$<br>其中 $l$ 的范围是 $0$ 到 $L − 1$，表示不同的粒度级别。通过这种方式，每个话语或响应中的单词会反复发挥作用，以合成越来越多的整体表示形式，我们将这些多粒度表示形式表示为 $U_i^0,…,U_i^L$ 和 $R^0,…,R^L$ 之后。</p>\n<h2 id=\"Utterance-Response-匹配\"><a href=\"#Utterance-Response-匹配\" class=\"headerlink\" title=\"Utterance-Response 匹配\"></a>Utterance-Response 匹配</h2><p>给定 $[U_i^l]<em>{l=0}^L$ 和 $[R^l]</em>{l=0}^L$ ，在每个粒度级别 $l$ 上构造了两种segment-segment匹配矩阵，即self-attention匹配 $M_{self}^{u_i,r,l}$ 和cross-attention匹配 $M_{corss}^{u_i,r,l}$。其中， $M_{self}^{u_i,r,l}$ 被定义为：<br>$$M_{self}^{u_i,r,l}={U_i^l[k]^T\\cdot R^l[t]}<em>{n</em>{u_i}\\times n_r}$$<br>其中，矩阵中的每个元素都是 $U_i^l[k]$ 和 $R^l[t]$ 的点积，第 $k$ 个嵌入在 $U_i^l$ 中，第 $t$ 个嵌入在 $R^l$ 中，反映了 $u_i$中 第 $k$ 个分段和 $r$ 中第 $t$ 个分段之间，在 第$l$ 层粒度的相关性。 cross-attention匹配矩阵基于cross-attention，其定义为：<br>$$\\tilde{U}<em>i^l=AttentiveModule(U_i^l,R^l,R^l)$$  $$\\tilde{R}^l=AttentiveModule(R^l,U_i^l,U_i^l)$$  $$M_{corss}^{u_i,r,l}={\\tilde{U}_i^l[k]^T\\cdot \\tilde{R}^l[t]}</em>{n_{u_i}\\times n_r}$$<br>其中，我们使用注意力模块将 $U_i^l$ 和 $R^l$ 相互交叉，分别为它们构造两个新的表示形式，分别写为 $\\tilde{U}_i^l$ 和 $\\tilde{R}^l$。 $\\tilde{U}_i^l$ 和 $\\tilde{R}^l$ 都隐式捕获了跨语句和响应的语义结构信息。这样，那些相互依存的片段对在表示上彼此接近，并且那些潜在相互依存的对之间的点积可以增加，从而提供了依赖于感知的匹配信息。</p>\n<h2 id=\"Aggregation\"><a href=\"#Aggregation\" class=\"headerlink\" title=\"Aggregation\"></a>Aggregation</h2><p>在得到 $M_{self}$ 和 $M_{cross}$ 后，就需要将它们聚合起来，做法是将所有的 $2D$ 匹配矩阵聚合成一个大的 $3D$ 匹配图像 $Q$，具体的聚合方法就是将 $M_{self}$ 和 $M_{cross}$ 所有的矩阵排列起来，所以就增加了一个维度，新的维度（可以称之为深度）大小是 $2(L+1)$ ，具体公式如下：<br>$$Q={Q_{i,k,t}}<em>{n\\times n</em>{u_i}\\times n_r}$$<br>其中 $Q_{i,k,t}$ 可以表示为：<br>$$Q_{i,k,t}=[M_{self}^{u_i,r,l}[k,t]]<em>{l=0}^L\\oplus [M</em>{corss}^{u_i,r,l}[k,t]]_{l=0}^L$$</p>\n<p>聚合成 $3D$ 匹配图像后，采用了两次的 $3D$ 卷积和最大池化去提取特征，在实际试验中，第一次 $3D$ 卷积的输入通道数为 $2(L+1)$ ，输出通道数为 $32$，卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。第二次 $3D$ 卷积的输入通道数为 $32$，输出通道数为 $16$，卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。通过卷积和池化提取到特征后（用 $f_{match}(c,r)$ 表示提取后的特征），后面接一层线性层将维度转化成 $1$，用来表示匹配的分数，具体公式如下：<br>$$g(c,r)=\\sigma(W_3f_{match}(c,r)+b_3)$$</p>\n<p>其中 $W_3$ 和 $b_3$ 是学习参数，$\\sigma$ 是sigmoid函数，如果 $r$ 是 $c$ 的合适候选相应，则给出概率。DAM的损失函数为负对数似然，定义为：<br>$$p(y|c,r)=g(c,r)y+(1-g(c,r))(1-y)$$  $$L(\\cdot)=-\\sum_{(c,r,y)\\in D}log(p(y|c,r))$$</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>实验要求每个比较模型从给定对话上下文 $c$ 的 $n$ 个可用候选中选择 $k$ 个最匹配的响应，然后我们将在 $k$ 个选定的真实响应中的正确回复的召回率作为主要评估指标来计算，记为 $R_n@k=\\frac{\\sum_{i=1}^ky_i}{\\sum_{i=1}^ny_i}$，其中 $y_i$ 是每个候选项的二进制标签。除此之外，我们还使用MAP（Mean Average Precision），MRR (Mean Reciprocal Rank)，Precision-at-one P@1。</p>\n<h2 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h2><p>使用的词汇表和词嵌入大小和SMN模型一致。上下文中语句最大设置为 $9$，每个语句最多 $50$ 个单词，并使用word2vec进行词嵌入。我们使用零填充来处理可变大小的输入，并且将FFN中的参数设置为200，与词嵌入大小相同。 我们测试了$1-7$层自注意力层，其中 $5$ 层自注意力层在验证集上获得了最佳分数。试验中两次的 $3D$ 卷积和最大池化，第一次 $3D$ 卷积的卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。第二次 $3D$ 卷积的卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。我们使用adam优化器，学习率初始化为 $1e-3$，并在训练过程中逐渐降低，并且批大小为 $256$。</p>\n<p>下表显示DAM的评估结果以及所有比较模型：<br><img src=\"https://img-blog.csdnimg.cn/20201122233518572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>论文还使用Ubuntu语料通过数量分析和可视化分析DAM中的self-attention和cross-attention的工作方式。下图的左侧部分显示了在具有不同语句数量的上下文中，Ubuntu 语料上 $R_{10} @1$ 的变化，右侧提供了在具有不同平均语句长度和self-attention数量的性能比较：<br><img src=\"https://img-blog.csdnimg.cn/20201122233939735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图给出了第 $0$，第 $2$ 和第 $4$ self-attention匹配矩阵，第 $4$ cross-attention匹配矩阵的可视化结果，以及第4层中的self-attention和cross-attention的分布。<br><img src=\"https://img-blog.csdnimg.cn/20201122234258665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>模型存在的不足：</p>\n<ul>\n<li>模糊候选响应：候选响应虽然基本上适合于会话上下文，但是还有一些不适当的细节。</li>\n<li>逻辑错误：在给定的对话上下文，由于逻辑不匹配，候选响应不合适。</li>\n</ul>\n<p>作者认为在训练过程中生成对抗性示例而非随机抽样可能是解决模糊候选和逻辑错误的一个好主意，并且捕获隐藏在对话文本后的逻辑信息也值得在将来进行研究 。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Transformer","Paper","多轮对话","DAM","检索式"]},{"title":"论文阅读笔记：语音应用中Transformer与RNN的比较研究","url":"/Paper-Reading/8e77a991aae6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：A Comparative Study on Transformer vs RNN in Speech Applications<br>原文链接：<a href=\"https://arxiv.org/pdf/1909.06317.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>序列到序列模型已广泛用于端到端语音处理中，例如自动语音识别（ASR），语音翻译（ST）和文本到语音（TTS）。本文着重介绍把Transformer应用在语音领域上并与RNN进行对比。与传统的基于RNN的模型相比，将Transformer应用于语音的主要困难之一是，它需要更复杂的配置（例如优化器，网络结构，数据增强）。在语音应用实验中，论文研究了基于Transformer和RNN的系统的几个方面，例如，根据所有标注数据、训练曲线和多个GPU的可伸缩性来计算单词/字符/回归错误。本文的几个主要贡献：</p>\n<ul>\n<li>将Transformer和RNN进行了大规模的比较研究，尤其是在ASR相关任务方面，它们具有显着的性能提升。</li>\n<li>提供了针对语音应用的Transformer的训练技巧：包括ASR，TTS和ST</li>\n<li>在开放源代码工具包<a href=\"https://github.com/espnet/espnet\">ESPnet</a>中提供了可复制的端到端配置和模型，这些配置和模型已在大量可公开获得的数据集中进行了预训练。</li>\n</ul>\n<h1 id=\"端到端RNN\"><a href=\"#端到端RNN\" class=\"headerlink\" title=\"端到端RNN\"></a>端到端RNN</h1><p>如下图中，说明了实验用于ASR，TTS和ST任务的通用S2S结构。<br><img src=\"https://img-blog.csdnimg.cn/20201123224519278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br> S2S包含两个神经网络：其中编码器如下：<br> $$(1):X_0=EncPre(X)$$   $$(2):X_e=EncBody(X_0)$$<br> 解码器如下：<br> $$(3):Y_0[1:t-1]=DecPre(Y[1:t-1])$$   $$(4):Y_d[t]=DecBody(X_e,Y_0[1:t-1])$$    $$(5):Y_{post}[1:t]=DecPost(Y_d[1:t])$$</p>\n<p>其中 $X$ 是源序列，例如，语音特征序列（对于ASR和ST）或字符序列（对于TTS），$e$ 是EncBody层数，$d$ 是DecBody中的层数，$t$ 是目标帧索引，以上等式中的所有方法均由神经网络实现。对于解码器输入 $Y [1：t − 1]$，我们在训练阶段使用一个真实标注的前缀，而在解码阶段使用一个生成的前缀。在训练过程中，S2S模型学习是将在生成的序列 $Y_{post}$ 和目标序列 $Y$ 之间标量损失值最小化：<br>$$(6):L=Loss(Y_{post},Y)$$<br>本节的其余部分描述了基于RNN的通用模块：“EncBody”和“DecBody”。而将“EncPre”，“DecPre”，“DecPost”和“Loss”视为特定于任务的模块，我们将在后面的部分中介绍。</p>\n<p>等式(2)中的EncBody将源序列 $X_0$ 转换为中间序列 $X_e$，现有的基于RNN的EncBody实现通常采用双向长短记忆（BLSTM）。对于ASR，编码序列 $X_e$ 还可以在进行联合训练和解码中，用基于神经网络的时序类分类（CTC）进行逐帧预测。</p>\n<p>等式(4)中的DecBody()将生成具有编码序列 $X_e$ 和目标前缀 $Y_0 [1：t − 1]$ 的前缀的下一个目标帧。对于序列生成，解码器通常是单向的。 例如，具有注意力机制的单向LSTM通常用于基于RNN的DecBody()实现中。该注意力机制计算逐帧权重，以将编码后的帧 $X_e$ 求和，并作为要以前缀 $Y0 [0：t-1]$ 进行转换的逐帧目标向量，我们称这种注意为“encoder-decoder attention”</p>\n<h1 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h1><p>Transformer包含多个dot-attention层：<br>$$(7):att(X^q,X^k,X^v)=softmax(\\frac{X^qX^{kT}}{\\sqrt{d^{att}}})X^v$$<br>其中 $X^k,X^v\\in \\mathbb{R}^{n^k\\times d^{att}}$ 和 $X^q\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是attention层的输入，$d^{att}$是特征维数，$n^q$是 $X^q$ 的长度，$n^k$ 是 $X^k$ 和 $X^v$ 的长度。我们将 $X^qX^{kT}$ 称为“attention matrix”。多头注意力机制如下：<br>$$(8):MHA(Q, K, V )=[H_1,H_2,…,H_{d^{head}}]W^{head}$$  $$(9):H_h=att(QW_h^q,KW_h^k,VW_h^v)$$<br>其中， $K,V \\in \\mathbb{R}^{n^k\\times d^{att}}$ 和 $Q\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是 $MHA$ 层的输入，$H_h\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是第 $h$ 个attention层的输出$(h =1,…,dhead )$。$W_h^q,W_h^k,W_h^v \\in \\mathbb{R}^{d^{att}\\times d^{att}}$和 $W^{head} \\in \\mathbb{R}^{d^{att}d^{head}\\times d^{att}}$ 是可学习的权重矩阵，而 $d^{head}$ 是该层中的注意力头数。</p>\n<p>我们定义用于等式(2)的基于Transformer的EncBody()，如下所示：<br>$$(10):X_i^{‘}=X_i+MHA_i(X_i,X_i,X_i)$$   $$X_{i+1}=X_i^{‘}+FF_i(X_i^{‘})$$</p>\n<p>其中 $i = 0,…,e − 1$ 是编码器层的索引，而 $FF_i$ 是第 $i$ 个两层前馈网络。<br>$$(12):FF(X[t])=ReLU(X[t]W_1^{ff}+b_1^{ff})W_2^{ff}+b_2^{ff}$$<br>其中 $X[t]\\in \\mathbb{R}^{d^{att}}$ 是输入序列 $X$ 的第 $t$ 帧，$W_1^{ff} \\in \\mathbb{R}^{d^{att}\\times d^{ff}},W_2^{ff} \\in \\mathbb{R}^{d^{ff}\\times d^{att}}$ 是可学习的权重矩阵，$b_1^{ff}\\in \\mathbb{R}^{d^{ff}},b_2^{ff}\\in \\mathbb{R}^{d^{att}}$ 是可学习的偏差向量。等式(10)中的 $MHA_i(X_i,X_i,X_i)$ 为“self-attention”</p>\n<p>用于等式(4)的基于Transformer的DecBody()包含两个attention模块：<br>$$(12):Y_j[t]^{‘}=Y_j[t]+MHA_j^{self}(Y_j[t],Y_j[1:t],Y_j[1:t])$$   $$Y_j[t]^{‘’}=Y_j+MHA_j^{src}(Y_j^{‘},X_e,X_e)$$   $$Y_{j+1}=Y_j^{‘’}+FF_j(Y_j^{‘’})$$<br>其中 $j = 0,…,d − 1$ 是解码器层的索引，我们将 $MHA_j^{src}(Y_j^{‘},X_e,X_e)$ 中的解码器输入与编码器输出之间的attention矩阵称为“encoder-decoder attention”。因为单向解码器对于序列生成很有用，所以它在第 $t$ 个目标帧处的attention矩阵被屏蔽，因此它们不会与 $t$ 之后的帧进行连接。可以使用带有三角形二进制矩阵的元素乘积并行地进行序列mask。 因为它不需要顺序操作，所以它提供了比RNN更快的实现。</p>\n<p>Transformer采用正弦位置编码：<br>$$PE[t]=\\left{\\begin{matrix} sin\\frac{t}{10000^{t/d^{att}}} &amp; 当t是偶数\\ cos\\frac{t}{10000^{t/d^{att}}} &amp; 当t是奇数 \\end{matrix}\\right.$$<br>在EncBody()和DecBody()模块之前，输入序列 $X_0,Y_0$ 与（$PE[1],PE[2],…$）串联在一起。</p>\n<h1 id=\"Transformer应用于ASR\"><a href=\"#Transformer应用于ASR\" class=\"headerlink\" title=\"Transformer应用于ASR\"></a>Transformer应用于ASR</h1><p>S2S从log-mel滤波器组语音特征的输入序列 $X^{fbank}$ 预测字符或SentencePiece的目标序列 $Y$。ASR中的源 $X$ 表示为具有音高特征的83维log-mel滤波器阵列帧的序列。首先，EncPre()通过使用具有256维，步长为2且内核大小为3的两层CNN或类似于VGG的最大池化的两层CNN，将源序列 $X$ 转换为子采样序列 $X_0\\in \\mathbb{R}^{n^{sub}\\times d^{att}}$，其中 $n^{sub}$ 是CNN输出序列的长度。然后，EncBody()将 $X_0$ 转换为CTC和解码器的一系列编码特征 $X_e\\in \\mathbb{R}^{n^{sub}\\times d^{att}}$。</p>\n<p>解码器接收编码序列 $X_e$ 和目标序列 $Y[1:t-1]$：字符或SentencePiece。首先，等式(3)中的DecPre()将，进行词嵌入。接下来，DecBody()和单线性层DecPost()在给定 $X_e$ 和目标序列 $Y[1:t-1]$ 的情况下，预测下一个token，即 $Y_{post}[t]$ 的后验分布。解码器和CTC模块的后验分布预测为：$p_{s2s}(Y|X)$和 $p_{ctc}(Y|X)$，训练阶段两个loss加权求和：<br>$$L^{ASR}=-alogp_{s2s}(Y|X)-(1-a)logp_{ctc}(Y|X)$$</p>\n<blockquote>\n<p>In the decoding stage, the decoder predicts the next token given the speech feature X and the previous predicted tokens using beam search, which combines the scores of S2S, CTC and the RNN language model (LM)</p>\n</blockquote>\n<p>解码阶段除了ctc和s2s以外还需要一个语言模型<br>$$\\hat{Y}=argmax{\\lambda logp_{s2s}(Y|X_e)+(1-\\lambda)logp_{ctc}(Y|X_e)+\\gamma logp_{lm}(Y)}$$</p>\n<h1 id=\"Transformer应用于TTS\"><a href=\"#Transformer应用于TTS\" class=\"headerlink\" title=\"Transformer应用于TTS\"></a>Transformer应用于TTS</h1><p>TTS中编码器的输入是包含EOS符号的token序列，首先，将token序列进行词嵌入，然后将通过权重参数缩放的位置编码添加到向量中，即等式(1)的EncPre()实现。随后，等式(2)中的编码器EncBody()将这个输入序列转换为解码器的编码特征序列。TTS中解码器的输入是一系列编码器特征和log-mel滤波器组特征。在训练中，标记数据log-mel滤波器组特征以teacher forcing方式使用，而推理中，预测特征以自回归方式使用。</p>\n<p>解码器中，作为等式(3)中的DecPre()的TTS实现，Prenet将80维log-mel滤波器组特征的目标序列转换为隐藏特征序列。该网络由两个具有256个单位的线性层，一个ReLU激活函数和一个dropout组成，后面是具有 $d^{att}$ 单位的投影线性层。然后在等式(4)中的解码器DecBody()，其架构与编码器相同，将编码器特征和隐藏特征的序列转换为解码器特征的序列。对 $Y_d$ 的每一帧应用两个线性层以分别计算目标特征和EOS的概率。最后，将Postnet应用于预测目标特征的序列。Postnet是一个五层的CNN，其每一层都是一维卷积，具有256个通道，内核大小为5，然后进行批量归一化，tanh激活函数和dropout。这些模块是等式(5)中DecPost()的TTS实现。</p>\n<p>在TTS训练中，优化了整个网络，以最大程度地减少TTS中的两个损失功能。</p>\n<ul>\n<li>目标特征的L1损失</li>\n<li>EOS概率的二元交叉熵（BCE）损失</li>\n<li>guided attention loss </li>\n</ul>\n<p>为了解决BCE计算中类别不平衡的问题，对正样本使用恒定权重。除此之外，作者表示没有引入任何超参数来平衡这三个损耗值。推理中，模型以自回归方式预测下一帧的目标特征，并且如果EOS的概率变得高于某个阈值（例如0.5），则模型将停止预测。</p>\n<h1 id=\"ASR相关的实验以及调参方法\"><a href=\"#ASR相关的实验以及调参方法\" class=\"headerlink\" title=\"ASR相关的实验以及调参方法\"></a>ASR相关的实验以及调参方法</h1><p><strong>下表中，作者总结了在ASR实验中使用的15个数据集</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201124103745154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>实验配置如下</strong>：<br>除了最大的LibriSpeech（$d^{head}= 8,d^{att} = 512$）外，我们对每个语料库的Transformer采用了相同的架构配置（$e = 12,d = 6,d^{ff} = 2048,d^{head} = 4,d^{att} = 256$）。</p>\n<ul>\n<li>Transformer更快收敛<blockquote>\n<p>Transformer requires a different optimizer configuration from RNN because Transformer’s training iteration is eight times faster and its update is more fine-grained than RNN. </p>\n</blockquote>\n</li>\n<li>参方法主要参考之前论文方法，参数是最后10个epoch的模型的求平均<blockquote>\n<p>To train Transformer, we basically followed the previous literature (e.g., dropout, learning rate, warmup steps). We did not use development sets for early stopping in Transformer. We simply ran 20 – 200 epochs (mostly 100 epochs) and averaged the model parameters stored at the last 10 epochs as the final model.</p>\n</blockquote>\n</li>\n<li>解码阶段Transformer和RNN用相同的配置。<blockquote>\n<p>In the decoding stage, Transformer and RNN share the same configuration for each corpus, for example, beam size (e.g., 20 – 40), CTC weight λ (e.g., 0.3), and LM weight γ (e.g., 0.3 – 1.0)</p>\n</blockquote>\n</li>\n</ul>\n<p>如下表中展示根据每个语料库的字符/单词错误率（CER / WER）的ASR结果，非常好的实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201124105806182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><strong>从图中我们可以得出结论</strong>：</p>\n<ul>\n<li>Transformer更大的minibatch效果更好<blockquote>\n<p>We observed that Transformer trained with a larger minibatch became more accurate while RNN did not.</p>\n</blockquote>\n</li>\n<li>8倍速于RNN<blockquote>\n<p>In this task, Transformer achieved the best accuracy provided by RNN about eight times faster than RNN with a single GPU.</p>\n</blockquote>\n</li>\n</ul>\n<p>下表总结了LibriSpeech ASR基准，因为它是最具竞争力的任务之一。<br><img src=\"https://img-blog.csdnimg.cn/20201124105628908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图显示了在LibriSpeech上使用多个GPU获得的ASR训练曲线：<br><img src=\"https://img-blog.csdnimg.cn/20201124105134691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><strong>训练技巧</strong>：</p>\n<ul>\n<li>扩大minibatch的大小</li>\n<li>使用accumulating gradient strategy</li>\n<li>使用dropout</li>\n<li>使用数据增强方法可以在很大程度上提升性能</li>\n<li>解码阶段RNN和Transformer在相同参数集上性能都比较好 </li>\n</ul>\n<p>下图清楚地表明，在9种语言中，Transformer明显优于RNN：<br><img src=\"https://img-blog.csdnimg.cn/20201124110919724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>TTS中Transformer的配置为：$e=6, d=6, d^{att}=384, d^{ff}=1536, d^{head}=4$，且系统的输入都是字符序列。下面两个图显示了两个语料库中的训练曲线：<br><img src=\"https://img-blog.csdnimg.cn/20201124111444848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下面两个图展示了生成的语音频谱图<br><img src=\"https://img-blog.csdnimg.cn/20201124111649703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201124111723982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>把Transformer应用在语音领域上与RNN对比的论文，结果也是比较喜人，重点是它在ESPnet上面开源了，模型、代码都给出来了，还给出了各种训练方法和技巧，是一篇实用性很强的文章。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","RNN","Transformer","语音"]},{"title":"论文阅读笔记：需要推理的MuTual多轮对话数据集","url":"/Paper-Reading/64d1bd606b34/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：MuTual: A Dataset for Multi-Turn Dialogue Reasoning<br>原文链接：<a href=\"https://arxiv.org/pdf/2004.04494.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>面向非任务的对话系统在给定上下文的情况下，当前系统能够产生相关且流畅的回复，但是由于推理能力较弱，有时会出现逻辑错误。为了促进对话推理研究，发布了多轮对话推理数据集 MuTual，针对性地评测模型在多轮对话中的推理能力。它由基于中国学生英语听力理解考试的8,860个手动注释的对话组成，数据集的<a href=\"https://github.com/Nealcly/MuTual\">GitHub仓库</a>。</p>\n<p>神经对话系统在大型对话语料库上进行训练，并用于预测给定上下文的回复。一般情况下，构建聊天机器人主要有两种解决方案：</p>\n<ul>\n<li>检索式的方法依赖文本匹配技术，在诸多候选回复中，选择匹配分数最高的作为回复；</li>\n<li>生成式的方法使用 Seq2Seq 模型，编码器读取对话历史，解码器直接生成相应回复。</li>\n</ul>\n<p>检索式的方法凭借回复相关性高，流利度高等优势，在工业界取得了更多的应用。不过，虽然在以BERT为代表的预训练模型，在检索式多轮对话任务中，已经基本接近了人类的表现。但实际应用中，当前的对话模型选择出的回复往往相关性较好，但是经常出现常识和逻辑错误等问题。由于现有的大部分检索式对话数据集都没有关注这种对话逻辑问题，导致评价指标也无法直接反映模型对对话逻辑的掌握程度。针对此问题，提出了多轮对话推理数据集 MuTual。</p>\n<p>相比现有的其他检索式聊天数据集，MuTual 要求对话模型具备常识推理能力；相比阅读理解式的推理数据集，MuTual 的输入输出则完全符合标准检索式聊天机器人的流程。因此，MuTual 也是目前最具挑战性的对话式数据集。由于任务不同，目前现有的推理数据集并不能直接帮助指导训练聊天机器人。下表为对话和基于推理的阅读理解的常用数据集：<br><img src=\"https://img-blog.csdnimg.cn/20201109234130781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>MuTual是第一个基于人标签推理的多轮对话数据集，用最佳方法在此数据集上运行的 $R@1$ 为71％，明显不如人类的表现（94％）。</p>\n<h1 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h1><p>原始的听力理解材料和问答对是由语言专家设计的，学生需要根据一段音频从三个选项中选择最佳答案，为了确保学生完全理解音频，大部分问题都需要具备推理能力。原始数据的格式设置为三元组 &lt;Conversation (audio),Question and Choices (text), Answer (image)&gt;<br><img src=\"https://img-blog.csdnimg.cn/20201110090755102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>听力考试要求学生根据一段双人多轮对话，回答额外提出的问题（图1左），并通过学生能否正确答对问题衡量学生是否理解了对话内容。为了更自然的模拟开放领域对话，我们进一步将听力题中额外的问题转化为对话中的回复（图1右）。标注者截选原对话中具备回答问题信息的片段，根据正确选项构造正确的回复（图1右回复 A），根据两个错误选项构造两个错误的回复（回复 C 和回复 D）。</p>\n<p>为了进一步提升难度，引入额外的推理信息，标注者还需根据正确选项构建一个负面的回复（回复 B）。另外，标注者需要保证在无上文信息情况下，所有候选回复在逻辑上皆合理。这样可以让数据集聚焦于检测模型在多轮对话中的推理能力，而非判断单个句子是否具有逻辑性。作者还在标注过程中控制正确和错误的回复与上文的词汇重叠率相似，防止模型可以通过简单的根据文本匹配选出候选回复。<br><img src=\"https://img-blog.csdnimg.cn/20201110091857685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上图是MuTual的详细统计汇总，直观上感觉词汇量比其他数据集要小得多，是由于MuTual是从英语作为外语的听力测试中修改而来的，因此形态和语法的复杂性比其他数据集要简单得多。</p>\n<p>为了评估不同推理类型的分布，我们注释了所涉及的特定推理类型，例如，从测试集中取样并将其分为六类。MuTual 数据集主要包含聊天机器人需要的六种推理能力：态度推理(13%)、数值推理(7%)、意图预测(31%)、多事实推理(24%)和常识等其他推理类型（9%）。</p>\n<ul>\n<li>态度推理（Attitude Reasoning）：这种类型的实例测试模型是否知道说话者对物体的态度。</li>\n<li>数值推理（Algebraic Reasoning）：这种类型的实例测试模型在选择回复时是否具备数值推理能力</li>\n<li>意图预测（Intention Prediction）：此类型测试模型是否可以预测说话者接下来要做什么</li>\n<li>多事实推理（Situational Reasoning）：在这种类型的实例中考虑情况信息（例如，位置，两个讲话者之间的关系），模型应从先前的上下文中挖掘隐式信息。</li>\n<li>常识等其他推理类型（Multi-fact Reasoning and Others）：在这种情况下，正确的响应与上下文中的多个事实有关，这要求模型深刻理解上下文，而不是简单地进行文本匹配。</li>\n</ul>\n<p>这六种类型的推理被认为与真正的聊天机器人最相关。例如，如果机器知道用户的姿势，它可使聊天机器人提出个人建议。意图预测功能使聊天机器人在长时间的对话中能够更智能地做出回复。</p>\n<p>如下图，所有的回复都与上下文相关，但其中只有一个是逻辑正确的。一些错误的回复在极端情况下可能是合理的，但正确的回复是最合适的。<br><img src=\"https://img-blog.csdnimg.cn/20201110092743942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在真实应用场景中，检索式对话模型无法检索所有可能的回复，如果没有检索到合适的回复，系统应具有给予安全回复（safe response）的能力。为了模拟这一场景，我们提出了 MuTual plus。对于每个实例，MuTual plus 随机替换掉 MuTual 中一个候选回复。如果正确回复被替换，安全回复即为新的正确回复。如果错误回复被替换，原正确回复仍为四个回复中最合适的。</p>\n<p>这里说明一下论文中R@1、R@2、MRR等指标的含义</p>\n<blockquote>\n<p>数据集以 Recall@1（正确检索结果出现在检索结果第一位），Recall@2（正确检索结果出现在检索结果前两位），MRR（Mean Reciprocal Rank, 正确检索结果在检索结果中的排名的倒数）作为评价指标。</p>\n</blockquote>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>我们将数据分为训练集，开发集和测试集，比例分别为80％，10％和10％。我们在拆分过程中打包了从同一会话构造的实例，以避免数据泄漏。</p>\n<ul>\n<li><p>不同方法在 MuTual 数据集上的表现对比。<br><img src=\"https://img-blog.csdnimg.cn/20201110094123186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们发现，选择题方法的性能明显优于个人评分方法。一种可能的解释是，多项选择方法将候选回复同时考虑在内，因此他们可以区分safe response是否是最佳选择。相比之下，个人评分方法并不稳健，在训练阶段，safe response容易使其混淆。</p>\n</li>\n<li><p>不同方法在 MuTual plus 数据集上的表现对比，实验调查了模型在训练语料库中从未见过的情况下是否能够很好地处理safe response。<br><img src=\"https://img-blog.csdnimg.cn/2020111010335387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n</ul>\n<p>根据表3，4的结果可以看到，之前的检索式对话模型在 MuTual 上，表现只比随机猜的情况好一点。不过预训练模型也不能取得很好的效果，甚至 RoBERTa 也只能达到71%的 Recall@1。然而未经培训的非母语者可以轻松达到94%。</p>\n<p>下图，我们发现，不同类别的BERT-MC和RoBERTa-MC的趋势相似，RoBERTa-MC在态度推理和多事实推理方面明显优于BERT-MC<br><img src=\"https://img-blog.csdnimg.cn/20201110103744714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>如下图，通过简单的减法步骤即可得出时间差（5:00 pm-6h = 11:00 am），但这对RoBERTa-MC来说是一个巨大的挑战。模型在不同上下文轮数数据的 R@1 对比。#T 表示上下文轮数。#Instances 表示实例的数量。<br><img src=\"https://img-blog.csdnimg.cn/20201110104418149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>进一步研究发现，与其他多轮对话数据集不同的是，在 MuTual 中，模型表现不会随着对话轮数增加而变差，RoBERTa 在两轮和六轮以上的数据上 R@1 相似。这表示推理能力并不依赖复杂的对话历史。如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201110104742960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在推理类型方面，模型在数值推理和意图推测中表现的较差。上图第一个例子中，时差运算只需简单的减法(5:00pm - 6h = 11:00am)，第二个例子需要推理出对话出现在租房场景中，然而对现有的深度学习模型来说依然十分困难。下图展示了前 n 轮对话被删除情况下模型表现显著下滑，证明了解决 MuTual 中的问题需要依赖多轮推理而不是单轮推理。<br><img src=\"https://img-blog.csdnimg.cn/20201110105031354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>MuTual 数据集，用于针对性地评测模型在多轮对话中的推理能力，该数据集有助于将来进行多轮对话推理问题的研究。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","多轮对话","MuTual"]},{"title":"LDA、PCA、ZCA、ICA回顾","url":"/Deep-Learning/f5b6040bcaf2/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>白化是一种线性变换，用于对源信号进行去相关，是一种重要的预处理过程，其目的就是降低输入数据的冗余性，使得经过白化处理的输入数据具有如下性质：</p>\n<ul>\n<li>特征之间相关性较低</li>\n<li>所有特征具有相同的方差。</li>\n</ul>\n<blockquote>\n<p>其中，PCA白化保证了所有特征分布均值为0，方差为1，而ZCA白化则保证了所有特征分布均值为0，方差相同。PCA白化可以用于降维也可以去相关性，而ZCA白化主要用于去相关性，且尽量使白化后的数据接近原始输入数据。</p>\n</blockquote>\n<p>白化是机器学习里面常用的一种规范化数据分布的方法，特别是用来在机器学习模型中用来解决Covariate Shift问题（Covariate Shift是啥可以看这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记：Covariate Shift: 基于机器学习分类器的回顾和分析</a>）</p>\n<p>在深度学习中，Covariate Shift问题也存在，不过叫做另一个名字Internal Covariate Shift，同样在深度学习中解决Internal Covariate Shift问题的办法有很多，比如我们熟悉的Batch Normalization、Layer Normalization等等归一化方法，感兴趣的小伙伴可以参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/340747455\">深度学习中眼花缭乱的Normalization学习总结</a></p>\n<p>这篇文章就来对PCA (主成分分析)、ZCA (零相成分分析)和ICA（独立成分分析）进行回顾和总结。本文涉及到的代码已放置在<a href=\"https://github.com/DengBoCong/Experiment\">GitHub：Experiment</a></p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>在我们接下来进行下面讲解之前，我们先来了解几个概念理解结论：</p>\n<ul>\n<li>如果假设 B 的模为 1，则<strong>A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小</strong>，这就是内积的一种几何解释，即让$|B|=1$，那么$A\\cdot B=|A||B|cos(\\alpha)=|A|cos(\\alpha)$。注意投影是一个标量，所以可以为负。</li>\n<li>要准确描述向量，首先<strong>要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。</li>\n<li>矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列向量 [公式] 变换到左边矩阵中以每一行行向量为基所表示的空间中去</strong>。也就是说一个矩阵可以表示一种线性变换，如下矩阵乘法。<br>$$\\begin{bmatrix}p_1\\p_2\\…\\p_R\\end{bmatrix}(a_1\\ a_2\\ …\\ a_M)=\\begin{bmatrix}p_1a_1 &amp; p_1a_2 &amp; … &amp; p_1a_M\\ p_2a_1 &amp; p_2a_2 &amp; … &amp; p_2a_M\\ … &amp; … &amp; … &amp; …\\ p_Ra_1 &amp; p_Ra_2 &amp; … &amp; p_Ra_M\\end{bmatrix}$$<br>其中 $p_i$ 是一个行向量，表示第 $i$ 个基， $a_j$ 是一个列向量，表示第 $j$ 个原始数据记录，实际上也就是做了一个向量矩阵化的操作。</li>\n<li>我们应该如何选择 K 个基才能最大程度保留原有的信息？问题可以形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></li>\n<li>降维问题的优化目标：<strong>将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）</strong>。</li>\n</ul>\n<h1 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h1><p>首先理解降维，降维就意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。</p>\n<p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。也就是我们接下来要讨论的PCA和ZCA，不过在此之前，我还想举一个更加生动的例子。下图的平面直角坐标系中有五条数据：<br><img src=\"https://img-blog.csdnimg.cn/20210109201656942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多，用数学来衡量就是使用方差和协方差。</p>\n<h1 id=\"LDA\"><a href=\"#LDA\" class=\"headerlink\" title=\"LDA\"></a>LDA</h1><p>LDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。有些资料上也称为是Fisher’s Linear Discriminant，因为它被Ronald Fisher发明自1936年，Discriminant这次词我个人的理解是，一个模型，不需要去通过概率的方法来训练、预测数据，比如说各种贝叶斯方法，就需要获取数据的先验、后验概率等等。</p>\n<p> LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。即给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，比如在二分类问题中，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20210109203942814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><p>PCA（Principal Component Analysis） 是一种常见的数据分析方式，通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。</p>\n<p>关于PCA的讲解，直接看这一篇文章就好了（<a href=\"http://blog.codinglabs.org/articles/pca-tutorial.html\">PCA的数学原理</a>），里面从基开始讲解，把每个点讲的通俗易懂，很值得阅读。我这里就单纯的陈列总结一下PCA的算法步骤：</p>\n<p>设有m条n维数据。</p>\n<ul>\n<li>将原始数据按列组成n行m列矩阵$X$</li>\n<li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li>\n<li>求出协方差矩阵 $C=\\frac{1}{m}XX^T$</li>\n<li>求出协方差矩阵的特征值及对应的特征向量</li>\n<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵$P$</li>\n<li>$Y=PX$ 即为降维到k维后的数据</li>\n</ul>\n<p>PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>\n<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>\n<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p>\n<h1 id=\"ZCA\"><a href=\"#ZCA\" class=\"headerlink\" title=\"ZCA\"></a>ZCA</h1><p>白化的目的是去相关和方差归一化，那么在上述PCA-Whtening中，只要达到这两个目的即可，计算方法并不唯一。换句话说，如果我们换一种方差归一化方法也是可以实现白化的，这就是下面要介绍的ZCA-Whitening。</p>\n<p>如果 $R$ 是任意正交矩阵（any orthogonal matrix），即满足 $RR^T = R^TR = I$ (说它正交不太严格， $R$ 可以是旋转或反射矩阵)，那么 $R$ 乘以 $x_{PCAwhite}$ 仍然具有单位协方差。在ZCA-Whitening中，令$R=U$（其中 U 是PCA白化中使用的特征向量矩阵） 。则我们定义ZCA白化的结果为：<br>$$x_{ZCAwhite}=Ux_{PCAwhite}$$</p>\n<p>此时，数据的协方差矩阵依然是单位矩阵。</p>\n<h1 id=\"ICA\"><a href=\"#ICA\" class=\"headerlink\" title=\"ICA\"></a>ICA</h1><p>ICA又称盲源分离(Blind source separation, BSS)，它假设观察到的随机信号 $x$ 服从模型 $x=As$，其中 $s$ 为未知源信号，其分量相互独立，$A$ 为一未知混合矩阵。ICA的目的是通过且仅通过观察 $x$ 来估计混合矩阵 $A$ 以及源信号 $s$。</p>\n<p>大多数ICA的算法需要进行“数据预处理”（data preprocessing）：先用PCA得到 $y$，再把 $y$ 的各个分量标准化（即让各分量除以自身的标准差）得到 $z$。预处理后得到的 $z$ 满足下面性质：</p>\n<ul>\n<li>$z$ 的各个分量不相关</li>\n<li>$z$ 的各个分量的方差都为1。</li>\n</ul>\n<p>有许多不同的ICA算法可以通过 $z$ 把 $A$ 和 $s$ 估计出来。以著名的FastICA算法为例，该算法寻找方向使得随机变量 $w^Tz$ 的某种“非高斯性”(non-Gaussianity)的度量最大化。一种常用的非高斯性的度量是四阶矩 $E[(w^Tx)^4]$。类似PCA的流程，我们首先找 $w_1$ 使得 $E[(w^Tx)^4]$ 最大；然后在 $w_1$ 与正交的空间里找 $w_2$，使得 $E[(w^Tx)^4]$ 最大，以此类推直到找到所有的 $w_1,…,w_n$。可以证明，用这种方法得到的 $w_1^Tz,…,w_n^T$ 是相互独立的。</p>\n<p>ICA认为一个信号可以被分解成若干个统计独立的分量的线性组合，而后者携带更多的信息。我们可以证明，只要源信号非高斯，那么这种分解是唯一的。若源信号为高斯的话，那么显然可能有无穷多这样的分解。</p>\n<p>总的来说，ICA认为观测信号是若干个统计独立的分量的线性组合，ICA要做的是一个解混过程。而PCA是一个信息提取的过程，将原始数据降维，现已成为ICA将数据标准化的预处理步骤。</p>\n<h1 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h1><p>实践中需要实现PCA白化或ZCA白化时，有时一些特征值在数值上接近于0，这样在缩放步骤时我们除以 $\\sqrt{\\lambda_i}$ 将导致除以一个接近0的值；这可能使数据上溢 (赋为大数值)或造成数值不稳定。因而在实践中，我们使用少量的正则化实现这个缩放过程，即在取平方根和倒数之前给特征值加上一个很小的常数：</p>\n<h1 id=\"PCA白化和ZCA白化可视化\"><a href=\"#PCA白化和ZCA白化可视化\" class=\"headerlink\" title=\"PCA白化和ZCA白化可视化\"></a>PCA白化和ZCA白化可视化</h1><p>为了说明PCA和ZCA白化之间的区别，我们创建一些玩具数据。 具体来说，我们生成了两个相关时间序列 $x_1$ 和 $x_2$ 的1000个样本。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">mu = [<span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">sigma = [[<span class=\"number\">5</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>]]  <span class=\"comment\"># must be positive semi-definite</span></span><br><span class=\"line\">n = <span class=\"number\">1000</span></span><br><span class=\"line\">x = np.random.multivariate_normal(mu, sigma, size=n).T</span><br></pre></td></tr></table></figure>\n<p>这两个时间序列存储在shape为(2,1000)的NumPy数组 $x$ 中，同时，为了便于稍后可视化，这里固定20个最极端的值，并将它们的索引表示为set1（其余数据点的索引存储在set2中）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">set1 = np.argsort(np.linalg.norm(x, axis=<span class=\"number\">0</span>))[-<span class=\"number\">20</span>:]</span><br><span class=\"line\">set2 = <span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(<span class=\"built_in\">range</span>(n)) - <span class=\"built_in\">set</span>(set1))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109160718843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>显然，这两个时间序列看起来高度相关，散点图的椭圆形表明，随着 $x_1$ 的值增加，$x_2$ 的值也趋于增加，实际上， $x_1$ 和 $x_2$ 之间的皮尔逊相关系数为0.80，可以通过以下公式计算：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">print(np.corrcoef(x)[<span class=\"number\">0</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\"># 0.8020186259500502</span></span><br></pre></td></tr></table></figure>\n<p>PCA和ZCA均基于（经验）协方差矩阵的特征向量和特征值，特别地，协方差矩阵可以分解为其特征向量 $U$ 和特征值 $\\Lambda$，例如（$\\Sigma$是协方差矩阵）：<br>$$\\Sigma = U\\Lambda U^T$$<br>我们来计算玩具数据的这些值：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">sigma = np.cov(x)</span><br><span class=\"line\">print(sigma)</span><br><span class=\"line\">evals, evecs = np.linalg.eigh(sigma)</span><br></pre></td></tr></table></figure>\n<p>请注意，在这里使用从数据得出的经验协方差矩阵，而不是普遍未知的真实协方差矩阵，此外，请注意，由于协方差矩阵始终是对称的，因此在这里使用优化的<code>np.linalg.eigh</code>函数，而不是更通用的<code>np.linalg.eig</code>版本（这也确保了我们总是得到真实的特征值而不是复杂的特征值）。当然，我们也可以直接在数据 $x$ 上使用<code>np.linalg.svd</code>（而不是协方差矩阵）来计算特征向量和特征值，在某些情况下，它们在数值上可能更稳定。</p>\n<h2 id=\"PCA白化\"><a href=\"#PCA白化\" class=\"headerlink\" title=\"PCA白化\"></a>PCA白化</h2><p>现在，我们的PCA白化矩阵 $W^{PCA}$ 可以写为：$W^{PCA}=\\Lambda^{-\\frac{1}{2}}U^T$，这也就意味着数据可以转换为：$z=W^{PCA}x=\\Lambda^{-\\frac{1}{2}}U^Tx$，因此，我们可以相应的白化我们的数据，代码实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">z = np.diag(evals**(-<span class=\"number\">1</span>/<span class=\"number\">2</span>)) @ evecs.T @ x</span><br></pre></td></tr></table></figure>\n<p>现在我们可以来看看白化之后的数据是啥样：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109163615638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以很明显的看到该变换消除了两个时间序列之间的相关性，因为散点图现在看起来像一个球体（二维圆），因此PCA的另一个名称叫做sphering。<code>np.corrcoef(z)[0,1]</code>产生的值实际上等于零，从数据分布图我们可以看到，PCA旋转了所有数据点，如红点的新位置所示，它们不再位于大约45度的对角线上，而是现在与垂直轴对齐。</p>\n<h2 id=\"ZCA白化\"><a href=\"#ZCA白化\" class=\"headerlink\" title=\"ZCA白化\"></a>ZCA白化</h2><p>同样的，ZCA的白化矩阵 $W^{ZCA}$ 可以表示为：$W^{ZCA}=U\\Lambda^{-\\frac{1}{2}}U^T$，ZCA白化类似于PCA白化，但不同点在于，$U$ 会进行额外的旋转。原始数据可以按如下方式转换：$z=W^{ZCA}x=U\\Lambda^{-\\frac{1}{2}}U^Tx$。接着，我们使用代码，将使用ZCA算法将白化数据并生成数据散点图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">z = evecs @ np.diag(evals**(-<span class=\"number\">1</span>/<span class=\"number\">2</span>)) @ evecs.T @ x</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109170835355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>同样，由于散点图看起来是球形的，因此ZCA已将数据解相关（且<code>np.corrcoef(z)[0,1]</code>产生的值实际上等于零），与PCA相比，ZCA保留了原始数据点的方向，此属性为其命名为 “zero-phase”，因为它最小化了数据的原始相位（即方向）。</p>\n<p>通过上面的实验，<strong>PCA和ZCA执行不同的旋转，所以如果目标是压缩原始数据，则使用PCA更好，而如果目标是使转换后尽可能与原始数据相似，则使用ZCA更好，因此ZCA无法用于压缩数据</strong>。值得一提的是，有时在白化之前对数据进行<a href=\"https://en.wikipedia.org/wiki/Standard_score\">标准化</a>可能会很有用，特别是如果各个源信号的比例不同。</p>\n<p><em>参考资料</em>：</p>\n<ul>\n<li><a href=\"https://cbrnr.github.io/2018/12/17/whitening-pca-zca/\">Whitening with PCA and ZCA</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/pca-tutorial.html\">PCA的数学原理</a></li>\n<li><a href=\"https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html\">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li>\n<li><a href=\"https://my.oschina.net/findbill/blog/535044\">从SVD到PCA——奇妙的数学游戏</a></li>\n<li><a href=\"https://www.cnblogs.com/robert-dlut/p/4211174.html\">预处理：主成分分析与白化</a></li>\n<li><a href=\"https://www.zhihu.com/question/28845451\">独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["白化","机器学习","PCA","LDA"]},{"title":"NLP中遇到的各类Attention结构汇总以及代码复现","url":"/Deep-Learning/2a74b1f93c8b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>我们所熟知的encoder和decoder结构中，通常采用RNN结构如GRU或LSTM等，在encoder RNN中将输入语句信息总结到最后一个hidden vector中，并将其作为decoder的初始hidden vector，从而利用decoder的解码成对应的其他语言中的文字。但是这样的结构会出现一些问题，比如老生常谈的长程梯度消失的问题，对于较长的句子很难寄希望于将输入的序列转化为定长的向量而保存所有的有效的信息，所以随着输入序列的长度增加，这种结构的效果就会显著下降。因此这个时候就是Attention出场了，用一个浅显描述总结Attention就是，分配权重系数，保留序列的有效信息，而不是局限于原来模型中的定长隐藏向量，并且不会丧失长程的信息。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201218114440343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p>本篇文章主要是汇总我目前在对话和语音方面遇到的各类Attention，针对这些Attention进行理解阐述、总结、论文、代码复现。本文只对各Attention的关键处进行阐述，具体细节可查阅资料或阅读原论文了解。**本文所述的结构不是很多，主要是目前我再学习中遇到的比较重要的Attention（一些用的不多的在最后提了一下），后续还会持续更新。</p>\n<h1 id=\"Bahdanau-Attention\"><a href=\"#Bahdanau-Attention\" class=\"headerlink\" title=\"Bahdanau Attention\"></a>Bahdanau Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\">Paper Link</a></li>\n</ul>\n<p>Bahdanau Attention实现可以说是Attention的开创者之一，该实现的论文名叫“Neural Machine Translation by Learning to Jointly Align and Translate”，其中使用到了“Align”一次，意思是在训练模型的同时调整直接影响得分的权重，下面是论文中的结构图：<br><img src=\"https://img-blog.csdnimg.cn/20201218201405508.png#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$c_t = \\sum_{j=1}^{T_x}a_{tj}h_j$$    $$a_{tj}=\\frac{exp(e_{tj})}{\\sum_{k=1}^{T_x}exp(e_{tk})}$$    $$e_{tj}=V_a^Ttanh(W_a[s_{t-1};h_j])$$</p>\n<p>其中，$c_t$ 是 $t$ 时刻的语义向量，$e_ij$ 是encoder中 $j$ 时刻Encoder隐藏层状态 $h_j$ 对decoder中 $t$ 时刻隐藏层状态 $s_t$ 的影响程度，然后通过softmax函数（第二个式子）将 $e_{tj}$ 概率归一化为 $a_{tj}$</p>\n<p>论文是使用Seq2seq结构对Attention进行阐述的，所以需要注意几点的是：</p>\n<ul>\n<li>在模型结构的encoder中，是使用双向RNN处理序列的，并将方向RNN的最后一个隐藏层作为decoder的初始化隐藏层。</li>\n<li>attention层中的分数计算方式是使用 <strong>additive/concat</strong></li>\n<li>解码器的下一个时间步的输入是前一个解码器时间步生成的单词（或ground-truth）与当前时间步的上下文向量之间的concat。</li>\n</ul>\n<p><strong>下面附一张更清晰的结构图</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201218212755333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bahdanau_attention</span>(<span class=\"params\">hidden_dim: <span class=\"built_in\">int</span>, units: <span class=\"built_in\">int</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    :param units: 全连接层单元数</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    query = tf.keras.Input(shape=(hidden_dim))</span><br><span class=\"line\">    values = tf.keras.Input(shape=(<span class=\"literal\">None</span>, hidden_dim))</span><br><span class=\"line\">    V = tf.keras.layers.Dense(<span class=\"number\">1</span>)</span><br><span class=\"line\">    W1 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">    W2 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">    <span class=\"comment\"># query其实就是decoder的前一个状态，decoder的第一个状态就是上</span></span><br><span class=\"line\">    <span class=\"comment\"># 面提到的encoder反向RNN的最后一层，它作为decoderRNN中的初始隐藏层状态</span></span><br><span class=\"line\">    <span class=\"comment\"># values其实就是encoder每个时间步的隐藏层状态，所以下面需要将query扩展一个时间步维度进行之后的操作</span></span><br><span class=\"line\">    hidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">    score = V(tf.nn.tanh(W1(values) + W2(hidden_with_time_axis)))</span><br><span class=\"line\">    attention_weights = tf.nn.softmax(score, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    context_vector = attention_weights * values</span><br><span class=\"line\">    context_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.keras.Model(inputs=[query, values], outputs=[context_vector, attention_weights])</span><br></pre></td></tr></table></figure>\n<h1 id=\"Luong-Attention\"><a href=\"#Luong-Attention\" class=\"headerlink\" title=\"Luong Attention\"></a>Luong Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1508.04025.pdf\">Paper Link</a></li>\n</ul>\n<p>论文名为“Effective Approaches to Attention-based Neural Machine Translation”，文章其实是基于Bahdanau Attention进行研究的，但在架构上更加简单。论文研究了两种简单有效的注意力机制：一种始终关注所有词的global方法和一种仅一次查看词子集的local方法。结构如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201218220809256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$a_t(s)=align(h_t,\\bar{h}<em>s)=\\frac{exp(score(h_t, \\bar{h}_s))}{\\sum</em>{s’}exp(score(h_t, \\bar{h}_{s’}))}$$    $$score(h_t, \\bar{h}_s)\\left{\\begin{matrix} h_t^T\\bar{h}_s &amp; dot \\ h_t^TW_a\\bar{h}_s &amp;general \\ v_a^Ttanh(W_a[h_t;\\bar{h}_s]) &amp;concat \\end{matrix}\\right.$$ </p>\n<p>同样的，论文中也是使用Seq2Seq结构进行阐述，需要注意如下几点：</p>\n<ul>\n<li>在encoder部分是使用两层堆叠的LSTM，decoder也是同样的结构，不过它使用encoder最后一个隐藏层作为初始化隐藏层。</li>\n<li>用作Attention计算的隐藏层向量是使用堆叠的最后一个LSTM的隐层</li>\n<li>论文中实验的注意力分数计算方式有：（1）additive/concat，（2）dot product，（3）location-based，（4）‘general’</li>\n<li>当前时间步的解码器输出与当前时间步的上下文向量之间的concat喂给前馈神经网络，从而给出当前时间步的解码器的最终输出。</li>\n</ul>\n<p><strong>下面附一张更清晰的结构图</strong>：你会发现和Bahdanau Attention很像区别在于score计算方法和最后decoder中和context vector合并部分。<br><img src=\"https://img-blog.csdnimg.cn/20201218234135371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">luong_attention_concat</span>(<span class=\"params\">hidden_dim: <span class=\"built_in\">int</span>, units: <span class=\"built_in\">int</span></span>) -&gt; tf.keras.Model:</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t:param units: 全连接层单元数</span></span><br><span class=\"line\"><span class=\"string\">\t&quot;&quot;&quot;</span></span><br><span class=\"line\">\tquery = tf.keras.Input(shape=(hidden_dim))</span><br><span class=\"line\">\tvalues = tf.keras.Input(shape=(<span class=\"literal\">None</span>, hidden_dim))</span><br><span class=\"line\">\tW1 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">\tV = tf.keras.layers.Dense(<span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"comment\"># query其实就是decoder的前一个状态，decoder的第一个状态就是上</span></span><br><span class=\"line\">\t<span class=\"comment\"># 面提到的encoder反向RNN的最后一层，它作为decoderRNN中的初始隐藏层状态</span></span><br><span class=\"line\">\t<span class=\"comment\"># values其实就是encoder每个时间步的隐藏层状态，所以下面需要将query扩展一个时间步维度进行之后的操作</span></span><br><span class=\"line\">\thidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">\tscores = V(tf.nn.tanh(W1(hidden_with_time_axis + values)))</span><br><span class=\"line\">\tattention_weights = tf.nn.softmax(scores, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\tcontext_vector = tf.matmul(attention_weights, values)</span><br><span class=\"line\">\tcontext_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> tf.keras.Model(inputs=[query, values], outputs=[attention_weights, context_vector])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">luong_attention_dot</span>(<span class=\"params\">query: tf.Tensor, value: tf.Tensor</span>) -&gt; tf.Tensor:</span></span><br><span class=\"line\">\t <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t :param query: decoder的前一个状态</span></span><br><span class=\"line\"><span class=\"string\">\t :param value: encoder的output</span></span><br><span class=\"line\"><span class=\"string\">\t &quot;&quot;&quot;</span></span><br><span class=\"line\">\t hidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">\t scores = tf.matmul(hidden_with_time_axis, value, transpose_b=<span class=\"literal\">True</span>)</span><br><span class=\"line\">\t attention_weights = tf.nn.softmax(scores, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\t context_vector = tf.matmul(attention_weights, value)</span><br><span class=\"line\">\t context_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"Self-Attention、Multi-Head-Attention\"><a href=\"#Self-Attention、Multi-Head-Attention\" class=\"headerlink\" title=\"Self-Attention、Multi-Head Attention\"></a>Self-Attention、Multi-Head Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Link</a></li>\n</ul>\n<p>Transformer用的就是Self-Attention、Multi-Head Attention。对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入，首先我们要计算Q与K之间的点乘，然后为了防止其结果过大，会除以一个尺度标度 $\\sqrt{d_k}$ ，其中 $d_k$ 为一个query和key向量的维度。再利用Softmax操作将其结果归一化为概率分布，然后再乘以矩阵V就得到权重求和的表示。多头Attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention，两个的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201219161334371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$   $$head_i=Attention(q_i,K,V)$$    $$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O$$</p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">scaled_dot_product_attention</span>(<span class=\"params\">query: tf.Tensor, key: tf.Tensor, value: tf.Tensor, mask: tf.Tensor=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t计算注意力权重。</span></span><br><span class=\"line\"><span class=\"string\">    q, k, v 必须具有匹配的前置维度。</span></span><br><span class=\"line\"><span class=\"string\">    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。</span></span><br><span class=\"line\"><span class=\"string\">    虽然 mask 根据其类型（填充或前瞻）有不同的形状，</span></span><br><span class=\"line\"><span class=\"string\">    但是 mask 必须能进行广播转换以便求和。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    参数:</span></span><br><span class=\"line\"><span class=\"string\">      q: 请求的形状 == (..., seq_len_q, depth)</span></span><br><span class=\"line\"><span class=\"string\">      k: 主键的形状 == (..., seq_len_k, depth)</span></span><br><span class=\"line\"><span class=\"string\">      v: 数值的形状 == (..., seq_len_v, depth_v)</span></span><br><span class=\"line\"><span class=\"string\">      mask: Float 张量，其形状能转换成</span></span><br><span class=\"line\"><span class=\"string\">            (..., seq_len_q, seq_len_k)。默认为None。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    返回值:</span></span><br><span class=\"line\"><span class=\"string\">      输出，注意力权重</span></span><br><span class=\"line\"><span class=\"string\">\t&quot;&quot;&quot;</span></span><br><span class=\"line\">\tmatmul_qk = tf.matmul(q, k, transpose_b=<span class=\"literal\">True</span>)  <span class=\"comment\"># (..., seq_len_q, seq_len_k)</span></span><br><span class=\"line\">    <span class=\"comment\"># 缩放 matmul_qk</span></span><br><span class=\"line\">    dk = tf.cast(tf.shape(k)[-<span class=\"number\">1</span>], tf.float32)</span><br><span class=\"line\">    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)</span><br><span class=\"line\">    <span class=\"comment\"># 将 mask 加入到缩放的张量上。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scaled_attention_logits += (mask * -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    <span class=\"comment\"># softmax 在最后一个轴（seq_len_k）上归一化，因此分数相加等于1。</span></span><br><span class=\"line\">    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-<span class=\"number\">1</span>)  <span class=\"comment\"># (..., seq_len_q, seq_len_k)</span></span><br><span class=\"line\">    output = tf.matmul(attention_weights, v)  <span class=\"comment\"># (..., seq_len_q, depth_v)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output, attention_weights</span><br></pre></td></tr></table></figure>\n<h1 id=\"Location-Sensitive-Attention\"><a href=\"#Location-Sensitive-Attention\" class=\"headerlink\" title=\"Location Sensitive Attention\"></a>Location Sensitive Attention</h1><ul>\n<li><a href=\"https://proceedings.neurips.cc/paper/2015/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf\">Link</a><br>语音合成中的Tacotron2用的就是Location Sensitive Attention，即对位置敏感的Attention，也就是说加入了位置特征，是一种混合注意力机制（见最后一节说明）。原论文中提出，基于内容的Attention对于所输入内容的输入序列中的绝对位置能够跟踪捕获信息，但是在较长的语音片段中性能迅速下降，所以作者为了解决这个问题，通过将辅助的卷积特征作为输入添加到注意机制中来实现的，而这些卷积特征是通过将前一步的注意力权重进行卷积而提取的。结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201219221547858.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>计算公式如下：<br>$$e_{ij}=score(s_{i-1},ca_{i-1}, h_j)=v_a^Ttanh(Ws_i+Vh_j+Uf_{i,j}+b)$$</p>\n<p>其中，$s_i$ 为当前解码器隐状态而非上一步解码器隐状态，偏置值 $b$ 被初始化为 $0$。位置特征 $f_i$ 使用累加注意力权重 $ca_i$ 卷积而来：<br>$$f_i=F*ca_{i-1}$$    $$ca_i=\\sum_{j=1}^{i-1}a_j$$</p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Attention</span>(<span class=\"params\">tf.keras.layers.Layer</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, attention_dim, attention_filters, attention_kernel</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Attention, self).__init__()</span><br><span class=\"line\">        self.attention_dim = attention_dim</span><br><span class=\"line\">        self.attention_location_n_filters = attention_filters</span><br><span class=\"line\">        self.attention_location_kernel_size = attention_kernel</span><br><span class=\"line\">        self.query_layer = tf.keras.layers.Dense(</span><br><span class=\"line\">            self.attention_dim, use_bias=<span class=\"literal\">False</span>, activation=<span class=\"string\">&quot;tanh&quot;</span>)</span><br><span class=\"line\">        self.memory_layer = tf.keras.layers.Dense(</span><br><span class=\"line\">            self.attention_dim, use_bias=<span class=\"literal\">False</span>, activation=<span class=\"string\">&quot;tanh&quot;</span>)</span><br><span class=\"line\">        self.V = tf.keras.layers.Dense(<span class=\"number\">1</span>, use_bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.location_layer = LocationLayer(self.attention_location_n_filters, self.attention_location_kernel_size,</span><br><span class=\"line\">                                            self.attention_dim)</span><br><span class=\"line\">        self.score_mask_value = -<span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_alignment_energies</span>(<span class=\"params\">self, query, memory, attention_weights_cat</span>):</span></span><br><span class=\"line\">        processed_query = self.query_layer(tf.expand_dims(query, axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">        processed_memory = self.memory_layer(memory)</span><br><span class=\"line\"></span><br><span class=\"line\">        attention_weights_cat = tf.transpose(attention_weights_cat, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        processed_attention_weights = self.location_layer(</span><br><span class=\"line\">            attention_weights_cat)</span><br><span class=\"line\">        energies = tf.squeeze(self.V(tf.nn.tanh(</span><br><span class=\"line\">            processed_query + processed_attention_weights + processed_memory)), -<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> energies</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span>(<span class=\"params\">self, attention_hidden_state, memory, attention_weights_cat</span>):</span></span><br><span class=\"line\">        alignment = self.get_alignment_energies(</span><br><span class=\"line\">            attention_hidden_state, memory, attention_weights_cat)</span><br><span class=\"line\">        attention_weights = tf.nn.softmax(alignment, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention_context = tf.expand_dims(attention_weights, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        attention_context = tf.matmul(attention_context, memory)</span><br><span class=\"line\">        attention_context = tf.squeeze(attention_context, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> attention_context, attention_weights</span><br></pre></td></tr></table></figure>\n<h1 id=\"Attention形式\"><a href=\"#Attention形式\" class=\"headerlink\" title=\"Attention形式\"></a>Attention形式</h1><p>关于Attention形式和获取信息方式的总结，可参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/35739040\">Attention用于NLP的一些小结</a>。我接下来陈列出具体形式下的相关论文（这里的陈列的论文我并没有全部研读，单纯在这里汇总，往后有空或者需要用到对应Attention时，再仔细研读）。</p>\n<h2 id=\"Soft-attention、global-attention、动态attention\"><a href=\"#Soft-attention、global-attention、动态attention\" class=\"headerlink\" title=\"Soft attention、global attention、动态attention\"></a>Soft attention、global attention、动态attention</h2><p>这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量可能会比较大一些。</p>\n<h2 id=\"Hard-attention\"><a href=\"#Hard-attention\" class=\"headerlink\" title=\"Hard attention\"></a>Hard attention</h2><p>这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）</p>\n<h2 id=\"Local-Attention（半软半硬attention）\"><a href=\"#Local-Attention（半软半硬attention）\" class=\"headerlink\" title=\"Local Attention（半软半硬attention）\"></a>Local Attention（半软半硬attention）</h2><p>这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">A Context-aware Attention Network for Interactive Question Answering</a></li>\n<li><a href=\"https://discovery.ucl.ac.uk/id/eprint/10066102/1/Wang_Dynamic%20attention%20deep%20model%20for%20article%20recommendation%20by%20learning%20human%20editors%27%20demonstration_AAM.pdf\">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors’ Demonstration</a></li>\n</ul>\n<h2 id=\"Concatenation-based-Attention\"><a href=\"#Concatenation-based-Attention\" class=\"headerlink\" title=\"Concatenation-based Attention\"></a>Concatenation-based Attention</h2><ul>\n<li><a href=\"https://ai.tencent.com/ailab/media/publications/Wei_Liu-Attentive_Collaborative_Filtering_Multimedia_Recommendation-SIGIR17.pdf\">Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention</a></li>\n<li><a href=\"https://arxiv.org/pdf/1706.05764.pdf\">Dipole: Diagnosis Prediction in Healthcare via Aention-based Bidirectional Recurrent Neural Networks</a></li>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3077136.3080699\">Enhancing Recurrent Neural Networks with Positional Attention for Question Answering</a></li>\n<li><a href=\"http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2031.pdf\">Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.06664.pdf\">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a></li>\n</ul>\n<h2 id=\"静态attention\"><a href=\"#静态attention\" class=\"headerlink\" title=\"静态attention\"></a>静态attention</h2><p>对输出句子共用一个 $s_t$ 的attention就够了，一般用在Bilstm的首位hidden state输出拼接起来作为 $s_t$ </p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">Teaching Machines to Read and Comprehend</a></li>\n<li><a href=\"https://pdfs.semanticscholar.org/a97b/5db17acc731ef67321832dbbaf5766153135.pdf\">Supervised Sequence Labelling with Recurrent Neural Networks</a></li>\n</ul>\n<h2 id=\"多层Attention\"><a href=\"#多层Attention\" class=\"headerlink\" title=\"多层Attention\"></a>多层Attention</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">A Context-aware Attention Network for Interactive Question Answering</a></li>\n<li><a href=\"http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2031.pdf\">Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention</a></li>\n<li><a href=\"https://ai.tencent.com/ailab/media/publications/Wei_Liu-Attentive_Collaborative_Filtering_Multimedia_Recommendation-SIGIR17.pdf\">Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention</a></li>\n<li><a href=\"https://www.researchgate.net/profile/Pengjie_Ren/publication/318764168_Leveraging_Contextual_Sentence_Relations_for_Extractive_Summarization_Using_a_Neural_Attention_Model/links/5d890c34299bf1996f98c6d6/Leveraging-Contextual-Sentence-Relations-for-Extractive-Summarization-Using-a-Neural-Attention-Model.pdf\">Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model</a><h1 id=\"说在最后\"><a href=\"#说在最后\" class=\"headerlink\" title=\"说在最后\"></a>说在最后</h1>Attention的提出到现在拥有很多的变种，但是经典的还是Bahdanau Attention和Luong Attention，很多Attention都是对这两个进行改进的。其实学习了Attention的伙伴会发现，对于Attention而言，重要的是Score计算方法，对于不同的计算方法在下面做个总结：</li>\n<li>基于内容的注意力机制(content-based attention)：<br>$$e_{ij}=score(s_{i-1}, h_j)=v_a^Ttanh(W_as_{i-1}+U_ah_j)$$<br>其中，$s_{i−1}$ 为上一个时间步中解码器的输出(解码器隐状态，decoder hidden states)，$h_j$ 是编码器此刻输入(编码器隐状态，encoder hidden state j)，$v_a$、$W_a$ 和 $U_a$ 是待训练参数张量。由于 $U_ah_j$ 是独立于解码步i的，因此可以独立提前计算。基于内容的注意力机制能够将不同的输出与相应的输入元素连接，而与其位置无关。</li>\n<li>基于位置的注意力机制(location-based attention)：<br>$$e_{ij}=score(a_{i-1}, h_j)=v_a^Ttanh(Wh_j+Uf_{i,j})$$<br>其中，$f_{i,j}$ 是之前的注意力权重，$a_{i-1}$ 是经卷积而得的位置特征，$f_i=F∗α_{i−1}$，$v_a$、$W_a$、$U_a$ 和 $F$ 是待训练参数。基于位置的注意力机制仅关心序列元素的位置和它们之间的距离。基于位置的注意力机制会忽略静音或减少它们，因为该注意力机制没有发现输入的内容。</li>\n<li>混合注意力机制(hybrid attention)：<br>$$e_{ij}=score(s_{i-1},a_{i-1}, h_j)=v_a^Ttanh(Ws_{i-1}+Uh_j+Uf_{i,j})$$<br>顾名思义，混合注意力机制是上述两者注意力机制的结合。其中，$s_{i-1}$ 为之前的解码器隐状态，$a_{i-1}$ 是之前的注意力权重，$h_j$ 是第j个编码器隐状态。为其添加偏置值b，最终的score函数计算如下：<br>$$e_{ij}=v_a^Ttanh(Ws_{i-1}+Vh_j+Uf_{i,j}+b)$$<br>其中，$v_a$、$W$、$V$、$U$ 和 $b$ 为待训练参数，$s_{i−1}$ 为上一个时间步中解码器隐状态，$h_j$ 是当前编码器隐状态，$f_{i,j}$ 是之前的注意力权重 $a_{i-1}$ 经卷积而得的位置特征(location feature)，$f_i=F∗α_{i−1}$。混合注意力机制能够同时考虑内容和输入元素的位置。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\">Attn: Illustrated Attention</a></li>\n<li><a href=\"https://blog.floydhub.com/attention-mechanism/\">Attention Mechanism</a></li>\n<li><a href=\"https://www.cnblogs.com/mengnan/p/9527797.html\">声谱预测网络</a></li>\n<li><a href=\"https://krntneja.github.io/posts/2018/attention-based-models-1\">Tutorial on Attention-based Models</a> </li>\n<li><a href=\"https://blog.csdn.net/BVL10101111/article/details/78470716\">Attention Model（mechanism） 的 套路</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/319866371\">Performer: 基于正交随机特征的快速注意力计算</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","NLP","Attention","注意力机制","Paper"]},{"title":"好好琢磨一下TF-IDF，结合Sklearn","url":"/Deep-Learning/c6980c31d7a3/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>首先我们需要了解TF-IDF的相关知识和原理，最后我们通过代码来学习使用。</p>\n<h1 id=\"词集、词袋、词汇表模型\"><a href=\"#词集、词袋、词汇表模型\" class=\"headerlink\" title=\"词集、词袋、词汇表模型\"></a>词集、词袋、词汇表模型</h1><p>文本类的分类任务，特征提取几种方式：</p>\n<ul>\n<li>词集模型（SOW）：单词构成的集合，集合中每个元素只有一个，即词集中的每个单词都只有一个。</li>\n<li>词袋模型 （BOW）：在词集的基础上加入了频率这个维度，即统计单词在文档中出现的次数（token化和出现频数统计），通常我们在应用中都选用词袋模型。</li>\n<li>词汇表模型：前面两个以及TF-IDF中模型没有表达单词间的关系，于是又了词汇表模型。该模型在词袋模型思想的基础上，按照句子中单词顺序进行排序输出特征</li>\n</ul>\n<p>词集模型和词袋模型两者本质上的区别，词袋是在词集的基础上增加了频率的维度，词集只关注有和没有，词袋还要关注有几个。词袋模型可以很好的表现文本由哪些单词组成，但是却无法表达出单词之间的前后关系，于是人们借鉴了词袋模型的思想，使用生成的词汇表对原有句子按照单词逐个进行编码。</p>\n<p>其实我们经常在模型中使用到的词嵌入模型，也和上述的词统计方法密切相关，有兴趣的可以看我另一篇写<a href=\"https://dengbocong.blog.csdn.net/article/details/109319937\">词嵌入的文章</a>。</p>\n<h1 id=\"TF-IDF模型\"><a href=\"#TF-IDF模型\" class=\"headerlink\" title=\"TF-IDF模型\"></a>TF-IDF模型</h1><p>TF-IDF（Term Frequency-Inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。一个词的重要程度跟它在文章中出现的次数成正比，跟它在语料库出现的次数成反比。这种计算方式能有效避免常用词对关键词的影响，提高了关键词与文章之间的相关性。原理说简单点，不难理解。</p>\n<p>向量空间模型就是希望把查询关键字和文档都表达成向量，然后利用向量之间的运算来进一步表达向量间的关系。比如，一个比较常用的运算就是计算查询关键字所对应的向量和文档所对应的向量之间的 “相关度”，如下。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201108160531502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li><strong>词频（TF）</strong><br>在一份给定的文件里，词频（Term Frequency，即TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（Term count）的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 $t_i$ 来说，它的重要性可表示为：<br>$$TF_{i,j}=\\frac{n_{i,j}}{\\sum_kn_{k,j}}$$<br>其中，$n_{i,j}$ 是该词在文件 $d_j$ 中的出现次数，而分母则是在文件 $d_j$ 中所有字词的出现次数之和。</li>\n<li><strong>逆向文件频率（IDF）</strong><br>逆向文件频率（Inverse Document Frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的 IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取以10为底的对数得到，如下：<br>$$IDF_i=lg\\frac{|D|}{|{j:t_i\\in d_j}|}$$<br>其中，$|D|$ 是语料库中的文件总数，$|{j:t_i\\in d_j}|$ 表示包含词语 $t_i$ 的文件数目（即 $n_{i,j}\\neq 0$ 的文件数目），如果词语不在资料中，就导致分母为零，因此一般情况下使用 $1+|j:t_i\\in d_j|$</li>\n</ul>\n<p>有了上述TF和IDF的计算值之后，我们就可以计算TF-IDF值了，如下：<br>$$TFIDF_{i,j}=tf_{i,j}\\times idf_i$$<br>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。</p>\n<h1 id=\"TF-IDF的不足之处\"><a href=\"#TF-IDF的不足之处\" class=\"headerlink\" title=\"TF-IDF的不足之处\"></a>TF-IDF的不足之处</h1><p>TF-IDF算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取TF词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，TF-IDF法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。</p>\n<p>因此引入了逆文本频度IDF的概念，以TF和IDF的乘积作为特征空间坐标系的取值测度，并用它完成对权值TF的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上IDF是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。IDF的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以TF-IDF法的精度并不是很高。</p>\n<p>此外，在TF-IDF算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。总结而言就是：</p>\n<ul>\n<li>没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。</li>\n<li>按照传统TF-IDF，往往一些生僻词的IDF(逆文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。</li>\n<li>传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。</li>\n<li>对于文档中出现次数较少的重要人名、地名信息提取效果不佳。</li>\n</ul>\n<p>当然TF-IDF算法被广泛使用的原因是因为它简单快速，结果比较符合实际情况，所以结合很多其他的方法进行应用，比如结合余弦相似性，应用于搜索相似文章等。在Sklearn的TF-IDF算法实现中，我们可以通过正则表达式表规定过滤的词，这个操作有助于我们更好的利用和提升TF-IDF的准确度，后续会讲到。</p>\n<h1 id=\"TF-IDF-的4个变种\"><a href=\"#TF-IDF-的4个变种\" class=\"headerlink\" title=\"TF-IDF 的4个变种\"></a>TF-IDF 的4个变种</h1><ul>\n<li><strong>通过对数函数避免 TF 线性增长</strong></li>\n</ul>\n<p>很多人注意到 TF 的值在原始的定义中没有任何上限。虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。例如，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，很多人意识到，超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>\n<p>所以这里用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 5.6，而出现 200 次，新的值是 6.3。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>\n<ul>\n<li><strong>标准化解决长文档、短文档问题</strong></li>\n</ul>\n<p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧。</p>\n<ul>\n<li><strong>对数函数处理 IDF线性增长问题</strong></li>\n</ul>\n<p>第三个常用的技巧，也是利用了对数函数进行变换的，是对 IDF 进行处理。相对于直接使用 IDF 来作为 “惩罚因素”，我们可以使用 N+1 然后除以 DF 作为一个新的 DF 的倒数，并且再在这个基础上通过一个对数变化。这里的 N 是所有文档的总数。这样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>\n<ul>\n<li><strong>查询词及文档向量标准化解决长短文档问题</strong></li>\n</ul>\n<p>还有一个重要的 TF-IDF 变种，则是对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>\n<h1 id=\"结合Sklearn实现TF-IDF算法\"><a href=\"#结合Sklearn实现TF-IDF算法\" class=\"headerlink\" title=\"结合Sklearn实现TF-IDF算法\"></a>结合Sklearn实现TF-IDF算法</h1><p>首先引入Sklearn中的CountVectorizer、TfidfTransformer和TfidfVectorizer</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction.text <span class=\"keyword\">import</span> CountVectorizer, TfidfTransformer</span><br><span class=\"line\"><span class=\"keyword\">from</span>  sklearn.feature_extraction.text <span class=\"keyword\">import</span> TfidfVectorizer</span><br><span class=\"line\"></span><br><span class=\"line\">corpus=[<span class=\"string\">&quot;I come to China to travel&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class=\"line\">    <span class=\"string\">&quot;I love tea and Apple &quot;</span>,   </span><br><span class=\"line\">    <span class=\"string\">&quot;The work is to write some papers in science&quot;</span>]</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\">CountVectorizer</a>搭配<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\">TfidfTransformer</a></li>\n</ul>\n<p>CountVectorizer会将文本中的词语转换为词频矩阵，它通过<code>fit_transform</code>函数计算各个词语出现的次数，通过<code>get_feature_names()</code>可获得所有文本的关键词，通过<code>toarray()</code>可看到词频矩阵的结果。TfidfTransformer用于统计vectorizer中每个词语的TFIDF值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">vectorizer=CountVectorizer()</span><br><span class=\"line\">transformer = TfidfTransformer()</span><br><span class=\"line\">tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus)) </span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">TfidfVectorizer</a></li>\n</ul>\n<p>将原始文档的集合转化为TF-IDF特性的矩阵，相当于CountVectorizer配合TfidfTransformer使用的效果。即TfidfVectorizer类将CountVectorizer和TfidfTransformer类封装在一起。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tfidf2 = TfidfVectorizer()</span><br><span class=\"line\">re = tfidf2.fit_transform(corpus)</span><br></pre></td></tr></table></figure>\n<p>上面两种方式的结果都是：<br><img src=\"https://img-blog.csdnimg.cn/20201108164025603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>下面是涉及到的一些比较关键的参数解释，更详细的参数情况可前往官网查看<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">input：string &#123;&#39;filename&#39;，&#39;file&#39;，&#39;content&#39;&#125;</span><br><span class=\"line\">    如果&#39;filename&#39;，作为参数传递的顺序适合，预计将是需要读取以获取原始内容进行分析的文件名列表。</span><br><span class=\"line\">    如果&#39;file&#39;，序列项必须有一个&#39;read&#39;方法（类文件对象），被调用来获取内存中的字节。</span><br><span class=\"line\">    否则，输入将被预期是顺序字符串或字节项预期直接分析。</span><br><span class=\"line\">encoding：string，&#39;utf-8&#39;。</span><br><span class=\"line\">    如果要分配字节或文件，则使用该编码进行解码。</span><br><span class=\"line\">decode_error：&#123;&#39;strict&#39;，&#39;ignore&#39;，&#39;replace&#39;&#125;</span><br><span class=\"line\">    如果给出分析字节序列包含不是给定编码的字符，该怎么做。默认情况下，它是&#39;strict&#39;，这意味着将会引发一个UnicodeDecodeError。其他值是“忽略”和“替换”。</span><br><span class=\"line\">strip_accents：&#123;&#39;ascii&#39;，&#39;unicode&#39;，无&#125;</span><br><span class=\"line\">    在预处理步骤中删除口音。&#39;ascii&#39;是一种快速的方法，只适用于具有直接ASCII映射的字符。&#39;unicode&#39;是一种稍慢的方法，适用于任何字符。无（默认）不起作用。</span><br><span class=\"line\">analyzer：string，&#123;&#39;word&#39;，&#39;char&#39;&#125;或可调用</span><br><span class=\"line\">    该功能是否应由字符或字符n-gram组成。</span><br><span class=\"line\">    如果传递了一个可调用函数，它将用于从原始未处理的输入中提取特征序列。</span><br><span class=\"line\">预处理器：可调用或无（默认）</span><br><span class=\"line\">    覆盖预处理（字符串转换）阶段，同时保留令牌化和n-gram生成步骤。</span><br><span class=\"line\">tokenizer：可调用或无（默认）</span><br><span class=\"line\">    覆盖字符串标记化步骤，同时保留预处理和n-gram生成步骤。仅适用如果。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">ngram_range：tuple（min_n，max_n）</span><br><span class=\"line\">    不同n值的n值范围的下边界和上边界被提取。将使用所有n值，使得min_n &lt;&#x3D; n &lt;&#x3D; max_n。</span><br><span class=\"line\">stop_words：string &#123;&#39;english&#39;&#125;，list或None（默认）</span><br><span class=\"line\">    如果是字符串，则将其传递给_check_stop_list，并返回相应的停止列表。&#39;english&#39;是目前唯一支持的字符串值。</span><br><span class=\"line\">    如果一个列表，该列表被假定为包含停止词，所有这些都将从生成的令牌中删除。仅适用如果。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">    如果没有，将不会使用停止的单词。max_df可以设置为[0.7,1.0]范围内的值，以根据术语的语料库文档频率自动检测和过滤停止词。</span><br><span class=\"line\">小写：布尔值，默认值为True</span><br><span class=\"line\">    在标记化之前将所有字符转换为小写。</span><br><span class=\"line\">token_pattern：string</span><br><span class=\"line\">    表示什么构成“令牌”的正则表达式，仅用于。默认正则表达式选择2个或更多字母数字字符的标记（标点符号被完全忽略，并始终作为令牌分隔符处理）。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">max_df：float in range [ 0.0，1.0 ]或int，default &#x3D; 1.0</span><br><span class=\"line\">    当构建词汇时，忽略文档频率严格高于给定阈值（语料库特定停止词）的术语。如果为float，则该参数代表一定比例的文档，整数绝对计数。如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">min_df：float in range [ 0.0，1.0 ]或int，default &#x3D; 1</span><br><span class=\"line\">    当构建词汇时，忽略文档频率严格低于给定阈值的术语。这个值在文献中也被称为截止值。如果为float，则该参数代表一定比例的文档，整数绝对计数。如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">max_features：int或None，default &#x3D; None</span><br><span class=\"line\">    如果不是无，建立一个词汇，只考虑由词汇频率排序的顶级max_feature。</span><br><span class=\"line\">    如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">词汇表：映射或迭代，可选</span><br><span class=\"line\">    键是术语和值的映射（例如，dict）是特征矩阵中的索引，或者可迭代的术语。如果没有给出，则从输入文档确定词汇表。</span><br><span class=\"line\">binary：boolean，default &#x3D; False</span><br><span class=\"line\">    如果为True，则所有非零项计数都设置为1.这并不意味着输出将只有0&#x2F;1值，只有tf-idf中的tf项是二进制的。（将idf归一化为False，得到0&#x2F;1输出。）</span><br><span class=\"line\">dtype：type，可选</span><br><span class=\"line\">    由fit_transform（）或transform（）返回的矩阵的类型。</span><br><span class=\"line\">规范：&#39;l1&#39;，&#39;l2&#39;或无，可选</span><br><span class=\"line\">    用于规范化术语向量的规范。没有没有规范化。</span><br><span class=\"line\">use_idf：boolean，default &#x3D; True</span><br><span class=\"line\">    启用逆文档频率重新加权。</span><br><span class=\"line\">smooth_idf：boolean，default &#x3D; True</span><br><span class=\"line\">    通过将文档频率添加一个平滑的idf权重，就好像一个额外的文档被看到包含一个集合中的每个术语一次。防止零分。</span><br><span class=\"line\">sublinear_tf：boolean，default &#x3D; False</span><br><span class=\"line\">    应用子线性tf缩放，即用1 + log（tf）替换tf。</span><br></pre></td></tr></table></figure>\n<h1 id=\"补充问题\"><a href=\"#补充问题\" class=\"headerlink\" title=\"补充问题\"></a>补充问题</h1>在使用TfidfVectorizer和CountVectorizer的时候，可能会出现了错误<code>ValueError: empty vocabulary; perhaps the documents only contain stop words</code>。原因是，创建CountVectorizer实例时，有一个默认参数<code>analyzer=&#39;word&#39;</code>，在该参数作用下，词频矩阵构建过程会默认过滤所有的单字token，例如<code>&#39;a b c d&#39;</code>以空格分隔以后全是单字，也就全被过滤了，所以就empty vocabulary了。解决的办法我们就要来看看其中的一个参数：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">analyzer : &#123;‘word’, ‘char’, ‘char_wb’&#125; or callable, default&#x3D;’word’</span><br><span class=\"line\">Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</span><br><span class=\"line\">If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</span><br><span class=\"line\">Since v0.21, if input is filename or file, the data is first read from the file and then passed to the given callable analyzer.</span><br></pre></td></tr></table></figure>\n<p>当然，如果上述三种不能满足需求，可以使用正则表达式达到，即使用<code>token_pattern</code>参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">token_pattern</span><br><span class=\"line\">Regular expression denoting what constitutes a “token”, only used if analyzer &#x3D;&#x3D; &#39;word&#39;. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).</span><br></pre></td></tr></table></figure>\n<p>通过正则的方式来解决，具体解决代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">CountVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>,token_pattern=<span class=\"string\">u&quot;(?u)\\\\b\\\\w+\\\\b&quot;</span>)</span><br><span class=\"line\">TfidfVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>,token_pattern=<span class=\"string\">u&quot;(?u)\\\\b\\\\w+\\\\b&quot;</span>)</span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["词袋","TensorFlow","TF-IDF","Sklearn"]},{"title":"损失函数理解汇总，结合PyTorch和TensorFlow2","url":"/Deep-Learning/f3029f0768bb/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文打算讨论在深度学习中常用的十余种损失函数（含变种），结合PyTorch和TensorFlow2对其概念、公式及用途进行阐述，希望能达到看过的伙伴对各种损失函数有个大致的了解以及使用。本文对原理只是浅尝辄止，不进行深挖，感兴趣的伙伴可以针对每个部分深入翻阅资料。</p>\n<p>使用版本：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0</li>\n</ul>\n<h1 id=\"交叉熵损失（CrossEntropyLoss）\"><a href=\"#交叉熵损失（CrossEntropyLoss）\" class=\"headerlink\" title=\"交叉熵损失（CrossEntropyLoss）\"></a>交叉熵损失（CrossEntropyLoss）</h1><p>对于单事件的信息量而言，当事件发生的概率越大时，信息量越小，需要明确的是，信息量是对于单个事件来说的，实际事件存在很多种可能，所以这个时候熵就派上用场了，熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。<strong>交叉熵用来描述两个分布之间的差距，交叉熵越小，假设分布离真实分布越近，模型越好</strong>。</p>\n<p>在分类问题模型中（不一定是二分类），如逻辑回归、神经网络等，在这些模型的最后通常会经过一个sigmoid函数（softmax函数），输出一个概率值（一组概率值），这个概率值反映了预测为正类的可能性（一组概率值反应了所有分类的可能性）。而对于预测的概率分布和真实的概率分布之间，使用交叉熵来计算他们之间的差距，换句不严谨的话来说，交叉熵损失函数的输入，是softmax或者sigmoid函数的输出。交叉熵损失可以从理论公式推导出几个结论（优点），具体公式推导不在这里详细讲解，如下：</p>\n<ul>\n<li>预测的值跟目标值越远时，参数调整就越快，收敛就越快；</li>\n<li>不会陷入局部最优解</li>\n</ul>\n<p>交叉熵损失函数的标准形式（也就是二分类交叉熵损失）如下:<br>$$L = \\frac{1}{N}\\sum_{i}L_i=\\frac{1}{N}\\sum_{i}-[y_i\\cdot log(p_i)+(1-y_i)\\cdot log(1-p_i)]$$<br>其中，$y_i$ 表示样本 $i$ 的标签，正类为1，负类为0，$p_i$ 表示样本 $i$ 预测为正的概率。<br>多分类交叉熵损失如下：<br>$$L=\\frac{1}{N}\\sum_{i}L_i=\\frac{1}{N}\\sum_{i}-\\sum_{c=1}^{M}y_{ic}log(p_{ic})$$<br>其中，$M$ 表示类别的数量，$y_{ic}$ 表示变量（0或1），如果该类别和样本i的类别相同就是1，否则是0，$p_{ic}$ 表示对于观测样本 $i$ 属于类别 $c$ 的预测概率。</p>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy\">BinaryCrossentropy</a>：二分类，经常搭配Sigmoid使用<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.BinaryCrossentropy(from_logits&#x3D;False, label_smoothing&#x3D;0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;binary_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy\">binary_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\">CategoricalCrossentropy</a>：多分类，经常搭配Softmax使用<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CategoricalCrossentropy(from_logits&#x3D;False, label_smoothing&#x3D;0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;categorical_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy\">categorical_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\">SparseCategoricalCrossentropy</a>：多分类，经常搭配Softmax使用，和CategoricalCrossentropy不同之处在于，CategoricalCrossentropy是one-hot编码，而SparseCategoricalCrossentropy使用一个位置整数表示类别</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.SparseCategoricalCrossentropy(from_logits&#x3D;False, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;sparse_categorical_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy\">sparse_categorical_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits&#x3D;False, axis&#x3D;-1)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\taxis：默认是-1，计算交叉熵的维度</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\">BCELoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.BCELoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\">BCEWithLogitsLoss</a>：其实和TensorFlow是的<code>from_logits</code>参数很像，在BCELoss的基础上合并了Sigmoid</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.BCEWithLogitsLoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, pos_weight: Optional[torch.Tensor] &#x3D; None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br><span class=\"line\">\tpos_weight：正样本的权重, 当p&gt;1，提高召回率，当p&lt;1，提高精确度。可达到权衡召回率(Recall)和精确度(Precision)的作用。 </span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">CrossEntropyLoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.CrossEntropyLoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, ignore_index: int &#x3D; -100, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\tignore_index：忽略某一类别，不计算其loss，其loss会为0，并且，在采用size_average时，不会计算那一类的loss，除的时候的分母也不会统计那一类的样本</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br></pre></td></tr></table></figure>\n<h1 id=\"KL散度\"><a href=\"#KL散度\" class=\"headerlink\" title=\"KL散度\"></a>KL散度</h1><p>我们在计算预测和真实标签之间损失时，需要拉近他们分布之间的差距，即模型得到的预测分布应该与数据的实际分布情况尽可能相近。KL散度(相对熵)是用来衡量两个概率分布之间的差异。模型需要得到最大似然估计，乘以负Log以后就相当于求最小值，此时等价于求最小化KL散度(相对熵)。所以得到KL散度就得到了最大似然。又因为KL散度中包含两个部分，第一部分是交叉熵，第二部分是信息熵，即KL=交叉熵−信息熵。信息熵是消除不确定性所需信息量的度量，简单来说就是真实的概率分布，而这部分是固定的，所以优化KL散度就是近似于优化交叉熵。下面是KL散度的公式：<br>$$D_{KL}(p||q)=\\sum_{i=1}^Np(x_i)\\cdot (logp(x_i)-logq(x_i))$$<br>联系上面的交叉熵，我们可以将公式简化为（KL散度 = 交叉熵 - 熵）：<br>$$D_{KL}(A||B)=H(A,B)-S(A)$$<br>监督学习中，因为训练集中每个样本的标签是已知的，此时标签和预测的标签之间的KL散度等价于交叉熵。<br><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLD\">KLD | kullback_leibler_divergence</a></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.KLD(y_true, y_pred)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLDivergence\">KLDivergence</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.KLDivergence(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;kl_divergence&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<p><strong>Pytorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\">KLDivLoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.KLDivLoss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, log_target: bool &#x3D; False)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br><span class=\"line\">\tlog_target：默认False，指定是否在日志空间中传递目标</span><br></pre></td></tr></table></figure>\n<h1 id=\"平均绝对误差（L1范数损失）\"><a href=\"#平均绝对误差（L1范数损失）\" class=\"headerlink\" title=\"平均绝对误差（L1范数损失）\"></a>平均绝对误差（L1范数损失）</h1><p>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值 $Y_i$ 与估计值 $f(x_i)$ 的绝对差值的总和 $S$ 最小化：<br>$$S=\\sum_{i=1}^n|Y_i-f(x_i)|$$<br>缺点：</p>\n<ul>\n<li>梯度恒定，不论预测值是否接近真实值，这很容易导致发散，或者错过极值点。</li>\n<li>导数不连续，导致求解困难。这也是L1损失函数不广泛使用的主要原因。</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>收敛速度比L2损失函数要快，这是通过对比函数图像得出来的，L1能提供更大且稳定的梯度。</li>\n<li>对异常的离群点有更好的鲁棒性，下面会以例子证实。</li>\n</ul>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAE\">MAE | mean_absolute_error</a><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MAE(y_true, y_pred)</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\">MeanAbsoluteError</a></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanAbsoluteError(reduction=losses_utils.ReductionV2.AUTO, name=<span class=\"string\">&#x27;mean_absolute_error&#x27;</span>)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError\">MeanAbsolutePercentageError</a>：平均绝对百分比误差</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanAbsolutePercentageError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_absolute_percentage_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; 100 * abs(y_true - y_pred) &#x2F; y_true</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE\">MAPE | mean_absolute_percentage_error</a>：平均绝对百分比误差</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MAPE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; 100 * mean(abs((y_true - y_pred) &#x2F; y_true), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\">Huber</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.Huber(delta&#x3D;1.0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;huber_loss&#39;)</span><br><span class=\"line\">公式：error &#x3D; y_true - y_pred</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tdelta：float类型，Huber损失函数从二次变为线性的点。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html\">L1Loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.L1Loss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.l1_loss\">l1_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.l1_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;) → Tensor</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html\">SmoothL1Loss</a>：平滑版L1损失，也被称为 Huber 损失函数。<br>$$loss(x,y)=\\frac{1}{n}\\sum_iz_i$$<br>其中，当 $|x_i-y_i|&lt;beta$ 时， $0.5(x_i-y_i)^2/beta$，否则 $|x_i-y_i|-0.5*beta$<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.SmoothL1Loss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, beta: float &#x3D; 1.0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br><span class=\"line\">\tbeta：默认为1，指定在L1和L2损耗之间切换的阈值</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.smooth_l1_loss\">smooth_l1_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.smooth_l1_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;, beta&#x3D;1.0)</span><br></pre></td></tr></table></figure>\n<h1 id=\"均方误差损失（L2范数损失）\"><a href=\"#均方误差损失（L2范数损失）\" class=\"headerlink\" title=\"均方误差损失（L2范数损失）\"></a>均方误差损失（L2范数损失）</h1><p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值 $Y_i$ 与估计值 $f(x_i)$ 的差值的平方和 $S$ 最小化：<br>$$S=\\sum_{i=1}^n(Y_i-f(x_i))^2$$<br>缺点：</p>\n<ul>\n<li>收敛速度比L1慢，因为梯度会随着预测值接近真实值而不断减小。</li>\n<li>对异常数据比L1敏感，这是平方项引起的，异常数据会引起很大的损失。</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>它使训练更容易，因为它的梯度随着预测值接近真实值而不断减小，那么它不会轻易错过极值点，但也容易陷入局部最优。</li>\n<li>它的导数具有封闭解，优化和编程非常容易，所以很多回归任务都是用MSE作为损失函数。</li>\n</ul>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\">MeanSquaredError</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanSquaredError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_squared_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; square(y_true - y_pred)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE\">MSE | mean_squared_error</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MSE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(y_true - y_pred), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\">MeanSquaredLogarithmicError</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanSquaredLogarithmicError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_squared_logarithmic_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; square(log(y_true + 1.) - log(y_pred + 1.))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSLE\">MSLE | mean_squared_logarithmic_error</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MSLE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(log(y_true + 1) - log(y_pred + 1)), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\">MSELoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.MSELoss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.mse_loss\">mse_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.mse_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;)</span><br></pre></td></tr></table></figure>\n<h1 id=\"Hinge-loss\"><a href=\"#Hinge-loss\" class=\"headerlink\" title=\"Hinge loss\"></a>Hinge loss</h1><p>有人把hinge loss称为铰链损失函数，它可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的损失函数。hinge loss专用于二分类问题，标签值 $y=\\pm 1$，预测值 $\\hat{y}\\in R$。二分类问题的目标函数的要求如下：当 $\\hat{y}$ 大于等于 $\\pm 1$或者小于等于 $-1$时，都是分类器确定的分类结果，此时的损失函数loss为0。而当预测值 $\\hat{y}\\in(-1,1)$ 时，分类器对分类结果不确定，loss不为0。显然，当 $\\hat{y} = 0$ 时，loss达到最大值。对于输出 $y=\\pm 1$，当前 $\\hat{y}$ 的损失为：<br>$$L(y)=max(0,1-y\\cdot \\hat{y})$$<br>扩展到多分类问题上就需要多加一个边界值，然后叠加起来。公式如下：<br>$$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+\\Delta)$$</p>\n<p><strong>Tensorflow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalHinge\">CategoricalHinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CategoricalHinge(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;categorical_hinge&#39;)</span><br><span class=\"line\">公式：loss &#x3D; maximum(neg - pos + 1, 0) where neg&#x3D;maximum((1-y_true)*y_pred) and pos&#x3D;sum(y_true*y_pred)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge\">categorical_hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.categorical_hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; maximum(neg - pos + 1, 0) where neg&#x3D;maximum((1-y_true)*y_pred) and pos&#x3D;sum(y_true*y_pred)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/Hinge\">Hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.Hinge(</span><br><span class=\"line\">    reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;hinge&#39;</span><br><span class=\"line\">)</span><br><span class=\"line\">公式：loss &#x3D; maximum(1 - y_true * y_pred, 0)，y_true值应为-1或1。如果提供了二进制（0或1）标签，会将其转换为-1或1</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/hinge\">hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(maximum(1 - y_true * y_pred, 0), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/SquaredHinge\">SquaredHinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.SquaredHinge(</span><br><span class=\"line\">    reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;squared_hinge&#39;</span><br><span class=\"line\">)</span><br><span class=\"line\">公式：loss &#x3D; square(maximum(1 - y_true * y_pred, 0))，y_true值应为-1或1。如果提供了二进制（0或1）标签，会将其转换为-1或1。</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/squared_hinge\">squared_hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.squared_hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(maximum(1 - y_true * y_pred, 0)), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html\">HingeEmbeddingLoss</a>：当 $y_n=1$时，$l_n=x_n$，当 $y_n=-1$ 时， $l_n=max{0,\\Delta-x_n}$<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.HingeEmbeddingLoss(margin: float &#x3D; 1.0, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tmargin：float类型，默认为1.</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<h1 id=\"余弦相似度\"><a href=\"#余弦相似度\" class=\"headerlink\" title=\"余弦相似度\"></a>余弦相似度</h1>余弦相似度是机器学习中的一个重要概念，在Mahout等MLlib中有几种常用的相似度计算方法，如欧氏相似度，皮尔逊相似度，余弦相似度，Tanimoto相似度等。其中，余弦相似度是其中重要的一种。余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。</li>\n</ul>\n<p>余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感），公式如下：<br>$$sim(X,Y)=cos\\theta=\\frac{\\vec{x}\\cdot \\vec{y}}{||x||\\cdot ||y||}$$</p>\n<p><strong>Tensorflow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\">CosineSimilarity</a>：请注意，所得值是介于-1和0之间的负数，其中0表示正交性，而接近-1的值表示更大的相似性。 如果y_true或y_pred是零向量，则余弦相似度将为0，而与预测值和目标值之间的接近程度无关。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CosineSimilarity(axis&#x3D;-1, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;cosine_similarity&#39;)</span><br><span class=\"line\">公式：loss &#x3D; -sum(l2_norm(y_true) * l2_norm(y_pred))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\taxis：默认-1，沿其计算余弦相似度的维</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity\">cosine_similarity</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.cosine_similarity(y_true, y_pred, axis&#x3D;-1)</span><br><span class=\"line\">公式：loss &#x3D; -sum(l2_norm(y_true) * l2_norm(y_pred))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\taxis：默认-1，沿其计算余弦相似度的维</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\">CosineEmbeddingLoss</a>：当 $y=1$时，$loss(x,y)1-cos(x_1,x_2)$，当 $y=-1$ 时，$loss(x,y)=max(0,cos(x_1,x_2)-margin)$ </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.CosineEmbeddingLoss(margin: float &#x3D; 0.0, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tmargin：float类型，应为-1到1之间的数字，建议为0到0.5，默认值为0</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["TensorFlow","PyTorch","损失函数","梯度下降"]},{"title":"搞定检索式对话系统的候选response检索--使用pysolr调用Solr","url":"/Deep-Learning/351df5ecefe5/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>构建对话机器人的现有方法中，可以分为 generation-based（生成式）和retrieval-based（检索式），相对于生成式而言，检索式拥有的信息更加丰富，且运行流畅的特点。本篇文章不具体讲解模型，而是来好好阐述关于检索候选回复的实现，比如SMN模型、DAM模型等中，关于检索候选回复的实现。关于SMN模型的论文笔记和实现代码可以参考我的<a href=\"https://dengbocong.blog.csdn.net/article/details/109392033\">另一篇文章</a>和<a href=\"https://github.com/DengBoCong\">GitHub</a>，后续我还会对DAM论文和模型写一篇文章。</p>\n<p>使用到的工具版本如下：</p>\n<ul>\n<li>Solr：8.6.3</li>\n<li>pysolr：3.9.0</li>\n<li>python：3.7</li>\n<li>CentOS：7.6</li>\n<li>Docker：19.03.9</li>\n</ul>\n<h1 id=\"整体流程\"><a href=\"#整体流程\" class=\"headerlink\" title=\"整体流程\"></a>整体流程</h1><p>我们讲解工具使用之前，首先简要的阐述一下我们的目的，如果已经了解过检索式对话系统或者阅读过相应论文，就不用看了。首先我们知道目的是检索候选回复，用什么检索呢？这个和具体模型结构和需求有关。拿SMN模型为例，利用启发式方法从索引中获取候选response，将前一轮的utterances ${u_1,…,u_{n-1}}$ （也就是对话的历史）和 $u_n$ 进行计算，根据他们的<strong>tf-idf</strong>得分，从 ${u_1,…,u_{n-1}}$ 中提取前 $5$ 个关键字，然后将扩展后的message用于索引，并使用索引的内联检索算法来检索候选response。</p>\n<p>模型结构和训练至关重要，但是检索候选回复也是使得整个对话流程实现闭环的关键。我们了解了检索的目的和整体流程，那我们从何实现？方式有很多，可以自行编写一个脚本从数据集中生成一个索引候选数据集（这个是我最开始用的方法，但毕竟没专门研究过检索，所以写的很粗糙，勉强验证功能可以，用作正式使用就不行了），还有一种就是使用现有的检索工具，比如Lucene、Solr、ElasticSearch等等。所以这篇文章就是来讲解部署solr和使用python实现检索（为什么选用Solr？不是说那种工具好坏，而是佛系使用，貌似ElasticSearch现在很火的样子，哈哈哈）。</p>\n<h1 id=\"Solr和Pysolr\"><a href=\"#Solr和Pysolr\" class=\"headerlink\" title=\"Solr和Pysolr\"></a>Solr和Pysolr</h1><p>Solr它是一种开放源码的、基于 Lucene Java 的搜索服务器，易于加入到 Web 应用程序中。Lucene很底层，从底层代码层面来实现需求，而Solr在其上进行了封装，你如果想要实现脱机检索，那还是使用Lucene吧。Solr 提供了层面搜索(就是统计)、命中醒目显示并且支持多种输出格式（包括XML/XSLT 和JSON等格式）。它易于安装和配置，而且附带了一个基于 HTTP 的管理界面。Solr已经在众多大型的网站中使用，较为成熟和稳定。Solr 包装并扩展了 Lucene，所以Solr的基本上沿用了Lucene的相关术语。更重要的是，Solr 创建的索引与 Lucene 搜索引擎库完全兼容。通过对Solr 进行适当的配置，某些情况下可能需要进行编码，Solr 可以阅读和使用构建到其他 Lucene 应用程序中的索引。此外，很多 Lucene 工具（如Nutch、 Luke）也可以使用Solr 创建的索引。可以使用 Solr 的表现优异的基本搜索功能，也可以对它进行扩展从而满足企业的需要，<a href=\"https://lucene.apache.org/\">Solr官网</a>（官方将其和Lucene并列放在一起，嘿嘿嘿，万变不离其宗，看官方文档）。</p>\n<p>而Pysolr是基于Python的Solr轻量级封装，它提供了服务器查询并返回基于查询的结果接口。简单来说就是Pysolr封装了Solr的各种http请求，使用起来非常方便，你可以直接从pypi中直接导入（这个就要吐槽一下Pylucene了，不能从pipy直接导入），<a href=\"https://pypi.org/project/pysolr/\">PySolr官方地址</a>，上面有使用的示例和API，可以自行去看，这里配一张Solr的示意图，方面后面理解：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201118232526492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"部署Solr\"><a href=\"#部署Solr\" class=\"headerlink\" title=\"部署Solr\"></a>部署Solr</h1><h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><p>都0202年了，部署服务应用都是用容器了吧，我这里讲解用Docker部署solr，不了解的可以参考我的关于<a href=\"https://blog.csdn.net/dbc_121/category_9650661.html\">Docker的几篇文章</a>，我这里就不介绍Docker了，默认会就接着往下讲了。</p>\n<p>有了docker环境之后，首先先将solr拉下来，我这里拉的是8.6.3的版本（ps：不喜欢拉最新的，因为最新的可能其他的附属库跟不上更新，出问题）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker pull solr:8.6.3</span><br><span class=\"line\"><span class=\"comment\"># 然后启动solr</span></span><br><span class=\"line\">docker run -itd --name solr -p 8983:8983 solr:8.6.3</span><br><span class=\"line\"><span class=\"comment\"># 然后创建core核心选择器，我这里因为以SMN模型讲解，所以取名SMN</span></span><br><span class=\"line\"><span class=\"comment\"># exec -it ：交互式执行容器</span></span><br><span class=\"line\"><span class=\"comment\"># -c  内核的名称（必须）</span></span><br><span class=\"line\">docker <span class=\"built_in\">exec</span> -it --user=solr solr bin/solr create_core -c smn</span><br></pre></td></tr></table></figure>\n<p>指令具体含义以及core是啥，请自行查阅资料，或者研究一下solr，毕竟先学习基础再来实战。上面构建solr运行容器是简单粗暴且实用的方法，也可以和我一样使用Dockerfile进行构建镜像和容器，Dockerfile内容如下（内容来自<a href=\"https://github.com/docker-solr/docker-solr\">docker-solr</a>项目，官方的docker镜像项目）：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">FROM openjdk:11-jre</span><br><span class=\"line\"></span><br><span class=\"line\">LABEL maintainer=<span class=\"string\">&quot;The Apache Lucene/Solr Project&quot;</span></span><br><span class=\"line\">LABEL repository=<span class=\"string\">&quot;https://github.com/docker-solr/docker-solr&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">ARG SOLR_VERSION=<span class=\"string\">&quot;8.6.3&quot;</span></span><br><span class=\"line\">ARG SOLR_SHA512=<span class=\"string\">&quot;f040d4489118b655bd27451a717c1f22f180c398638d944a53889a1a449e7032b016cecbff1979c2e8bfd51fc037dd613f3b968254001d34fe0e8fc4f6761dcf&quot;</span></span><br><span class=\"line\">ARG SOLR_KEYS=<span class=\"string\">&quot;902CC51935C140BF820230961FD5295281436075&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># If specified, this will override SOLR_DOWNLOAD_SERVER and all ASF mirrors. Typically used downstream for custom builds</span></span><br><span class=\"line\">ARG SOLR_DOWNLOAD_URL</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Override the solr download location with e.g.:</span></span><br><span class=\"line\"><span class=\"comment\">#   docker build -t mine --build-arg SOLR_DOWNLOAD_SERVER=http://www-eu.apache.org/dist/lucene/solr .</span></span><br><span class=\"line\">ARG SOLR_DOWNLOAD_SERVER</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  apt-get update; \\</span><br><span class=\"line\">  apt-get -y install acl dirmngr gpg lsof procps wget netcat gosu tini; \\</span><br><span class=\"line\">  rm -rf /var/lib/apt/lists/*; \\</span><br><span class=\"line\">  <span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/bin; wget -nv https://github.com/apangin/jattach/releases/download/v1.5/jattach; chmod 755 jattach; \\</span><br><span class=\"line\">  <span class=\"built_in\">echo</span> &gt;jattach.sha512 <span class=\"string\">&quot;d8eedbb3e192a8596c08efedff99b9acf1075331e1747107c07cdb1718db2abe259ef168109e46bd4cf80d47d43028ff469f95e6ddcbdda4d7ffa73a20e852f9  jattach&quot;</span>; \\</span><br><span class=\"line\">  sha512sum -c jattach.sha512; rm jattach.sha512</span><br><span class=\"line\"></span><br><span class=\"line\">ENV SOLR_USER=<span class=\"string\">&quot;solr&quot;</span> \\</span><br><span class=\"line\">    SOLR_UID=<span class=\"string\">&quot;8983&quot;</span> \\</span><br><span class=\"line\">    SOLR_GROUP=<span class=\"string\">&quot;solr&quot;</span> \\</span><br><span class=\"line\">    SOLR_GID=<span class=\"string\">&quot;8983&quot;</span> \\</span><br><span class=\"line\">    SOLR_CLOSER_URL=<span class=\"string\">&quot;http://www.apache.org/dyn/closer.lua?filename=lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&amp;action=download&quot;</span> \\</span><br><span class=\"line\">    SOLR_DIST_URL=<span class=\"string\">&quot;https://www.apache.org/dist/lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> \\</span><br><span class=\"line\">    SOLR_ARCHIVE_URL=<span class=\"string\">&quot;https://archive.apache.org/dist/lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> \\</span><br><span class=\"line\">    PATH=<span class=\"string\">&quot;/opt/solr/bin:/opt/docker-solr/scripts:<span class=\"variable\">$PATH</span>&quot;</span> \\</span><br><span class=\"line\">    SOLR_INCLUDE=/etc/default/solr.in.sh \\</span><br><span class=\"line\">    SOLR_HOME=/var/solr/data \\</span><br><span class=\"line\">    SOLR_PID_DIR=/var/solr \\</span><br><span class=\"line\">    SOLR_LOGS_DIR=/var/solr/logs \\</span><br><span class=\"line\">    LOG4J_PROPS=/var/solr/log4j2.xml</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  groupadd -r --gid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GID</span>&quot;</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GROUP</span>&quot;</span>; \\</span><br><span class=\"line\">  useradd -r --uid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_UID</span>&quot;</span> --gid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GID</span>&quot;</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_USER</span>&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  <span class=\"built_in\">export</span> GNUPGHOME=<span class=\"string\">&quot;/tmp/gnupg_home&quot;</span>; \\</span><br><span class=\"line\">  mkdir -p <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span>; \\</span><br><span class=\"line\">  chmod 700 <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">&quot;disable-ipv6&quot;</span> &gt;&gt; <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>/dirmngr.conf&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"variable\">$SOLR_KEYS</span>; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">    found=<span class=\"string\">&#x27;&#x27;</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">for</span> server <span class=\"keyword\">in</span> \\</span><br><span class=\"line\">      ha.pool.sks-keyservers.net \\</span><br><span class=\"line\">      hkp://keyserver.ubuntu.com:80 \\</span><br><span class=\"line\">      hkp://p80.pool.sks-keyservers.net:80 \\</span><br><span class=\"line\">      pgp.mit.edu \\</span><br><span class=\"line\">    ; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">      <span class=\"built_in\">echo</span> <span class=\"string\">&quot;  trying <span class=\"variable\">$server</span> for <span class=\"variable\">$key</span>&quot;</span>; \\</span><br><span class=\"line\">      gpg --batch --keyserver <span class=\"string\">&quot;<span class=\"variable\">$server</span>&quot;</span> --keyserver-options timeout=10 --recv-keys <span class=\"string\">&quot;<span class=\"variable\">$key</span>&quot;</span> &amp;&amp; found=yes &amp;&amp; <span class=\"built_in\">break</span>; \\</span><br><span class=\"line\">      gpg --batch --keyserver <span class=\"string\">&quot;<span class=\"variable\">$server</span>&quot;</span> --keyserver-options timeout=10 --recv-keys <span class=\"string\">&quot;<span class=\"variable\">$key</span>&quot;</span> &amp;&amp; found=yes &amp;&amp; <span class=\"built_in\">break</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">test</span> -z <span class=\"string\">&quot;<span class=\"variable\">$found</span>&quot;</span> &amp;&amp; <span class=\"built_in\">echo</span> &gt;&amp;2 <span class=\"string\">&quot;error: failed to fetch <span class=\"variable\">$key</span> from several disparate servers -- network issues?&quot;</span> &amp;&amp; <span class=\"built_in\">exit</span> 1; \\</span><br><span class=\"line\">  <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">  <span class=\"built_in\">exit</span> 0</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  <span class=\"built_in\">export</span> GNUPGHOME=<span class=\"string\">&quot;/tmp/gnupg_home&quot;</span>; \\</span><br><span class=\"line\">  MAX_REDIRECTS=1; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ -n <span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_URL</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    <span class=\"comment\"># If a custom URL is defined, we download from non-ASF mirror URL and allow more redirects and skip GPG step</span></span><br><span class=\"line\">    <span class=\"comment\"># This takes effect only if the SOLR_DOWNLOAD_URL build-arg is specified, typically in downstream Dockerfiles</span></span><br><span class=\"line\">    MAX_REDIRECTS=4; \\</span><br><span class=\"line\">    SKIP_GPG_CHECK=<span class=\"literal\">true</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">elif</span> [ -n <span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_SERVER</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    SOLR_DOWNLOAD_URL=<span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_SERVER</span>/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">for</span> url <span class=\"keyword\">in</span> <span class=\"variable\">$SOLR_DOWNLOAD_URL</span> <span class=\"variable\">$SOLR_CLOSER_URL</span> <span class=\"variable\">$SOLR_DIST_URL</span> <span class=\"variable\">$SOLR_ARCHIVE_URL</span>; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">    <span class=\"keyword\">if</span> [ -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> ]; <span class=\"keyword\">then</span> <span class=\"built_in\">break</span>; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;downloading <span class=\"variable\">$url</span>&quot;</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wget -t 10 --max-redirect <span class=\"variable\">$MAX_REDIRECTS</span> --retry-connrefused -nv <span class=\"string\">&quot;<span class=\"variable\">$url</span>&quot;</span> -O <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"keyword\">then</span> <span class=\"built_in\">break</span>; <span class=\"keyword\">else</span> rm -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ ! -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> ]; <span class=\"keyword\">then</span> <span class=\"built_in\">echo</span> <span class=\"string\">&quot;failed all download attempts for solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"built_in\">exit</span> 1; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ -z <span class=\"string\">&quot;<span class=\"variable\">$SKIP_GPG_CHECK</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;downloading <span class=\"variable\">$SOLR_ARCHIVE_URL</span>.asc&quot;</span>; \\</span><br><span class=\"line\">    wget -nv <span class=\"string\">&quot;<span class=\"variable\">$SOLR_ARCHIVE_URL</span>.asc&quot;</span> -O <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_SHA512</span> */opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> | sha512sum -c -; \\</span><br><span class=\"line\">    (&gt;&amp;2 ls -l <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span>); \\</span><br><span class=\"line\">    gpg --batch --verify <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;Skipping GPG validation due to non-Apache build&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  tar -C /opt --extract --file <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  (<span class=\"built_in\">cd</span> /opt; ln -s <span class=\"string\">&quot;solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> solr); \\</span><br><span class=\"line\">  rm <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>*; \\</span><br><span class=\"line\">  rm -Rf /opt/solr/docs/ /opt/solr/dist/&#123;solr-core-<span class=\"variable\">$SOLR_VERSION</span>.jar,solr-solrj-<span class=\"variable\">$SOLR_VERSION</span>.jar,solrj-lib,solr-test-framework-<span class=\"variable\">$SOLR_VERSION</span>.jar,test-framework&#125;; \\</span><br><span class=\"line\">  mkdir -p /opt/solr/server/solr/lib /docker-entrypoint-initdb.d /opt/docker-solr; \\</span><br><span class=\"line\">  chown -R 0:0 <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span>; \\</span><br><span class=\"line\">  find <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> -<span class=\"built_in\">type</span> d -print0 | xargs -0 chmod 0755; \\</span><br><span class=\"line\">  find <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> -<span class=\"built_in\">type</span> f -print0 | xargs -0 chmod 0644; \\</span><br><span class=\"line\">  chmod -R 0755 <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/bin&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/contrib/prometheus-exporter/bin/solr-exporter&quot;</span> /opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/server/scripts/cloud-scripts; \\</span><br><span class=\"line\">  cp /opt/solr/bin/solr.in.sh /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  mv /opt/solr/bin/solr.in.sh /opt/solr/bin/solr.in.sh.orig; \\</span><br><span class=\"line\">  mv /opt/solr/bin/solr.in.cmd /opt/solr/bin/solr.in.cmd.orig; \\</span><br><span class=\"line\">  chown root:0 /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  chmod 0664 /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  mkdir -p /var/solr/data /var/solr/logs; \\</span><br><span class=\"line\">  (<span class=\"built_in\">cd</span> /opt/solr/server/solr; cp solr.xml zoo.cfg /var/solr/data/); \\</span><br><span class=\"line\">  cp /opt/solr/server/resources/log4j2.xml /var/solr/log4j2.xml; \\</span><br><span class=\"line\">  find /var/solr -<span class=\"built_in\">type</span> d -print0 | xargs -0 chmod 0770; \\</span><br><span class=\"line\">  find /var/solr -<span class=\"built_in\">type</span> f -print0 | xargs -0 chmod 0660; \\</span><br><span class=\"line\">  sed -i -e <span class=\"string\">&quot;s/\\&quot;\\$(whoami)\\&quot; == \\&quot;root\\&quot;/\\$(id -u) == 0/&quot;</span> /opt/solr/bin/solr; \\</span><br><span class=\"line\">  sed -i -e <span class=\"string\">&#x27;s/lsof -PniTCP:/lsof -t -PniTCP:/&#x27;</span> /opt/solr/bin/solr; \\</span><br><span class=\"line\">  chown -R <span class=\"string\">&quot;0:0&quot;</span> /opt/solr-<span class=\"variable\">$SOLR_VERSION</span> /docker-entrypoint-initdb.d /opt/docker-solr; \\</span><br><span class=\"line\">  chown -R <span class=\"string\">&quot;<span class=\"variable\">$SOLR_USER</span>:0&quot;</span> /var/solr; \\</span><br><span class=\"line\">  &#123; <span class=\"built_in\">command</span> -v gpgconf; gpgconf --<span class=\"built_in\">kill</span> all || :; &#125;; \\</span><br><span class=\"line\">  rm -r <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">COPY --chown=0:0 scripts /opt/docker-solr/scripts</span><br><span class=\"line\"></span><br><span class=\"line\">VOLUME /var/solr</span><br><span class=\"line\">EXPOSE 8983</span><br><span class=\"line\">WORKDIR /opt/solr</span><br><span class=\"line\">USER <span class=\"variable\">$SOLR_USER</span></span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [<span class=\"string\">&quot;docker-entrypoint.sh&quot;</span>]</span><br><span class=\"line\">CMD [<span class=\"string\">&quot;solr-foreground&quot;</span>]</span><br></pre></td></tr></table></figure>\n<p>容器运行情况如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118231257526.png#pic_center\" alt=\"在这里插入图片描述\"><br>接下来可以访问：<a href=\"http://xxxxxx:8983/solr/%EF%BC%8C%E8%BF%9B%E5%85%A5%E5%88%B0solr%E7%95%8C%E9%9D%A2%EF%BC%8C%E5%A6%82%E4%B8%8B%EF%BC%9A\">http://xxxxxx:8983/solr/，进入到solr界面，如下：</a><br><img src=\"https://img-blog.csdnimg.cn/20201118231420490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后点击Core Admin，查看一下自己刚刚创建的Core，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118231532138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后选择smn就可以使用了，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020111823163747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>结束了？当然没有，哪有那么简单的事儿，首先我们上面算是基本部署好了solr，但是我们需要进行一些必要的使得我们能更好的使用，比如我们需要对文档进行分词，添加相似度计算类（用于tf-idf计算），接下来就说明如何配置这两个东西。</p>\n<h2 id=\"配置IK\"><a href=\"#配置IK\" class=\"headerlink\" title=\"配置IK\"></a>配置IK</h2><p>首先是IK，IK Analyzer(中文分词器)是一个开源的，基于java语言开发的轻量级的中文分词工具包。最初，它是以开源项目 Lucene为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer3.0则发展为 面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。</p>\n<ul>\n<li>Solr 5以前的可以装上<a href=\"https://pan.baidu.com/s/1WAtY5kjI75Kg-e6OAH69cw\">老版本</a>，提取码：g5ib</li>\n<li>Solr 6使用这个<a href=\"https://github.com/cj96248/ik-analyzer-solr6\">版本</a></li>\n<li>Solr 7&amp;8使用这个<a href=\"https://github.com/magese/ik-analyzer-solr\">版本</a></li>\n</ul>\n<p>注意要将IK源码打成JAR包（作为一个老Java，打包还是不难的）。接着将jar包通过传输软件或其它方式传入宿主机的某一文件夹内，然后使用指令将jar包复制到Solr容器的分词包文件夹中：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker cp ik-analyzer.jar solr:/opt/solr-8.6.3/contrib/analysis-extras/lucene-libs</span><br></pre></td></tr></table></figure>\n<p>查看 Solr 容器在宿主机中数据卷的位置：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker inspect solr</span><br><span class=\"line\"><span class=\"comment\"># 找到 Mounts</span></span><br><span class=\"line\"><span class=\"comment\"># Destination : 容器里的路径</span></span><br><span class=\"line\"><span class=\"comment\"># Source : 对应宿主机里的路径</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20201118233446485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>将IK分词器配置到 Solr 的核心配置文件中，Source为上面的Mounts中的：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> &#123;Source&#125;/data/myIKCore/conf/</span><br><span class=\"line\">vim solrconfig.xml</span><br></pre></td></tr></table></figure>\n<p>然后添加如下内容：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dir\t容器存放自带分词JAR包的目录</span></span><br><span class=\"line\"><span class=\"comment\"># regex\tJAR包名</span></span><br><span class=\"line\">&lt;lib dir=<span class=\"string\">&quot;<span class=\"variable\">$&#123;solr.install.dir:../../../..&#125;</span>/contrib/analysis-extras/lucene-libs/&quot;</span> regex=<span class=\"string\">&quot;ik-analyzer.jar&quot;</span> /&gt;</span><br></pre></td></tr></table></figure>\n<p>声明中文分词器</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">vim managed-schema</span><br></pre></td></tr></table></figure>\n<p>找到指定位置添加配置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!-- IKAnalyzer --&gt;</span><br><span class=\"line\">&lt;fieldType name =<span class=\"string\">&quot;text_ik&quot;</span> class =<span class=\"string\">&quot;solr.TextField&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 索引时候的分词器 --&gt;</span><br><span class=\"line\">    &lt;analyzer <span class=\"built_in\">type</span> =<span class=\"string\">&quot;index&quot;</span> isMaxWordLength =<span class=\"string\">&quot;false&quot;</span> class=<span class=\"string\">&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;!-- 查询时候的分词器 --&gt;</span><br><span class=\"line\">    &lt;analyzer <span class=\"built_in\">type</span> =<span class=\"string\">&quot;query&quot;</span> isMaxWordLength =<span class=\"string\">&quot;true&quot;</span> class=<span class=\"string\">&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;</span>/&gt;</span><br><span class=\"line\">&lt;/fieldType&gt;</span><br></pre></td></tr></table></figure>\n<p>重启 Solr 容器</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker restart solr</span><br></pre></td></tr></table></figure>\n<p>选择刚刚创建的核心选择器<br><img src=\"https://img-blog.csdnimg.cn/202011182340266.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"配置相似度\"><a href=\"#配置相似度\" class=\"headerlink\" title=\"配置相似度\"></a>配置相似度</h2><p>到了这里，你其实可以直接用了，但是如果使用tf-idf的话，会报错，如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request</span><br><span class=\"line\">null:java.lang.UnsupportedOperationException: requires a TFIDFSimilarity (such as ClassicSimilarity)</span><br></pre></td></tr></table></figure>\n<p>所以还是需要配置，打开 <code>managed-schema</code> 将下面一行添加进就可以了：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;similarity class=<span class=\"string\">&quot;solr.ClassicSimilarityFactory&quot;</span>/&gt;</span><br></pre></td></tr></table></figure>\n<h1 id=\"pysolr使用\"><a href=\"#pysolr使用\" class=\"headerlink\" title=\"pysolr使用\"></a>pysolr使用</h1><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建solr</span></span><br><span class=\"line\">solr = pysolr.Solr(url=solr_server, always_commit=True, timeout=10)</span><br><span class=\"line\"><span class=\"comment\"># 使用前习惯性安全检查</span></span><br><span class=\"line\">solr.ping()</span><br><span class=\"line\"><span class=\"comment\"># 将回复数据添加索引，responses是一个json,形式如：[&#123;&#125;,&#123;&#125;,&#123;&#125;,...]，里面每个对象构建按照你回复的需求即可</span></span><br><span class=\"line\">solr.add(docs=responses)</span><br></pre></td></tr></table></figure>\n<p>接下来进行查询，首先我们提取关键词词，我这里将我的tf-idf方法的代码贴出来，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_tf_idf_top_k</span>(<span class=\"params\">history: <span class=\"built_in\">list</span>, k: <span class=\"built_in\">int</span> = <span class=\"number\">5</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    使用tf_idf算法计算权重最高的k个词，并返回</span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        history: 上下文语句</span></span><br><span class=\"line\"><span class=\"string\">        k: 返回词数量</span></span><br><span class=\"line\"><span class=\"string\">    Returns: top_5_key</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    tf_idf = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    vectorizer = TfidfVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>)</span><br><span class=\"line\">    weights = vectorizer.fit_transform(history).toarray()[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    key_words = vectorizer.get_feature_names()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(weights)):</span><br><span class=\"line\">        tf_idf[key_words[i]] = weights[i]</span><br><span class=\"line\"></span><br><span class=\"line\">    top_k_key = []</span><br><span class=\"line\">    tf_idf_sorted = <span class=\"built_in\">sorted</span>(tf_idf.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)[:k]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> tf_idf_sorted:</span><br><span class=\"line\">        top_k_key.append(element[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> top_k_key</span><br></pre></td></tr></table></figure>\n<p>然后将得到的五个关键词通过query的语法进行组合，得到查询语句，我这里只返回前十个分数最高的候选回复：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">query = <span class=\"string\">&quot;&#123;!func&#125;sum(&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> keyin tf_idf:</span><br><span class=\"line\">    query += <span class=\"string\">&quot;product(idf(utterance,&quot;</span> + key + <span class=\"string\">&quot;),tf(utterance,&quot;</span> + key + <span class=\"string\">&quot;)),&quot;</span></span><br><span class=\"line\">query += <span class=\"string\">&quot;)&quot;</span></span><br><span class=\"line\">candidates = self.solr.search(q=query, start=<span class=\"number\">0</span>, rows=<span class=\"number\">10</span>).docs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#query合起来长这样：&#123;!func&#125;sum(product(idf(utterance,key1),tf(utterance,key1),product(idf(utterance,key2),tf(utterance,key2),...)</span></span><br></pre></td></tr></table></figure>\n<p>查询回复格式如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118235834184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后检索得到了候选回复就可以喂给模型了。</p>\n","categories":["Deep-Learning"],"tags":["NLP","对话系统","检索式对话","solr"]},{"title":"教你如何估计各种神经网络的计算量和参数量","url":"/Deep-Learning/3bb4a04f32b7/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于深度学习模型来说，拥有一个非常好的设计思路和体系架构非常重要，对模型性能的影响非常之大，所以对于模型的研究倾向于在模型性能上的表现。但是对于商业应用来说，算法模型落地的另一个重要考量就是在满足业务场景内存占用、计算量等需要的同时，保证算法的性能，这个研究对于移动端的模型部署更加明显，这有利于压缩应用的体积。最近这段时间正好在研究关于移动端模型部署（TensorFlow Lite用的不是很顺心呀），所以要仔细研究一下模型的参数量等，这不仅可以让我们对模型的大小进行了解，还能更好的调整结构使得模型响应的更快。</p>\n<p>可能有时候觉得，模型的大小需要计算嘛，直接保存大小直接看不就完事儿了？运行速度就更直接了，多运行几次取平均速度不就行了嘛?so easy？这个想法也没啥错，但是大前提是你得有个整型的模型呀（训练成本多高心里没数嘛），因此很多时候我们想要在模型设计之初就估计一下模型的大小以及可能的运行速度（通过一些指标侧面反应速度），这个时候我们就需要更深入的理解模型的内部结构和原理，从而通过估算模型内部的参数量和计算量来对模型的大小和速度进行一个初步评估。</p>\n<p>一个朴素的评估模型速度的想法是评估它的计算量。一般我们用FLOPS，即每秒浮点操作次数FLoating point OPerations per Second这个指标来衡量GPU的运算能力。这里我们用MACC，即乘加数Multiply-ACCumulate operation，或者叫MADD，来衡量模型的计算量。不过这里要说明一下，用MACC来估算模型的计算量只能大致地估算一下模型的速度。模型最终的的速度，不仅仅是和计算量多少有关系，还和诸如内存带宽、优化程度、CPU流水线、Cache之类的因素也有很大关系。</p>\n<p><strong>下面我们对计算量来进行介绍和定义，方便我们后续展开层的讲解：</strong></p>\n<p>神经网络中的许多计算都是点积，例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">y = w[<span class=\"number\">0</span>]*x[<span class=\"number\">0</span>] + w[<span class=\"number\">1</span>]*x[<span class=\"number\">1</span>] + w[<span class=\"number\">2</span>]*x[<span class=\"number\">2</span>] + ... + w[n-<span class=\"number\">1</span>]*x[n-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>此处，$w$ 和 $x$ 是两个向量，结果 $y$ 是标量（单个数字）。对于卷积层或完全连接的层（现代神经网络中两种主要类型的层），$w$ 是该层的学习权重，$x$ 是该层的输入。我们将$w[0]*x[0]+…$计数为一个乘法累加或1个MACC，这里的“累加”运算是加法运算，因为我们将所有乘法的结果相加，所以上式具有 $n$ 个这样的MACC（从技术上讲，上式中只有 $n-1$ 个加法，比乘法数少一个，所以这里知识认为MACC的数量是一个近似值）。就FLOPS而言，因为有 $n$ 个乘法和 $n-1$ 个加法，所以点积执行 $2n-1$ FLOPS，因此，MACC大约是两个FLOPS。现在，我们来看几种不同的层类型，以了解如何计算这些层的MACC数量。</p>\n<p><strong>注意了：下面的阐述如果没有特别说明，默认都是batch为1。</strong></p>\n<h1 id=\"全连接层计算量和参数量估计\"><a href=\"#全连接层计算量和参数量估计\" class=\"headerlink\" title=\"全连接层计算量和参数量估计\"></a>全连接层计算量和参数量估计</h1><p>在完全连接的层中，所有输入都连接到所有输出。 对于具有 $I$ 输入值和 $J$ 输出值的图层，其权重 $W$ 可以存储在 $I×J$ 矩阵中。 全连接层执行的计算为：<br>$$y = matmul(x, W) + b$$<br>在这里，$x$ 是 $I$ 输入值的向量，$W$ 是包含图层权重的 $I×J$ 矩阵，$b$ 是 $J$ 偏差值的向量，这些值也被相加。 结果 $y$ 包含由图层计算的输出值，并且也是大小 $J$ 的向量。对于完全连接层来说，矩阵乘法为 $matmul(x,W)$ ，其中具有 $I\\times J$ 个MACC（和权重矩阵 $W$ 大小一样），对于偏置 $b$，正好补齐了前面我们所说的点积中正好少一个加法操作。因此，比如一个具有300个输入神经元和100个输出神经元的全连接层执行 $300\\times 100 = 30,000$ 个MACC。特别提示：上面我们讨论的批次大小 $Batch=1$ 需要具体计算需要乘上Batch。</p>\n<blockquote>\n<p>也就是说，通常，将长度为 $I$ 的向量与 $I×J$ 矩阵相乘以获得长度为 $J$ 的向量，则需要 $I×J$ MACC或$(2I-1)\\times J$ FLOPS。</p>\n</blockquote>\n<p>上面我们讨论了全连接层的计算量，那么它的参数量是多少呢？这个应该很容易就算出来，对于全连接层而言，它的参数分别是权重 $W$ 和 偏置$b$，所以对于上面的例子中具有300个输入神经元和100个输出神经元的全连接层的参数量是： $300 \\times 100 + 100=30100$ ，这个很容易进行验证，下图是使用TensorFlow进行验证参数量：<br><img src=\"https://img-blog.csdnimg.cn/20210107113022453.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"激活函数计算量估计\"><a href=\"#激活函数计算量估计\" class=\"headerlink\" title=\"激活函数计算量估计\"></a>激活函数计算量估计</h1><p>通常，一个层后面紧接着就是非线性激活函数，例如ReLU或sigmoid，理所当然的计算这些激活函数需要时间，但在这里我们不用MACC进行度量，而是使用FLOPS进行度量，原因是它们不做点积，一些激活函数比其他激活函数更难计算，例如一个ReLU只是：<br>$$y = max(x, 0)$$<br>这是在GPU上的一项操作，激活函数仅应用于层的输出，例如在具有 $J$ 个输出神经元的完全连接层上，ReLU计算 $J$ 次，因此我们将其判定为 $J$ FLOPS。而对于Sigmoid激活函数来说，有不一样了，它涉及到了一个指数，所以成本更高：<br>$$y = 1 / (1 + exp(-x))$$<br>在计算FLOPS时，我们通常将加，减，乘，除，求幂，平方根等作为单个FLOP进行计数，由于在Sigmoid激活函数中有四个不同的运算，因此将其判定为每个函数输出4 FLOPS或总层输出 $J\\times 4$ FLOPS。所以实际上，通常不计这些操作，因为它们只占总时间的一小部分，更多时候我们主要对（大）矩阵乘法和点积感兴趣，所以其实我们通常都是忽略激活函数的计算量。</p>\n<p>对于参数量？注意了它压根没有参数，请看它们的公式，用TensorFlow验证如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107114606186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"LSTM计算量和参数量估计\"><a href=\"#LSTM计算量和参数量估计\" class=\"headerlink\" title=\"LSTM计算量和参数量估计\"></a>LSTM计算量和参数量估计</h1><p>关于LSTM的原理可以参考这一篇文章：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>，LSTM结构如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107204715551.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>实际上LSTM里面有 4 个非线性变换（3 个 门 + 1 个 tanh），每一个非线性变换说白了就是一个全连接网络，形如：$W[h_{t-1},x_t] + b$。其中，第一层是 $x_i$ 和 $h_i$ 的结合，维度就是embedding_size + hidden_size，第二层就是输出层，维度为 hidden_size，则它的计算量按照上文我们对全连接层的阐述，易得MACC为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">(embedding_size + hidden_size) * hidden_size * <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<p>四个非线性变换中，还会对全连接层的输出进行激活函数计算（三个sigmoid和一个tanh），由上面讨论的sigmoid我们知道，对于sigmoid的计算量为：(embedding_size + hidden_size) * hidden_size * 3 *4个FLOPS，而tanh的计算公式为：$\\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$，其中共有八个加，减，乘，除，求幂，平方根等计算，所以计算量为：(embedding_size + hidden_size) * hidden_size * 8个FLOPS。除此之外，LSTM除了在四个非线性变换中的计算，还有三个矩阵乘法（不是点积）、一个加法、一个tanh计算，其中三个矩阵乘法都是shape为(batch, hidden_size)，则这四个运算的计算量为：batch * hidden_size + batch * hidden_size + batch * hidden_size + batch * hidden_size + batch * hidden_size * 8，综上所述，LSTM的计算量为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">(embedding_size + hidden_size) * hidden_size * <span class=\"number\">4</span> 个MACC</span><br><span class=\"line\">embedding_size * hidden_size * <span class=\"number\">8</span> + hidden_size * (hidden_size + <span class=\"number\">20</span>) 个FLOPS</span><br></pre></td></tr></table></figure>\n<p>而该网络的参数量就是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">((embedding_size + hidden_size) * hidden_size + hidden_size) * <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<p>对于特征维128的输入，LSTM单元数为64的网络来说，LSTM的参数量为：((128 + 64) * 64 + 64) * 4 = 49408，通过TensorFlow验证如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107211244797.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"卷积层计算量和参数量估计\"><a href=\"#卷积层计算量和参数量估计\" class=\"headerlink\" title=\"卷积层计算量和参数量估计\"></a>卷积层计算量和参数量估计</h1><p>卷积层的输入和输出不是矢量，而是shape为 $H\\times W\\times C$的三维特征图，其中 $H$是特征图的高度，$W$ 是宽度，$C$ 是每个位置的通道数，正如我们所见今天使用的大多数卷积层都是二维正方内核，对于内核大小为 $K$ 的转换层，MACC的数量为：<br>$$K \\times K \\times Cin \\times Hout \\times Wout \\times Cout$$</p>\n<ul>\n<li>输出特征图中有Hout × Wout × Cout个像素；</li>\n<li>每个像素对应一个立体卷积核K x K x Cin在输入特征图上做立体卷积卷积出来的；</li>\n<li>而这个立体卷积操作，卷积核上每个点都对应一次MACC操作</li>\n</ul>\n<p>同样，我们在这里为了方便忽略了偏置和激活。我们不应该忽略的是层的stride，以及任何dilation因子，padding等。这就是为什么我们需要参看层的输出特征图的尺寸Hout × Wout，因它考虑到了stride等因素。比如，对于 $3\\times 3$，128个filter的卷积，在 $112\\times 112$ 带有64个通道的输入特征图上，我们执行MACC的次数是：</p>\n<p>$$3 \\times 3\\times 64\\times 112\\times 112\\times 128 = 924,844,032$$</p>\n<p>这几乎是10亿次累积运算！注意：在此示例中，我们使用“same”填充和$stride = 1$，以便输出特征图与输入特征图具有相同的大小。通常看到卷积层使用$stride = 2$，这会将输出特征图大小减少一半，在上面的计算中，我们将使用 $56 \\times 56$ 而不是 $112 \\times 112$。</p>\n<p>那我们现在来计算一下参数量，如果了解卷积的原理，应该也不难算出它的参数量（可能有人会说卷积原理怎么理解，这里推荐一篇写得通俗易懂的文章：<a href=\"https://zhuanlan.zhihu.com/p/77471866\">CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride)</a>），根据卷积的原理，对于上面的例子中， $3\\times 3$，128个filter的卷积，在 $112\\times 112$ 带有64个通道的输入特征图上的参数量为：$3\\times 3 \\times 64 \\times 128 + 128 = 73856$，用TensorFlow验证结果如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210107152351432.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"深度可分离卷积层\"><a href=\"#深度可分离卷积层\" class=\"headerlink\" title=\"深度可分离卷积层\"></a>深度可分离卷积层</h1><p>深度可分离卷积是将常规卷积因式分解为两个较小的运算，它们在一起占用的内存更少（权重更少），并且速度更快，这些层在移动设备上可以很好地工作，既是MobileNet的基础，也是Xception等大型模型的基础。深度可分离卷积中，第一个操作是深度卷积，它在很多方面与常规卷积相似，不同之处在于我们不合并输入通道，也就是说输出通道数始终与输入通道数相同，深度卷积的MACC总数为：<br>$$K × K × Cin × Hout × Wout$$<br>这减少了 $Cin$ 的工作量，比常规的卷积层效率更高。当然，仅深度卷积是不够的，我们还需要增加“可分离”，第二个操作是常规卷积，但始终使用内核大小 $1\\times 1$，即 $K=1$，也称为“逐点”卷积，MACC的数量为：<br>$$Cin × Hout × Wout × Cout$$<br>深度可分离卷积分为两个操作，深度卷积和可分离，所以现在我们对两种操作分别就上面的例子计算，和并和常规 $3\\times 3$ 卷积进行比较：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">3</span>×<span class=\"number\">3</span> depthwise          : <span class=\"number\">7</span>,<span class=\"number\">225</span>,<span class=\"number\">344</span></span><br><span class=\"line\"><span class=\"number\">1</span>×<span class=\"number\">1</span> pointwise          : <span class=\"number\">102</span>,<span class=\"number\">760</span>,<span class=\"number\">448</span></span><br><span class=\"line\">深度可分离卷积          : <span class=\"number\">109</span>,<span class=\"number\">985</span>,<span class=\"number\">792</span> MACCs</span><br><span class=\"line\"></span><br><span class=\"line\">常规 <span class=\"number\">3</span>×<span class=\"number\">3</span> 卷积           : <span class=\"number\">924</span>,<span class=\"number\">844</span>,032 MACCs</span><br></pre></td></tr></table></figure>\n<p>所以深度可分离卷积的计算量简化为：<br>$$Cin × Hout × Wout × (K × K + Cout)$$</p>\n<p>我们来看看MobileNet V2中的对深度可分离卷积的拓展，MobileNet V2相比与V1，主要是由DW+PW两层变成了下面的三层PW+DW+PW：</p>\n<ul>\n<li>一个 $1\\times 1$ 卷积，为特征图添加更多通道（称为expansion layer）</li>\n<li>$3\\times 3$ 深度卷积，用于过滤数据（depthwise convolution）</li>\n<li>$1\\times 1$ 卷积，再次减少通道数（projection layer，bottleneck convolution）</li>\n</ul>\n<p>这种扩展块中MACC数量的公式：Cexp = (Cin × expansion_factor)，（expansion_factor用于创建深度层要处理的额外通道，使得Cexp在此块内使用的通道数量）</p>\n<ul>\n<li>$MACC\\ expansion\\ layer = Cin \\times Hin \\times Win \\times Cexp$，(参照上面传统卷积，把卷积核设置为1x1即得)</li>\n<li>$MACC\\ depthwise\\ layer = K \\times K \\times Cexp \\times Hout \\times Wout$(参照MoblieNet V1分析)</li>\n<li>$MACC\\ projection\\ layer = Cexp \\times Hout \\times Wout \\times Cout$(参照MoblieNet V1分析，或者传统卷积把卷积核设置为1x1即得)</li>\n</ul>\n<p>把所有这些放在一起：</p>\n<p>$$MACC_v2 = Cin \\times Hin \\times Win \\times Cexp + (K \\times K + Cout) \\times Cexp \\times Hout \\times Wout$$</p>\n<p>这与MobileNet V1使用的深度可分层相比如何？如果我们使用输入特征图 $112\\times 112\\times 64$ 扩展因子6，以及 $stride = 1$ 的 $3\\times 3$ 深度卷积和128输出通道，那么MACC的总数是：</p>\n<p>$$(3 \\times 3 + 128 + 64) \\times (64 \\times 6) \\times 112 \\times 112 = 968,196,096$$</p>\n<p>这不是比以前更多吗？是的，它甚至超过了最初的 $3\\times 3$ 卷积。但是……请注意，由于扩展层，在这个块内，我们实际上使用了 $64 \\times 6 = 384$ 通道。因此，这组层比原始的$3\\times3$ 卷积做得更多（从64到128个通道），而计算成本大致相同。</p>\n<h1 id=\"Batch-normalization\"><a href=\"#Batch-normalization\" class=\"headerlink\" title=\"Batch normalization\"></a>Batch normalization</h1><p>在现代网络中，通常在每个卷积层之后都包含一个batch norm层。对于想要详细了解Batch normalization的原理，可参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/340219662\">论文阅读笔记：看完也许能进一步了解Batch Normalization</a></p>\n<p>Batch normalization在层的输出上进行计算，针对每个输出值，计算公式如下：<br>$$z = gamma * (y - mean) / sqrt(variance + epsilon) + beta$$<br>此处，$y$ 是上一层的输出图中的元素。 我们首先通过减去该输出通道的平均值并除以标准偏差来对该值进行归一化（epsilon 用于确保不除以0，通常为0.001），然后，我们将系数gamma缩放，然后添加一个偏差或偏移beta。每个通道都有自己的gamma，beta，均值和方差值，因此，如果卷积层的输出中有 $C$ 个通道，则Batch normalization层将学习 $C\\times 4$ 参数，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20210107164949918.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>通常将Batch normalization应用于卷积层的输出，而在ReLU之前，在这种情况下，我们可以做一些数学运算以使Batch normalization层消失！由于在全连接层中进行的卷积或矩阵乘法只是一堆点积，它们是线性变换，而上面给出的Batch normalization公式也是线性变换，因此我们可以将这两个公式组合为一个变换。我们只需要将Batch normalization参数合并到前面各层的权重中，其数学运算非常简单。 具体可以参见上面的那篇关于Batch Normalization的文章，也就是说我们可以完全忽略Batch Normalization层的影响，因为我们在进行推理时实际上将其从模型中删除了。</p>\n<blockquote>\n<p>注意：此trick仅在层的顺序为：卷积-&gt;BN-&gt;ReLU时才有效；不适用于：卷积-&gt;ReLU-&gt;BN。ReLU是一个非线性操作，它会把数据弄乱。（但如果批量标准化后面紧跟一个新的卷积层，你可以反过来折叠参数）</p>\n</blockquote>\n<h1 id=\"其它层\"><a href=\"#其它层\" class=\"headerlink\" title=\"其它层\"></a>其它层</h1><p><strong>池化层</strong><br>到此我们研究了卷积层和全连接层，这两个是现代神经网络中最重要的组成部分。但是也有其他类型的层，例如池化层。这些其他层类型肯定需要时间，但它们不使用点积，因此不能用MACC测量。如果你对计算FLOPS感兴趣，只需获取特征图大小并将其乘以表示处理单个输入元素的难度的常量。</p>\n<p>示例：在 $112\\times 112$ 具有128通道的特征图上具有过滤器大小2和步幅2的最大池化层需要 $112 \\times 112 \\times 128 = 1,605,632$ FLOPS或1.6兆FLOPS。当然，如果步幅与滤波器尺寸不同（例如 $3\\times 3$窗口，$2\\times 2$步幅），则这些数字会稍微改变。</p>\n<p>但是，在确定网络的复杂性时，通常会忽略这些附加层。毕竟，与具有100个MFLOPS的卷积/全连接层相比，1.6 MFLOPS非常小。因此，它成为网络总计算复杂度的舍入误差。</p>\n<p><strong>Concate层</strong><br>某些类型的操作，例如结果的连接，通常甚至可以免费完成。不是将两个层分别写入自己的输出张量中，然后有一个将这两个张量复制到一个大张量的连接层。相反，第一层可以直接写入大张量的前半部分，第二层可以直接写入后半部分。不需要单独的复制步骤。</p>\n<ul>\n<li>减少参数</li>\n<li>降低精度</li>\n<li>融合计算单元步骤</li>\n</ul>\n<p><em>参考资料</em>：</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/b8d48c99a47c\">卷积神经网络中参数量的计算与模型占用显存的计算</a></li>\n<li><a href=\"http://machinethink.net/blog/how-fast-is-my-model/\">How fast is my model?</a></li>\n<li><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a></li>\n<li><a href=\"https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\">Number of parameters in an LSTM model</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/77471991\">CNN卷积层、全连接层的参数量、计算量</a></li>\n<li><a href=\"https://arxiv.org/abs/1506.02626\">Learning both Weights and Connections for Efficient Neural Networks</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","LSTM","神经网络","CNN","参数量","计算量"]},{"title":"深度学习中眼花缭乱的Normalization学习总结","url":"/Deep-Learning/32dcc7d3f168/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于深度学习而言，正则化方法就是“通过把一部分不重要的复杂信息损失掉，以此来降低拟合难度以及过拟合的风险，从而加速了模型的收敛”，而本篇文章我们要讲的Normalization方法的目的就是让分布稳定下来（降低各维度数据的方差），不同的正则化方法的区别只是操作的信息维度不同，即选择损失信息的维度不同。</p>\n<p>本篇文章将结合TensorFlow和Pytorch计算框架，阐述几种归一化算法：Batch Normalization(BN)、Layer Normalization(LN)、Weight Normalization(WN)、Instance Normalization(IN)、Group Normalization(GN)、Cosine Normalization(CN)。</p>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>“Covariate Shift: A Review and Analysis on Classifiers”论文中对Covariate Shift的描述：</p>\n<blockquote>\n<p>In real world, the joint distribution of inputs to the model and outputs of the model differs between training and test data, which is called dataset shift</p>\n</blockquote>\n<p>关于Covariate Shift的一篇方法综述性论文，可参考我的另一篇<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记</a>。</p>\n<p>下面我用白话简单的阐述一下Covariate Shift和Internal Covariate Shift</p>\n<ul>\n<li>Covariate Shift：假设 $q_1(x)$ 是测试集中一个样本点的概率密度，$q_0(x)$ 是训练集中一个样本点的概率密度。最终我们估计一个条件概率密度 $p(y|x,\\theta)$，它由 $x$ 和一组参数 $\\theta={\\theta_1,\\theta_2,…,\\theta_m}$ 所决定。对于一组参数来说，对应 $loss(\\theta)$ 函数评估性能的好坏。综上，当我们找出在 $q_0(x)$ 分布上最优的一组 $\\theta^{‘}$ 时，能否保证 $q_1(x)$ 上测试时也最好呢？传统机器学习假设训练集和测试集是独立同分布的，即 $q_0(x)=q_1(x)$，所以可以推出最优 $\\theta^{‘}$ 依然可以保证 $q_1(x)$ 最优。但现实当中这个假设往往不成立，伴随新数据产生，老数据会过时，当 $q_0(x)$ 不再等于 $q_1(x)$ 时，就被称作Covariate Shift。</li>\n<li>Internal Covariate Shift：深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略，Google 将这一现象总结为Internal Covariate Shift，简称 ICS。</li>\n</ul>\n<p>ICS会导致什么问题？简而言之，每个神经元的输入数据不再是“独立同分布”。</p>\n<ul>\n<li>上层参数需要不断适应新的输入数据分布，降低学习速度。</li>\n<li>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</li>\n<li>每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li>\n</ul>\n<p>我们以神经网络中的一个普通神经元为例，神经元接收一组输入向量$x=(x_1,x_2,…,x_d)$，通过某种运算后，输出一个标量值：$y=f(x)$。由于 ICS 问题的存在，$x$ 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 $x$ 喂给神经元之前，先对其做平移和伸缩变换， 将 $x$ 的分布规范化成在固定区间范围的标准分布。通用变换框架就如下所示：<br>$$h=f(g\\cdot \\frac{x-\\mu}{\\sigma}+b)$$<br>其中，$\\mu$ 是平移参数，$\\sigma$ 是缩放参数，通过这两个参数进行 shift 和 scale 变换：$\\hat{x}=\\frac{x-\\mu}{\\sigma}$得到的数据符合均值为 0、方差为 1 的标准分布。$b$ 是再平移参数，$g$ 是再缩放参数。将上一步得到的 $\\hat{x}$ 进一步变换为：$y=g\\cdot \\hat{x}+b$，最终得到的数据符合均值为 $b$ 、方差为 $g^2$ 的分布。</p>\n<h1 id=\"Batch-Normalization-BN\"><a href=\"#Batch-Normalization-BN\" class=\"headerlink\" title=\"Batch Normalization(BN)\"></a>Batch Normalization(BN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>\n</ul>\n<p>针对Batch Normalization的详细介绍，可以参考这一篇文章：论文阅读笔记：<a href=\"https://zhuanlan.zhihu.com/p/340219662\">看完也许能进一步了解Batch Normalization</a></p>\n<p>Batch Normalization 于2015年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 $x_i$ 的均值和方差，因而称为 Batch Normalization。<br>$$\\mu_i=\\frac{1}{M}\\sum x_i \\\\ \\ \\sigma_i=\\sqrt{\\frac{1}{M}\\sum(x_i-\\mu_i)^2+\\epsilon}$$<br>其中$M$是mini-batch 的大小。按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。</p>\n<p>BN 独立地规范化每一个输入维度 $x_i$ ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。因此，<strong>BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多</strong>。</p>\n<p>另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。不过，也有研究者专门提出了适用于 RNN 的 BN 使用方法，这里先不展开了，附一张Batch Normalization的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201230094634732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Layer-Normalization-LN\"><a href=\"#Layer-Normalization-LN\" class=\"headerlink\" title=\"Layer Normalization(LN)\"></a>Layer Normalization(LN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1607.06450.pdf\">Layer Normalization</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/258977332\">论文阅读笔记</a></li>\n</ul>\n<p>Layer Normalization就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。<br>$$\\mu=\\sum_ix_i,\\quad \\sigma=\\sqrt{\\sum_i(x_i-\\mu)^2+\\epsilon}$$</p>\n<p>其中 $i$ 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 $\\mu$, $\\sigma$, $g$, $b$ 均为标量（BN中是向量），所有输入共享一个normalization变换。</p>\n<p>LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。</p>\n<p>但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。</p>\n<p>在NLP任务中，对于不同的训练案例，通常有不同的句子长度。这在RNN中很容易处理，因为每个时间步使用相同的权重。但是，当我们以明显的方式将批归一化应用于RNN时，我们需要为序列中的每个时间步计算并存储单独的统计信息。如果测试序列比任何训练序列都要长，这是有问题的。层归一化不存在此类问题，因为其归一化项仅取决于当前时间步对层的求和输入。 在所有时间步中，它也只有一组增益和偏置参数共享。</p>\n<p>这里附一张Batch Normalization和Layer Normalization之间计算差异的图：<br><img src=\"https://img-blog.csdnimg.cn/20201230161155494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>LN用于RNN进行Normalization时，取得了比BN更好的效果。但用于CNN时，效果并不如BN明显。</p>\n<h1 id=\"Weight-Normalization-WN\"><a href=\"#Weight-Normalization-WN\" class=\"headerlink\" title=\"Weight Normalization(WN)\"></a>Weight Normalization(WN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1602.07868.pdf\">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a></li>\n</ul>\n<p>在前面列的变换框架中：<br>$$h=f(g\\cdot \\frac{x-\\mu}{\\sigma}+b)$$<br>中，经过规范化之后的 $y$ 作为输入送到下一个神经元，应用以 $w$ 为参数的 $f_w(\\cdot)$ 函数定义的变换。最普遍的变换是线性变换，即 $f_w(x)=w\\cdot x$。BN 和 LN 均将规范化应用于输入的特征数据 $x$ ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 $w$ ，这就是 WN 名称的来源。具体而言，WN 提出的方案是，将权重向量 $w$ 分解为向量方向 $\\hat{v}$ 和向量模 $g$ 两部分：<br>$$w=g\\cdot \\hat{v}=g\\cdot \\frac{v}{||v||}$$<br>其中 $v$ 是与 $w$ 同维度的向量， $||v||$ 是欧氏范数，因此 $\\hat{v}$ 是单位向量，决定了 $w$ 的方向。$g$ 是标量，决定了 $w$ 的长度。由于 $||w||\\equiv |g|$ ，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。通过推导（如下），我们会发现WN其实只是在前述框架上令 $\\sigma=||v||,\\quad \\mu=0, \\quad b=0$。<br>$$f_w(WN(x))=w\\cdot WN(x)=g\\cdot \\frac{v}{||v||}\\cdot x=v\\cdot g\\cdot \\frac{x}{||v||}=f_v(g\\cdot \\frac{x}{||v||})$$</p>\n<p>BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。<strong>虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同</strong>。另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 $\\mu=0$。 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 $\\mu$ 。WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。</p>\n<p>和目前主流归一化方法不同的是，WN的归一化操作作用在了权值矩阵之上。从其计算方法上来看，WN完全不像是一个归一化方法，更像是基于矩阵分解的一种优化策略，它带来了四点好处：</p>\n<ul>\n<li>更快的收敛速度；</li>\n<li>更强的学习率鲁棒性；</li>\n<li>可以应用在RNN等动态网络中；</li>\n<li>对噪声更不敏感，更适用在GAN，RL等场景中。</li>\n</ul>\n<p>说WN不像归一化的原因是它并没有对得到的特征范围进行约束的功能，所以WN依旧对参数的初始值非常敏感，这也是WN一个比较严重的问题。所以针对这个问题，论文中作者实验采用WN+Mean-Only BN的方法。在每一层的layer的激活函数之前，我们发现$t=w\\cdot x=\\frac{g}{||v||}v\\cdot x$，我们发现虽然我们将权重 $w$ 进行分离，但是每一层的激活函数之前的输出的均值仍然与 $v$ 有关。因此作者将WN与BN进行结合，采用移动平均去计算每个mini-batch上的均值 $\\mu [t]$，因此：<br>$$t=wx$$    $$\\hat{t}=t-\\mu [t] + b$$    $$y=\\phi (\\hat{t})$$<br>对激活函数之前的t的反向传播的损失函数为：<br>$$\\bigtriangledown_tL=\\bigtriangledown_{\\hat{t}}L-\\mu [\\bigtriangledown_{\\hat{t}}L]$$<br>对损失函数来说，mean-only batch normalization 可以有效的在反向传播中centering梯度。并且对于mean-only的BN来说，其计算量要小于full-BN的方法。另外，这个方法比Full-BN引入更轻微的噪声。</p>\n<h1 id=\"Cosine-Normalization-CN\"><a href=\"#Cosine-Normalization-CN\" class=\"headerlink\" title=\"Cosine Normalization(CN)\"></a>Cosine Normalization(CN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1702.05870v5.pdf\">Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks</a><br><img src=\"https://img-blog.csdnimg.cn/20201230201656422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量 $w$ 和 特征数据向量 $x$ 的点积，向量点积是无界（unbounded）的啊！那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围。于是，Cosine Normalization 就出世了。他们不处理权重向量 $w$ ，也不处理特征数据向量 $x$ ，就改了一下线性变换的函数：<br>$$f_w(x)=cos\\theta=\\frac{w\\cdot x}{||w||\\cdot ||x||}$$<br>其中 $\\theta$ 是  $w$ 和 $x$ 的夹角。不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是  $w$ 和 $x$ 的内积，而分母则可以看做用  $w$ 和 $x$ 二者的模之积进行规范化。对比一下 WN 的公式：<br>$$f_w(WN(x))=f_v(g\\cdot \\frac{x}{||v||})$$<br>一定程度上可以理解为，WN 用 权重的模 $||v||$ 对输入向量进行 scale，而 CN 在此基础上用输入向量的模 $||x||$ 对输入向量进行了进一步的 scale。CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。</p>\n<h1 id=\"Instance-Normalization\"><a href=\"#Instance-Normalization\" class=\"headerlink\" title=\"Instance Normalization\"></a>Instance Normalization</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1607.08022.pdf\">Instance Normalization:The Missing Ingredient for Fast Stylization</a></li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201230212443641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>IN在计算归一化统计量时并没有像BN那样跨样本、单通道，也没有像LN那样单样本、跨通道。它是取的单通道，单样本上的数据进行计算，如图1最右侧所示。所以对比BN的公式，它只需要它只需要去掉批量维的求和即可：<br>$$u_{ti}=\\frac{1}{HW}\\sum_{l=1}^W\\sum_{m=1}^Hx_{tilm}\\quad \\sigma_{ti}^2=\\frac{1}{HW}\\sum_{l=1}^W\\sum_{m=1}^H(x_{tilm}-u_{ti})^2 \\quad y_{tijk}=\\frac{x_{tilm}-u_{ti}}{\\sqrt{\\sigma_{ti}^2+\\epsilon}}$$<br>在TensorFlow实现中，对于是否使用BN中的可学习参数 $\\beta$ 和 $\\gamma$ ，从LN的TensorFlow中源码中我们可以看出这两个参数是要使用的。但是我们也可以通过将其值置为False来停用它们，这一点和其它归一化方法在TensorFlow中的实现是相同的。</p>\n<h1 id=\"Group-Normalization-GN\"><a href=\"#Group-Normalization-GN\" class=\"headerlink\" title=\"Group Normalization(GN)\"></a>Group Normalization(GN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1803.08494.pdf\">Group Normalization</a></li>\n</ul>\n<p>组归一化 (GN) 将输入的通道分成较小的子组，并根据其均值和方差归一化这些值。由于 GN 只对单一样本起作用，因此该技术与batch大小无关。在图像分类任务中，GN 的实验得分与BN十分接近。如果整体batch大小很小，则使用 GN 而不是BN可能更为有利，因为较小的batch大小会导致BN的性能不佳，下面附一张原论文中的图：<br><img src=\"https://img-blog.csdnimg.cn/20201230205516507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<blockquote>\n<p>从左到右一次是BN，LN，IN，GN。众所周知，深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式，N是batch size，H/W是feature的高/宽，C是feature的channel，压缩H/W至一个维度，其三维的表示如上图，假设单个方格的长度是1，那么其表示的是[6, 6，*, * ]</p>\n</blockquote>\n<ul>\n<li>BN在batch的维度上norm，归一化维度为[N，H，W]，对batch中对应的channel归一化；</li>\n<li>LN避开了batch维度，归一化的维度为[C，H，W]；</li>\n<li>IN 归一化的维度为[H，W]；</li>\n<li>而GN介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]</li>\n</ul>\n<p>传统角度来讲，在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。而更高维的特征比如VLAD和Fisher Vectors(FV)也可以看作是group-wise feature，此处的group可以被认为是每个聚类（cluster）下的子向量sub-vector。</p>\n<p>从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p>\n<p>导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。作者基于此，提出了组归一化（Group Normalization）的方式，且效果表明，显著优于BN、LN、IN等。</p>\n<h1 id=\"Normalization的不变性\"><a href=\"#Normalization的不变性\" class=\"headerlink\" title=\"Normalization的不变性\"></a>Normalization的不变性</h1><ul>\n<li><strong>权重伸缩不变性</strong>（weight scale invariance）指的是，当权重 $W$ 按照常量 $\\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：<br>$$Norm(W^{‘}x)=Norm(Wx)$$<br>其中，$W^{‘}=\\lambda W$。上述规范化方法均有这一性质，这是因为，当权重 $W$ 伸缩时，对应的均值和标准差均等比例伸缩，分子分母相抵：<br>$$Norm(W^{‘}x)=Norm(g\\cdot \\frac{W^{‘}x-\\mu^{‘}}{\\sigma^{‘}}+b)=Norm(g\\cdot \\frac{\\lambda Wx-\\lambda\\mu}{\\lambda \\sigma}+b)=Norm(g\\cdot \\frac{Wx-\\mu}{\\sigma}+b)=Norm(Wx)$$<br>权重伸缩不变性可以有效地提高反向传播的效率，由于：<br>$$\\frac{\\partial Norm(W^{‘}x)}{\\partial x}=\\cdot\\frac{\\partial Norm(Wx)}{\\partial x}$$<br>因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率。由于：<br>$$\\frac{\\partial Norm(W^{‘}x)}{\\partial W^{‘}}=\\frac{1}{\\lambda}\\cdot\\frac{\\partial Norm(Wx)}{\\partial W}$$<br>因此，下层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。</li>\n<li><strong>数据伸缩不变性</strong>：数据伸缩不变性（data scale invariance）指的是，当数据 $x$ 按照常量 $\\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：<br>$$Norm(Wx^{‘})=Norm(Wx)$$<br>其中，$x^{‘}=\\lambda x$。数据伸缩不变性仅对 BN、LN 和 CN 成立。因为这三者对输入数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母互相抵消。而 WN 不具有这一性质。数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。对于某一层神经元 $h_l=f_{W_l}(x_l)$ 而言，展开可得：<br>$$h_l=f_{W_l}(x_l)=f_{W_l}(f_{W_{l\\ 1}}(x_l\\ 1))=…=x_0 \\coprod_{k=0}^{l}W_k$$<br>每一层神经元的输出依赖于底下各层的计算结果。如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。加入 Normalization 之后，不论底层的数据如何变化，对于某一层神经元  $h_l=f_{W_l}(x_l)$ 而言，其输入 $x_l$ 永远保持标准的分布，这就使得高层的训练更加简单。从梯度的计算公式来看：<br>$$\\frac{\\partial Norm(Wx^{‘})}{\\partial W}=\\cdot\\frac{\\partial Norm(Wx)}{\\partial W}$$<br>数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1>选择什么样的归一化方式，取决于你关注数据的哪部分信息，如果某个维度信息的差异性很重要，需要被拟合，那就别在那个维度进行归一化。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://theaisummer.com/normalization/\">In-layer normalization techniques for training very deep neural networks</a></li>\n<li><a href=\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\">归一化</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/166101119\">2020 年 BatchNorm 还能大水漫灌，吗</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/33173246\">详解深度学习中的Normalization，BN/LN/WN</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/56542480\">模型优化之Instance Normalization</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35005794\">全面解读Group Normalization-（吴育昕-何恺明 ）</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247491157&idx=1&sn=5ab87d7847666aac2cef8463a2f0f078&chksm=ec1ff3acdb687aba68bfeb32a79421cd350217cfd4804d98eab85fd7c8aa95923fbd6790788a&scene=21#wechat_redirect\">深度学习中的Normalization模型</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","归一化","机器学习","Normalization"]},{"title":"深度学习矩阵乘法的终极奥义einsum，结合多个计算框架上的使用","url":"/Deep-Learning/8d94140b2f7f/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>这里我们来好好探讨一下深度学习中，矩阵乘法的使用，其实主要是围绕einsum来进行探讨，即通过爱因斯坦求和约定来指导矩阵乘法，同时附带陈列其他矩阵乘法的API，方便进行直观感受。本文中的计算框架及版本如下：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0</li>\n<li>Numpy1.19</li>\n</ul>\n<h1 id=\"爱因斯坦求和约定\"><a href=\"#爱因斯坦求和约定\" class=\"headerlink\" title=\"爱因斯坦求和约定\"></a>爱因斯坦求和约定</h1><p>我们讨论einsum绕不开爱因斯坦求和约定的，爱因斯坦求和约定（Einstein summation convention）是一种标记的约定，又称为爱因斯坦标记法（Einstein notation），在处理关于坐标的方程式时非常有用，用一句话来总结爱因斯坦求和约定，就是：</p>\n<blockquote>\n<p>当式子中任何一个角标出现了两次，并且一次是上标、一次是下标时，那么该式表示的实际上是对这个角标一切可能值的求和。换言之，如果角标 $i$ 作为上标和下标各出现了一次，那么式子相当于添加了一个关于 $i$ 的求和符号</p>\n</blockquote>\n<p>我们下面使用线性函数和矩阵运算来对爱因斯坦求和约定进行举例说明：</p>\n<ul>\n<li>线性函数：从张量中我们知道，一个1-线性函数可以表示为一个向量，这样的向量常被称为余向量、补向量或者1-形式。通常，我们用下标来表示一个余向量的各分量：$a=(a_1,a_2,a_3)$ ，而用上标来表示一个通常的几何向量：$v=(v^1,v^2,v^3)$。注意，上标不是乘方，则 $a$ 和 $v$ 的内积是：<br>$$\\sum_{i=1,2,3}^{}a_iv^i$$<br>用爱因斯坦求和约定， $a$ 和 $v$ 的内积就可以写为 $a_iv^i$</li>\n<li>矩阵运算：对于矩阵 $A$，我们把其第 $i$ 行第 $j$ 列的元素表示为 $A_j^i$。则矩阵乘法表示为：如果 $A=BC$，那么 $A_j^i=B_k^iC_j^k$。矩阵 $A$ 的迹为 $A_i^i$</li>\n</ul>\n<p>由于重复出现而实际上应该是求和的指标，被称为赝指标或者哑指标（dummy index），因为它们不是真正的指标，而是可以用任意字母代替的。没有求和的指标是固定的，是真正的指标．比如说 $B_k^iC_j^k$ 中 $k$ 可以是任何字母，但是 $i$ 和 $j$ 是不可以替换成别的字母的，因为它们由 $A_j^i$ 决定了。在这里，哑指标实际上是表示遍历全部可能的真指标。</p>\n<p>爱因斯坦求和约定的表示方法脱胎于矩阵乘法的要求，但是却不依赖于矩阵行和列的形式，转而关注指标间的配合，相比传统的矩阵表达，能更方便地推广到高阶张量的情形中。本文关于爱因斯坦求和约定的相关点到为止，如果感兴趣其公式，可以参考这一篇文章：<a href=\"https://zhuanlan.zhihu.com/p/46006162\">爱因斯坦求和约定</a>。</p>\n<h1 id=\"einsum介绍\"><a href=\"#einsum介绍\" class=\"headerlink\" title=\"einsum介绍\"></a>einsum介绍</h1><p>通过使用einsum函数，我们可以使用爱因斯坦求和约定（Einstein summation convention）在NumPy数组上指定操作。einsum函数由于其强大的表现力和智能循环，它在速度和内存效率方面通常可以超越我们常见的array函数。但缺点是，可能需要一段时间才能理解符号，有时需要尝试才能将其正确的应用于棘手的问题，当然熟悉之后得心应手才是使用关键。</p>\n<p>einsum以一种优雅的方式，表示各种矩阵运算，好处在于你不需要去记和使用计算框架中（TensorFlow|PyTorch|Numpy）点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法的函数名字和签名。从某种程度上解决引入不必要的张量变形或转置运算，以及可以省略的中间张量的现象。不仅如此，einsum有时可以编译到高性能代码，事实上，PyTorch最近引入的能够自动生成GPU代码并为特定输入尺寸自动调整代码的张量理解（Tensor Comprehensions）就基于类似einsum的领域特定语言。此外，可以使用opt einsum和tf einsum opt这样的项目优化einsum表达式的构造顺序。</p>\n<p>假设我们有两个多维数组A和B，现在让我们要进行如下操作：</p>\n<ul>\n<li>以特定方式将A与B相乘以创建新的乘积数组</li>\n<li>沿特定轴求和该新数组</li>\n<li>以特定顺序转置新数组的轴</li>\n</ul>\n<p>einsum帮助我们更快，更高效地执行此操作，当然，NumPy函数的组合（例如multiply，sum和transpose）也可以实现。</p>\n<h1 id=\"einsum应用\"><a href=\"#einsum应用\" class=\"headerlink\" title=\"einsum应用\"></a>einsum应用</h1><p>我们在这一节用Numpy的einsum进行讲解说明（Numpy中einsum先被开发出来，TensorFlow和PyTorch都在一定程度上参考了它），在下一节，我会将TensorFlow、PyTorch和Numpy的API都贴出来。</p>\n<p>在Numpy中，einsum使用格式字符串和任意数量的Numpy张量作为参数调用，并返回结果张量。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109145722330.png#pic_center\" alt=\"在这里插入图片描述\"><br>格式字符串包含分隔参数说明的逗号（，）和将参数说明与张量的参数分开的箭头（-&gt;）。参数说明中的数量和参数的数量必须匹配，并且必须精确地出现一个箭头，后跟一个结果说明</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109145901549.png#pic_center\" alt=\"在这里插入图片描述\"><br>格式字符串中的字符数完全等于该张量的维数。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109150049280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下面展示einsum()格式字符串的完整示例。 尝试猜测结果将是什么？注意了，0维张量（标量）对于参数和结果均有效，由空字符串“”表示。 再次提醒您，格式字符串必须仅由ASCII小写或大写字母组成。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109150320881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们这里使用一个实际的例子来进行说明，首先我们要相乘的两个​​数组是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">A &#x3D; array([[1, 1, 1],</span><br><span class=\"line\">           [2, 2, 2],</span><br><span class=\"line\">           [5, 5, 5]])</span><br><span class=\"line\">B &#x3D; array([[0, 1, 0],</span><br><span class=\"line\">           [1, 1, 0],</span><br><span class=\"line\">           [1, 1, 1]])</span><br></pre></td></tr></table></figure>\n<p>我们的矩阵乘法np.einsum(‘ij,jk-&gt;ik’, A, B)大致如下：<br><img src=\"https://img-blog.csdnimg.cn/20201109154121134.png#pic_center\" alt=\"在这里插入图片描述\"><br>要了解输出数组的计算方法，请记住以下三个规则：</p>\n<ul>\n<li><p><strong>在输入数组中重复的字母意味着值沿这些轴相乘</strong>。乘积结果为输出数组的值。在本例中，我们使用字母 j 两次：A和B各一次。这意味着我们将A每一行与B每列相乘。这只在标记为 j 的轴在两个数组中的长度相同（或者任一数组长度为1）时才有效。</p>\n</li>\n<li><p><strong>输出中省略的字母意味着沿该轴的值将相加</strong>。在这里，j 不包含在输出数组的标签中。通过累加的方式将它从轴上除去，最终数组中的维数减少1。如果输出是’ijk’，我们得到的结果是3x3x3数组（如果我们不提供输出标签，只写箭头，则对整个数组求和）。</p>\n</li>\n<li><p><strong>我们可以按照我们喜欢的任何顺序返回未没进行累加的轴</strong>。如果我们省略箭头’-&gt;’，NumPy会将只出现一次的标签按照字母顺序排列（因此实际上’ij,jk-&gt;ik’相当于’ij,jk’）。如果我们想控制输出的样子，我们可以自己选择输出标签的顺序。例如，’ij,jk-&gt;ki’为矩阵乘法的转置。</p>\n</li>\n</ul>\n<p>现在，我们已经知道矩阵乘法是如何工作的。下图显示了如果我们不对 j 轴进行求和，而是通过写<code>np.einsum(‘ij,jk-&gt;ijk’, A, B)</code>将其包含在输出中，我们会得到什么。右边代表 j 轴已经求和：<br><img src=\"https://img-blog.csdnimg.cn/20201109154140281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>注意，由于<code>np.einsum(‘ij,jk-&gt;ik’, A, B)</code>函数不构造3维数组然后求和，它只是将总和累加到2维数组中。下面是两个表格展示了einsum如何进行各种NumPy操作。我们可以用它来熟悉符号。</p>\n<ul>\n<li>让A和B是两个形状兼容的一维数组（也就是说，我们相应的轴的长度要么相等，要么其中一个长度为1）：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>调用</th>\n<th>Numpy等式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>(‘i’, A)</td>\n<td>A</td>\n<td>返回数组A的视图</td>\n</tr>\n<tr>\n<td>(‘i-&gt;’, A)</td>\n<td>sum(A)</td>\n<td>数组A值的总和</td>\n</tr>\n<tr>\n<td>(‘i,i-&gt;i’, A, B)</td>\n<td>A * B</td>\n<td>A和B的数组元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘i,i’, A, B)</td>\n<td>inner(A, B)</td>\n<td>A和B的点积（内积）</td>\n</tr>\n<tr>\n<td>(‘i,j-&gt;ij’, A, B)</td>\n<td>outer(A, B)</td>\n<td>A和B的外积（叉积）</td>\n</tr>\n</tbody></table>\n<ul>\n<li>现在，我们A和B是与之兼容形状的两个二维数组：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>调用</th>\n<th>Numpy等式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>(‘ij’, A)</td>\n<td>A</td>\n<td>返回A的视图</td>\n</tr>\n<tr>\n<td>(‘ji’, A)</td>\n<td>A.T</td>\n<td>A的转置</td>\n</tr>\n<tr>\n<td>(‘ii-&gt;i’, A)</td>\n<td>diag(A)</td>\n<td>A的主对角线</td>\n</tr>\n<tr>\n<td>(‘ii’, A)</td>\n<td>trace(A)</td>\n<td>A的主对角线的和</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;’, A)</td>\n<td>sum(A)</td>\n<td>A的值相加</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;j’, A)</td>\n<td>sum(A, axis=0)</td>\n<td>通过A的轴竖直（列）求和</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;i’, A)</td>\n<td>sum(A, axis=1)</td>\n<td>通过A的轴水平（行）求和</td>\n</tr>\n<tr>\n<td>(‘ij,ij-&gt;ij’, A, B)</td>\n<td>A * B</td>\n<td>A和B逐元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘ij,ji-&gt;ij’, A, B)</td>\n<td>A * B.T</td>\n<td>A和B的转置逐个元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘ij,jk’, A, B)</td>\n<td>dot(A, B)</td>\n<td>A和B的矩阵乘法</td>\n</tr>\n<tr>\n<td>(‘ij,kj-&gt;ik’, A, B)</td>\n<td>inner(A, B)</td>\n<td>A和B点积（内积）</td>\n</tr>\n<tr>\n<td>(‘ij,kj-&gt;ijk’, A, B)</td>\n<td>A[:, None] * B</td>\n<td>A的每一行乘以B</td>\n</tr>\n<tr>\n<td>(‘ij,kl-&gt;ijkl’, A, B)</td>\n<td>A[:, :, None, None] * B</td>\n<td>A的每个值乘以B</td>\n</tr>\n</tbody></table>\n<p>当处理大量维度时，别忘了einsum允许使用省略号语法’…’。这提供了一种变量的方式标记我们不大感兴趣的轴，例如<code>np.einsum(‘…ij,ji-&gt;…’, a, b)</code>，仅将a的最后两个轴与2维数组b相乘。</p>\n<h1 id=\"TensorFlow、PyTorch和Numpy\"><a href=\"#TensorFlow、PyTorch和Numpy\" class=\"headerlink\" title=\"TensorFlow、PyTorch和Numpy\"></a>TensorFlow、PyTorch和Numpy</h1><p>通常，要将元素式方程式转换为方程式字符串，可以使用以下过程（右侧是矩阵乘法示例的中间字符串）</p>\n<ol>\n<li>原始元素式方程式：C[i,k] = sum_j A[i,j] * B[j,k]</li>\n<li>删除变量名称，方括号和逗号：ik = sum_j ij * jk</li>\n<li>将“*”替换成“,”：ik = ij, jk</li>\n<li>去掉求和符号：ik = ij, jk</li>\n<li>输出移到右边，将将“=”替换成“-&gt;”：ij,jk-&gt;ik</li>\n</ol>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/einsum?hl=en\">TensorFlow2.3：tf.einsum()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.einsum(equation, *inputs, **kwargs)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tequation：描述计算的字符串，格式与numpy.einsum相同。</span><br><span class=\"line\">\t*inputs：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">\t**kwargs：</span><br><span class=\"line\">\t\toptimize：用于使用opt_einsum查找计算路径的优化策略，可选项包括 &#39;greedy&#39;, &#39;optimal&#39;, &#39;branch-2&#39;, &#39;branch-all&#39; ,&#39;auto&#39;，默认是&#39;greedy&#39;</span><br><span class=\"line\">\t\tname：操作名称（可选）</span><br><span class=\"line\">返回值：张量，形状与方程中一致</span><br><span class=\"line\">示例：</span><br><span class=\"line\">\t# 矩阵相乘</span><br><span class=\"line\">\teinsum(&#39;ij,jk-&gt;ik&#39;, m0, m1)  # output[i,k] &#x3D; sum_j m0[i,j] * m1[j, k]</span><br><span class=\"line\">\t# 点积</span><br><span class=\"line\">\teinsum(&#39;i,i-&gt;&#39;, u, v)  # output &#x3D; sum_i u[i]*v[i]</span><br><span class=\"line\">\t# 外积</span><br><span class=\"line\">\teinsum(&#39;i,j-&gt;ij&#39;, u, v)  # output[i,j] &#x3D; u[i]*v[j]</span><br><span class=\"line\">\t# 转置</span><br><span class=\"line\">\teinsum(&#39;ij-&gt;ji&#39;, m)  # output[j,i] &#x3D; m[i,j]</span><br><span class=\"line\">\t# 主对角线的和</span><br><span class=\"line\">\teinsum(&#39;ii&#39;, m)  # output[j,i] &#x3D; trace(m) &#x3D; sum_i m[i, i]</span><br><span class=\"line\">\t# 批量矩阵相乘</span><br><span class=\"line\">\teinsum(&#39;aij,ajk-&gt;aik&#39;, s, t)  # out[a,i,k] &#x3D; sum_j s[a,i,j] * t[a, j, k]</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.einsum.html?highlight=einsum#torch.einsum\">PyTorch1.7.0：torch.einsum()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.einsum(equation, *operands)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tequation：描述计算的字符串，格式与numpy.einsum相同。</span><br><span class=\"line\">\toperands：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">示例：</span><br><span class=\"line\">torch.einsum(&#39;i,j-&gt;ij&#39;, x, y)  # 外积</span><br><span class=\"line\">torch.einsum(&#39;bn,anm,bm-&gt;ba&#39;, l, A, r) # 计算torch.nn.functional.bilinear</span><br><span class=\"line\">torch.einsum(&#39;bij,bjk-&gt;bik&#39;, As, Bs) # 批量矩阵相乘</span><br><span class=\"line\">torch.einsum(&#39;ii-&gt;i&#39;, A) # 对角线之和</span><br><span class=\"line\">torch.einsum(&#39;...ii-&gt;...i&#39;, A) # 批量对角线之和</span><br><span class=\"line\">torch.einsum(&#39;...ij-&gt;...ji&#39;, A).shape # 批量转置</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://numpy.org/doc/1.19/reference/generated/numpy.einsum.html?highlight=einsum#numpy.einsum\">Numpy1.19：numpy.einsum</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">numpy.einsum(subscripts, *operands, out&#x3D;None, dtype&#x3D;None, order&#x3D;&#39;K&#39;, casting&#x3D;&#39;safe&#39;, optimize&#x3D;False)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsubscripts：描述计算的字符串，除非包含显式指示符“-&gt;”以及精确输出形式的下标标签，否则将执行隐式（经典的爱因斯坦求和）计算</span><br><span class=\"line\">\toperands：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">\tout：ndarray类型（可选），如果提供，则将计算结果放入此数组中</span><br><span class=\"line\">\tdtype：&#123;data-type, None&#125;（可选），如果提供，则强制使用指定的数据类型计算。 请注意，你可能还必须提供一个更宽松的转换参数以允许进行转换。 默认为None。</span><br><span class=\"line\">\torder：&#123;‘C’, ‘F’, ‘A’, ‘K’&#125;（可选），控制输出的内存布局。其中，‘C’表示C连续的。‘F’表示它应该是Fortran连续的。如果输入全为‘F’，则‘A’表示应为‘F’，否则为‘C’。‘K’表示它应尽可能与输入尽可能靠近布局，包括任意排列的轴。</span><br><span class=\"line\">\tcasting：&#123;‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’&#125;（可选），控制可能发生的数据类型转换。 不建议将其设置为“unsafe”，因为这会积聚不利影响。其中，“no”表示完全不应该转换数据类型，“equiv”表示仅允许按照字节顺序转换，“safe”表示只允许保留值的强制类型转换。“same_kind”表示仅允许安全类型转换或同一类型（例如float64到float32）内的类型转换。“unsafe”表示可能会进行任何数据转换。</span><br><span class=\"line\">\toptimize：&#123;False, True, ‘greedy’, ‘optimal’&#125;（可选），控制优化策略，如果True将默认设置为“贪心”算法，如果是False则不会进行优化。 还接受np.einsum_path函数的显式收缩列表。</span><br></pre></td></tr></table></figure>\n<h1 id=\"其他乘法\"><a href=\"#其他乘法\" class=\"headerlink\" title=\"其他乘法\"></a>其他乘法</h1><p><strong>TensorFlow2.3</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/linalg/matmul?hl=en\">tf.linalg.matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.linalg.matmul(a, b, transpose_a&#x3D;False, transpose_b&#x3D;False, adjoint_a&#x3D;False, adjoint_b&#x3D;False,a_is_sparse&#x3D;False, b_is_sparse&#x3D;False, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：类型为float16, float32, float64, int32, complex64, complex128，并且秩大于1</span><br><span class=\"line\">\tb：和a相同类型和秩</span><br><span class=\"line\">\ttranspose_a：如果为True，a在计算前会被转置</span><br><span class=\"line\">\ttranspose_b：如果为True，b在计算前会被转置</span><br><span class=\"line\">\tadjoint_a：如果为True，a在计算前会被共轭和转置</span><br><span class=\"line\">\tadjoint_b：如果为True，b在计算前会被共轭和转置</span><br><span class=\"line\">\ta_is_sparse：如果为True，则将a视为稀疏矩阵。</span><br><span class=\"line\">\tb_is_sparse：如果为True，则将b视为稀疏矩阵。</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul?hl=en\">tf.sparse.sparse_dense_matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.sparse.sparse_dense_matmul(sp_a, b, adjoint_a&#x3D;False, adjoint_b&#x3D;False, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：SparseTensor或者是密度矩阵，并且秩为2</span><br><span class=\"line\">\tb：和a相同类型，密度矩阵或者是a：SparseTensor</span><br><span class=\"line\">\tadjoint_a：如果为True，a在计算前会被共轭和转置</span><br><span class=\"line\">\tadjoint_b：如果为True，b在计算前会被共轭和转置</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/math/multiply?hl=en\">tf.math.multiply</a><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.math.multiply(x, y, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：类型为bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128的张量</span><br><span class=\"line\">\tb：和a相同类型</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><strong>Pytorch1.7.0</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul\">torch.matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.matmul(input, other, *, out&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tinput：第一个用于乘法的张量</span><br><span class=\"line\">\tother：第二个用于乘法的张量</span><br><span class=\"line\">\tout：可选，输出张量</span><br></pre></td></tr></table></figure>\n<p><strong>Numpy1.19</strong></p>\n<ul>\n<li><a href=\"https://numpy.org/doc/1.19/reference/generated/numpy.matmul.html?highlight=matmul#numpy.matmul\">numpy.matmul()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">numpy.matmul(x1, x2, &#x2F;, out&#x3D;None, *, casting&#x3D;&#39;same_kind&#39;, order&#x3D;&#39;K&#39;, dtype&#x3D;None, subok&#x3D;True[, signature, extobj])</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tx1, x2：输入数组</span><br><span class=\"line\">\tout：输出储存的位置，如果提供，则其形状必须与签名(n,k),(k,m)-&gt;(n,m)相匹配。 如果未提供或没有，则返回一个新分配的数组。</span><br><span class=\"line\">\t**kwargs：对于其他仅关键字参数，参考文档</span><br></pre></td></tr></table></figure>\n<p>参考文献</p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf?hl=en\">TensorFlow2.3</a></li>\n<li><a href=\"https://pytorch.org/docs/stable/index.html\">Pytorch1.7.0</a></li>\n<li><a href=\"https://numpy.org/doc/1.19/reference/index.html\">Numpy1.19</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/44954540\">einsum满足你一切需要：深度学习中的爱因斯坦求和约定</a></li>\n<li><a href=\"http://www.atyun.com/32288.html\">NumPy中einsum的基本介绍</a></li>\n<li><a href=\"https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/\">Einstein Summation in Numpy</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","TensorFlow","Pytorch","einsum"]},{"title":"看完这一篇，你就对Spring-Security略窥门径了","url":"/Spring-Boot/a86e53449298/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>开发Web应用，对页面的安全控制通常是必须的。比如：对于没有访问权限的用户需要转到登录表单页面。要实现访问控制的方法多种多样，可以通过Aop、拦截器实现，也可以通过框架实现，例如：Apache Shiro、Spring Security。我们这里要讲的Spring Security 就是一个Spring生态中关于安全方面的框架。它能够为基于Spring的企业应用系统提供声明式的安全访问控制解决方案。</p>\n<h2 id=\"默认认证用户名密码\"><a href=\"#默认认证用户名密码\" class=\"headerlink\" title=\"默认认证用户名密码\"></a>默认认证用户名密码</h2><p>项目pom.xml添加spring-boot-starter-security依赖</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>重启你的应用。再次打开页面，你讲看到一个登录页面</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308205647401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>既然跳到了登录页面，那么这个时候我们就会想，这个登录的用户名以及密码是什么呢？让我们来从SpringBoot源码寻找一下。你搜一下输出日志，会看到下面一段输出：<br><img src=\"https://img-blog.csdnimg.cn/20200308205717917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这段日志是UserDetailsServiceAutoConfiguration类里面的如下方法输出的：<br><img src=\"https://img-blog.csdnimg.cn/20200318115748850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>通过上面的这个类，我们可以看出，是SecurityProperties这个Bean管理了用户名和密码。在SecurityProperties里面的一个内部静态类User类里面，管理了默认的认证的用户名与密码。代码如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@ConfigurationProperties(</span></span><br><span class=\"line\"><span class=\"meta\">    prefix = &quot;spring.security&quot;</span></span><br><span class=\"line\"><span class=\"meta\">)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityProperties</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BASIC_AUTH_ORDER = <span class=\"number\">2147483642</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> IGNORED_ORDER = -<span class=\"number\">2147483648</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> DEFAULT_FILTER_ORDER = -<span class=\"number\">100</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SecurityProperties.Filter filter = <span class=\"keyword\">new</span> SecurityProperties.Filter();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SecurityProperties.User user = <span class=\"keyword\">new</span> SecurityProperties.User();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SecurityProperties</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SecurityProperties.<span class=\"function\">User <span class=\"title\">getUser</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.user;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SecurityProperties.<span class=\"function\">Filter <span class=\"title\">getFilter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.filter;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> String name = <span class=\"string\">&quot;user&quot;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> String password = UUID.randomUUID().toString();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> List&lt;String&gt; roles = <span class=\"keyword\">new</span> ArrayList();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> passwordGenerated = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">User</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.name;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setName</span><span class=\"params\">(String name)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.name = name;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getPassword</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.password;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPassword</span><span class=\"params\">(String password)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.hasLength(password)) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.passwordGenerated = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.password = password;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> List&lt;String&gt; <span class=\"title\">getRoles</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.roles;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRoles</span><span class=\"params\">(List&lt;String&gt; roles)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.roles = <span class=\"keyword\">new</span> ArrayList(roles);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isPasswordGenerated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.passwordGenerated;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Filter</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> order = -<span class=\"number\">100</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> Set&lt;DispatcherType&gt; dispatcherTypes;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Filter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.dispatcherTypes = <span class=\"keyword\">new</span> HashSet(Arrays.asList(DispatcherType.ASYNC, DispatcherType.ERROR, DispatcherType.REQUEST));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getOrder</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.order;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setOrder</span><span class=\"params\">(<span class=\"keyword\">int</span> order)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.order = order;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Set&lt;DispatcherType&gt; <span class=\"title\">getDispatcherTypes</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.dispatcherTypes;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDispatcherTypes</span><span class=\"params\">(Set&lt;DispatcherType&gt; dispatcherTypes)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.dispatcherTypes = dispatcherTypes;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>综上所述，security默认的用户名是user, 默认密码是应用启动的时候，通过UUID算法随机生成的，默认的role是”USER”。当然，如果我们想简单改一下这个用户名密码，可以在<code>application.properties</code>配置你的用户名密码，例如<br><img src=\"https://img-blog.csdnimg.cn/20200318120347187.png#pic_center\" alt=\"在这里插入图片描述\"><br>当然这只是一个初级的配置，更复杂的配置，可以分不用角色，在控制范围上，能够拦截到方法级别的权限控制。</p>\n<h2 id=\"内存用户名密码认证\"><a href=\"#内存用户名密码认证\" class=\"headerlink\" title=\"内存用户名密码认证\"></a>内存用户名密码认证</h2><p>在上面的内容，我们什么都没做，就添加了spring-boot-starter-security依赖，整个应用就有了默认的认证安全机制。下面，我们来定制用户名密码。写一个继承了 WebSecurityConfigurerAdapter的配置类，具体内容如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.builders.HttpSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableWebSecurity</span></span><br><span class=\"line\"><span class=\"meta\">@EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebSecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.configure(http);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        auth.inMemoryAuthentication()</span><br><span class=\"line\">                .passwordEncoder(<span class=\"keyword\">new</span> BCryptPasswordEncoder())</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;1234567&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;USER&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>这里对上面的代码进行简要说明</strong>：</p>\n<ul>\n<li><p>Spring security 5.0中新增了多种加密方式，也改变了默认的密码格式。需要修改一下configure中的代码，我们要将前端传过来的密码进行某种方式加密，Spring Security 官方推荐的是使用<code>bcrypt</code>加密方式。<code>inMemoryAuthentication().passwordEncoder(new BCryptPasswordEncoder())</code>，这相当于登陆时用BCrypt加密方式对用户密码进行处理。以前的”<code>.password(&quot;123&quot;)</code>“ 变成了 “<code>.password(new BCryptPasswordEncoder().encode(&quot;123&quot;))</code>“，这相当于对内存中的密码进行Bcrypt编码加密。如果比对时一致，说明密码正确，才允许登陆。</p>\n</li>\n<li><p>通过 <code>@EnableWebSecurity</code>注解开启Spring Security的功能。使用<code>@EnableGlobalMethodSecurity(prePostEnabled = true)</code>这个注解，可以开启security的注解，我们可以在需要控制权限的方法上面使用<code>@PreAuthorize</code>，<code>@PreFilter</code>这些注解。</p>\n</li>\n<li><p>继承 WebSecurityConfigurerAdapter 类，并重写它的方法来设置一些web安全的细节。我们结合<code>@EnableWebSecurity</code>注解和继承WebSecurityConfigurerAdapter，来给我们的系统加上基于web的安全机制。</p>\n</li>\n<li><p>在<code>configure(HttpSecurity http)</code>方法里面，我们进入到源码中，就会看到默认的认证代码是：</p>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200318121502574.png\" alt=\"在这里插入图片描述\"><br>从方法名我们基本可以看懂这些方法的功能。上面的那个默认的登录页面，就是SpringBoot默认的用户名密码认证的login页面。我们使用SpringBoot默认的配置<code>super.configure(http)</code>，它通过 <code>authorizeRequests()</code> 定义哪些URL需要被保护、哪些不需要被保护。默认配置是所有访问页面都需要认证，才可以访问。</p>\n<ul>\n<li><p>通过 <code>formLogin()</code> 定义当需要用户登录时候，转到的登录页面。</p>\n</li>\n<li><p><code>configureGlobal(AuthenticationManagerBuilder auth)</code> 方法，在内存中创建了一个用户，该用户的名称为root，密码为root，用户角色为USER。这个默认的登录页面是怎么冒出来的呢？是的，SpringBoot内置的，SpringBoot甚至给我们做好了一个极简的登录页面。这个登录页面是通过Filter实现的。具体的实现类是<code>org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter</code>。同时，这个DefaultLoginPageGeneratingFilter也是SpringBoot的默认内置的Filter。</p>\n</li>\n</ul>\n<p>输入用户名，密码，点击Login。不过，我们发现，SpringBoot应用的启动日志还是打印了如下一段：<br><img src=\"https://img-blog.csdnimg.cn/20200318122300838.png\" alt=\"在这里插入图片描述\"><br>但实际上，已经使用了我们定制的用户名密码了。如果我们要配置多个用户，多个角色，可参考使用如下示例的代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        auth.inMemoryAuthentication()</span><br><span class=\"line\">                .passwordEncoder(<span class=\"keyword\">new</span> BCryptPasswordEncoder())</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;1234567&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;USER&quot;</span>)</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin1&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;123&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;ADMIN&quot;</span>, <span class=\"string\">&quot;USER&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"角色权限控制\"><a href=\"#角色权限控制\" class=\"headerlink\" title=\"角色权限控制\"></a>角色权限控制</h2><p>当我们的系统功能模块当需求发展到一定程度时，会不同的用户，不同角色使用我们的系统。这样就要求我们的系统可以做到，能够对不同的系统功能模块，开放给对应的拥有其访问权限的用户使用。Spring Security提供了Spring EL表达式，允许我们在定义URL路径访问(@RequestMapping)的方法上面添加注解，来控制访问权限。在标注访问权限时，根据对应的表达式返回结果，控制访问权限：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">true</span>，表示有权限</span><br><span class=\"line\">fasle，表示无权限</span><br></pre></td></tr></table></figure>\n<p>Spring Security可用表达式对象的基类是SecurityExpressionRoot。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityExpressionRoot</span> <span class=\"keyword\">implements</span> <span class=\"title\">SecurityExpressionOperations</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> Authentication authentication;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> AuthenticationTrustResolver trustResolver;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> RoleHierarchy roleHierarchy;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Set&lt;String&gt; roles;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String defaultRolePrefix = <span class=\"string\">&quot;ROLE_&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> permitAll = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> denyAll = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> PermissionEvaluator permissionEvaluator;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String read = <span class=\"string\">&quot;read&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String write = <span class=\"string\">&quot;write&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String create = <span class=\"string\">&quot;create&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String delete = <span class=\"string\">&quot;delete&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String admin = <span class=\"string\">&quot;administration&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SecurityExpressionRoot</span><span class=\"params\">(Authentication authentication)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (authentication == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">&quot;Authentication object cannot be null&quot;</span>);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.authentication = authentication;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAuthority</span><span class=\"params\">(String authority)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthority(authority);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyAuthority</span><span class=\"params\">(String... authorities)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthorityName((String)<span class=\"keyword\">null</span>, authorities);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasRole</span><span class=\"params\">(String role)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyRole(role);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyRole</span><span class=\"params\">(String... roles)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthorityName(<span class=\"keyword\">this</span>.defaultRolePrefix, roles);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyAuthorityName</span><span class=\"params\">(String prefix, String... roles)</span> </span>&#123;</span><br><span class=\"line\">        Set&lt;String&gt; roleSet = <span class=\"keyword\">this</span>.getAuthoritySet();</span><br><span class=\"line\">        String[] var4 = roles;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> var5 = roles.length;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> var6 = <span class=\"number\">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class=\"line\">            String role = var4[var6];</span><br><span class=\"line\">            String defaultedRole = getRoleWithDefaultPrefix(prefix, role);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (roleSet.contains(defaultedRole)) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Authentication <span class=\"title\">getAuthentication</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.authentication;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">permitAll</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">denyAll</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isAnonymous</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.trustResolver.isAnonymous(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isAuthenticated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> !<span class=\"keyword\">this</span>.isAnonymous();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isRememberMe</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.trustResolver.isRememberMe(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isFullyAuthenticated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> !<span class=\"keyword\">this</span>.trustResolver.isAnonymous(<span class=\"keyword\">this</span>.authentication) &amp;&amp; !<span class=\"keyword\">this</span>.trustResolver.isRememberMe(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Object <span class=\"title\">getPrincipal</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.authentication.getPrincipal();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setTrustResolver</span><span class=\"params\">(AuthenticationTrustResolver trustResolver)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.trustResolver = trustResolver;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRoleHierarchy</span><span class=\"params\">(RoleHierarchy roleHierarchy)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.roleHierarchy = roleHierarchy;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDefaultRolePrefix</span><span class=\"params\">(String defaultRolePrefix)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.defaultRolePrefix = defaultRolePrefix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> Set&lt;String&gt; <span class=\"title\">getAuthoritySet</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.roles == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            Collection&lt;? extends GrantedAuthority&gt; userAuthorities = <span class=\"keyword\">this</span>.authentication.getAuthorities();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.roleHierarchy != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                userAuthorities = <span class=\"keyword\">this</span>.roleHierarchy.getReachableGrantedAuthorities(userAuthorities);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">this</span>.roles = AuthorityUtils.authorityListToSet(userAuthorities);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.roles;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasPermission</span><span class=\"params\">(Object target, Object permission)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.permissionEvaluator.hasPermission(<span class=\"keyword\">this</span>.authentication, target, permission);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasPermission</span><span class=\"params\">(Object targetId, String targetType, Object permission)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.permissionEvaluator.hasPermission(<span class=\"keyword\">this</span>.authentication, (Serializable)targetId, targetType, permission);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPermissionEvaluator</span><span class=\"params\">(PermissionEvaluator permissionEvaluator)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.permissionEvaluator = permissionEvaluator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String <span class=\"title\">getRoleWithDefaultPrefix</span><span class=\"params\">(String defaultRolePrefix, String role)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (role == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (defaultRolePrefix != <span class=\"keyword\">null</span> &amp;&amp; defaultRolePrefix.length() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role.startsWith(defaultRolePrefix) ? role : defaultRolePrefix + role;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过阅读源码，我们可以更加深刻的理解其EL写法，并在写代码的时候正确的使用。变量defaultRolePrefix硬编码约定了role的前缀是”ROLE_”。同时，我们可以看出hasRole跟hasAnyRole是一样的。hasAnyRole是调用的<code>hasAnyAuthorityName(defaultRolePrefix, roles)</code>。所以，我们在学习一个框架或者一门技术的时候，最准确的就是源码。通过源码，我们可以更好更深入的理解技术的本质。</p>\n<p>SecurityExpressionRoot为我们提供的使用Spring EL表达式总结如下：<br>| 表达式 | 描述 |<br>|–|–|<br>|hasRole([role])    |当前用户是否拥有指定角色。|<br>|hasAnyRole([role1,role2])    |多个角色是一个以逗号进行分隔的字符串。如果当前用户拥有指定角色中的任意一个则返回true。|<br>|hasAuthority([auth])    |等同于hasRole|<br>|hasAnyAuthority([auth1,auth2])    |等同于hasAnyRole|<br>|Principle    |代表当前用户的principle对象|<br>|authentication    |直接从SecurityContext获取的当前Authentication对象|<br>|permitAll    |总是返回true，表示允许所有的|<br>|denyAll|    总是返回false，表示拒绝所有的|<br>|isAnonymous()    |当前用户是否是一个匿名用户|<br>|isRememberMe()|    表示当前用户是否是通过Remember-Me自动登录的|<br>|isAuthenticated()    |表示当前用户是否已经登录认证成功了。|<br>|isFullyAuthenticated()    |如果当前用户既不是一个匿名用户，同时又不是通过Remember-Me自动登录的，则返回true。|</p>\n<p>在Controller方法上添加@PreAuthorize这个注解，<code>value=&quot;hasRole(&#39;ADMIN&#39;)&quot;)</code>是Spring-EL expression，当表达式值为true，标识这个方法可以被调用。如果表达式值是false，标识此方法无权限访问。</p>\n<h2 id=\"在Spring-Security里面获取当前登录认证通过的用户信息\"><a href=\"#在Spring-Security里面获取当前登录认证通过的用户信息\" class=\"headerlink\" title=\"在Spring Security里面获取当前登录认证通过的用户信息\"></a>在Spring Security里面获取当前登录认证通过的用户信息</h2><p>如果我们想要在前端页面显示当前登录的用户怎么办呢？在在Spring Security里面怎样获取当前登录认证通过的用户信息？下面我们就来探讨这个问题。其实很好办。我们添加一个LoginFilter，默认拦截所有请求，把当前登录的用户放到系统session中即可。在Spring Security中，用户信息保存在SecurityContextHolder中。Spring Security使用一个Authentication对象来持有所有系统的安全认证相关的信息。这个信息的内容格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;accountNonExpired&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;accountNonLocked&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;authorities&quot;</span>:[&#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;authority&quot;</span>:<span class=\"string\">&quot;ROLE_ADMIN&quot;</span></span><br><span class=\"line\">    &#125;,&#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;authority&quot;</span>:<span class=\"string\">&quot;ROLE_USER&quot;</span></span><br><span class=\"line\">    &#125;],</span><br><span class=\"line\">    <span class=\"string\">&quot;credentialsNonExpired&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;enabled&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;username&quot;</span>:<span class=\"string\">&quot;root&quot;</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个Authentication对象信息其实就是User实体的信息，类似如下(当然，密码没放进来)。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> <span class=\"keyword\">implements</span> <span class=\"title\">UserDetails</span>, <span class=\"title\">CredentialsContainer</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String password;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> String username;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Set&lt;GrantedAuthority&gt; authorities;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> accountNonExpired;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> accountNonLocked;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> credentialsNonExpired;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> enabled;</span><br><span class=\"line\">        ....</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们可以使用下面的代码（Java）获得当前身份验证的用户的名称:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (principal <span class=\"keyword\">instanceof</span> UserDetails) &#123;</span><br><span class=\"line\">    String username = ((UserDetails)principal).getUsername();</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    String username = principal.toString();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过调用getContext()返回的对象是SecurityContext的实例对象，该实例对象保存在ThreadLocal线程本地存储中。使用Spring Security框架，通常的认证机制都是返回UserDetails实例，通过如上这种方式，我们就可以拿到认证登录的用户信息。</p>\n<h2 id=\"用数据库存储用户和角色，实现安全认证\"><a href=\"#用数据库存储用户和角色，实现安全认证\" class=\"headerlink\" title=\"用数据库存储用户和角色，实现安全认证\"></a>用数据库存储用户和角色，实现安全认证</h2><p>很多时候，我们需要的是实现一个用数据库存储用户和角色，实现系统的安全认证。为了简化讲解，本例中在权限角色上，我们简单设计两个用户角色：USER，ADMIN。我们设计页面的权限如下：</p>\n<ul>\n<li>首页/ : 所有人可访问</li>\n<li>登录页 /login: 所有人可访问</li>\n<li>普通用户权限页 /httpapi, /httpsuite: 登录后的用户都可访问</li>\n<li>管理员权限页 /httpreport ： 仅管理员可访问</li>\n<li>无权限提醒页： 当一个用户访问了其没有权限的页面，我们使用全局统一的异常处理页面提示。</li>\n</ul>\n<h3 id=\"配置Spring-Security\"><a href=\"#配置Spring-Security\" class=\"headerlink\" title=\"配置Spring Security\"></a>配置Spring Security</h3><p>我们首先使用Spring Security帮我们做登录、登出的处理，以及当用户未登录时只能访问: <a href=\"http://localhost:8080/\">http://localhost:8080/</a> 以及 <a href=\"http://localhost:8080/login\">http://localhost:8080/login</a> 两个页面。同样的，我们要写一个继承WebSecurityConfigurerAdapter的配置类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.springboot.in.action.service.LightSwordUserDetailService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.builders.HttpSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.core.userdetails.UserDetailsService;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Created by jack on 2017/4/27.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableWebSecurity</span></span><br><span class=\"line\"><span class=\"meta\">@EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)</span></span><br><span class=\"line\"><span class=\"comment\">//使用@EnableGlobalMethodSecurity(prePostEnabled = true)</span></span><br><span class=\"line\"><span class=\"comment\">// 这个注解，可以开启security的注解，我们可以在需要控制权限的方法上面使用@PreAuthorize，@PreFilter这些注解。</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebSecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> UserDetailsService <span class=\"title\">userDetailsService</span><span class=\"params\">()</span> </span>&#123; <span class=\"comment\">//覆盖写userDetailsService方法 (1)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> AdminUserDetailService();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * If subclassed this will potentially override subclass configure(HttpSecurity)</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> http</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//super.configure(http);</span></span><br><span class=\"line\">        http.csrf().disable();</span><br><span class=\"line\"></span><br><span class=\"line\">        http.authorizeRequests()</span><br><span class=\"line\">            .antMatchers(<span class=\"string\">&quot;/&quot;</span>).permitAll()</span><br><span class=\"line\">            .antMatchers(<span class=\"string\">&quot;/amchart/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/bootstrap/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/build/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/css/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/dist/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/documentation/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/fonts/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/js/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/pages/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/plugins/**&quot;</span></span><br><span class=\"line\">            ).permitAll() <span class=\"comment\">//默认不拦截静态资源的url pattern （2）</span></span><br><span class=\"line\">            .anyRequest().authenticated().and()</span><br><span class=\"line\">            .formLogin().loginPage(<span class=\"string\">&quot;/login&quot;</span>)<span class=\"comment\">// 登录url请求路径 (3)</span></span><br><span class=\"line\">            .defaultSuccessUrl(<span class=\"string\">&quot;/httpapi&quot;</span>).permitAll().and() <span class=\"comment\">// 登录成功跳转路径url(4)</span></span><br><span class=\"line\">            .logout().permitAll();</span><br><span class=\"line\"></span><br><span class=\"line\">        http.logout().logoutSuccessUrl(<span class=\"string\">&quot;/&quot;</span>); <span class=\"comment\">// 退出默认跳转页面 (5)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//AuthenticationManager使用我们的 Service来获取用户信息，Service可以自己写，其实就是简单的读取数据库的操作</span></span><br><span class=\"line\">        auth.userDetailsService(()); <span class=\"comment\">// （6）</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码只做了基本的配置，其中：</p>\n<ul>\n<li>覆盖写userDetailsService方法，具体的AdminUserDetailsService实现类，就是之前说的获取用户信息的service层类。</li>\n<li>默认不拦截静态资源的url pattern。我们也可以用下面的WebSecurity这个方式跳过静态资源的认证。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(WebSecurity web)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">    web</span><br><span class=\"line\">        .ignoring()</span><br><span class=\"line\">        .antMatchers(<span class=\"string\">&quot;/resourcesDir/**&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>跳转登录页面url请求路径为/login，我们需要定义一个Controller把路径映射到login.html。</li>\n<li>登录成功后跳转的路径为/httpapi</li>\n<li>退出后跳转到的url为/</li>\n<li>认证鉴权信息的Bean，采用我们自定义的从数据库中获取用户信息的AdminUserDetailService类。</li>\n</ul>\n<p>我们同样使用<code>@EnableGlobalMethodSecurity(prePostEnabled = true)</code>这个注解，开启security的注解，这样我们可以在需要控制权限的方法上面使用<code>@PreAuthorize</code>，<code>@PreFilter</code>这些注解。</p>\n<h3 id=\"用户退出\"><a href=\"#用户退出\" class=\"headerlink\" title=\"用户退出\"></a>用户退出</h3><p>我们在configure(HttpSecurity http)方法里面定义了任何权限都允许退出，当然SpringBoot集成Security的默认退出请求是/logout </p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">http.logout().logoutSuccessUrl(<span class=\"string\">&quot;/&quot;</span>); <span class=\"comment\">// 退出默认跳转页面 (4)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置错误处理页面\"><a href=\"#配置错误处理页面\" class=\"headerlink\" title=\"配置错误处理页面\"></a>配置错误处理页面</h3><p>访问发生错误时，跳转到系统统一异常处理页面。我们首先添加一个<code>GlobalExceptionHandlerAdvice</code>，使用<code>@ControllerAdvice</code>注解：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.&#123;ControllerAdvice, ExceptionHandler&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.context.request.WebRequest</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.servlet.ModelAndView</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Created by jack on 2017/4/27.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"><span class=\"meta\">@ControllerAdvice</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GlobalExceptionHandlerAdvice</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"meta\">@ExceptionHandler(value = Exception.class)</span><span class=\"comment\">//表示捕捉到所有的异常，你也可以捕捉一个你自定义的异常</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> ModelAndView <span class=\"title\">exception</span><span class=\"params\">(Exception exception, WebRequest request)</span></span>&#123;</span><br><span class=\"line\">        ModelAndView modelAndView = <span class=\"keyword\">new</span> ModelAndView(<span class=\"string\">&quot;/error&quot;</span>);</span><br><span class=\"line\">        modelAndView.addObject(<span class=\"string\">&quot;errorMessage&quot;</span>, exception.getMessage());</span><br><span class=\"line\">        modelAndView.addObject(<span class=\"string\">&quot;stackTrace&quot;</span>, exception.getStackTrace());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> modelAndView;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中，@ExceptionHandler(value = Exception.class)，表示捕捉到所有的异常，这里你也可以捕捉一个你自定义的异常。比如说，针对安全认证的Exception，我们可以单独定义处理。此处不再赘述。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Spring Security","Spring"]},{"title":"论文阅读笔记：各种Optimizer梯度下降优化算法回顾和总结","url":"/Paper-Reading/d9b52c160292/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：An overview of gradient descent optimization algorithms<br>原文链接：<a href=\"https://arxiv.org/pdf/1609.04747.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>不管是使用PyTorch还是TensorFlow，用多了Optimizer优化器封装好的函数，对其内部使用的优化算法却没有仔细研究过，也很难对其优点和缺点进行实用的解释。所以打算以这一篇论文为主线并结合多篇优秀博文，回顾和总结目前主流的优化算法，对于没有深入了解过的算法，正好借这个机会学习一下。</p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>当前使用的许多优化算法，是对梯度下降法的衍生和优化。在微积分中，对多元函数的参数求 $\\theta$ 偏导数，把求得的各个参数的导数以向量的形式写出来就是梯度。梯度就是函数变化最快的地方。梯度下降是迭代法的一种，在求解机器学习算法的模型参数 $\\theta$ 时，即无约束问题时，梯度下降是最常采用的方法之一。 </p>\n<p>这里定义一个通用的思路框架，方便我们后面理解各算法之间的关系和改进。首先定义待优化参数 $\\theta$ ，目标函数 $J(\\theta)$，学习率为 $\\eta$，然后我们进行迭代优化，假设当前的epoch为 $t$ ，则有：</p>\n<ul>\n<li>计算目标函数关于当前参数的梯度： $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$</li>\n<li>根据历史梯度计算一阶动量和二阶动量：$m_t=\\phi(g_1,g_2,…,g_t);V_t=\\psi(g_1,g_2,…,g_t)$，</li>\n<li>计算当前时刻的下降梯度： $\\triangledown_t=\\eta\\cdot \\frac{m_t}{\\sqrt{V_t}}$</li>\n<li>根据下降梯度进行更新： $\\theta_{t+1} = \\theta_t -\\triangledown_t$</li>\n</ul>\n<p>其中，$\\theta_{t+1}$为下一个时刻的参数，$\\theta_t$为当前时刻 $t$ 参数，后面的描述我们都将结合这个框架来进行。</p>\n<p>这里提一下一些概念：</p>\n<ul>\n<li>鞍点：一个光滑函数的鞍点邻域的曲线，曲面，或超曲面，都位于这点的切线的不同边。例如这个二维图形，像个马鞍：在x-轴方向往上曲，在y-轴方向往下曲，鞍点就是（0，0）。<br><img src=\"https://img-blog.csdnimg.cn/20210111155320118.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>指数加权平均、偏差修正：可参见这篇文章：<a href=\"https://www.cnblogs.com/guoyaohua/p/8544835.html\">什么是指数加权平均、偏差修正？</a></li>\n</ul>\n<h1 id=\"Gradient-Descent（GD）\"><a href=\"#Gradient-Descent（GD）\" class=\"headerlink\" title=\"Gradient Descent（GD）\"></a>Gradient Descent（GD）</h1><p>在GD中没有动量的概念，也就是说在上述框架中：$m_t=g_t；V_t=I^2$，则我们在当前时刻需要下降的梯度就是 $\\triangledown_t=\\eta\\cdot g_t$ ，则使用梯度下降法更新参数为（假设当前样本为 $(x_i,y_i)$，每当样本输入时，参数即进行更新）：<br>$$\\theta_{t+1}=\\theta_t-\\triangledown_t=\\theta_t-\\eta\\cdot g_t=\\theta_t-\\eta\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t,x_i,y_i)$$</p>\n<p>梯度下降算法中，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步，更形象的如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210111142747267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>标准的梯度下降主要有两个缺点：</p>\n<ul>\n<li>训练速度慢：在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本，会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。</li>\n<li>容易陷入局部最优解：由于是在有限视距内寻找下山的反向，当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点，落入鞍点，梯度为0，使得模型参数不在继续更新。</li>\n</ul>\n<h1 id=\"Batch-Gradient-Descent（BGD）\"><a href=\"#Batch-Gradient-Descent（BGD）\" class=\"headerlink\" title=\"Batch Gradient Descent（BGD）\"></a>Batch Gradient Descent（BGD）</h1><p>BGD相对于标准GD进行了改进，改进的地方通过它的名字应该也能看出来，也就是不再是想标准GD一样，对每个样本输入都进行参数更新，而是针对一个批量的数据输入进行参数更新。我们假设<strong>批量训练样本总数</strong>为 $n$，样本为 ${(x_1,y_1),..,(x_n, y_n)}$  ，则在第 $i$ 对样本 $(x_i,y_i)$ 上损失函数关于参数的梯度为 $\\triangledown_\\theta J_i(\\theta, x_i, y_i)$ , 则使用BGD更新参数为：<br>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\frac{1}{n}\\cdot\\sum_{i=1}^{n}\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$<br>从上面的公式我们可以看到，BGD其实是在一个批量的样本数据中，求取该批量样本梯度的均值来更新参数，即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数，这样就会大大加快训练速度，但是还是不够，我们接着往下看。</p>\n<h1 id=\"Stochastic-Gradient-Descent（SGD）\"><a href=\"#Stochastic-Gradient-Descent（SGD）\" class=\"headerlink\" title=\"Stochastic Gradient Descent（SGD）\"></a>Stochastic Gradient Descent（SGD）</h1><p>随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本 $(x_i,y_i)$ 计算其梯度，参数更新公式为：</p>\n<p>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$</p>\n<p>公式看起来和上面标准GD一样，但是注意了，这里的样本是从批量中随机选取一个，而标准GD是所有的输入样本都进行计算。可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大（如下图），更容易从一个局部最优跳到另一个局部最优，准确度下降。<br><img src=\"https://img-blog.csdnimg.cn/20210111150957576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>论文中提到，当缓慢降低学习率时，SGD会显示与BGD相同的收敛行为，几乎一定会收敛到局部（非凸优化）或全局最小值（凸优化）。</p>\n<p>SGD的优点：</p>\n<ul>\n<li>虽然看起来SGD波动非常大，会走很多弯路，但是对梯度的要求很低（计算梯度快），而且对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。</li>\n<li>应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。</li>\n</ul>\n<p>SGD的缺点：</p>\n<ul>\n<li>SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确（次要）。</li>\n<li>SGD也没能单独克服局部最优解的问题（主要）。</li>\n</ul>\n<h1 id=\"Mini-batch-Gradient-Descent（MBGD，也叫作SGD）\"><a href=\"#Mini-batch-Gradient-Descent（MBGD，也叫作SGD）\" class=\"headerlink\" title=\"Mini-batch Gradient Descent（MBGD，也叫作SGD）\"></a>Mini-batch Gradient Descent（MBGD，也叫作SGD）</h1><p>小批量梯度下降法就是结合BGD和SGD的折中，对于含有 $n$ 个训练样本的数据集，每次参数更新，选择一个大小为 $m(m&lt;n)$的mini-batch数据样本计算其梯度，其参数更新公式如下：<br>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\frac{1}{m}\\cdot\\sum_{i=x}^{i=x+m-1}\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$<br>小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法。常用的小批量尺寸范围在50到256之间，但可能因不同的应用而异。</p>\n<p>MBGD的缺点：</p>\n<ul>\n<li>Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点）。对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点所有维度的梯度都接近于0，SGD 很容易被困在这里（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是BGD的训练集全集带入，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动）。</li>\n<li>SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新， 且learning rate会随着更新的次数逐渐变小。</li>\n</ul>\n<h1 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h1><p>momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。从这里开始，我们引入一阶动量的概念（在mini-batch SGD的基础之上），也就是说，在最开始说的框架中， $m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot g_t$，而 $V_t=I^2$ 不变，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$        $$\\theta_{t+1}=\\theta_t-m_{t+1}$$</p>\n<p>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 $\\frac{1}{(1-\\beta_1)}$ 个时刻的梯度向量和的平均值（移动平均是啥看最上面的文章）。也就是说，$t$ 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $\\beta_1$ 的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡，在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210111162859514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>动量主要解决SGD的两个问题：</p>\n<ul>\n<li>随机梯度的方法（引入的噪声）</li>\n<li>Hessian矩阵病态问题（可以理解为SGD在收敛过程中和正确梯度相比来回摆动比较大的问题）。</li>\n</ul>\n<h1 id=\"Nesterov-Accelerated-Gradient\"><a href=\"#Nesterov-Accelerated-Gradient\" class=\"headerlink\" title=\"Nesterov Accelerated Gradient\"></a>Nesterov Accelerated Gradient</h1><p>牛顿加速梯度（NAG, Nesterov accelerated gradient）算法，是Momentum动量算法的变种。momentum保留了上一时刻的梯度  $\\triangledown_\\theta J(\\theta)$  ，对其没有进行任何改变，NAG是momentum的改进，在梯度更新时做一个矫正，具体做法就是在当前的梯度上添加上一时刻的动量 $\\beta_1\\cdot m_t$ ，梯度改变为  $\\triangledown_\\theta J(\\theta-\\beta_1\\cdot m_t)$  ，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J(\\theta_t-\\beta_1\\cdot m_t)$$    $$\\theta_{t+1}=\\theta_t-m_{t+1}$$</p>\n<p>加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。 下图是momentum和nesterrov的对比表述图如下：<br><img src=\"https://img-blog.csdnimg.cn/20210111164437433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Nesterov动量梯度的计算在模型参数施加当前速度之后，因此可以理解为往标准动量中添加了一个校正因子。在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从 $O(1/k)$ (k步后)改进到 $O(1/k^2)$，然而，在随机梯度情况下，Nesterov动量对收敛率的作用却不是很大。</p>\n<p>Momentum和Nexterov都是为了使梯度更新更灵活。但是人工设计的学习率总是有些生硬，下面介绍几种自适应学习率的方法。</p>\n<h1 id=\"Adagrad\"><a href=\"#Adagrad\" class=\"headerlink\" title=\"Adagrad\"></a>Adagrad</h1><p>Adagrad其实是对学习率进行了一个约束，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。而该方法中开始使用二阶动量，才意味着“自适应学习率”优化算法时代的到来。</p>\n<p>我们前面都没有好好的讨论二阶动量，二阶动量是个啥？它是用来度量历史更新频率的，二阶动量是迄今为止所有梯度值的平方和，即 $V_t = \\sum_{i=1}^tg_t^2$，在最上面的框架中 $\\triangledown_t=\\eta\\cdot \\frac{m_t}{\\sqrt{V_t}}$（在这里$m_t=I$）， 也就是说，我们的学习率现在是 $\\frac{\\eta}{\\sqrt{V_t+\\epsilon}}$（一般为了避免分母为0，会在分母上加一个小的平滑项 $\\epsilon$），从这里我们就会发现 $\\sqrt{V_t+\\epsilon}$ 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小，这一方法在稀疏数据场景下表现非常好，参数更新公式如下：<br>$$V_t = \\sum_{i=1}^tg_t^2$$   $$\\theta_{t+1}=\\theta_t-\\eta\\frac{1}{\\sqrt{V_t+\\epsilon}}$$</p>\n<p>细心的小伙伴应该会发现Adagrad还是存在一个很明显的缺点：</p>\n<ul>\n<li>仍需要手工设置一个全局学习率 $\\eta$ , 如果 $\\eta$ 设置过大的话，会使regularizer过于敏感，对梯度的调节太大</li>\n<li>中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习</li>\n</ul>\n<h1 id=\"Adadelta\"><a href=\"#Adadelta\" class=\"headerlink\" title=\"Adadelta\"></a>Adadelta</h1><p>由于AdaGrad调整学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度，即Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值（指数移动平均值），这就避免了二阶动量持续累积、导致训练过程提前结束的问题了，参数更新公式如下：<br>$$V_t = \\beta_2\\cdot V_{t-1} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$   $$\\theta_{t+1}=\\theta_t-\\eta\\frac{1}{\\sqrt{V_t+\\epsilon}}$$</p>\n<p>观察上面的参数更新公式，我们发现还是依赖于全局学习率 $\\eta$ ，但是原作者在此基础之上做出了一定的处理，上式经过牛顿迭代法之后，得到Adadelta最终迭代公式如下式，其中 $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$：<br>$$E[g_t^2]<em>t=\\rho\\cdot E[g_t^2]_{t-1}+(1-\\rho)\\cdot g_t^2$$    $$\\triangledown_t=\\frac{\\sum</em>{i=1}^{t-1}\\triangle\\theta_r}{\\sqrt{E[g_t^2]_t+\\epsilon}}$$</p>\n<p><strong>此时可以看出Adadelta已经不依赖全局learning rate了</strong>，Adadelta有如下特点：</p>\n<ul>\n<li>训练初中期，加速效果不错，很快</li>\n<li>训练后期，反复在局部最小值附近抖动</li>\n</ul>\n<h1 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h1><p>RMSProp算法修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好。设定参数：全局初始率 $\\eta$ , 默认设为0.001，decay rate $\\rho$ ，默认设置为0.9，一个极小的常量 $\\epsilon$ ，通常为10e-6，参数更新公式如下，其中 $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$：<br>$$E[g_t^2]_t=\\rho\\cdot E[g_t^2]_{t-1}+(1-\\rho)\\cdot g_t^2$$    $$\\triangledown_t=\\frac{\\eta}{\\sqrt{E[g_t^2]_t+\\epsilon}}\\cdot g_t$$</p>\n<ul>\n<li>其实RMSprop依然依赖于全局学习率 $\\eta$</li>\n<li>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</li>\n<li>适合处理非平稳目标(包括季节性和周期性)——对于RNN效果很好</li>\n</ul>\n<h1 id=\"Adaptive-Moment-Estimation（Adam）\"><a href=\"#Adaptive-Moment-Estimation（Adam）\" class=\"headerlink\" title=\"Adaptive Moment Estimation（Adam）\"></a>Adaptive Moment Estimation（Adam）</h1><p>其实有了前面的方法，Adam和Nadam的出现就很理所当然的了，因为它们结合了前面方法的一阶动量和二阶动量。我们看到，SGD-M和NAG在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量，参数更新公式如下（按照最开始总结的计算框架）：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2\\cdot V_{t} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>通常情况下，默认值为$\\beta_1=0.9$、$\\beta_2=0.999$ 和 $\\epsilon=10^{-8}$，Adam通常被认为对超参数的选择相当鲁棒，特点如下：</p>\n<ul>\n<li>Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。</li>\n<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>\n<li>为不同的参数计算不同的自适应学习率</li>\n<li>也适用于大多非凸优化问题——适用于大数据集和高维空间。</li>\n</ul>\n<h1 id=\"AdaMax\"><a href=\"#AdaMax\" class=\"headerlink\" title=\"AdaMax\"></a>AdaMax</h1><p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围，即使用无穷范式，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2^\\infty\\cdot V_{t} + (1-\\beta_2^\\infty)(\\triangledown_{\\theta_t} J(\\theta_t))^\\infty=max(\\beta_2\\cdot V_t, |\\triangledown_{\\theta_t}J(\\theta_t)|)$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>通常情况下，默认值为$\\beta_1=0.9$、$\\beta_2=0.999$ 和 $\\eta=0.002$</p>\n<h1 id=\"Nadam\"><a href=\"#Nadam\" class=\"headerlink\" title=\"Nadam\"></a>Nadam</h1><p>其实如果说要集成所有方法的优点于一身的话，Nadam应该就是了，Adam遗漏了啥？没错，就是Nesterov项，我们在Adam的基础上，加上Nesterov项就是Nadam了，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+\\frac{(1-\\beta_1)}{(1-\\beta_1^t)}\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2\\cdot V_{t} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>可以看出，Nadam对学习率有更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在使用带动量的RMSprop或Adam的问题上，使用Nadam可以取得更好的结果。</p>\n<p>来张直观的动态图展示上述优化算法的效果：</p>\n<ul>\n<li>下图描述了在一个曲面上，6种优化器的表现：<br><img src=\"https://img-blog.csdnimg.cn/20210111213136496.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>下图在一个存在鞍点的曲面，比较6中优化器的性能表现：<br><img src=\"https://img-blog.csdnimg.cn/20210111212630400.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>下图图比较了6种优化器收敛到目标点（五角星）的运行过程<br><img src=\"https://img-blog.csdnimg.cn/20210111212612731.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>那种优化器最好？该选择哪种优化算法？目前还没能够达达成共识。Schaul et al (2014)展示了许多优化算法在大量学习任务上极具价值的比较。虽然结果表明，具有自适应学习率的优化器表现的很鲁棒，不分伯仲，但是没有哪种算法能够脱颖而出。</p>\n<p>目前，最流行并且使用很高的优化器（算法）包括SGD、具有动量的SGD、RMSprop、具有动量的RMSProp、AdaDelta和Adam。在实际应用中，选择哪种优化器应结合具体问题；同时，也优化器的选择也取决于使用者对优化器的熟悉程度（比如参数的调节等等）。</p>\n<ul>\n<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>\n<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>\n<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>\n<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>\n<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>\n<li>如果验证损失较长时间没有得到改善，可以停止训练。</li>\n<li>添加梯度噪声（高斯分布$N(0,\\sigma_t^2)$）到参数更新，可使网络对不良初始化更加健壮，并有助于训练特别深而复杂的网络。</li>\n</ul>\n<p><em>参考文献</em>：</p>\n<ul>\n<li><a href=\"https://ruder.io/optimizing-gradient-descent/\">An overview of gradient descent optimization algorithms</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/22252270\">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></li>\n<li><a href=\"https://github.com/snnclsr/visualize_optimizers\">visualize_optimizers</a></li>\n<li><a href=\"https://lossfunctions.tumblr.com/\">lossfunctions</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/55150256\">优化算法Optimizer比较和总结</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32230623\">一个框架看懂优化算法之异同 SGD/AdaGrad/Adam</a></li>\n<li><a href=\"https://www.cnblogs.com/guoyaohua/p/8542554.html\">深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）</a></li>\n<li><a href=\"https://blog.csdn.net/weixin_40170902/article/details/80092628\">机器学习：各种优化器Optimizer的总结与比较</a></li>\n<li><a href=\"https://blog.csdn.net/muyu709287760/article/details/62531509#%E4%B8%89%E7%A7%8Dgradient-descent%E5%AF%B9%E6%AF%94\">optimizer优化算法总结</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","TensorFlow","机器学习","梯度下降","优化算法","Optimizer"]},{"title":"论文阅读笔记：大名鼎鼎的BERT模型","url":"/Paper-Reading/1fc44f71c5c6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>原文链接：<a href=\"https://arxiv.org/pdf/1810.04805.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>本文介绍的language representation model就是大名鼎鼎的BERT，其模型结构是利用Transformer的双向Encoder表示。BERT有个明显的特点就是，它通过在所有层的左侧和右侧上下文中共同进行条件预处理，从而在未标记的文本中预训练深层双向表示。</p>\n<p>现有两种将预训练语言表示应用于下游任务的策略：<strong>feature-based</strong>和<strong>fine-tuning</strong>，基于特征的方法，比较典型的就是ELMo，使用特定于任务的架构，其中包括预训练的表示形式作为附加功能。微调的方式，众所周知的是GPT了，这种方式通过引入最少的特定于任务的参数，并通过简单地微调所有预训练的参数来对下游任务进行训练。两种方法在预训练过程中具有相同的目标功能，它们使用<strong>单向语言模型</strong>学习通用语言表示形式。而BERT作为预训练模型，就是定位于通过微调的方式，不过和其他模型有个区别就是（前面我加粗字体），BERT是双向结合上下文的架构，论文里面说双向有利于句子级任务。</p>\n<p>BERT的特别之处：</p>\n<ul>\n<li>BERT受到完型填空任务的启发，通过使用一个“masked language model”(MLM)预训练目标来减轻上面提到的单向约束问题。MLM随机masks掉input中的一些tokens，目标是从这些tokens的上下文中预测出它们在原始词汇表中的id。与从左到右的语言模型预训练不同，MLM目标使表示形式能够融合左右上下文，这使我们可以预训练深度双向Transformer。</li>\n<li>除了MLM，作者还使用了一个“next sentence prediction”任务，连带的预训练text-pair表征</li>\n</ul>\n<h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><ul>\n<li>深度双向：深度双向和浅度双向的区别在于，后者仅仅是将分开训练好的left-to-right和right-to-left的表征简单的串联，而前者是一起训练得到的。</li>\n<li>feature-based: 又称feature-extraction 特征提取。就是用预训练好的网络在新样本上提取出相关的特征，然后将这些特征输入一个新的分类器，从头开始训练的过程。也就是说在训练的过程中，网络的特征提取层是被冻结的，只有后面的密集链接分类器部分是可以参与训练的。</li>\n<li>fine-tuning: 微调。和feature-based的区别是，训练好新的分类器后，还要解冻特征提取层的顶部的几层，然后和分类器再次进行联合训练。之所以称为微调，就是因为在预训练好的参数上进行训练更新的参数，比预训练好的参数的变化相对小，这个相对是指相对于不采用预训练模型参数来初始化下游任务的模型参数的情况。也有一种情况，如果你有大量的数据样本可以训练，那么就可以解冻所有的特征提取层，全部的参数都参与训练，但由于是基于预训练的模型参数，所以仍然比随机初始化的方式训练全部的参数要快的多。对于作者团队使用BERT模型在下游任务的微调时，就采用了解冻所有层，微调所有参数的方法。</li>\n<li>warmup:学习率热身。规定前多少个热身步骤内，对学习率采取逐步递增的过程。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1></li>\n<li><strong>无监督的基于特征的方法</strong>：ELMo和它的前身从不同的维度概括了传统的词嵌入研究。它们从left-to-right和right-to-left语言模型中提取上下文敏感的特征。每个token(单词、符号等)的上下文表征是通过<strong>串联left-to-right和right-to-left</strong>的表征得到的。Melamud等人在2016年提出了使用LSTMs模型通过一个预测单词左右上下文的任务来学习上下文表征。与ELMo类似，他们的模型也是基于feature-based方法，并且没有深度双向</li>\n<li><strong>无监督的微调方法</strong>：与feature-based方法一样，该方向刚开始只是在未标记的文本上预训练词嵌入参数(无监督学习)。最近，句子和文档等生成上下文token表征的编码器已经从未标记的文本中预训练出来，并且通过fine-tuned的方式用在下游任务中。</li>\n<li><strong>监督数据的迁移学习</strong>：也有工作展示了从大数据集的监督任务的做迁移学习的有效性，就像自然语言推理(NLI)，和机器翻译。</li>\n</ul>\n<h1 id=\"具体实现结构\"><a href=\"#具体实现结构\" class=\"headerlink\" title=\"具体实现结构\"></a>具体实现结构</h1><p>框架分为两个步骤：</p>\n<ul>\n<li>pre-training</li>\n<li>fine-tuning</li>\n</ul>\n<p>在预训练期间，BERT模型在不同任务的未标记数据上进行训练。微调的时候，BERT模型用预训练好的参数进行初始化，并且是基于下游任务的有标签的数据来训练的。问答领域的例子作为本节一个运行示例，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020102911482744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>BERT的一个特性是其跨不同任务的统一体系结构，即在预训练架构和下游的架构之间的差异很小。BERT是基于原始Transformer实现中的Encoder（Transformer的那篇文章我也有阅读笔记，分别在<a href=\"https://dengbocong.blog.csdn.net/article/details/108639979\">CSDN</a>和<a href=\"https://zhuanlan.zhihu.com/p/250946855\">知乎</a>发布），改造成多层双向Transformer Encoder。<strong>BERT Transformer使用双向self-attention，而GPT Transformer 使用带约束的self-attention，每个token只能注意到它左边的上下文。</strong></p>\n<blockquote>\n<p>本文中，$L$表示层数，$H$表示每个隐藏单元的维数大小，$A$表示self-attention头数。BERT有2种参数，分别是BERT($base$，$L=12$, $H=768$, $A=12$, $Total Parameters=110M$)和BERT($large$，$L=24$, $H=1024$, $A=16$, $Total Parameters=340M$)。</p>\n</blockquote>\n<p>使用BERT做各种下游任务，输入表征可以在一个token序列里清楚的表示一个句子或者一对句子(比如&lt;Question,Answer&gt;)。在整个工作中，“句子”可以是任意连续的文本范围，而不是实际的语言句子。论文中BERT关于输入输出的操作：</p>\n<ul>\n<li>使用大小为30000的WordPiece作为词嵌入层</li>\n<li>句子对打包成一个序列</li>\n<li>每个序列的首个token总是一个特定的classification token([CLS])，这个token对应的最后的隐藏状态被用作分类任务的聚合序列表征。</li>\n<li>区分句子对中句子的方法有两种：<ul>\n<li>通过分隔符[SEP]</li>\n<li>模型架构中添加了一个经过学习的嵌入(learned embedding)到每个token，以表示它是属于句子A或者句子B。</li>\n</ul>\n</li>\n</ul>\n<p>如上面图1中，$E$表示输入的词嵌入，$C$表示最后隐藏层的[CLS]的向量，$T_i$ 表示第 $i$ 个输入token在最后隐藏层的向量。对于给定的token，其输入表示是通过将相应的token，segment和position embeddings求和而构造的，如下图。<br><img src=\"https://img-blog.csdnimg.cn/2020102913010227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Pre-training-BERT\"><a href=\"#Pre-training-BERT\" class=\"headerlink\" title=\"Pre-training BERT\"></a>Pre-training BERT</h3><ul>\n<li><p> <strong>Masked LM（MLM–掩盖式语言模型–Cloze任务）</strong>：本文作者直觉上认为，深层双向模型比left-to-right模型或left-to-right模型和right-to-left模型的浅层连接更强大。标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件将使模型“间接的看到自己”。为了训练深度双向表示，做法是简单的随机mask一些百分比的输入tokens，然后预测那些被mask掉的tokens。在实验中，作者为每个序列随机mask掉了15%的WordPiece tokens。注意了，<strong>BERT只预测被mask掉的词，而不是重建完整的输入</strong>。但由于[MASK]符号作为token不会出现在微调阶段，所以要想办法让那些被mask掉的词的原本的表征也被模型学习到，所以这里需要采用一些策略，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201029193110597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p><strong>Next Sentence Prediction（NSP）</strong>：许多下游任务，比如问答，自然语言推理等，需要基于对两个句子之间的关系的理解，而这种关系不能直接通过语言建模来获取到。为了训练一个可以理解句子间关系的模型，做法是为一个二分类的下一个句子预测任务进行了预训练。特别是，当为每个预测样例选择一个句子对A和B，50%的时间B是A后面的下一个句子(标记为IsNext)， 50%的时间B是语料库中的一个随机句子(标记为NotNext)。在图1中，C用来预测下一个句子（NSP）。尽管简单，但是该方法QA和NLI任务都非常有帮助。<br><img src=\"https://img-blog.csdnimg.cn/20201029210744426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p><strong>Pre-training data</strong>：预训练语料使用了BooksCorpus(800M words)，English Wikipedia(2500M words) 。之所以使用这种文档级语料而不是使用语句级语料，就是为了提取长连续序列。</p>\n</li>\n</ul>\n<h3 id=\"Fine-tuning-BERT\"><a href=\"#Fine-tuning-BERT\" class=\"headerlink\" title=\"Fine-tuning BERT\"></a>Fine-tuning BERT</h3><p>对于涉及到文本对的应用，常见的模式是先分辨编码文本对中的文本，然后应用双向交叉的注意力。BERT使用self-attention机制统一了这两个步骤，BERT使用self-attention编码一个串联的文本对，其过程中就包含了2个句子之间的双向交叉注意力。</p>\n<ul>\n<li>输入端：句子A和句子B可以是：（1）释义句子对（2）假设条件句子对（3）问答句子对 （4）文本分类或序列标注中的text-∅对。</li>\n<li>输出端：token表征喂给一个针对token级别的任务的输出层，序列标注和问答是类似的，而[CLS]表征喂给一个分类器输出层，比如情感分析。</li>\n</ul>\n<p>BERT在不同任务上微调的图解<br><img src=\"https://img-blog.csdnimg.cn/20201029195812498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h3 id=\"GLUE\"><a href=\"#GLUE\" class=\"headerlink\" title=\"GLUE\"></a>GLUE</h3><p>在所有的GLUE任务上，作者使用了$batch-size=32,epochs=3$。对于每个任务，都通过开发集的验证来选择了最佳的微调学习率(在$5e- 5$，$4e - 5$，$3e -5$和$2e-5$之间)。另外，对于BERT的large模型，作者发现微调有时候在小数据集上不稳定，所以随机重启了几次，并选择了开发集上表现最佳的模型。<br><img src=\"https://img-blog.csdnimg.cn/20201029201521980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>在模型架构方面，除了注意力掩盖之外，BERT base版本的模型架构和OpenAI GPT几乎相同。</li>\n<li>BERT large 版本明显比base版本要表现的更好。</li>\n</ul>\n<h3 id=\"SQuAD-v1-1\"><a href=\"#SQuAD-v1-1\" class=\"headerlink\" title=\"SQuAD v1.1\"></a>SQuAD v1.1</h3><p>标准的SQuAD v1.1是一个100k的问答对集合，给定一个问题和一篇短文，以及对应的答案，任务是预测出短文中的答案文本span。图1所示，在问答任务中，作者将输入问题和短文表示成一个序列，其中，使用A嵌入表示问题，B嵌入表示短文。在微调的时候，作者引入一个start向量S，和一个end向量E，维数都为H。</p>\n<ul>\n<li>answer span的起始词 $i$ 的概率计算公式（答案末尾词的概率表示原理一样。）：$P_i=\\frac{e^{S\\cdot T_i}}{\\sum_je^{S\\cdot T_j}}$。</li>\n<li>位置 $i$ 到位置 $j$ 的候选span的分数定义如下：$S\\cdot T_i+E\\cdot T_j$。</li>\n</ul>\n<p>并将满足 $j&gt;i$ 的最大得分的span最为预测结果。训练目标是正确的开始和结束位置的对数似然估计的和。<br><img src=\"https://img-blog.csdnimg.cn/20201029204255261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"SQuAD-2-0\"><a href=\"#SQuAD-2-0\" class=\"headerlink\" title=\"SQuAD 2.0\"></a>SQuAD 2.0</h3><p><img src=\"https://img-blog.csdnimg.cn/20201029204512621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"SWAG\"><a href=\"#SWAG\" class=\"headerlink\" title=\"SWAG\"></a>SWAG</h3><p>The Situations With Adversarial Generations (SWAG)数据集包含113k个句子对完整示例，用于评估基于常识的推理。给定一个句子，任务是从四个选项中选择出最有可能是对的的continuation(延续/扩展)。在微调的时候，作者构造了4个输入序列，每个包含给定句子A的序列和continuation(句子B)。引入的唯一特定于任务的参数是一个向量，它与[CLS]token做点积，得到每个选项的分数，该分数会通过一个softmax层来归一化。<br><img src=\"https://img-blog.csdnimg.cn/20201029204522481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Ablation-Studies（消融研究）\"><a href=\"#Ablation-Studies（消融研究）\" class=\"headerlink\" title=\"Ablation Studies（消融研究）\"></a>Ablation Studies（消融研究）</h1><p>简单而言就是通过控制变量法证明算法的有效性。</p>\n<h3 id=\"预训练任务的影响\"><a href=\"#预训练任务的影响\" class=\"headerlink\" title=\"预训练任务的影响\"></a>预训练任务的影响</h3><p>通过去掉NSP后，对比BERT的双向表征和Left-to-Right表征，作者得证明了有NSP更好，且双向表征更有效。通过引入一个双向的LSTM，作者证明了BILSTM比Left-to-Right能得到更好的结果，但是仍然没有BERT的base版本效果好，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201029205352261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>ELMo是分别训练LTR和RTL的方式，以下列举了其不如BERT的地方：</p>\n<ul>\n<li>ELMo这种分别训练在串联的方式比单个双向模型代价高的两倍</li>\n<li>对于QA这样的任务而言不直观，因为RTL模型将无法确定问题的答案</li>\n<li>绝对不如深度双向模型强大，因为深度双向模型可以在每一层使用左右上下文</li>\n</ul>\n<h3 id=\"模型大小的影响\"><a href=\"#模型大小的影响\" class=\"headerlink\" title=\"模型大小的影响\"></a>模型大小的影响</h3><p>论文中训练了一些不同层数、隐藏单元数、注意力头的BERT模型，但使用相同的超参数和训练过程。<br><img src=\"https://img-blog.csdnimg.cn/20201029205942994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"训练步数的影响\"><a href=\"#训练步数的影响\" class=\"headerlink\" title=\"训练步数的影响\"></a>训练步数的影响</h3><p>从如下图总结出，BERT是需要巨大的预训练量级(128,000 words/batch * 1000,000 steps)进行训练的。还有就是虽然MLM预训练收敛速度比LTR慢（因为每个batch中只有15%的单词被预测，而不是所有单词都参与），但是准确度超过了LTR模型。<br><img src=\"https://img-blog.csdnimg.cn/20201029211329960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"不同Masking过程的影响\"><a href=\"#不同Masking过程的影响\" class=\"headerlink\" title=\"不同Masking过程的影响\"></a>不同Masking过程的影响</h3><p><img src=\"https://img-blog.csdnimg.cn/20201029211959996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"BERT用于feature-based方法\"><a href=\"#BERT用于feature-based方法\" class=\"headerlink\" title=\"BERT用于feature-based方法\"></a>BERT用于feature-based方法</h3><p>相对于fine-tuning的方式而言，feature-based的方式也有着其关键的优势。首先，不是所有的任务都可以轻易的表示成Trasformer encoder 架构，所以会有需要添加一个基于特定任务的模型架构的需求。其次，预先计算一次训练数据的昂贵表示，然后在此表示之上使用更便宜的模型运行许多实验，这对计算有很大的好处。<br><img src=\"https://img-blog.csdnimg.cn/20201029210454454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>BERT的大名想必早就知道了，了解BERT相关的结构以及威力还有有必要的，必备不时之需。BERT和GPT相反，使用的Transformer的Encoder（可见Transformer有多牛逼）。BERT的主要贡献是进一步将这些发现推广到深层双向架构，使得相同的预训练模型可以成功应对一组广泛的NLP任务。附录的相关信息我也直接穿插在上面各节的论述中，然后最后补充一下一个BERT,ELMo,OpenAI GPT模型架构对比图<br><img src=\"https://img-blog.csdnimg.cn/20201029210915719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>BERT使用了双向的Transformer架构</li>\n<li>OpenAI GPT使用了left-to-right的Transformer</li>\n<li>ELMo分别使用了left-to-right和right-to-left进行独立训练，然后将输出拼接起来，为下游任务提供序列特征<br>上面的三个模型架构中，只有BERT模型的表征在每一层都联合考虑到了左边和右边的上下文信息。</li>\n</ul>\n<p>除了架构不同，另外的区别在于BERT和OpenAI GPT是基于fine-tuning的方法，而ELMo是基于feature-based的方法。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","NLP","Transformer","BERT"]},{"title":"论文阅读笔记：大模型指导小模型--ProjectionNet的联合框架","url":"/Paper-Reading/86116a512b8e/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections<br>原文链接：<a href=\"https://arxiv.org/pdf/1708.00630.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>众所周知，深度学习网络通常很大，包括涉及许多层的参数，并经过大量数据训练，以学习可用于在推理时预测输出的有用表示形式，所以为了达到高效率，我们使用分布式计算达到目标，这将需要多个CPU内核或图形处理单元（GPU）。随着移动端设备的普及，我们自然而然的想将这些模型应用到移动端设备上，但是，与在云上运行的高性能群集不同，这些设备在低功耗模式下运行，并且存在显着的内存限制，所以如果还是使用老方法，完全是行不通的。</p>\n<p>即使将模型部署在云端，通过网络连接的方式进行使用，也会涉及到连接性问题（数据无法发送到服务器）或隐私原因（某些数据类型和处理需要限制在某些范围内），而且在许多实际场景中，将计算密集型操作从设备委派给云并不可行。还有就是模型压缩，降低浮点精度等等手段去缩减模型体积，其实在某些情况下，达不到应用场景的精度需要。所以需要有一个学习高效，具有低内存占用量的设备上机器学习模型的能力，这些模型可以直接在设备上运行以进行推理，并且计算成本较低</p>\n<p>论文中介绍了一种叫ProjectionNet的联合框架，可以为不同机器学习模型架构训练轻量的设备端模型。其使用复杂的前馈/循环架构（就像 LSTM）作为训练模型，联合一个简单的投影（projection）架构——其中包含动态投影操作以及一些窄带全连接层。整个架构使用反向传播在 TensorFlow 上进行端到端训练，在训练完成后，我们就可以直接使用紧凑的 ProjectionNet 进行推理了。通过这种方法，我们可以训练尺寸很小的 ProjectionNet 模型，兼顾小尺寸（比常规模型小几个数量级）与高性能，在一些视觉和语言分类任务中达到满意的效果。</p>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>许多相关工作通过在有限的大小或内存限制下学习有效的模型，比如简单的字典查找、特征修剪或散列的神经网络压缩的技术，以及降低的数字精度、矢量量化、网络的二值化策略、权重共享来实现神经网络的紧凑表示。这些方法大多数旨在通过使用低秩分解或哈希技巧对连接进行分组来利用网络权重中的冗余。</p>\n<p>相反，本论文中建议学习一个简单的基于投影（projection）的网络，该网络可以有效地编码中间网络表示形式（即隐藏单元）和所涉及的操作，而不是权重。同时还为设备模型引入了新的训练范例，其中简单网络经过耦合和联合训练可以模仿现有的深度网络，而且该深度网络非常灵活，可以根据不同的体系结构或任务进行自定义。</p>\n<h1 id=\"神经投影网络（Neural-Projection-Networks）\"><a href=\"#神经投影网络（Neural-Projection-Networks）\" class=\"headerlink\" title=\"神经投影网络（Neural Projection Networks）\"></a>神经投影网络（Neural Projection Networks）</h1><h2 id=\"ProjectionNets\"><a href=\"#ProjectionNets\" class=\"headerlink\" title=\"ProjectionNets\"></a>ProjectionNets</h2><p>神经网络是一类非线性模型，用于学习从输入 $\\vec{x}_i$ 到输出 $y_i$ 的映射，其中 $\\vec{x}_i$ 表示输入特征向量或序列（在递归神经网络的情况下），而 $y_i$ 是分类任务的输出类别或预测的序列。通常，这些网络由多层隐藏的单元或神经元组成，并在一对层之间建立连接。例如，在完全连接的前馈神经网络中，经过训练的加权连接或网络参数的数量为 $O(n^2)$，其中 $n$ 是每层隐藏单元的数量。而论文提出的联合优化框架，该架构结合了projection网络和trainer网络进行联合训练。如下图：<br><img src=\"https://img-blog.csdnimg.cn/2020111520400533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上图说明了神经投影网络架构，使用前馈NN作为trainer网络，这个耦合网络进行联合训练以优化组合损耗函数（公式1）：<br>$$L(\\theta,p)=\\lambda_1\\cdot L_\\theta(.)+\\lambda_2 \\cdot L^p(.)+\\lambda_3 \\cdot \\hat{L^p}(.)$$<br>其中$L_\\theta(.)$，$L^p(.)$和$\\hat{L^p}(.)$，对应于以下定义的两个网络的损失函数（公式2）：<br>$$L_\\theta(.)=\\sum_{i\\in N}D(h_\\theta(\\vec{x}_i), \\hat{y_i})$$  $$L^p(.)=\\sum_{i\\in N}D(h^p(\\vec{x}<em>i),h_\\theta(\\vec{x}_i))$$  $$\\hat{L^p}(.)=\\sum</em>{i\\in N}D(h^p(\\vec{x}_i), \\hat{y_i})$$<br>其中，$N$ 表示数据集中训练实例的数量， $\\vec{x}_i$ 表示前馈网络中的输入特征向量或RNN中的序列输入，而 $\\hat{y_i}$ 表示用于网络训练的真实输出类别。$h_\\theta(\\vec{x}_i)$，表示训练器网络中隐藏单元的参数化表示，将 $\\vec{x}_i$ 转换为输出预测 $y_i$，同样的，$h^p(\\vec{x}_i)$ 表示将输入转换为相应预测 $y_i^p$ 的投影网络参数。我们在两个网络的最后一层应用softmax激活来计算预测 $y_i$ 和 $y_i^p$。</p>\n<p>D表示距离函数，作为损失函数用于计算预测误差，误差分为三个部分：训练器预测误差，投影模拟误差和投影预测误差。减少第一个误差会得到更好的训练器网络，而减少后两个会反过来学习更好的投影网络，该网络更简单，但预测能力大致相同。实际上，我们在所有实验中对 $D(.)$ 使用交叉熵。</p>\n<p>对于等式2中的投影 $L^p$，我们遵循一种蒸馏方法来优化 $D(.)$ ，因为它已经显示出比仅在标签 $\\hat{y_i}$ 上训练的模型具有更好的泛化能力。$\\lambda_1$，$\\lambda_2$和 $\\lambda_3$ 是影响这些不同类型误差之间权衡的超参数，这些是在一个小的保留开发集上进行调整的，在我们的实验中，我们将它们设置为$\\lambda_1=1.0$，$\\lambda_2=0.1$和 $\\lambda_3=1.0$ 。</p>\n<ul>\n<li>**Trainer Network (θ)**：训练器模型是一个完整的神经网络（前馈，RNN或CNN），其选择灵活，取决于任务。图1演示了使用前馈网络的训练器，但可以与LSTM RNN（我们稍后介绍）或其他深度神经网络互换。对于图中所示的网络，层 $l_{k+1}$ 中 $h_\\theta(.)$ 的激活计算如下（公式3）：<br>$$A_{\\theta_{l_{k+1}}}=\\sigma(W_{\\theta_{l_{k+1}}}\\cdot A_{\\theta_{l_k}}+B_{\\theta_{l_{k+1}}})$$<br>其中，其中 $\\sigma$ 是除最后一个层以外应用于每一层的ReLU激活函数，$A$ 表示计算得出的隐藏单元的激活值。该网络中的 权重/偏差 参数 $W_\\theta$，$B_\\theta$ 的数量可以任意大，因为这只会在训练阶段使用，而这可以通过使用具有CPU或GPU的高性能分布式计算来有效地完成。</li>\n<li>**Projection Network ( p )**：投影模型是一个简单的网络，对一组有效的计算操作进行编码，这些操作将直接在设备上进行推断。模型本身定义了一组有效的“投影”函数 $\\mathbb{P}(\\vec{x}_i)$，将每个输入实例 $\\vec{x}_i$ 投影到不同的空间 $\\Omega_\\mathbb{P}$ ，然后在该空间中执行学习以将其映射到相应的输出 $y_i^p$。我们使用简化的投影网络，几乎没有操作，如图1所示。输入 $\\vec{x}_i$ 使用一系列 $T$ 投影函数 $\\mathbb{P}^1,…,\\mathbb{P}^T$ 进行转换，然后再进行单层激活（公式4和公式5）。<br>$$\\vec{x}_i^p=\\mathbb{P}^1(\\vec{x}_i),…,\\mathbb{P}^T(\\vec{x}_i)$$  $$y_i^p=softmax(W^p\\cdot \\vec{x}_i^p + B^p)$$<br>投影转换使用预先计算的参数化方法，即在学习过程中未对其进行训练，并且将其输出连接起来以形成用于后续操作的隐藏单元。在训练期间，较简单的投影网络将学习选择和应用特定的投影操作 $\\mathbb{P}^j$（通过激活），这些操作对于给定任务更具预测性。可以堆叠连接到该网络中间层的其他层，以实现投影的非线性组合。</li>\n</ul>\n<p>投影模型是与训练器共同训练的，并学会模仿整个训练器网络进行的预测，训练器预测网具有更多的参数，因此具有更大的预测能力。一旦学习完成，就从投影网络中提取变换函数 $\\mathbb{P}(.)$ 和相应的训练权重 $W^p$，$B^p$，以创建一个轻量级模型，并将其转移到设备。</p>\n<p>在我们的设置中，选择投影矩阵 $\\mathbb{P}$ 以及表示投影空间 $ΩP$ 会对计算成本和模型大小产生直接影响。我们建议利用局部敏感哈希（LSH）的修改版本，作为有效的随机投影方法来定义 $\\mathbb{P}(.)$ 。结合起来，我们使用 $1^d$ 表示 $\\Omega_\\mathbb{P}$ ，即网络的隐藏单元本身使用投影的位向量表示。与整个网络相比，这在参数的数量和大小方面都大大降低了内存占用量。我们在下面重点介绍此方法的一些关键属性：</p>\n<ul>\n<li>与典型的机器学习方法不同，不需要依靠预设的词汇表或特征空间，典型的机器学习方法采用较小的词汇表作为缩放机制。例如，LSTM RNN模型通常应用修剪，并在输入编码步骤中使用较小且固定大小的词汇表来降低模型的复杂性。</li>\n<li>所提出的学习方法可有效地缩放到大数据大小和高维空间。这对于涉及稀疏高维特征空间的自然语言应用程序特别有用。对于密集的特征空间（例如图像像素），可以在不依赖大量参数的情况下有效地近似现有操作（例如全连接层（甚至卷积））进行预测。在限制存储需求下，这种操作还可以与投影功能结合使用，以产生更复杂的投影网络。</li>\n<li> $\\mathbb{P}(x_i)$ 的计算与训练数据的大小无关。</li>\n<li>我们确保 $\\mathbb{P}(.)$ 可以高效地进行即时计算，以便在设备上进行推理。</li>\n</ul>\n<p>接下来，将更详细地描述投影方法和相关的操作。</p>\n<h2 id=\"局部敏感投影网络（Locality-Sensitive-Projection-Network）\"><a href=\"#局部敏感投影网络（Locality-Sensitive-Projection-Network）\" class=\"headerlink\" title=\"局部敏感投影网络（Locality Sensitive Projection Network）\"></a>局部敏感投影网络（Locality Sensitive Projection Network）</h2><p>前面描述的投影网络依赖于一组转换函数 $\\mathbb{P}$，这些函数将输入 $\\vec{x}_i$ 投影到隐藏的单位表示 $\\Omega_\\mathbb{P}$ 中。可以使用不同类型的函数来执行公式4中概述的投影操作。一种可能性是使用通过word2vec或类似技术得到预训练的特征嵌入矩阵，模型 $\\mathbb{P}$ 作为 $\\vec{x}_i$ 中特征的嵌入查找，然后进行诸如矢量平均之类的聚合操作。</p>\n<p>相反，我们在此步骤中采用了有效的随机投影方法，我们使用局部敏感哈希（LSH）来建模基础的投影操作，LSH通常用作诸如聚类之类的应用程序的降维技术。我们在Projection Nets中使用LSH的动机是，它允许我们将类似的输入 $\\vec{x}_i$ 或中间网络层投影到接近度量空间的隐藏单位向量中。这使我们能够转换输入并学习有效而紧凑的网络表示形式，该表示形式仅取决于数据的固有维数（即观察到的特征），而不是实例数或实际数据矢量的维数（即所有特征或词汇量）。我们通过二进制散列函数来实现。</p>\n<ul>\n<li><strong>定理1</strong>：对于 $\\vec{x}_i,\\vec{x}_j\\in \\mathbb{R}^n$ 和从 $\\mathbb{R}^n$ 上的球对称分布绘制的矢量 $\\mathbb{P}_k$，内积符号与矢量之间的角度 $\\measuredangle(\\vec{x}_i,\\vec{x}_j)$ 之间的关系可以表示为：<br>$$\\measuredangle(\\vec{x}_i,\\vec{x}_j)=\\pi Pr{sgn[&lt;\\vec{x}_i,\\mathbb{P}_k&gt;]\\neq sgn[&lt;\\vec{x}_j,\\mathbb{P}_k&gt;]}$$<br>此属性适用于简单的几何，即每当投影矩阵 $\\mathbb{P}$ 的行向量落入单位向量之间在 $\\vec{x}_i$ 和 $\\vec{x}_j$ 方向上的角度之内时，它们将产生相反的符号。与包含 $\\vec{x}_i\\vec{x}_j$ 的平面正交的任何投影矢量都将无效。由于可以使用内积来确定附近的参数表示形式，所以 $&lt;\\vec{x}_i,\\vec{x}_j&gt;=||\\vec{x}_i||\\cdot ||\\vec{x}_j||\\cdot cos\\measuredangle(\\vec{x}_i,\\vec{x}_j)$，因此我们可以通过使用矢量的签名来有效地建模和存储网络隐藏的激活单元矢量。</li>\n<li><strong>计算投影</strong>：遵循上述属性，我们反复使用二进制哈希，然后将 $\\mathbb{P}$ 中的投影向量应用于将输入 $\\vec{x}<em>i$ 转换为由 $\\mathbb{P}_k(\\vec{x}_i)\\in {0,1}^d$ 表示的二进制哈希表示，其中 $[\\mathbb{P}_k(\\vec{x}_i)] :=sgn[&lt;\\vec{x}_i,\\mathbb{P}_k&gt;]$ 。使得 d-bit 矢量表示，对应于每个投影行 $\\mathbb{P}</em>{k=1…d}$ 的一位。投影矩阵 $\\mathbb{P}$ 在训练和推理之前是固定的，请注意，我们无需显式存储随机投影矢量 $\\mathbb{P}_k$ ，因为我们可以使用哈希函数动态计算它们，而不用调用随机数生成器。此外，这还使我们能够执行在观察到的特征尺寸上呈线性的投影操作，而不是对高维数据而言可能过大的整体特征尺寸，从而节省了内存和计算成本。二进制表示很重要，因为与训练器网络相比，这使得投影网络参数表示的非常紧凑，从而大大减小了模型大小。注意，其他方法，例如量化或权重共享，可以叠加在此方法之上，以在内存减少方面提供较小的进一步收益。</li>\n<li><strong>投影参数</strong>：实际上，我们采用 $T$ 个不同的投影函数 $\\mathbb{P}^{j=1…T}$ ，如图1所示。每一个都产生 d-bit 向量，将其连接起来以形成等式4中的预测激活单元 $\\vec{x}_i^p$。$T$ 和 $d$ 取决于为 $\\mathbb{P}$ 指定的投影网络参数配置，并且可以调整为在预测质量和模型大小之间进行权衡。</li>\n</ul>\n<h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2><p>如前所述，我们使用紧凑的位单元来表示投影网络。在训练过程中，该网络学习在投影的位空间 $\\Omega_\\mathbb{P}$ 中彼此相邻的点沿相同方向移动梯度。梯度的方向和大小由可访问更多参数集和更复杂架构的训练器网络确定。使用反向传播对两个网络进行联合训练。尽管有联合优化目标，但在高性能CPU或GPU上进行分布式计算时，随机梯度下降可以有效地进行训练。经过训练后，这两个网络将解耦并用于不同的目的。 训练器模型可以部署在使用标准神经网络的任何地方。提取更简单的投影网络模型权重以及变换函数 $\\mathbb{P}(.)$ ，以创建轻量级模型，并将其推送到设备。</p>\n<ul>\n<li>复杂度：推理的总体复杂度为 $O(n\\cdot T\\cdot d)$，其中 $n$ 是观察到的特征大小（<strong>不是</strong>总词汇量大小），它在输入大小上呈线性关系，$d$ 是为每个投影矢量 $\\mathbb{P}_k$ 指定的LSH位数 ，$T$ 是 $\\mathbb{P}$ 中使用的投影函数的数量。在此设置中，投影推断模型所需的模型大小（根据参数的数量）和存储空间为 $O(T\\cdot d)$。</li>\n</ul>\n<p>作为位向量表示 $\\Omega_\\mathbb{P}$ 的替代，投影矩阵 $\\mathbb{P}$ 可以用来生成投影网络中隐藏单元的稀疏表示。每个 d-bit 块都可以编码为整数而不是位向量。这样会导致整体参数空间为 $O(T\\cdot 2^d)$（反而更大），但是对于实际学习的参数数量很少，并且可以通过有效的稀疏查找操作进行推理的应用程序仍然是有益的。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>下表显示了基线（MNIST手写数字识别）的结果以及与具有不同大小 $(T,d)$ 的ProjectionNet模型的比较。结果表明，小型的ProjectionNet的压缩率高达388x，可以实现92.3％的高精度，而内存占用量明显更大的基准可以达到98.9％。此外，ProjectionNet模型能够实现模型尺寸的进一步减小（最大2000x-3500x），同时对top-1的预测的精度约为70-80％，而对top-3的预测的精度约为90-94％。<br><img src=\"https://img-blog.csdnimg.cn/20201115230624884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了ProjectionNets与基准（ CIFAR-100图像分类）之间的结果比较。 如前所述，此任务比MNIST更复杂，因此精度数较低。<br><img src=\"https://img-blog.csdnimg.cn/20201115231337199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表使用神经投影网络和基线进行语义意图分类的结果。表中显示，使用LSTM RNN训练的ProjectionNet达到82.3％的precision@ 1，与基线LSTM相比仅下降了15％，但减少了内存占用和计算量（与LSTM展开步骤相比）。<br><img src=\"https://img-blog.csdnimg.cn/20201115231834990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在MNIST的识别任务中，80-100个神经投影bits足以得到70-80％的精度解决该任务，将其增加到720bits可达到92.3％的精度，通过使用更深的投影进一步提高了精度网络。对于涉及序列输入的语义分类的语言任务，需要720个神经投影bits才能达到82.3％的top-1精度。</p>\n<p>下图显示了MNIST和CIFAR-100任务的图。 该图表明，可以使用简单的100位ProjectionNet进行MNIST分类，从而简洁地获得具有3-5M参数的3层前馈网络的预测能力（比率=〜0.8）。 需要恢复90％以上的基础深度网络质量。<br><img src=\"https://img-blog.csdnimg.cn/20201115232738807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文引入了一种新的神经投影方法来训练轻量级神经网络模型，从而以较低的计算和内存成本在设备上执行有效的推理。论文展示了这种方法在模型大小和深度网络体系结构变化方面的灵活性。第3节末尾讨论了该框架的一些可能的将来扩展。除了深度学习之外，还可以将此框架应用于其他类型的学习场景中的轻量级模型训练。例如，训练范例可以更改为半监督或无监督设置。可以修改训练器模型本身，以合并在图形或概率图形模型（而非深度神经网络）上定义的结构化损失函数。下图展示了使用图优化损失函数学习轻量模型的端到端投影图方法，可以使用大规模分布图算法甚至神经图方法有效地训练它们。<br><img src=\"https://img-blog.csdnimg.cn/20201115233402806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Paper-Reading"],"tags":["深度学习","神经网络","ProjectionNet","移动端模型","部署"]},{"title":"论文阅读笔记：看完也许能进一步了解Batch-Normalization","url":"/Paper-Reading/c766928db46f/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<br>原文链接：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>训练深度神经网络非常复杂，因为在训练过程中，随着先前各层的参数发生变化，各层输入的分布也会发生变化，导致调参工作要做的很小心，训练更加困难，论文中将这种现象称为“internal covariate shift”，而Batch Normalization正式用来解决深度神经网络中internal covariate shift现象的方法。有关covariate shift的内容，可以参阅我另一篇<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记</a>。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Batch Normalization是在每个mini-batch进行归一化操作，并将归一化操作作为模型体系结构的一部分，使用BN可以获得如下的好处：</p>\n<ul>\n<li><strong>可以使用更大的学习率</strong>，训练过程更加稳定，极大提高了训练速度。</li>\n<li><strong>可以将bias置为0</strong>，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。</li>\n<li><strong>对权重初始化不再敏感</strong>，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差也会放缩同样的倍数，相除抵消。</li>\n<li><strong>对权重的尺度不再敏感</strong>。</li>\n<li><strong>深层网络可以使用sigmoid和tanh了</strong>，BN抑制了梯度消失。</li>\n<li><strong>Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。</strong></li>\n</ul>\n<p>我们从梯度计算开始看起，如在SGD中是优化参数 $\\theta$，从而最小化损失，如下公式：<br>$$\\theta=arg\\underset{\\theta}{min}\\frac{1}{N}\\sum_{i=1}^{N}l(x_i,\\theta)$$<br>其中，$x_1…x_N$是训练数据集。使用SGD，训练将逐步进行，并且在每个步骤中，我们考虑大小为 $m$ 的mini-batch，即$x_1…m$，通过计算$\\frac{1}{m}\\frac{\\partial(x_i,\\theta)}{\\partial\\theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算 $m$ 次效率更高。</p>\n<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p>\n<h1 id=\"BN之前的一些减少Covariate-Shift的方法\"><a href=\"#BN之前的一些减少Covariate-Shift的方法\" class=\"headerlink\" title=\"BN之前的一些减少Covariate Shift的方法\"></a>BN之前的一些减少Covariate Shift的方法</h1><p>对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除Internal Covariate Shift的不良影响。那么如何消除呢？考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数，但这样会弱化梯度下降步骤。</p>\n<p>例如：例如，考虑一个层，其输入u加上学习到的偏置 $b$，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\\hat x=x - E[x]$，$x = u+b$，$X={x_{1\\ldots N}}$ 是训练集上$x$ 值的集合，$E[x] = \\frac{1}{N}\\sum_{i=1}^N x_i$。如果梯度下降步骤忽略了 $E[x]$ 对 $b$的依赖，那它将更新$b\\leftarrow b+\\Delta b$，其中$\\Delta b\\propto -\\partial{\\ell}/\\partial{\\hat x}$。然后 $u+(b+\\Delta b) -E[u+(b+\\Delta b)] = u+b-E[u+b]$。因此，结合 $b$ 的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，$b$ 将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。在最初的实验中，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p>\n<p>总结而言就是使用白话来缓解ICS问题，白化是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：</p>\n<ul>\n<li>使得输入特征分布具有相同的均值与方差，其中PCA白化保证了所有特征分布均值为0，方差为1，而ZCA白化则保证了所有特征分布均值为0，方差相同。</li>\n<li>去除特征之间的相关性。</li>\n</ul>\n<p><strong>通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛。但是白话过程的计算成本太高，并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作，这未免过于奢侈。而且白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力，底层网络学习到的参数信息会被白化操作丢失掉。</strong></p>\n<h1 id=\"BN算法描述\"><a href=\"#BN算法描述\" class=\"headerlink\" title=\"BN算法描述\"></a>BN算法描述</h1><p>文中使用了类似z-score的归一化方式：每一维度减去自身均值，再除以自身标准差，由于使用的是随机梯度下降法，这些均值和方差也只能在当前迭代的batch中计算，故作者给这个算法命名为Batch Normalization。BN变换的算法如下所示，其中，为了数值稳定，$\\epsilon$ 是一个加到小批量数据方差上的常量。<br><img src=\"https://img-blog.csdnimg.cn/20201228154526445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以将上面的算法总结为两步：</p>\n<ul>\n<li><strong>Standardizatio</strong>n：首先对 $m$ 个 $x$ 进行Standardization，得到 zero mean unit variance的分布 $\\hat{x}$。</li>\n<li><strong>scale and shift</strong>：然后再对 $\\hat{x}$ 进行scale and shift，缩放并平移到新的分布 $y$，具有新的均值 $\\beta$ 方差 $\\gamma$。</li>\n</ul>\n<p>更形象一点，假设BN层有 $d$ 个输入节点，则 $x$ 可构成 $d\\times m$大小的矩阵 $X$，BN层相当于通过行操作将其映射为另一个 $d\\times m$ 大小的矩阵$Y$，如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228154941139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>将2个过程写在一个公式里如下：<br>$$y_i^{(b)}=BN(x_i)^{(b)}=\\gamma (\\frac{x_i^{(b)}-\\mu(x_i)}{\\sqrt{\\sigma(x_i)^2+\\epsilon}})+\\beta$$</p>\n<p>其中，$x_i^{(b)}$ 表示输入当前batch的b-th样本时该层i-th输入节点的值，$x_i$ 为 $[x_i^{(1)},x_i^{(2)},…,x_i^{(m)}]$ 构成的行向量，长度为batch size $m$，$\\mu$和$\\sigma$为该行的均值和标准差，$\\epsilon$ 为防止除零引入的极小量（可忽略），$\\gamma$和$\\beta$为该行的scale和shift参数，可知</p>\n<ul>\n<li>$\\mu$ 和 $\\sigma$ 为当前行的统计量，不可学习。</li>\n<li>$\\gamma$ 和 $\\beta$ 为待学习的scale和shift参数，用于控制 $y_i$ 的方差和均值。</li>\n<li>BN层中，$x_i$ 和 $x_j$ 之间不存在信息交流 $(i\\neq j)$</li>\n</ul>\n<p>可见，**无论xi原本的均值和方差是多少，通过BatchNorm后其均值和方差分别变为待学习的 $\\gamma$ 和 $\\beta$**。为什么需要 $\\gamma$ 和 $\\beta$ 的可训练参数？Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，单纯通过让每一层的输入分布均值为0，方差为1，而不做缩放和移位，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。</p>\n<blockquote>\n<p>在训练初期，分界面还在剧烈变化时，计算出的参数不稳定，所以退而求其次，<strong>在 $Wx+b$ 之后，ReLU激活层前面进行归一化</strong>。因为初始的 $W$ 是从标准高斯分布中采样得到的，而 $W$ 中元素的数量远大于 $x$，$Wx+b$ 每维的均值本身就接近 $0$、方差接近 $1$，所以在 $Wx+b$ 后使用Batch Normalization能得到更稳定的结果，如下图所示：</p>\n</blockquote>\n<p><img src=\"https://img-blog.csdnimg.cn/20201228154233138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Batch-Normalization的反向传播\"><a href=\"#Batch-Normalization的反向传播\" class=\"headerlink\" title=\"Batch Normalization的反向传播\"></a>Batch Normalization的反向传播</h1><p>讲反向传播之前，我们先来简单的写一下正向传递的代码，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">batchnorm_forward</span>(<span class=\"params\">x, gamma, beta, eps</span>):</span></span><br><span class=\"line\">    N, D = x.shape</span><br><span class=\"line\">    <span class=\"comment\"># 第一步：计算平均</span></span><br><span class=\"line\">    mu = <span class=\"number\">1.</span>/N * np.<span class=\"built_in\">sum</span>(x, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 第二步：每个训练样本减去平均</span></span><br><span class=\"line\">    xmu = x - mu</span><br><span class=\"line\">    <span class=\"comment\"># 第三步：计算分母</span></span><br><span class=\"line\">    sq = xmu ** <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"comment\"># 第四步：计算方差</span></span><br><span class=\"line\">    var = <span class=\"number\">1.</span>/N * np.<span class=\"built_in\">sum</span>(sq, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 第五步：加上eps保证数值稳定性，然后计算开方</span></span><br><span class=\"line\">    sqrtvar = np.sqrt(var + eps)</span><br><span class=\"line\">    <span class=\"comment\"># 第六步：倒转sqrtvar</span></span><br><span class=\"line\">    ivar = <span class=\"number\">1.</span>/sqrtvar</span><br><span class=\"line\">    <span class=\"comment\"># 第七步：计算归一化</span></span><br><span class=\"line\">    xhat = xmu * ivar</span><br><span class=\"line\">    <span class=\"comment\"># 第八步：加上两个参数</span></span><br><span class=\"line\">    gammax = gamma * xhat</span><br><span class=\"line\">    out = gammax + beta</span><br><span class=\"line\">    <span class=\"comment\"># cache储存计算反向传递所需要的一些内容</span></span><br><span class=\"line\">    cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out, cache</span><br></pre></td></tr></table></figure>\n<p>我们都知道，对于目前的神经网络计算框架，一个层要想加入到网络中，要保证其是可微的，即可以求梯度。BatchNorm的梯度该如何求取？反向传播求梯度只需抓住一个关键点，如果一个变量对另一个变量有影响，那么他们之间就存在偏导数，找到直接相关的变量，再配合链式法则，公式就很容易写出了。<br><img src=\"https://img-blog.csdnimg.cn/20201228161718394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>根据反向传播的顺序，首先求取损失 $l$ 对BN层输出 $y_i$ 的偏导 $\\frac{\\partial l}{\\partial y_i}$，然后是对可学习参数的偏导 $\\frac{\\partial l}{\\partial \\gamma}$ 和 $\\frac{\\partial l}{\\partial \\beta}$，用于对参数进行更新，想继续回传的话还需要求对输入 $x$ 偏导，于是引出对变量 $\\mu$、$\\sigma^2$ 和 $\\hat{x}$ 的偏导，根据链式法则再求这些变量对 $x$ 的偏导，计算图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201228205034787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>通过链式法则，我们可以对上面的正向传递的代码进行运算，得到反向传播的代码，如下（结合代码理解更方便）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">batchnorm_backward</span>(<span class=\"params\">dout, cache</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 展开存储在cache中的变量</span></span><br><span class=\"line\">    xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache</span><br><span class=\"line\">    <span class=\"comment\"># 获得输入输出的维度</span></span><br><span class=\"line\">    N, D = dout.shape</span><br><span class=\"line\">    dbeta = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dgammax = dout</span><br><span class=\"line\">    dgamma = np.<span class=\"built_in\">sum</span>(dgammax * xhat, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dxhat = dgammax * gamma</span><br><span class=\"line\">    divar = np.<span class=\"built_in\">sum</span>(dxhat * xmu, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dxmu1 = dxhat * ivar</span><br><span class=\"line\">    dsqrtvar = -<span class=\"number\">1.</span>/(sqrtvar ** <span class=\"number\">2</span>) * divar</span><br><span class=\"line\">    dvar = <span class=\"number\">0.5</span> * <span class=\"number\">1.</span> / np.sqrt(var + eps) * dsqrtvar</span><br><span class=\"line\">    dsq = <span class=\"number\">1.</span> / N * np.ones((N, D)) * dvar</span><br><span class=\"line\">    dxmu2 = <span class=\"number\">2</span> * xmu * dsq</span><br><span class=\"line\">    dx1 = (dxmu1 + dxmu2)</span><br><span class=\"line\">    dmu = -<span class=\"number\">1</span> * np.<span class=\"built_in\">sum</span>(dxmu1+dxmu2, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dx2 = <span class=\"number\">1.</span> / N * np.ones((N, D)) * dmu</span><br><span class=\"line\">    dx = dx1 + dx2</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>\n<h1 id=\"Batch-Normalization的预测阶段\"><a href=\"#Batch-Normalization的预测阶段\" class=\"headerlink\" title=\"Batch Normalization的预测阶段\"></a>Batch Normalization的预测阶段</h1><p>在预测阶段，所有参数的取值是固定的，对BN层而言，意味着$\\mu$、$\\sigma$、$\\gamma$、$\\beta$ 都是固定值。$\\gamma$和$\\beta$ 比较好理解，随着训练结束，两者最终收敛，预测阶段使用训练结束时的值即可。对于 $\\mu$ 和 $\\sigma$，在训练阶段，它们为当前mini batch的统计量，随着输入batch的不同， $\\mu$ 和 $\\sigma$ 一直在变化。在预测阶段，输入数据可能只有1条，该使用哪个 $\\mu$ 和 $\\sigma$ ，或者说，每个BN层的 $\\mu$ 和 $\\sigma$ 该如何取值？可以采用训练收敛最后几批mini batch的 $\\mu$ 和 $\\sigma$ 的期望，作为预测阶段的 $\\mu$ 和 $\\sigma$ ，如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228162916396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>因为Standardization和scale and shift均为线性变换，在预测阶段所有参数均固定的情况下，参数可以合并成 $y=kx+b$ 的形式，如上图中行号11所示。</p>\n<p>这里多说一句，BN在卷积中使用时，1个卷积核产生1个feature map，1个feature map有1对 $\\gamma$和$\\beta$ 参数，同一batch同channel的feature map共享同一对 $\\gamma$和$\\beta$ 参数，若卷积层有 $n$ 个卷积核，则有 $n$ 对 $\\gamma$和$\\beta$ 参数。</p>\n<p>对于测试集均值和方差已经不是针对某一个Batch了，而是针对整个数据集而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录每一个Batch的均值和方差，以便训练完成之后按照下式计算整体的均值和方差：<br>$$E[x]\\leftarrow E_\\beta[\\mu_\\beta]$$    $$Var[x]\\leftarrow \\frac{m}{m-1}E_\\beta[\\sigma_\\beta^2]$$<br> 上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是行号11所示。</p>\n<h1 id=\"网络inference阶段conv层和BN层的融合\"><a href=\"#网络inference阶段conv层和BN层的融合\" class=\"headerlink\" title=\"网络inference阶段conv层和BN层的融合\"></a>网络inference阶段conv层和BN层的融合</h1><p>现在很多的网络结构都将BN层直接放在卷积层和激活层之间，这种做法可以在网络的inference阶段，将BN层的运算直接嵌入到卷积层中，减少运算量，提升网络的运行速度。在inference阶段，已知某层卷积层的kernel参数 $w$， $b$ ，以及输入 $x$ ，紧随其后的BN层参数（已学习到）：尺度参数 $\\gamma$ 、偏移参数 $\\beta$ 、以及样本均值 $\\hat{\\mu}$ 和标准差 $\\hat{\\sigma}$ 。</p>\n<ul>\n<li>卷积层输出为：$w<em>x+b$<br>bn层输出为： $\\gamma \\frac{w</em>x+b-\\hat{\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta$<br>bn层的输出可以化为如下形式：<br>$$\\gamma \\frac{w*x+b-\\hat{\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta=(\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}w)<em>x+\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}b-\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}\\hat{\\mu}+\\beta=kw</em>x+b^{‘}$$<br>所以，在inference阶段，如果BN直接跟在卷积层后，可以将BN层直接嵌入到卷积层的计算中，相当于将卷积核缩放一定倍数，并对偏置进行一定改变。</li>\n</ul>\n<p>将BN层融合到卷积层中，就相当于对卷积核进行一定的修改，并没有增加卷积层的计算量，同时整个BN层的计算量都省去了。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>下图是使用三层全连接层，在每层之后添加BN以及无添加的实验对比：<br><img src=\"https://img-blog.csdnimg.cn/20201228170635438.png#pic_center\" alt=\"在这里插入图片描述\"><br>下图是训练步和精度的实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201228172923853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图是使用BN在Inception上的相关实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201228195302919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"关于BN的几个讨论\"><a href=\"#关于BN的几个讨论\" class=\"headerlink\" title=\"关于BN的几个讨论\"></a>关于BN的几个讨论</h2><ul>\n<li>没有scale and shift过程可不可以？<br>BatchNorm有两个过程，Standardization和scale and shift，前者是机器学习常用的数据预处理技术，在浅层模型中，只需对数据进行Standardization即可，Batch Normalization可不可以只有Standardization呢？答案是可以，但网络的表达能力会下降。直觉上理解，浅层模型中，只需要模型适应数据分布即可。对深度神经网络，每层的输入分布和权重要相互协调，强制把分布限制在zero mean unit variance并不见得是最好的选择，加入参数 $\\gamma$和$\\beta$ ，对输入进行scale and shift，有利于分布与权重的相互协调，特别地，令 $\\gamma=1$和$\\beta=0$ 等价于只用Standardization，令 $\\gamma=\\sigma$和$\\beta=\\mu$ 等价于没有BN层，scale and shift涵盖了这2种特殊情况，在训练过程中决定什么样的分布是适合的，所以使用scale and shift增强了网络的表达能力。表达能力更强，在实践中性能就会更好吗？并不见得，就像曾经参数越多不见得性能越好一样。在<a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a>中，作者实验发现没有scale and shift性能可能还更好一些，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201228195444340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>BN层放在ReLU前面还是后面？实验表明，放在前后的差异似乎不大，甚至放在ReLU后还好一些（如上图），放在ReLU后相当于直接对每层的输入进行归一化，这与浅层模型的Standardization是一致的。<a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a>中有很多组合实验结果，可以看看。BN究竟应该放在激活的前面还是后面？以及，BN与其他变量，如激活函数、初始化方法、dropout等，如何组合才是最优？可能只有直觉和经验性的指导意见，具体问题的具体答案可能还是得实验说了算</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>Batch Normalization的加速作用体现在两个方面：一是归一化了每层和每维度的scale，所以可以整体使用一个较高的学习率，而不必像以前那样迁就小scale的维度；二是归一化后使得更多的权重分界面落在了数据中，降低了overfit的可能性，因此一些防止overfit但会降低速度的方法，例如dropout和权重衰减就可以不使用或者降低其权重。</p>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>BN层的有效性已有目共睹，但为什么有效可能还需要进一步研究，还需要进一步研究，这里整理了一些关于BN为什么有效的论文，贴在这：</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1805.11604.pdf\">How Does Batch Normalization Help Optimization?</a>：<strong>BN层让损失函数更平滑</strong>。<br>论文中通过分析训练过程中每步梯度方向上步长变化引起的损失变化范围、梯度幅值的变化范围、光滑度的变化，认为添加BN层后，损失函数的landscape(loss surface)变得更平滑，相比高低不平上下起伏的loss surface，平滑loss surface的梯度预测性更好，可以选取较大的步长。如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228201948418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1612.04010.pdf\">An empirical analysis of the optimization of deep network loss surfaces</a>：<strong>BN更有利于梯度下降</strong>。<br>论文中绘制了VGG和NIN网络在有无BN层的情况下，loss surface的差异，包含初始点位置以及不同优化算法最终收敛到的local minima位置，如下图所示。没有BN层的，其loss surface存在较大的高原，有BN层的则没有高原，而是山峰，因此更容易下降。<br><img src=\"https://img-blog.csdnimg.cn/2020122820245825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>参考文献：</p>\n<ul>\n<li><a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a></li>\n<li><a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\">Understanding the backward pass through Batch Normalization Layer</a></li>\n<li><a href=\"https://abay.tech/blog/2018/07/01/why-does-batch-normalization-work/\">Why Does Batch Normalization Work?</a></li>\n<li><a href=\"https://www.cnblogs.com/shine-lee/p/11989612.html\">Batch Normalization详解</a></li>\n<li><a href=\"https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b\">Batch Normalization — What the hey</a></li>\n<li><a href=\"http://gradientscience.org/batchnorm/\">How does Batch Normalization Help Optimization?</a></li>\n<li><a href=\"https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/\">How does Batch Normalization Help Optimization?</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["计算图","Covariate Shift","机器学习","Batch-Normalization"]},{"title":"论文阅读笔记：端到端可训练的面向任务的对话系统","url":"/Paper-Reading/79c2e935e8d0/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：A Network-based End-to-End Trainable Task-oriented Dialogue System<br>原文链接：<a href=\"https://arxiv.org/pdf/1604.04562.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>论文作者将对话建模成一个seq2seq的映射问题，该seq2seq框架以对话历史数据（通过belief tracker建模）和数据库查询结果（通过Database Operator得到结果）作为支撑。</p>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>教会机器完成与人自然交流的任务是充满挑战性的，当前，开发面向任务的对话系统需要创建多个组件，通常这涉及大量的手工制作或获取昂贵的标记数据集以解决每个组件的统计学习问题。在这项工作中，我们介绍了基于神经网络的文本输入，文本输出的端到端可训练的面向目标的对话系统，以及一种基于pipeline的Wizard-of-Oz框架的收集对话数据的新方法。这种方法使我们能够轻松开发对话系统，而无需对手头的任务做太多假设。结果表明，该模型可以自然地与人类交谈，同时帮助他们完成餐馆搜索领域的任务。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>建立面向任务的对话系统（例如酒店预订或技术支持服务）很困难，因为它是针对特定应用的，并且训练数据的可用性通常有限。为了缓解这个问题，最近的面向任务的对话系统设计的机器学习方法已经将该问题作为部分可观察到的马尔可夫决策过程（POMDP）进行了研究，目的是使用强化学习（RL）进行训练，通过与真实用户的互动在线对话策略。然而，语言理解和语言生成模块仍然依赖于监督学习，因此需要语料库来训练。此外，为了使RL易于处理，必须仔细设计状态和动作空间，这可能会限制模型的表达能力和可学习性，而且训练此类模型所需的奖励方法难以设计且难以在运行时进行衡量。</p>\n<p>另一方面，从序列到序列学习激发了一些努力来构建端到端可训练的，非任务导向的对话系统。该系列方法将对话视为目标序列转导问题的来源，应用编码器网络将用户查询编码为代表其语义的分布矢量，然后使用解码器网络以生成每个系统响应，这些模型通常需要大量数据来训练。它们允许创建有效的聊天机器人类型的系统，但是它们缺乏支持领域特定任务的任何功能，例如，能够与数据库进行交互，并将有用的信息汇总到他们的系统中回应。</p>\n<p>在这项工作中，我们通过平衡两个研究方向的优势和劣势，为面向任务的对话系统提出了一种基于神经网络的模型。</p>\n<ul>\n<li>该模型是端到端可训练的，但仍模块化连接</li>\n<li>它并不能直接为用户目标建模，但是尽管如此，它仍然可以通过在每一步提供<strong>relevant</strong>且<strong>appropriate</strong>的响应来学习完成所需的任务</li>\n<li>它具有数据库（DB）属性（槽-值对）的显式表示形式，用于实现较高的任务成功率，但具有用户意图的分布表示（对话行为）允许模棱两可的输入</li>\n<li>并且它使用了词法分解和权重绑定策略来减少训练模型所需的数据，但是如果有更多数据可用，它仍然保持较高的自由度。</li>\n</ul>\n<h1 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h1><p>模型结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/2020100322492021.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在每个回合中，系统都会从用户那里获得token序列作为输入，并将其转换为两个内部表示形式：</p>\n<ul>\n<li>由 intent network生成的分布表示</li>\n<li>由一组belief trackers生成的称为belief state的槽值对上的概率分布</li>\n</ul>\n<p>然后数据库operator挑选belief state中最可能的值以形成对数据库的查询，策略网络将搜索结果、意图表示和信念状态进行转换和组合，以形成代表下一个系统动作的单个向量。然后，该系统动作向量用于调节响应生成网络，该网络以骨架(skeletal)形式中的token生成所需的系统token输出。然后，通过将数据库实体的实际值代入骨架句结构来形成最终的系统响应。</p>\n<blockquote>\n<p>具体而言，在每一轮对话中，通过Intent Network得到一个用户输入的向量表征，通过Belief Tracker得到一个slot-value的概率分布，随后database operator针对概率最大的slot-value在数据库中进行查询，得到的结果将会和Intent Network输出的向量表征以及Belief Tracker输出的slot-value分布概率共同输入给policy network，随后获得一个向量，该向量表征了该系统的下一个action，然后该action被输入到Generation Network中产生回复。</p>\n</blockquote>\n<p>每个组件的详细说明如下。</p>\n<h3 id=\"Intent-Network\"><a href=\"#Intent-Network\" class=\"headerlink\" title=\"Intent Network\"></a>Intent Network</h3><p>Intent Network可以看作是序列到序列学习框架中的编码器，其工作是在 $t$回合， 将输入tokens为 $w_0^t，w_1^t,…,w_N^t$ 的序列编码为分布向量表示 $z_t$。通常，使用长短期记忆（LSTM）网络，其中最后一个时间步中隐藏层 $z_t^N$ 被表示为：<br>$$z_t=z_t^N=LSTM(w_0^1,w_1^t,…w_N^t)$$<br>或者，可以使用卷积神经网络（CNN）代替LSTM作为编码器<br>$$z_t=CNN(w_0^1,w_1^t,…w_N^t)$$<br>本文中都进行探讨。</p>\n<h3 id=\"Belief-Trackers\"><a href=\"#Belief-Trackers\" class=\"headerlink\" title=\"Belief Trackers\"></a>Belief Trackers</h3><p><img src=\"https://img-blog.csdnimg.cn/20201004101353442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Belief Tracker也叫做Dialogue State Tracker，它的详细结构如上图所示，包含两个主要结构：</p>\n<ul>\n<li>Delexicalised CNN</li>\n<li>Jordan-type RNN</li>\n</ul>\n<p>在Delexicalised CNN中（<strong>delexicalised主要指句子中同一类型的实体词都被替换成该类型的统一符号，以在slot类型中共享参数</strong>），在当前对话轮次 $t$ 的用户输入 $u$，以及上一轮次系统的回复 $m$，分别通过该CNN结构后的输出进行concatenation，只不过需要注意的是，在各自的CNN结构中，除了使用CNN的最终输出外，也会利用各自输入句子中被delexicalised处的浅层CNN特征（为了保证各层卷积的输出能够与输入句子长度对应一致，在每一层卷积输入的首尾两端进行padding），如果当前句子没有被delexicalised的词则进行padding。<br>$$f_{v,cnn}^t=CNN_{s,v}^{(u)}(u_t)⊕CNN_{s,v}^{(m)}(m_{t-1})$$<br>在Jordan-type RNN中可以看到，和普通RNN结构不同，Jordan-type RNN更为简单，没有输入层，直接将上一个时刻的输出，以及来自于Delexicalised CNN的输出进行concatenation后当做RNN的隐藏层状态，并通过softmax后得到当前时刻的输出，具体计算过程公式如下所示：<br>$$f_v^t=f_{v,cnn}^t⊕p_v^{t-1}⊕p_∅^{t-1}$$ $$g_v^t=w_s \\cdot sigmoid(W_sf_v^t+b_s)+b_s^{‘}$$ $$p_v^t=\\frac{exp(g_v^t)}{exp(g_{∅,s})+\\sum_{v’∈V_s}exp(g_{v’}^t)}$$</p>\n<p>其中concat到Jordan-type RNN隐藏层的除了CNN的输出外，还有两个概率，一个是上一轮的该槽位取值某个 $v$ 的概率大小，另一个是直到当前轮次 $t$ 时，用户还未提及该槽位，也可以用一个概率大小来表征，直接在第三个公式中利用分母中多余的一个参数代替普通的 $g(v)$ 计算即可（这样的话，该槽对应所有可能取值的概率之和，以及用户未提及该槽的概率，才能够使得所有概率之和为1）。</p>\n<p>特别需要注意的是，论文中采用的方法是，先针对每个具体的task构建一个本体库$G$，它是一个知识图谱，在这个知识图谱中，定义了该task下存在的各种可能槽位以及槽位对应的可能取值。而槽位分为两种类型：informable slot和requestable slot，前者是用户用来限定查询范围的一些信息（比如订餐task中的食物类型，价格区间等等），后者是用户想要咨询的信息（比如询问地址和电话等，那么地址和电话此时便是一个requestable slot）。此后针对该本体知识图谱$G$中的每一个槽位$s$，有两种处理办法：</p>\n<ul>\n<li>对于informable slot，每一个槽位$s$都有一个专门的Jordan type RNN进行跟踪。例如针对食物类型有一个专门的RNN进行跟踪，在跟踪过程中，每一个轮次$t$都会计算一次RNN在当前时刻$t$的输出，用以更新食物类型这个槽位上所有可能取值的概率分布</li>\n<li>对于requestable slot，因为不需要跟踪，并未使用RNN结构，然而原文未做详细解读，个人猜测就是每个时刻做一个简单的二分类，输出一个binary distribution，大概意思就是用户当前是否向系统请求了该槽位的信息</li>\n</ul>\n<h3 id=\"Database-Operator\"><a href=\"#Database-Operator\" class=\"headerlink\" title=\"Database Operator\"></a>Database Operator</h3><p>通过Belief Tracker后，可以针对所有informable slot的所有可能取值，通过下式形成一个查询语句 $q$（不过个人在这里有些疑问，按照下式的意思，大概是针对每一个槽位都取一个概率最大的值，并将所有informable slot全部合并形成一个 $query$，这样的话，岂不就会在每一轮的查询语句中，都包含了所有的informable slot，只不过每一轮的查询语句 $q$ 中各个槽位的具体取值不一样而已。如果是这样个人感觉不太合理，如果不是这样那是否公式的argmax应该放到取合集符号的外边来呢？），使用查询语句在数据库中进行查询后，会得到一个针对数据库中实体的一个 $01$ 向量（类似于bag-of-words中，该向量的每一位表示数据库中的某个实体，如果命中了数据库中的某个实体，则该位置1）。<br>$$\\bigcup_{s’\\in S_1 }{\\underset{v}{argmax}p_{s’}^t}$$<br>此外，如果查询结果只要有命中（即向量x不全为0），则这里还会维护一个DB pointer，它将随机指向一个命中的实体。并且根据每轮查询的结果进行动态更新。这个pointer主要是在后面生成网络中用来显示一个有意义的句子（因为生成的句子是类似于模板一样的，并没有显示具体的实体信息）。</p>\n<h3 id=\"Policy-network\"><a href=\"#Policy-network\" class=\"headerlink\" title=\"Policy network\"></a>Policy network</h3><p>在该模块中，实际上和强化学习中的policy action还有点不一样，这里的policy实际上就是一个融合另外三个模块，并输出的一个向量。公式如下：<br>$$o_t=tanh(W_{zo}z_t+W_{po}\\hat{p}<em>t+W</em>{xo}\\hat{x}_t)$$</p>\n<ul>\n<li>$z$ 便是intent network输出的向量。</li>\n<li>其中对于belief tracker模块的处理如下（也就是将各个informable slot的概率进行concatenation）$\\hat{p}<em>t=\\oplus</em>{s\\in G}\\hat{p}_s^t$，而针对每一个具体的slot，它也是一个向量，其由三部分组成：（该slot下各个可能取值的概率之和，用户表示对当前槽位无所谓的概率，用户到当前轮次并未提及该槽位的概率）。这里为什么要针对每一个slot下的各个可能取值的概率大小进行求和，就是因为在generation network中，对于槽位信息很重要，但是对于每个槽位下的可能取值则不重要（因为无论是输入的句子还是生成的句子都是delexicalised）</li>\n<li>对于database operator的输出而言，同样的，对于查询语句得到的结果能够查询到的实体个数很重要，但是具体查询到的是什么实体并不重要。因此最后 $x$ 便转化为了一个6位的one-hot向量，每一位分别表示没有查询到任意实体，查询结果命中一个实体，查询结果命中两个实体，…，查询结果命中等于或超过五个实体。</li>\n</ul>\n<h3 id=\"Generation-Network\"><a href=\"#Generation-Network\" class=\"headerlink\" title=\"Generation Network\"></a>Generation Network</h3><p>生成网络就是一个普通的decoder，只不过在输入中加入Policy Network输出的action vector，也就是向量 $o$，公式如下：<br>$$P(w_{j+1}^t|w_j^t,h_{j-1}^t,o_t)=LSTM_j(w_j^t,h_{j-1}^t,o_t)$$<br>在每一个时刻输出一个token，该token有三种可能：</p>\n<ul>\n<li>一个正常的词；</li>\n<li>一个delexicalised slot name，在最终输出的时候会将其替换为实际的一个有意义的词，比如&lt;s.food&gt;会替换为”food”或”type of food”；</li>\n<li>一个delexicalised slot value，在最终输出的时候会将其替换为在Database Operator中DB pointer维护的一个实体。</li>\n</ul>\n<h1 id=\"Wizard-of-Oz-Data-Collection\"><a href=\"#Wizard-of-Oz-Data-Collection\" class=\"headerlink\" title=\"Wizard-of-Oz Data Collection\"></a>Wizard-of-Oz Data Collection</h1><p>Wizard-of-Oz数据集搜集范式这个就不做描述和介绍了，应该已经很熟悉了。</p>\n<h1 id=\"Empirical-Experiment\"><a href=\"#Empirical-Experiment\" class=\"headerlink\" title=\"Empirical Experiment\"></a>Empirical Experiment</h1><ul>\n<li>使用交叉熵预先训练belief tracker的参数<br>$$L_1(\\Theta_b)=-{\\sum}_t{\\sum}_s(y_s^t)^Tlogp_s^t$$<br>其中，y代表真实label。对于完整的模型，我们有三个informable追踪器（食品，价格范围，面积）和七个requestable追踪器（地址，电话，邮编，姓名，以及三个slot）。</li>\n<li>在固定了tracker的参数之后，使用来自生成网络语言模型的交叉熵损失函数对模型的其余部分进行训练<br>$$L_1(\\Theta_{/b})=-{\\sum}_t{\\sum}_j(y_j^t)^Tlogp_j^t$$<br>其中$y_j^t$和$p_j^t$分别是在第 $t$ 轮decoder第 $j$ 步的时候输出的target token和预测 token。我们将每个对话视为一个批次，并使用随机梯度下降和小的l2正则化项来训练模型。</li>\n</ul>\n<p>收集的语料库按3：1：1的比例划分为训练，验证和测试集。early stopping是基于正则化的验证集来实现的，并且梯度裁剪被设置为1.所有隐藏层大小被设置为50，并且所有权重在-0.3和0.3之间被随机初始化，包括字嵌入。输入和输出的词汇大小大约为500，其中可以灵活化的去掉罕见单词和可以被delexicalisation的单词。我们对实验中的所有CNN使用了三个卷积层，并且所有滤波器大小都设置为3.池化操作仅在最后的卷积层之后应用。</p>\n<blockquote>\n<p>梯度裁剪是一种在非常深度的网络（通常是循环神经网络）中用于防止梯度爆炸（exploding gradient）的技术。执行梯度裁剪的方法有很多，但常见的一种是当参数矢量的 L2 范数（L2 norm）超过一个特定阈值时对参数矢量的梯度进行标准化，这个特定阈值根据函数：新梯度=梯度* 阈值/L2范数（梯度）<br>{new_gradients = gradients * threshold / l2_norm(gradients)}确定。</p>\n</blockquote>\n<p>在该评估中，模型使用了三个评估指标：</p>\n<ul>\n<li>BLEU评分（on top-1 and top-5 candidates）：我们使用实体值替换进行lexicalising之前，在模板输出句子上计算BLEU分数。</li>\n<li>实体匹配率：通过确定每个对话结束时实际选择的实体是否与用户指定的任务相匹配来计算实体匹配率。我们通过确定每个对话结束时实际选择的实体是否与指定给用户的任务相匹配来计算实体匹配率。 如果（1）所提供的实体匹配，并且（2）系统回答来自用户的所有相关信息请求（例如，地址是什么），则对话被标记为成功。</li>\n<li>客观任务成功率。</li>\n</ul>\n<p><strong>下表是对tracker的评估结果</strong><br><img src=\"https://img-blog.csdnimg.cn/2020100412021267.png#pic_center\" alt=\"在这里插入图片描述\"><br><strong>下表是基于语料的评估结果</strong><br><img src=\"https://img-blog.csdnimg.cn/2020100412030771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用t-SNE生成一个降维视图，该视图嵌入了前三个生成的输出词（完整模型，不注意）嵌入，绘制和标记，该图如下图所示。<br><img src=\"https://img-blog.csdnimg.cn/2020100416220381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是认为评估的结果<br><img src=\"https://img-blog.csdnimg.cn/20201004162227849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们还对NN模型和由handcrafted 语义分析器，基于规则的策略和信念跟踪器以及基于模板的生成器组成的人工模块化基准系统（HDC）进行比较。 结果如下表：<br><img src=\"https://img-blog.csdnimg.cn/20201004162258400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusions-and-Future-Work\"><a href=\"#Conclusions-and-Future-Work\" class=\"headerlink\" title=\"Conclusions and Future Work\"></a>Conclusions and Future Work</h1><p>目前的模型是一个基于文本的对话系统，它不能直接处理噪声语音识别输入，也不能在用户不确定时要求用户确认。 事实上，这种类型的模型在多大程度上可以扩展到更大更广的领域，这仍然是希望在今后的工作中追求的一个悬而未决的问题。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","seq2seq","任务对话"]}]