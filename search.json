[{"title":"Embedding和Word2vec的理解","url":"/Deep-Learning/2873c4549d8b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文并不是深挖Word2vec和Embedding原理公式推导的文章，网上已经有很多针对性的原理讲解文章，大家可以自行了解。本文主要是针对词嵌入的相关概念、原理及模式进行提炼陈列，扫盲和抛砖引玉。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Embedding就是用一个低维的向量表示一个物体，这个Embedding向量的性质是能使距离相近的向量对应的物体有相近的含义，正是因为Embedding能够用低维向量对物体进行编码还能保留其含义的特点，所以其非常适合用于深度学习。由于我们熟知的one hot encoding、multi hot encoding是一种稀疏向量的编码方式，所以不适合用来深度学习进行特征表示，相反Embedding概括而言，就是一种高效的低维稠密的操作。</p>\n<blockquote>\n<p>词嵌入（ word embedding）：基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。</p>\n</blockquote>\n<p>Embedding很早之前就有人研究了，相关资料文章特别的多，不过让Embedding在行内如此流行的功劳还要归功于google的Word2vec。这里需要先说说神经网络语言模型与Word2vec的关系，神经网络语言模型做词向量有以下几种方式：</p>\n<ul>\n<li>Neural Network Language Model ，NNLM</li>\n<li>Log-Bilinear Language Model， LBL</li>\n<li>Recurrent Neural Network based Language Model，RNNLM</li>\n<li>C&amp;W 模型</li>\n<li>CBOW（ Continuous Bagof-Words）和 Skip-gram 模型</li>\n</ul>\n<p>上面几种模型只是个在逻辑概念上的东西，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具就是Word2vec。</p>\n<blockquote>\n<p>在广告、推荐、搜索等领域，由于用户数据的稀疏性，几乎必然要求在构建DNN之前对user和item进行embedding后才能进行有效的训练，所以诞生一种叫做“item2vec”的方法，感兴趣的可以自行了解，这里不展开讲。</p>\n</blockquote>\n<h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><ul>\n<li>Matrix Factorization：矩阵分解<ul>\n<li>简单来讲，MF就是将一个矩阵分解成多个矩阵的乘积，典型的算法有SVD、LSA，是一种常见的技术，在推荐、NLP都有应用。后面你就会知道，在Word2vec中，最终每一个word都会有两个向量：V_word和V_context，假设存在一个矩阵W = V_word * V_context，word2vec也可以理解成是对矩阵W的矩阵分解。</li>\n</ul>\n</li>\n<li>Pointwise Mutual Information（PMI）：点互信息<ul>\n<li>指标来衡量两个事物之间的相关性，如下公式中，w和c同时出现的次数/(w出现的次数 + c出现的次数)，和MF对比，就多了个常量logk。</li>\n</ul>\n</li>\n<li>Cosine similarity：内积空间的两个非零向量之间相似度的量度（这个自己实现也比较好实现，Word2vec用这个计算相似度）<br><img src=\"https://img-blog.csdnimg.cn/20201027230949373.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>这里多说一下余弦相似度计算，下面举个例子：<br><img src=\"https://img-blog.csdnimg.cn/20201027232354991.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"keras中的Embedding和Word2vec的区别\"><a href=\"#keras中的Embedding和Word2vec的区别\" class=\"headerlink\" title=\"keras中的Embedding和Word2vec的区别\"></a>keras中的Embedding和Word2vec的区别</h1><p>其实二者的目标是一样的，都是我们为了学到词的稠密的嵌入表示。只不过学习的方式不一样。Word2vec是无监督的学习方式，利用上下文环境来学习词的嵌入表示，因此可以学到相关词，但是只能捕捉到局部分布信息。而在keras的Embedding层中，权重的更新是基于标签的信息进行学习，为了达到较高的监督学习的效果，会将Embedding作为网络的一层，根据target进行学习和调整。比如LSTM中对词向量的微调。简单来说，Word2vec一般单独提前训练好，而Embedding一般作为模型中的层随着模型一同训练。</p>\n<h1 id=\"CBOW和Skip-gram\"><a href=\"#CBOW和Skip-gram\" class=\"headerlink\" title=\"CBOW和Skip-gram\"></a>CBOW和Skip-gram</h1><p>CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。这样讲可能比较抽象，这里借用jalammar的文章里的内容和图进行说明。</p>\n<h3 id=\"CBOW\"><a href=\"#CBOW\" class=\"headerlink\" title=\"CBOW\"></a>CBOW</h3><p>我们通过找常出现在每个单词附近的词，就能获得它们的映射关系。机制如下：</p>\n<ul>\n<li>先是获取大量文本数据</li>\n<li>后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)</li>\n<li>利用这样的滑动窗就能为训练模型生成大量样本数据。</li>\n</ul>\n<p>当这个窗口沿着文本滑动时，我们就能(真实地)生成一套用于模型训练的数据集。为了明确理解这个过程，我们看下滑动窗是如何处理这个短语的，我们把前两个单词单做特征，第三个单词单做标签:<br><img src=\"https://img-blog.csdnimg.cn/20201027233856280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如上，以此产生一个样本，然后以此慢慢的滑动下去，就会产生一个很大的数据集。接下来，我们不仅要考虑目标单词的前两个单词，还要考虑其后两个单词，像下面这样。<br><img src=\"https://img-blog.csdnimg.cn/20201027234252979.png#pic_center\" alt=\"在这里插入图片描述\"><br>通过这种方式，我们实际上构建并训练的模型就如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201027234317749.png#pic_center\" alt=\"在这里插入图片描述\"><br>上述的这种架构就被我们称为连续词袋(CBOW)。</p>\n<h3 id=\"Skip-gram\"><a href=\"#Skip-gram\" class=\"headerlink\" title=\"Skip-gram\"></a>Skip-gram</h3><p>Skip-gram不同的是，它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示（绿框中的词语是输入词，粉框则是可能的输出结果）：<br><img src=\"https://img-blog.csdnimg.cn/20201027234420824.png#pic_center\" alt=\"在这里插入图片描述\"><br>这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本<br><img src=\"https://img-blog.csdnimg.cn/20201027234453936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在移动几组位置之后，我们就能得到一批样本:<br><img src=\"https://img-blog.csdnimg.cn/20201027234533739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这种方式就被我们称为Skipgram架构。我们可以像下图这样将展示滑动窗的内容。</p>\n<h1 id=\"Skipgram-with-Negative-Sampling-SGNS\"><a href=\"#Skipgram-with-Negative-Sampling-SGNS\" class=\"headerlink\" title=\"Skipgram with Negative Sampling (SGNS)\"></a>Skipgram with Negative Sampling (SGNS)</h1><p>现在我们已经从现有的文本中获得了Skipgram模型的训练数据集，接下来让我们看看如何使用它来训练一个能预测相邻词汇的自然语言模型。从数据集中的第一个样本开始。我们将特征输入到未经训练的模型，让它预测一个可能的相邻单词。<br><img src=\"https://img-blog.csdnimg.cn/20201027234849691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>该模型会执行三个步骤并输入预测向量(对应于单词表中每个单词的概率)。因为模型未经训练，该阶段的预测肯定是错误的。但是没关系，我们知道应该猜出的是哪个单词——这个词就是我训练集数据中的输出标签:<br><img src=\"https://img-blog.csdnimg.cn/20201027234902801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。模型的偏差有多少？将两个向量相减，就能得到偏差向量:<br><img src=\"https://img-blog.csdnimg.cn/20201027234957849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。<br><img src=\"https://img-blog.csdnimg.cn/20201027235012137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这其实就是训练的第一步了。我们接下来继续对数据集内下一份样本进行同样的操作，直到我们遍历所有的样本。这就是一轮（epoch）了。我们再多做几轮（epoch），得到训练过的模型，于是就可以从中提取嵌入矩阵来用于其他应用了。但是真正的Word2vec在上面的基本流程上，还有一些关键思想：</p>\n<ul>\n<li>负样本采样：我们需要在数据集中引入负样本 - 不是邻居的单词样本。我们的模型需要为这些样本返回0，这些单词是从词汇表中随机抽取的单词，这个想法的灵感来自噪声对比估计。</li>\n</ul>\n<h1 id=\"Word2vec训练流程\"><a href=\"#Word2vec训练流程\" class=\"headerlink\" title=\"Word2vec训练流程\"></a>Word2vec训练流程</h1><p>在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。</p>\n<p>在训练阶段的开始，我们创建两个矩阵——Embedding矩阵和Context矩阵。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size——300是一个常见值，但我们在前文也看过50的例子）。<br><img src=\"https://img-blog.csdnimg.cn/2020102723540043.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在训练过程开始时，我们用随机值初始化这些矩阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：<br><img src=\"https://img-blog.csdnimg.cn/20201027235443866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们有四个单词：输入单词not和输出/上下文单词: thou（实际邻居词），aaron和taco（负面例子）。我们继续查找它们的嵌入——对于输入词，我们查看Embedding矩阵。对于上下文单词，我们查看Context矩阵（即使两个矩阵都在我们的词汇表中嵌入了每个单词）。<br><img src=\"https://img-blog.csdnimg.cn/20201027235452260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后，我们计算输入嵌入与每个上下文嵌入的点积。在每种情况下，结果都将是表示输入和上下文嵌入的相似性的数字。<br><img src=\"https://img-blog.csdnimg.cn/20201027235501133.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们需要一种方法将这些分数转化为看起来像概率的东西——我们需要它们都是正值，并且 处于0到1之间。sigmoid这一逻辑函数转换正适合用来做这样的事情啦。<br><img src=\"https://img-blog.csdnimg.cn/20201027235511889.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。</p>\n<p>既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们计算模型预测中的误差吧。为此我们只需从目标标签中减去sigmoid分数。<br><img src=\"https://img-blog.csdnimg.cn/20201027235528795.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在，我们可以利用这个错误分数来调整not、thou、aaron和taco的嵌入，使我们下一次做出这一计算时，结果会更接近目标分数。训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入。</p>\n<h1 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h1><p>word2vec训练过程中的两个关键超参数是窗口大小和负样本的数量。<br><img src=\"https://img-blog.csdnimg.cn/20201027235711617.png#pic_center\" alt=\"在这里插入图片描述\"><br>不同的任务适合不同的窗口大小。一种启发式方法是，使用较小的窗口大小（2-15）会得到这样的嵌入：两个嵌入之间的高相似性得分表明这些单词是可互换的（注意，如果我们只查看附近距离很近的单词，反义词通常可以互换——例如，好的和坏的经常出现在类似的语境中）。使用较大的窗口大小（15-50，甚至更多）会得到相似性更能指示单词相关性的嵌入。在实际操作中，你通常需要对嵌入过程提供指导以帮助读者得到相似的”语感“。Gensim默认窗口大小为5（除了输入字本身以外还包括输入字之前与之后的两个字）。<br><img src=\"https://img-blog.csdnimg.cn/20201027235722687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本的数量是训练训练过程的另一个因素。原始论文认为5-20个负样本是比较理想的数量。它还指出，当你拥有足够大的数据集时，2-5个似乎就已经足够了。Gensim默认为5个负样本。</p>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"http://jalammar.github.io/illustrated-word2vec/\">The Illustrated Word2vec</a></li>\n<li><a href=\"https://arxiv.org/abs/1411.2738\">word2vec Parameter Learning Explained</a></li>\n<li><a href=\"https://arxiv.org/pdf/1301.3781.pdf\">Efficient Estimation of Word Representations in Vector Space</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">Cosine similarity</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/26306795\">[NLP] 秒懂词向量Word2vec的本质</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["Embedding","Word2vec","词袋","深度学习"]},{"title":"HikariCP连接池各属性详解","url":"/Java/f8798d92e1fe/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>HikariCP是一个非常优秀的数据库连接池组件，这里我之所以要专门弄一篇文章来讲它的相关属性，是因为我最近使用SpringBoot架构了一个系统，需要比较细致的针对数据库连接池进行配置，所以查阅了相关在资料。GitHub上找到了一个属性介绍的<a href=\"https://github.com/brettwooldridge/HikariCP\">项目</a>，有兴趣的可以直接去看看，这里我就是基于这个项目进行翻译，在需要的地方加上我的一些理解。<strong>如果觉得有所帮助，记得点个赞以及点个关注哦。</strong></p>\n<h2 id=\"什么是HikariCP\"><a href=\"#什么是HikariCP\" class=\"headerlink\" title=\"什么是HikariCP\"></a>什么是HikariCP</h2><p>HikariCP是由日本程序员开源的一个数据库连接池组件，代码非常轻量，并且速度非常的快。根据官方提供的数据，在i7，开启32个线程32个连接的情况下，进行随机数据库读写操作，HikariCP的速度是现在常用的C3P0数据库连接池的数百倍。在SpringBoot2.0中，官方默认也是使用的HikariCP作为数据库连接池，可见HikariCP连接池的目的就是为了极致的数据库连接性能体验。HikariCP优化机制相对于其他的数据库连接池，有着以下的优势。</p>\n<ul>\n<li>字节码精简，HikariCP优化了代码，尽量减少了生成的字节码，使得cpu可以加载更多程序代码。</li>\n<li>优化了拦截和代理机制，HikariCP对拦截器机制和代理机制进行了代码优化处理，例如Statement proxy只有100行代码，大大减少了代码量，只有其他连接池例如BoneCP的十分之一。</li>\n<li>自定义数组，HikariCP针对数组操作进行了自定义数组–FastStatementList，用来替代jdk的ArrayList，优化了get、remove等方法，避免了每次调用get的时候进行范围检查，也避免了每次remove操作的时候会将数据从头到尾进行扫描的性能问题。</li>\n<li>自定义集合，同样的，针对jdk自带的集合类，HikariCP专门封装了无锁的集合类型 ，用来提高在使用中的读写并发的效率，减少并发造成的资源竞争问题。</li>\n<li>CPU时间片算法优化，HikariCP也对cpu时间片分配算法进行了优化，尽可能使得一个时间片内完成相关的操作。</li>\n</ul>\n<h2 id=\"主要的属性详解\"><a href=\"#主要的属性详解\" class=\"headerlink\" title=\"主要的属性详解\"></a>主要的属性详解</h2><ul>\n<li>dataSourceClassName</li>\n</ul>\n<p>这是DataSourceJDBC驱动程序提供的类的名称。请查阅您的特定JDBC驱动程序的文档以获取此类名称，或参阅下表。注XA数据源不受支持。XA需要像bitronix这样的真正的事务管理器 。请注意，如果您正在使用jdbcUrl“旧式”基于DriverManager的JDBC驱动程序配置，则不需要此属性 。 默认值：无</p>\n<ul>\n<li>jdbcUrl</li>\n</ul>\n<p>该属性指示HikariCP使用“基于DriverManager的”配置。我们认为基于DataSource的配置（上图）由于各种原因（参见下文）是优越的，但对于许多部署来说，几乎没有显着差异。 在“旧”驱动程序中使用此属性时，您可能还需要设置该driverClassName属性，但不要先尝试。 请注意，如果使用此属性，您仍然可以使用DataSource属性来配置您的驱动程序，实际上建议您使用URL本身中指定的驱动程序参数。 默认值：无</p>\n<ul>\n<li>username</li>\n</ul>\n<p>此属性设置从基础驱动程序获取连接时使用的默认身份验证用户名。请注意，对于DataSources，这通过调用DataSource.getConnection(<em>username</em>, password)基础DataSource 以非常确定的方式工作。但是，对于基于驱动程序的配置，每个驱动程序都不同。在基于驱动程序的情况下，HikariCP将使用此username属性来设置传递给驱动程序调用的user属性。如果这不是你所需要的，例如完全跳过这个方法并且调用。 默认值：无PropertiesDriverManager.getConnection(jdbcUrl, props)addDataSourceProperty(“username”, …)</p>\n<ul>\n<li>password</li>\n</ul>\n<p>此属性设置从基础驱动程序获取连接时使用的默认身份验证密码。请注意，对于DataSources，这通过调用DataSource.getConnection(username, <em>password</em>)基础DataSource 以非常确定的方式工作。但是，对于基于驱动程序的配置，每个驱动程序都不同。在基于驱动程序的情况下，HikariCP将使用此password属性来设置传递给驱动程序调用的password属性。如果这不是你所需要的，例如完全跳过这个方法并且调用。 默认值：无PropertiesDriverManager.getConnection(jdbcUrl, props)addDataSourceProperty(“pass”, …)</p>\n<ul>\n<li>autoCommit</li>\n</ul>\n<p>此属性控制从池返回的连接的默认自动提交行为。它是一个布尔值。 默认值：true</p>\n<ul>\n<li>connectionTimeout</li>\n</ul>\n<p>此属性控制客户端（即您）将等待来自池的连接的最大毫秒数。如果在没有可用连接的情况下超过此时间，则会抛出SQLException。最低可接受的连接超时时间为250 ms。 默认值：30000（30秒）</p>\n<ul>\n<li>idleTimeout</li>\n</ul>\n<p>此属性控制允许连接在池中闲置的最长时间。 此设置仅适用于minimumIdle定义为小于maximumPoolSize。一旦池达到连接，空闲连接将不会退出minimumIdle。连接是否因闲置而退出，最大变化量为+30秒，平均变化量为+15秒。在超时之前，连接永远不会退出。值为0意味着空闲连接永远不会从池中删除。允许的最小值是10000ms（10秒）。 默认值：600000（10分钟）</p>\n<ul>\n<li>maxLifetime</li>\n</ul>\n<p>此属性控制池中连接的最大生存期。正在使用的连接永远不会退休，只有在关闭后才会被删除。在逐个连接的基础上，应用较小的负面衰减来避免池中的大量消失。 我们强烈建议设置此值，并且应该比任何数据库或基础设施规定的连接时间限制短几秒。 值为0表示没有最大寿命（无限寿命），当然是idleTimeout设定的主题。 默认值：1800000（30分钟）</p>\n<ul>\n<li>connectionTestQuery</li>\n</ul>\n<p>如果您的驱动程序支持JDBC4，我们强烈建议您不要设置此属性。这是针对不支持JDBC4的“传统”驱动程序Connection.isValid() API。这是在连接从池中获得连接以确认与数据库的连接仍然存在之前将要执行的查询。再一次，尝试运行没有此属性的池，如果您的驱动程序不符合JDBC4的要求，HikariCP将记录一个错误以告知您。 默认值：无</p>\n<ul>\n<li>minimumIdle</li>\n</ul>\n<p>该属性控制HikariCP尝试在池中维护的最小空闲连接数。如果空闲连接低于此值并且连接池中的总连接数少于此值maximumPoolSize，则HikariCP将尽最大努力快速高效地添加其他连接。但是，为了获得最佳性能和响应尖峰需求，我们建议不要设置此值，而是允许HikariCP充当固定大小的连接池。 默认值：与maximumPoolSize相同</p>\n<ul>\n<li>maximumPoolSize</li>\n</ul>\n<p>此属性控制池允许达到的最大大小，包括空闲和正在使用的连接。基本上这个值将决定到数据库后端的最大实际连接数。对此的合理价值最好由您的执行环境决定。当池达到此大小并且没有空闲连接可用时，对getConnection（）的调用将connectionTimeout在超时前阻塞达几毫秒。请阅读关于游泳池尺寸。 默认值：10</p>\n<ul>\n<li>metricRegistry</li>\n</ul>\n<p>该属性仅通过编程配置或IoC容器可用。该属性允许您指定池使用的Codahale / Dropwizard 实例MetricRegistry来记录各种指标。有关 详细信息，请参阅Metrics维基页面。 默认值：无</p>\n<ul>\n<li>healthCheckRegistry</li>\n</ul>\n<p>该属性仅通过编程配置或IoC容器可用。该属性允许您指定池使用的Codahale / Dropwizard 的实例HealthCheckRegistry来报告当前的健康信息。有关 详细信息，请参阅健康检查 wiki页面。 默认值：无</p>\n<ul>\n<li>poolName</li>\n</ul>\n<p>此属性表示连接池的用户定义名称，主要出现在日志记录和JMX管理控制台中以识别池和池配置。 默认：自动生成</p>\n<ul>\n<li>initializationFailTimeout</li>\n</ul>\n<p>如果池无法成功初始化连接，则此属性控制池是否将“快速失败”。任何正数都取为尝试获取初始连接的毫秒数; 应用程序线程将在此期间被阻止。如果在超时发生之前无法获取连接，则会引发异常。此超时被应用后的connectionTimeout 期。如果值为零（0），HikariCP将尝试获取并验证连接。如果获得连接但未通过验证，将抛出异常并且池未启动。但是，如果无法获得连接，则会启动该池，但后续获取连接的操作可能会失败。小于零的值将绕过任何初始连接尝试，并且在尝试获取后台连接时，池将立即启动。因此，以后努力获得连接可能会失败。 默认值：1</p>\n<ul>\n<li>isolateInternalQueries</li>\n</ul>\n<p>此属性确定HikariCP是否在其自己的事务中隔离内部池查询，例如连接活动测试。由于这些通常是只读查询，因此很少有必要将它们封装在自己的事务中。该属性仅适用于autoCommit禁用的情况。 默认值：false</p>\n<ul>\n<li>allowPoolSuspension</li>\n</ul>\n<p>该属性控制池是否可以通过JMX暂停和恢复。这对于某些故障转移自动化方案很有用。当池被暂停时，呼叫 getConnection()将不会超时，并将一直保持到池恢复为止。 默认值：false</p>\n<ul>\n<li>readOnly</li>\n</ul>\n<p>此属性控制默认情况下从池中获取的连接是否处于只读模式。注意某些数据库不支持只读模式的概念，而其他数据库则在Connection设置为只读时提供查询优化。无论您是否需要此属性，都将主要取决于您的应用程序和数据库。 默认值：false</p>\n<ul>\n<li>registerMbeans</li>\n</ul>\n<p>该属性控制是否注册JMX管理Bean（“MBeans”）。 默认值：false</p>\n<ul>\n<li>catalog</li>\n</ul>\n<p>该属性设置默认目录为支持目录的概念数据库。如果未指定此属性，则使用由JDBC驱动程序定义的默认目录。 默认：驱动程序默认</p>\n<ul>\n<li>connectionInitSql</li>\n</ul>\n<p>该属性设置一个SQL语句，在将每个新连接创建后，将其添加到池中之前执行该语句。如果这个SQL无效或引发异常，它将被视为连接失败并且将遵循标准重试逻辑。 默认值：无</p>\n<ul>\n<li>driverClassName</li>\n</ul>\n<p>HikariCP将尝试通过DriverManager仅基于驱动程序来解析驱动程序jdbcUrl，但对于一些较旧的驱动程序，driverClassName还必须指定它。除非您收到明显的错误消息，指出找不到驱动程序，否则请忽略此属性。 默认值：无</p>\n<ul>\n<li>transactionIsolation</li>\n</ul>\n<p>此属性控制从池返回的连接的默认事务隔离级别。如果未指定此属性，则使用由JDBC驱动程序定义的默认事务隔离级别。如果您有针对所有查询通用的特定隔离要求，请仅使用此属性。此属性的值是从不断的名称Connection 类，如TRANSACTION_READ_COMMITTED，TRANSACTION_REPEATABLE_READ等 默认值：驱动程序默认</p>\n<ul>\n<li>validationTimeout</li>\n</ul>\n<p>此属性控制连接测试活动的最长时间。这个值必须小于connectionTimeout。最低可接受的验证超时时间为250 ms。 默认值：5000</p>\n<ul>\n<li> leakDetectionThreshold</li>\n</ul>\n<p>此属性控制在记录消息之前连接可能离开池的时间量，表明可能存在连接泄漏。值为0意味着泄漏检测被禁用。启用泄漏检测的最低可接受值为2000（2秒）。 默认值：0</p>\n<ul>\n<li> dataSource</li>\n</ul>\n<p>此属性仅通过编程配置或IoC容器可用。这个属性允许你直接设置DataSource池的实例，而不是让HikariCP通过反射来构造它。这在一些依赖注入框架中可能很有用。当指定此属性时，dataSourceClassName属性和所有DataSource特定的属性将被忽略。 默认值：无</p>\n<ul>\n<li>schema</li>\n</ul>\n<p>该属性设置的默认模式为支持模式的概念数据库。如果未指定此属性，则使用由JDBC驱动程序定义的默认模式。 默认：驱动程序默认</p>\n<ul>\n<li>threadFactory</li>\n</ul>\n<p>此属性仅通过编程配置或IoC容器可用。该属性允许您设置java.util.concurrent.ThreadFactory将用于创建池使用的所有线程的实例。在一些只能通过ThreadFactory应用程序容器提供的线程创建线程的有限执行环境中需要它。 默认值：无</p>\n<ul>\n<li>scheduledExecutor</li>\n</ul>\n<p>此属性仅通过编程配置或IoC容器可用。该属性允许您设置java.util.concurrent.ScheduledExecutorService将用于各种内部计划任务的实例。如果为ScheduledThreadPoolExecutor 实例提供HikariCP，建议setRemoveOnCancelPolicy(true)使用它。 默认值：无</p>\n","categories":["Java"],"tags":["HikariCP","连接池","Java"]},{"title":"SpringBoot整合RabbitMQ","url":"/Spring-Boot/9cf696e86b5a/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果对你有帮助，请给点个关注和点个赞哦，嘿嘿嘿</strong><br>MQ全称为Message Queue,消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写出入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过 队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求。而RabbitMQ是实现了高级消息队列协议的开源消息代理软件。RabbitMQ服务器是用Erlang语言编写的，而聚类和故障转移是构建在开放电信平台框架上的。</p>\n<h2 id=\"部署RabbitMQ\"><a href=\"#部署RabbitMQ\" class=\"headerlink\" title=\"部署RabbitMQ\"></a>部署RabbitMQ</h2><p>首先在用SpringBoot集成RabbitMQ之前呢，我们当然需要先部署一个RabbitMQ啦，你可以选择自己电脑安装，但是我还是比较推荐使用Docker进行部署的，比较方便，Docker部署RabbitMQ比较方便，这里我教一下如何使用Docker部署RabbitMQ，我使用的操作系统是Ubuntu19，首先把RabbitMQ的镜像pull下来，注意pull镜像的时候要获取management版本的，不要获取last版本的，management版本的才带有管理界面，比如</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker pull rabbitmq:<span class=\"number\">3.8</span><span class=\"number\">.2</span>-management</span><br></pre></td></tr></table></figure>\n<p>等待pull下来，pull下来之后，我们就可以直接运行了（这里只是简单的安装了一个单点Rabbitmq，如果要部署集群，当然没有这么简单啦，可以看我另一篇博文），运行的命令如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker run -d  -p <span class=\"number\">15672</span>:<span class=\"number\">15672</span>  -p  <span class=\"number\">5672</span>:<span class=\"number\">5672</span>  -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:<span class=\"number\">3.8</span><span class=\"number\">.2</span>-management</span><br></pre></td></tr></table></figure>\n<p>该镜像拥有一个基于 web 的控制台和 Http API。Http API 可以在地址看到如何使用：<code>http://localhost:15672/api/</code>，这里说明一下指令选项</p>\n<ul>\n<li>15672 ：表示 RabbitMQ 控制台端口号，可以在浏览器中通过控制台来执行 RabbitMQ 的相关操作。</li>\n<li>5672 : 表示 RabbitMQ 所监听的 TCP 端口号，应用程序可通过该端口与 RabbitMQ 建立 TCP 连接，并完成后续的异步消息通信</li>\n<li>RABBITMQDEFAULTUSER：用于设置登陆控制台的用户名，这里我设置 admin</li>\n<li>RABBITMQDEFAULTPASS：用于设置登陆控制台的密码，这里我设置 admin</li>\n</ul>\n<p>容器启动成功后，可以在浏览器输入地址：<a href=\"http://ip地址:15672/\">http://ip地址:15672/</a> 访问控制台<br><img src=\"https://img-blog.csdnimg.cn/20200307123034693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200307123108647.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里简单描述下上图中控制台的列表的作用：</p>\n<ul>\n<li>Overview ：用于查看 RabbitMQ 的一些基本信息（消息队列、消息发送速率、节点、端口和上下文信息等）</li>\n<li>Connections：用于查看 RabbitMQ 客户端的连接信息</li>\n<li>Channels：用户查看 RabbitMQ 的通道信息</li>\n<li>Exchange：用于查看 RabbitMQ 交换机</li>\n<li>Queues：用于查看 RabbitMQ 的队列</li>\n<li>Admin：用于管理用户，可增加用户</li>\n</ul>\n<h2 id=\"SpringBoot整合RabbitMQ\"><a href=\"#SpringBoot整合RabbitMQ\" class=\"headerlink\" title=\"SpringBoot整合RabbitMQ\"></a>SpringBoot整合RabbitMQ</h2><p>首先使用Idea创建一个只包含web模块的项目，然后引入rabbitmq的依赖，依赖如下：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>然后我们在application.properties主配置文件中进行如下配置</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">spring.rabbitmq.host=xxxxxxip地址</span><br><span class=\"line\">spring.rabbitmq.port=<span class=\"number\">5672</span></span><br><span class=\"line\">spring.rabbitmq.username=admin</span><br><span class=\"line\">spring.rabbitmq.password=admin</span><br></pre></td></tr></table></figure>\n<p>然后接下来我们写一个针对RabbitMQ的配置文件（用来管理消息队列），以及一个Sender和一个Receiver，具体内容如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.amqp.core.AmqpTemplate;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.stereotype.Component;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloSender</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> AmqpTemplate rabbitTemplate;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">send</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        String context = <span class=\"string\">&quot;hello &quot;</span> + <span class=\"keyword\">new</span> Date();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Sender : &quot;</span> + context);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.rabbitTemplate.convertAndSend(<span class=\"string\">&quot;hello&quot;</span>, context);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.amqp.rabbit.annotation.RabbitHandler;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.amqp.rabbit.annotation.RabbitListener;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.stereotype.Component;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"meta\">@RabbitListener(queues = &quot;hello&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloReceiver</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@RabbitHandler</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(String hello)</span></span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Receiver : &quot;</span> + hello);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.amqp.core.Queue;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RabbitConfig</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Queue <span class=\"title\">helloQueue</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Queue(<span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试及结果\"><a href=\"#测试及结果\" class=\"headerlink\" title=\"测试及结果\"></a>测试及结果</h2><p>这里测试发送的消息直接是 String 类型的，你也可以测试下 Bean 类，这需要注意需要序列化。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Autowired</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> HelloSender helloSender;</span><br><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">hello</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\thelloSender.send();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20200307163749828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200307163051949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200307163104789.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","RabbitMQ"]},{"title":"SpringBoot整合Swagger，不用手撕接口文档啦","url":"/Spring-Boot/e0188dfb231f/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。Swagger 让部署管理和使用功能强大的API从未如此简单，非常好用哦。不过这里要提醒一下的是，项目发布生产环境的时候，记得关闭swagger，以防泄漏项目接口文档，被攻击。<strong>如果觉得写得挺好，请给点个关注和点个赞哦，嘿嘿嘿</strong></p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>首先第一步要做的当然是创建一个只有web场景的新项目，然后引入swagger的依赖，这里的<code>swagger-ui</code>提供了一个非常好看的可视化UI，帮助我们整理Controller的API，从而生成API文档。</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;io.springfox&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.9.2&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;io.springfox&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.9.2&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h2><p>我们导入了swagger的依赖之后，当然是需要对swagger进行相关的配置，我们在Config包下创建一个<code>SwaggerConfig</code>配置类，用它来对swagger进行相关的配置，配置类的具体内容如下。当然，你还可对对固定接口进行过滤，不过我这里就不演示那么复杂了，就举个简单的例子。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.builders.ApiInfoBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.builders.PathSelectors;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.builders.RequestHandlerSelectors;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.service.ApiInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.service.Contact;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.spi.DocumentationType;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.spring.web.plugins.Docket;</span><br><span class=\"line\"><span class=\"keyword\">import</span> springfox.documentation.swagger2.annotations.EnableSwagger2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableSwagger2</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SwaggerConfig</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Docket <span class=\"title\">api</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Docket(DocumentationType.SWAGGER_2)</span><br><span class=\"line\">                .apiInfo(apiInfo())</span><br><span class=\"line\">                .select()</span><br><span class=\"line\">                .apis(RequestHandlerSelectors.basePackage(<span class=\"string\">&quot;com.dbc.ubiquity.Controller&quot;</span>))</span><br><span class=\"line\">                .paths(PathSelectors.any())</span><br><span class=\"line\">                .build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> ApiInfo <span class=\"title\">apiInfo</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ApiInfoBuilder()</span><br><span class=\"line\">                .title(<span class=\"string\">&quot;Ubiquity系统&quot;</span>)</span><br><span class=\"line\">                .description(<span class=\"string\">&quot;Ubiquity系统 API 1.0.0&quot;</span>)</span><br><span class=\"line\">                .termsOfServiceUrl(<span class=\"string\">&quot;填一个你的借口文档的地址，或者其他地址都可以&quot;</span>)</span><br><span class=\"line\">                .version(<span class=\"string\">&quot;1.0&quot;</span>)</span><br><span class=\"line\">                .contact(<span class=\"keyword\">new</span> Contact(<span class=\"string\">&quot;DengBoCong&quot;</span>, <span class=\"string\">&quot;个人网站地址&quot;</span>, <span class=\"string\">&quot;你的邮箱&quot;</span>))</span><br><span class=\"line\">                .build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>经过这2步配置后，我们启动服务后，访问：<a href=\"http://localhost:8080/swagger-ui.html%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E9%9B%86%E6%88%90%E3%80%82%E5%9C%A8%E4%B8%8A%E9%9D%A2%E7%9A%84%60basePackage(&quot;com.dbc.ubiquity.Controller&quot;)%60%EF%BC%8C%E6%88%91%E4%BB%AC%E9%85%8D%E7%BD%AE%E4%BA%86Swagger%E4%BC%9A%E9%BB%98%E8%AE%A4%E6%8A%8A%E6%89%80%E6%9C%89Controller%E4%B8%AD%E7%9A%84RequestMapping%E6%96%B9%E6%B3%95%E9%83%BD%E7%94%9F%E6%88%90API%E5%87%BA%E6%9D%A5%E3%80%82\">http://localhost:8080/swagger-ui.html就完成了集成。在上面的`basePackage(&quot;com.dbc.ubiquity.Controller&quot;)`，我们配置了Swagger会默认把所有Controller中的RequestMapping方法都生成API出来。</a></p>\n<h2 id=\"常用注解\"><a href=\"#常用注解\" class=\"headerlink\" title=\"常用注解\"></a>常用注解</h2><ul>\n<li><p>@Api：修饰整个类，描述Controller的作用</p>\n</li>\n<li><p>@ApiOperation：描述一个类的一个方法，或者说一个接口</p>\n</li>\n<li><p>@ApiParam：单个参数描述</p>\n</li>\n<li><p>@ApiModel：用对象来接收参数</p>\n</li>\n<li><p>@ApiProperty：用对象接收参数时，描述对象的一个字段</p>\n</li>\n<li><p>@ApiResponse：HTTP响应其中1个描述</p>\n</li>\n<li><p>@ApiResponses：HTTP响应整体描述</p>\n</li>\n<li><p>@ApiIgnore：使用该注解忽略这个API </p>\n</li>\n<li><p>@ApiClass</p>\n</li>\n<li><p>@ApiError</p>\n</li>\n<li><p>@ApiErrors</p>\n</li>\n<li><p>@ApiParamImplicit</p>\n</li>\n<li><p>@ApiParamsImplicit</p>\n</li>\n</ul>\n<h2 id=\"写Controller测试\"><a href=\"#写Controller测试\" class=\"headerlink\" title=\"写Controller测试\"></a>写Controller测试</h2><p>我们知道上面的一些注解的含义之后呢，我们现在可以来写Controller来测试一下，不过这里要说明一下的是，我们没有集成数据库，所以我这里使用模拟数据，Spring已经内含了模拟数据类，即<code>MockMvc</code>，首先我们先写两个实体类（这个实体类当然不能使用<code>@Entity</code> 来注释啦），具体内容如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> io.swagger.annotations.Api;</span><br><span class=\"line\"><span class=\"keyword\">import</span> io.swagger.annotations.ApiModelProperty;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Message</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long id;</span><br><span class=\"line\">    <span class=\"meta\">@ApiModelProperty(value = &quot;消息体&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String text;</span><br><span class=\"line\">    <span class=\"meta\">@ApiModelProperty(value = &quot;消息总结&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String summary;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Date createDate;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Long <span class=\"title\">getId</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setId</span><span class=\"params\">(Long id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.id = id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getText</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> text;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setText</span><span class=\"params\">(String text)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.text = text;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getSummary</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> summary;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setSummary</span><span class=\"params\">(String summary)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.summary = summary;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Date <span class=\"title\">getCreateDate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> createDate;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setCreateDate</span><span class=\"params\">(Date createDate)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.createDate = createDate;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Message&#123;&quot;</span> +</span><br><span class=\"line\">                <span class=\"string\">&quot;id=&quot;</span> + id +</span><br><span class=\"line\">                <span class=\"string\">&quot;, text=&#x27;&quot;</span> + text + <span class=\"string\">&#x27;\\&#x27;&#x27;</span> +</span><br><span class=\"line\">                <span class=\"string\">&quot;, summary=&#x27;&quot;</span> + summary + <span class=\"string\">&#x27;\\&#x27;&#x27;</span> +</span><br><span class=\"line\">                <span class=\"string\">&quot;, createDate=&quot;</span> + createDate +</span><br><span class=\"line\">                <span class=\"string\">&#x27;&#125;&#x27;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long id;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Long <span class=\"title\">getId</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setId</span><span class=\"params\">(Long id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.id = id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setName</span><span class=\"params\">(String name)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.name = name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getAge</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> age;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setAge</span><span class=\"params\">(<span class=\"keyword\">int</span> age)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.age = age;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后是Repository类，具体内容如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">MessageRepository</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\">List&lt;Message&gt; <span class=\"title\">findAll</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Message <span class=\"title\">save</span><span class=\"params\">(Message message)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Message <span class=\"title\">update</span><span class=\"params\">(Message message)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Message <span class=\"title\">updateText</span><span class=\"params\">(Message message)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Message <span class=\"title\">findMessage</span><span class=\"params\">(Long id)</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">deleteMessage</span><span class=\"params\">(Long id)</span></span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Service(&quot;messageRepository&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">InMemoryMessageRepository</span> <span class=\"keyword\">implements</span> <span class=\"title\">MessageRepository</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> AtomicLong counter = <span class=\"keyword\">new</span> AtomicLong();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> ConcurrentMap&lt;Long, Message&gt; messages = <span class=\"keyword\">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> List&lt;Message&gt; <span class=\"title\">findAll</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        List&lt;Message&gt; messages = <span class=\"keyword\">new</span> ArrayList&lt;&gt;(<span class=\"keyword\">this</span>.messages.values());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> messages;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Message <span class=\"title\">save</span><span class=\"params\">(Message message)</span> </span>&#123;</span><br><span class=\"line\">        Long id = message.getId();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (id == <span class=\"keyword\">null</span>)&#123;</span><br><span class=\"line\">            id = counter.incrementAndGet();</span><br><span class=\"line\">            message.setId(id);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.messages.put(id, message);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Message <span class=\"title\">update</span><span class=\"params\">(Message message)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.messages.put(message.getId(), message);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Message <span class=\"title\">updateText</span><span class=\"params\">(Message message)</span> </span>&#123;</span><br><span class=\"line\">        Message msg = <span class=\"keyword\">this</span>.messages.get(message.getId());</span><br><span class=\"line\">        msg.setText(message.getText());</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.messages.put(msg.getId(), msg);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> msg;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Message <span class=\"title\">findMessage</span><span class=\"params\">(Long id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.messages.get(id);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">deleteMessage</span><span class=\"params\">(Long id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.messages.remove(id);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后我们编写一个对各个实体类进行相应格式的基础类，具体内容如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> io.swagger.annotations.ApiModel;</span><br><span class=\"line\"><span class=\"keyword\">import</span> io.swagger.annotations.ApiModelProperty;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> * 通用相应对象</span></span><br><span class=\"line\"><span class=\"comment\"> * */</span></span><br><span class=\"line\"><span class=\"meta\">@ApiModel(description = &quot;响应对象&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BaseResult</span>&lt;<span class=\"title\">T</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> SUCCESS_CODE = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> String SUCCESS_MESSAGE = <span class=\"string\">&quot;成功&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@ApiModelProperty(value = &quot;响应码&quot;, name = &quot;code&quot;, required = true, example = &quot;&quot; + SUCCESS_CODE)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> code;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@ApiModelProperty(value = &quot;响应消息&quot;, name = &quot;msg&quot;, required = true, example = SUCCESS_MESSAGE)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@ApiModelProperty(value = &quot;响应数据&quot;, name = &quot;data&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> T data;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"title\">BaseResult</span><span class=\"params\">(<span class=\"keyword\">int</span> code, String msg, T data)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.code = code;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.msg = msg;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.data = data;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"title\">BaseResult</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>(SUCCESS_CODE, SUCCESS_MESSAGE);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"title\">BaseResult</span><span class=\"params\">(<span class=\"keyword\">int</span> code, String msg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>(code, msg, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"title\">BaseResult</span><span class=\"params\">(T data)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>(SUCCESS_CODE, SUCCESS_MESSAGE, data);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> &lt;T&gt; <span class=\"function\">BaseResult&lt;T&gt; <span class=\"title\">success</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> BaseResult&lt;&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> &lt;T&gt; <span class=\"function\">BaseResult&lt;T&gt; <span class=\"title\">successWithData</span><span class=\"params\">(T data)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> BaseResult&lt;&gt;(data);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> &lt;T&gt; <span class=\"function\">BaseResult&lt;T&gt; <span class=\"title\">failWithCodeAndMsg</span><span class=\"params\">(<span class=\"keyword\">int</span> code, String msg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> BaseResult&lt;&gt;(code, msg, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> &lt;T&gt; <span class=\"function\">BaseResult&lt;T&gt; <span class=\"title\">buildWithParam</span><span class=\"params\">(ResponseParam param)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> BaseResult&lt;&gt;(param.getCode(), param.getMsg(), <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getCode</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> code;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setCode</span><span class=\"params\">(<span class=\"keyword\">int</span> code)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.code = code;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getMsg</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> msg;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setMsg</span><span class=\"params\">(String msg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.msg = msg;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> T <span class=\"title\">getData</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> data;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setData</span><span class=\"params\">(T data)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.data = data;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ResponseParam</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> code;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"title\">ResponseParam</span><span class=\"params\">(<span class=\"keyword\">int</span> code, String msg)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.code = code;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.msg = msg;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> ResponseParam <span class=\"title\">buildParam</span><span class=\"params\">(<span class=\"keyword\">int</span> code, String msg)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ResponseParam(code, msg);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setCode</span><span class=\"params\">(<span class=\"keyword\">int</span> code)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.code = code;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getMsg</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> msg;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setMsg</span><span class=\"params\">(String msg)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.msg = msg;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getCode</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> code;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>到这里，我们所有的工作包括配置和逻辑都已经完成了，接下来就是测试我们编写的代码是否会生效了，测试类的具体内容如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.aspectj.lang.annotation.Before;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.jupiter.api.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.web.servlet.MockMvc;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.web.servlet.MvcResult;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.web.servlet.request.MockMvcRequestBuilders;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.web.servlet.setup.MockMvcBuilders;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.LinkedMultiValueMap;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.MultiValueMap;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.context.WebApplicationContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.springframework.test.web.servlet.result.MockMvcResultHandlers.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MessageControllerTest</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> WebApplicationContext wac;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> MockMvc mockMvc;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//    @Before(value = &quot;saveMessage()&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setup</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.mockMvc = MockMvcBuilders.webAppContextSetup(<span class=\"keyword\">this</span>.wac).build();</span><br><span class=\"line\">        saveMessages();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">saveMessage</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        setup();</span><br><span class=\"line\">        <span class=\"keyword\">final</span> MultiValueMap&lt;String, String&gt; params = <span class=\"keyword\">new</span> LinkedMultiValueMap&lt;&gt;();</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;text&quot;</span>, <span class=\"string\">&quot;text&quot;</span>);</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;summary&quot;</span>, <span class=\"string\">&quot;summary&quot;</span>);</span><br><span class=\"line\">        String mvcResult=  mockMvc.perform(MockMvcRequestBuilders.post(<span class=\"string\">&quot;/message&quot;</span>)</span><br><span class=\"line\">                .params(params)).andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getAllMessages</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        String mvcResult= mockMvc.perform(MockMvcRequestBuilders.get(<span class=\"string\">&quot;/messages&quot;</span>))</span><br><span class=\"line\">                .andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getMessage</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        String mvcResult= mockMvc.perform(MockMvcRequestBuilders.get(<span class=\"string\">&quot;/message/6&quot;</span>))</span><br><span class=\"line\">                .andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">modifyMessage</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> MultiValueMap&lt;String, String&gt; params = <span class=\"keyword\">new</span> LinkedMultiValueMap&lt;&gt;();</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;id&quot;</span>, <span class=\"string\">&quot;6&quot;</span>);</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;text&quot;</span>, <span class=\"string\">&quot;text&quot;</span>);</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;summary&quot;</span>, <span class=\"string\">&quot;summary&quot;</span>);</span><br><span class=\"line\">        String mvcResult= mockMvc.perform(MockMvcRequestBuilders.put(<span class=\"string\">&quot;/message&quot;</span>).params(params))</span><br><span class=\"line\">                .andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">patchMessage</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> MultiValueMap&lt;String, String&gt; params = <span class=\"keyword\">new</span> LinkedMultiValueMap&lt;&gt;();</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;id&quot;</span>, <span class=\"string\">&quot;6&quot;</span>);</span><br><span class=\"line\">        params.add(<span class=\"string\">&quot;text&quot;</span>, <span class=\"string\">&quot;text&quot;</span>);</span><br><span class=\"line\">        String mvcResult= mockMvc.perform(MockMvcRequestBuilders.patch(<span class=\"string\">&quot;/message/text&quot;</span>).params(params))</span><br><span class=\"line\">                .andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">deleteMessage</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        mockMvc.perform(MockMvcRequestBuilders.delete(<span class=\"string\">&quot;/message/6&quot;</span>))</span><br><span class=\"line\">                .andReturn();</span><br><span class=\"line\">        String mvcResult= mockMvc.perform(MockMvcRequestBuilders.get(<span class=\"string\">&quot;/messages&quot;</span>))</span><br><span class=\"line\">                .andReturn().getResponse().getContentAsString();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Result === &quot;</span>+mvcResult);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span>  <span class=\"title\">saveMessages</span><span class=\"params\">()</span>  </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;<span class=\"number\">10</span>;i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> MultiValueMap&lt;String, String&gt; params = <span class=\"keyword\">new</span> LinkedMultiValueMap&lt;&gt;();</span><br><span class=\"line\">            params.add(<span class=\"string\">&quot;id&quot;</span>,<span class=\"string\">&quot;&quot;</span>+i);</span><br><span class=\"line\">            params.add(<span class=\"string\">&quot;text&quot;</span>, <span class=\"string\">&quot;text&quot;</span>+i);</span><br><span class=\"line\">            params.add(<span class=\"string\">&quot;summary&quot;</span>, <span class=\"string\">&quot;summary&quot;</span>+i);</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                MvcResult mvcResult=  mockMvc.perform(MockMvcRequestBuilders.post(<span class=\"string\">&quot;/message&quot;</span>)</span><br><span class=\"line\">                        .params(params)).andReturn();</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来我们访问<a href=\"http://localhost:8080/swagger-ui.html\">http://localhost:8080/swagger-ui.html</a><br><img src=\"https://img-blog.csdnimg.cn/20200306213515941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h2><p>Swagger2默认将所有的Controller中的RequestMapping方法都会暴露，然而在实际开发中，我们并不一定需要把所有API都提现在文档中查看，这种情况下，使用注解@ApiIgnore来解决，如果应用在Controller范围上，则当前Controller中的所有方法都会被忽略，如果应用在方法上，则对应用的方法忽略暴露API。注解@ApiOperation和@ApiParam可以理解为API说明，多动手尝试就很容易理解了。如果我们不使用这样注解进行说明，Swagger2也是有默认值的，没什么可说的试试就知道了。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Swagger","接口文档"]},{"title":"SpringBoot的流行实践解读","url":"/Spring-Boot/fafc44587d95/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得点个关注和点个赞哦，非常感谢支持</strong><br>看完这篇文章，感觉受益匪浅，特别是文中提到的一些最佳实践文章以及技术，非常值得一看，来源：<a href=\"http://t.cn/EJWZNra\">http://t.cn/EJWZNra</a><br>Spring Boot是最流行的用于开发微服务的Java框架。在本文中，我将与你分享自2016年以来我在专业开发中使用Spring Boot所采用的最佳实践。这些内容是基于我的个人经验和一些熟知的Spring Boot专家的文章。</p>\n<p>在本文中，我将重点介绍Spring Boot特有的实践（大多数时候，也适用于Spring项目）。以下依次列出了最佳实践，排名不分先后。</p>\n<h2 id=\"使用自定义BOM来维护第三方依赖\"><a href=\"#使用自定义BOM来维护第三方依赖\" class=\"headerlink\" title=\"使用自定义BOM来维护第三方依赖\"></a>使用自定义BOM来维护第三方依赖</h2><p>这条实践是我根据实际项目中的经历总结出的。</p>\n<p>Spring Boot项目本身使用和集成了大量的开源项目，它帮助我们维护了这些第三方依赖。但是也有一部分在实际项目使用中并没有包括进来，这就需要我们在项目中自己维护版本。如果在一个大型的项目中，包括了很多未开发模块，那么维护起来就非常的繁琐。</p>\n<p>怎么办呢？事实上，Spring IO Platform就是做的这个事情，它本身就是Spring Boot的子项目，同时维护了其他第三方开源库。我们可以借鉴Spring IO Platform来编写自己的基础项目platform-bom，所有的业务模块项目应该以BOM的方式引入。这样在升级第三方依赖时，就只需要升级这一个依赖的版本而已。</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependencyManagement&gt;</span><br><span class=\"line\">   &lt;dependencies&gt;</span><br><span class=\"line\">       &lt;dependency&gt;</span><br><span class=\"line\">           &lt;groupId&gt;io.spring.platform&lt;/groupId&gt;</span><br><span class=\"line\">           &lt;artifactId&gt;platform-bom&lt;/artifactId&gt;</span><br><span class=\"line\">           &lt;version&gt;Cairo-SR3&lt;/version&gt;</span><br><span class=\"line\">           &lt;type&gt;pom&lt;/type&gt;</span><br><span class=\"line\">           &lt;scope&gt;import&lt;/scope&gt;</span><br><span class=\"line\">       &lt;/dependency&gt;</span><br><span class=\"line\">   &lt;/dependencies&gt;</span><br><span class=\"line\">&lt;/dependencyManagement&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用自动配置\"><a href=\"#使用自动配置\" class=\"headerlink\" title=\"使用自动配置\"></a>使用自动配置</h2><p>Spring Boot的一个主要特性是使用自动配置。这是Spring Boot的一部分，它可以简化你的代码并使之工作。当在类路径上检测到特定的jar文件时，自动配置就会被激活。</p>\n<p>使用它的最简单方法是依赖Spring Boot Starters。因此，如果你想与Redis进行集成，你可以首先包括：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">   &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>如果你想与MongoDB进行集成，需要这样：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">   &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>借助于这些starters，这些繁琐的配置就可以很好地集成起来并协同工作，而且它们都是经过测试和验证的。这非常有助于避免可怕的Jar地狱。</p>\n<blockquote>\n<p><a href=\"https://dzone.com/articles/what-is-jar-hell\">https://dzone.com/articles/what-is-jar-hell</a></p>\n</blockquote>\n<p>通过使用以下注解属性，可以从自动配置中排除某些配置类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration(exclude =&#123;ClassNotToAutoconfigure.class&#125;)</span></span><br></pre></td></tr></table></figure>\n<p>但只有在绝对必要时才应该这样做。</p>\n<p>有关自动配置的官方文档可在此处找到：</p>\n<blockquote>\n<p><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-auto-configuration.html\">https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-auto-configuration.html</a></p>\n</blockquote>\n<h2 id=\"使用Spring-Initializr来开始一个新的Spring-Boot项目\"><a href=\"#使用Spring-Initializr来开始一个新的Spring-Boot项目\" class=\"headerlink\" title=\"使用Spring Initializr来开始一个新的Spring Boot项目\"></a>使用Spring Initializr来开始一个新的Spring Boot项目</h2><blockquote>\n<p>这一条最佳实践来自Josh Long （Spring Advocate，@starbuxman）。</p>\n</blockquote>\n<p>Spring Initializr 提供了一个超级简单的方法来创建一个新的Spring Boot项目，并根据你的需要来加载可能使用到的依赖。</p>\n<blockquote>\n<p><a href=\"https://start.spring.io/\">https://start.spring.io/</a></p>\n</blockquote>\n<p>使用Initializr创建应用程序可确保你获得经过测试和验证的依赖项，这些依赖项适用于Spring自动配置。你甚至可能会发现一些新的集成，但你可能并没有意识到这些。</p>\n<h2 id=\"考虑为常见的组织问题创建自己的自动配置\"><a href=\"#考虑为常见的组织问题创建自己的自动配置\" class=\"headerlink\" title=\"考虑为常见的组织问题创建自己的自动配置\"></a>考虑为常见的组织问题创建自己的自动配置</h2><p>这一条也来自Josh Long（Spring Advocate，@starbuxman）——这个实践是针对高级用户的。</p>\n<p>如果你在一个严重依赖Spring Boot的公司或团队中工作，并且有共同的问题需要解决，那么你可以创建自己的自动配置。</p>\n<p>这项任务涉及较多工作，因此你需要考虑何时获益是值得投入的。与多个略有不同的定制配置相比，维护单个自动配置更容易。</p>\n<p>如果将这个提供Spring Boot配置以开源库的形式发布出去，那么将极大地简化数千个用户的配置工作。</p>\n<h2 id=\"正确设计代码目录结构\"><a href=\"#正确设计代码目录结构\" class=\"headerlink\" title=\"正确设计代码目录结构\"></a>正确设计代码目录结构</h2><p>尽管允许你有很大的自由，但是有一些基本规则值得遵守来设计你的源代码结构。</p>\n<p>避免使用默认包。确保所有内容（包括你的入口点）都位于一个名称很好的包中，这样就可以避免与装配和组件扫描相关的意外情况；</p>\n<p>将Application.java（应用的入口类）保留在顶级源代码目录中；</p>\n<p>我建议将控制器和服务放在以功能为导向的模块中，但这是可选的。一些非常好的开发人员建议将所有控制器放在一起。不论怎样，坚持一种风格！</p>\n<h2 id=\"保持-Controller的简洁和专注\"><a href=\"#保持-Controller的简洁和专注\" class=\"headerlink\" title=\"保持@Controller的简洁和专注\"></a>保持@Controller的简洁和专注</h2><p>Controller应该非常简单。你可以在此处阅读有关GRASP中有关控制器模式部分的说明。你希望控制器作为协调和委派的角色，而不是执行实际的业务逻辑。以下是主要做法：</p>\n<blockquote>\n<p><a href=\"https://en.wikipedia.org/wiki/GRASP(object-orienteddesign)#Controller\">https://en.wikipedia.org/wiki/GRASP(object-orienteddesign)#Controller</a></p>\n</blockquote>\n<ul>\n<li>控制器应该是无状态的！默认情况下，控制器是单例，并且任何状态都可能导致大量问题；</li>\n<li>控制器不应该执行业务逻辑，而是依赖委托；</li>\n<li>控制器应该处理应用程序的HTTP层，这不应该传递给服务；</li>\n<li>控制器应该围绕用例/业务能力来设计。</li>\n</ul>\n<p>要深入这个内容，需要进一步地了解设计REST API的最佳实践。无论你是否想要使用Spring Boot，都是值得学习的。</p>\n<h2 id=\"围绕业务功能构建-Service\"><a href=\"#围绕业务功能构建-Service\" class=\"headerlink\" title=\"围绕业务功能构建@Service\"></a>围绕业务功能构建@Service</h2><p>Service是Spring Boot的另一个核心概念。我发现最好围绕业务功能/领域/用例（无论你怎么称呼都行）来构建服务。</p>\n<p>在应用中设计名称类似 AccountService, UserService, PaymentService这样的服务，比起像 DatabaseService、 ValidationService、 CalculationService这样的会更合适一些。</p>\n<p>你可以决定使用Controler和Service之间的一对一映射，那将是理想的情况。但这并不意味着，Service之间不能互相调用！</p>\n<h2 id=\"使数据库独立于核心业务逻辑之外\"><a href=\"#使数据库独立于核心业务逻辑之外\" class=\"headerlink\" title=\"使数据库独立于核心业务逻辑之外\"></a>使数据库独立于核心业务逻辑之外</h2><p>我之前还不确定如何在Spring Boot中最好地处理数据库交互。在阅读了罗伯特·C·马丁的“Clear Architecture”之后，对我来说就清晰多了。</p>\n<p>你希望你的数据库逻辑与服务分离出来。理想情况下，你不希望服务知道它正在与哪个数据库通信，这需要一些抽象来封装对象的持久性。</p>\n<blockquote>\n<p>罗伯特C.马丁强烈地说明，你的数据库是一个“细节”，这意味着不将你的应用程序与特定数据库耦合。过去很少有人会切换数据库，我注意到，使用Spring<br>Boot和现代微服务开发会让事情变得更快。</p>\n</blockquote>\n<h2 id=\"保持业务逻辑不受Spring-Boot代码的影响\"><a href=\"#保持业务逻辑不受Spring-Boot代码的影响\" class=\"headerlink\" title=\"保持业务逻辑不受Spring Boot代码的影响\"></a>保持业务逻辑不受Spring Boot代码的影响</h2><p>考虑到“Clear Architecture”的教训，你还应该保护你的业务逻辑。将各种Spring Boot代码混合在一起是非常诱人的……不要这样做。如果你能抵制诱惑，你将保持你的业务逻辑可重用。</p>\n<p>部分服务通常成为库。如果不从代码中删除大量Spring注解，则更容易创建。</p>\n<h2 id=\"推荐使用构造函数注入\"><a href=\"#推荐使用构造函数注入\" class=\"headerlink\" title=\"推荐使用构造函数注入\"></a>推荐使用构造函数注入</h2><p>这一条实践来自Phil Webb（Spring Boot的项目负责人, @phillip_webb）。</p>\n<p>保持业务逻辑免受Spring Boot代码侵入的一种方法是使用构造函数注入。不仅是因为 @Autowired注解在构造函数上是可选的，而且还可以在没有Spring的情况下轻松实例化bean。</p>\n<h2 id=\"熟悉并发模型\"><a href=\"#熟悉并发模型\" class=\"headerlink\" title=\"熟悉并发模型\"></a>熟悉并发模型</h2><p>我写过的最受欢迎的文章之一是“介绍Spring Boot中的并发”。我认为这样做的原因是这个领域经常被误解和忽视。如果使用不当，就会出现问题。</p>\n<blockquote>\n<p><a href=\"https://www.e4developer.com/2018/03/30/introduction-to-concurrency-in-spring-boot/\">https://www.e4developer.com/2018/03/30/introduction-to-concurrency-in-spring-boot/</a></p>\n</blockquote>\n<p>在Spring Boot中，Controller和Service是默认是单例。如果你不小心，这会引入可能的并发问题。你通常也在处理有限的线程池。请熟悉这些概念。</p>\n<p>如果你正在使用新的WebFlux风格的Spring Boot应用程序，我已经解释了它在“Spring’s WebFlux/Reactor Parallelism and Backpressure”中是如何工作的。</p>\n<h2 id=\"加强配置管理的外部化\"><a href=\"#加强配置管理的外部化\" class=\"headerlink\" title=\"加强配置管理的外部化\"></a>加强配置管理的外部化</h2><p>这一点超出了Spring Boot，虽然这是人们开始创建多个类似服务时常见的问题……</p>\n<p>你可以手动处理Spring应用程序的配置。如果你正在处理多个Spring Boot应用程序，则需要使配置管理能力更加强大。</p>\n<p>我推荐两种主要方法：</p>\n<ul>\n<li>使用配置服务器，例如Spring Cloud Config；</li>\n<li>将所有配置存储在环境变量中（可以基于git仓库进行配置）。</li>\n</ul>\n<p>这些选项中的任何一个（第二个选项多一些）都要求你在DevOps更少工作量，但这在微服务领域是很常见的。</p>\n<h2 id=\"提供全局异常处理\"><a href=\"#提供全局异常处理\" class=\"headerlink\" title=\"提供全局异常处理\"></a>提供全局异常处理</h2><p>你真的需要一种处理异常的一致方法。Spring Boot提供了两种主要方法：</p>\n<ul>\n<li>你应该使用HandlerExceptionResolver定义全局异常处理策略；</li>\n<li>你也可以在控制器上添加@ExceptionHandler注解，这在某些特定场景下使用可能会很有用。</li>\n</ul>\n<p>这与Spring中的几乎相同，并且Baeldung有一篇关于REST与Spring的错误处理的详细文章，非常值得一读。</p>\n<blockquote>\n<p><a href=\"https://www.baeldung.com/exception-handling-for-rest-with-spring\">https://www.baeldung.com/exception-handling-for-rest-with-spring</a></p>\n</blockquote>\n<h2 id=\"使用日志框架\"><a href=\"#使用日志框架\" class=\"headerlink\" title=\"使用日志框架\"></a>使用日志框架</h2><p>你可能已经意识到这一点，但你应该使用Logger进行日志记录，而不是使用System.out.println()手动执行。这很容易在Spring Boot中完成，几乎没有配置。只需获取该类的记录器实例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Logger logger = LoggerFactory.getLogger(MyClass.class);</span><br></pre></td></tr></table></figure>\n<p>这很重要，因为它可以让你根据需要设置不同的日志记录级别。</p>\n<h2 id=\"测试你的代码\"><a href=\"#测试你的代码\" class=\"headerlink\" title=\"测试你的代码\"></a>测试你的代码</h2><p>这不是Spring Boot特有的，但它需要提醒——测试你的代码！如果你没有编写测试，那么你将从一开始就编写遗留代码。</p>\n<p>如果有其他人使用你的代码库，那边改变任何东西将会变得危险。当你有多个服务相互依赖时，这甚至可能更具风险。</p>\n<p>由于存在Spring Boot最佳实践，因此你应该考虑将Spring Cloud Contract用于你的消费者驱动契约，它将使你与其他服务的集成更容易使用。</p>\n<h2 id=\"使用测试切片让测试更容易，并且更专注\"><a href=\"#使用测试切片让测试更容易，并且更专注\" class=\"headerlink\" title=\"使用测试切片让测试更容易，并且更专注\"></a>使用测试切片让测试更容易，并且更专注</h2><p>这一条实践来自Madhura Bhave（Spring 开发者, @madhurabhave23）。</p>\n<p>使用Spring Boot测试代码可能很棘手——你需要初始化数据层，连接大量服务，模拟事物……实际上并不是那么难！答案是使用测试切片。</p>\n<p>使用测试切片，你可以根据需要仅连接部分应用程序。这可以为你节省大量时间，并确保你的测试不会与未使用的内容相关联。来自spring.io的一篇名为Custom test slice with Spring test 1.4的博客文章解释了这种技术。</p>\n<blockquote>\n<p><a href=\"https://spring.io/blog/2016/08/30/custom-test-slice-with-spring-boot-1-4\">https://spring.io/blog/2016/08/30/custom-test-slice-with-spring-boot-1-4</a></p>\n</blockquote>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>感谢Spring Boot，编写基于Spring的微服务正变得前所未有的简单。我希望通过这些最佳实践，你的实施过程不仅会变得很快，而且从长远来看也会更加强大和成功。祝你好运！</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","自动配置","BOM"]},{"title":"你会算对象的大小么？不会的话就看看这篇文章吧","url":"/Java/da615857c651/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得点个关注和点个赞，感谢支持。</strong><br>使用面向对象语言，离不开的就是对象，现在回过头来思考一下，真的了解所使用语言的对象么？我自己有点心虚，对于Java的对象，我了解的尚且还不够深入，对于一些底层的东西，还有待进一步了解学习。这一篇博文，来讲讲Java对象的大小，学习如何计算Java对象的大小。如果你想深入Java 对象的内存设计，以及在做内存优化时，需要知道每个对象占用的内存的大小，所以这一点还是很重要的，需要好好理解。要计算Java对象占用内存的大小，首先需要了解Java对象在内存中的实际存储方式和存储格式。接下来，我们就来学习相关的知识。</p>\n<h2 id=\"思考以及预备知识\"><a href=\"#思考以及预备知识\" class=\"headerlink\" title=\"思考以及预备知识\"></a>思考以及预备知识</h2><p>搞清楚存储在内存中的对象，它具体是如何存储的，存储时都需要存哪些信息，以及存这些信息的意义是什么。比如看下面这段代码，下面这两行代码当中的 list 对象是如何存储起来的？</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">List list = <span class=\"keyword\">new</span> ArrayList;</span><br><span class=\"line\">list.add(<span class=\"string\">&quot;hello, world!&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>要学习对象是怎么存储在内存当中的，就要从很原始的地方说起，先学习 JVM 的内存结构，这篇文章不讲很具体的JVM内存结构的知识，我这边只是提一下稍后需要用到的一些知识点。JVM 在运行 Java 程序时，会管理一块内存区域，这一片区域被称为运行时数据区域，从结构上可以分为五个部分，分别是：</p>\n<ul>\n<li>Java 虚拟机栈：线程私有，存储局部变量等</li>\n<li>本地方法栈：线程私有，存储本地方法的变量等</li>\n<li>程序计数器：线程私有，存储字节码的地址（程序执行到第几行了）</li>\n<li>堆：线程共享，存储几乎所有对象</li>\n<li>方法区：线程共享，存储类的结构信息（字段、构造方法等等）</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200410180306297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们今天要说的，只是栈和堆（栈指的是 Java 虚拟机栈）。非常浅薄地讲，栈存放的是局部变量以及对象的地址，堆存放的是对象的实体。（看书发现，栈中存放的并不一定是对象地址，但这是最常见的寻找堆对象的方式）简单制作了一张图，描述了代码、栈、堆之间的关系。<br><img src=\"https://img-blog.csdnimg.cn/20200410180418361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里要注意一点的是，内存结构和内存模型并不是一个概念：当我们说内存结构时，通常是指JVM 内存结构，这是真实存在的，指的是上文介绍的 Java 虚拟机栈、堆、本地方法栈等等那五部分构成的 JVM 运行时数据区域，这是在结构上把 JVM 的内存分成了多个部分。</p>\n<p>当我们说内存模型时，通常是指Java 内存模型，这是虚拟存在的，指的是面对并发时 Java 是如何实现内存访问一致的，牵扯到了主内存和工作内存等知识，这是在模型和概念上，屏蔽各种硬件和操作系统的内存访问差异，来实现并发内存一致性。</p>\n<h2 id=\"如何计算对象的大小\"><a href=\"#如何计算对象的大小\" class=\"headerlink\" title=\"如何计算对象的大小\"></a>如何计算对象的大小</h2><p>如何计算对象的大小。这个问题实际上可以拆成两个问题：</p>\n<ul>\n<li>对象由哪些部分组成？</li>\n<li>每部分各占多少字节？</li>\n</ul>\n<p>在这两个问题的基础上，自然会问出第三个问题：</p>\n<ul>\n<li>组成对象的这些基础部分，各自是做什么的？</li>\n</ul>\n<p>这里要多说一下的是，本篇文章提到的内容，实际上是 HotSpot 虚拟机的实现，而并非所有 Java 虚拟机的实现，但是目前基本上所有的 Java 程序都跑在 HostSpot 虚拟机上面。</p>\n<p>所有对象都可以笼统地切分成两部分：对象头（Header）和对象内容（Instance Data）。举一个实际的例子：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于上面这个 Person 类，它实例化出来的对象同样具有对象头和对象内容两部分，name 和 age 都是对象的内部变量，属于对象内容，而对象头是其余一些辅助信息。我绘制了一张图，画出了在最常见情况下（64 位虚拟机开启指针压缩），对象在内存中的结构，后文都是在解释这个结构的具体信息。<br><img src=\"https://img-blog.csdnimg.cn/20200410180836304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"对象内容\"><a href=\"#对象内容\" class=\"headerlink\" title=\"对象内容\"></a>对象内容</h3><p>对象内容准确地讲应该叫做实例数据（Instance Data），比较简单，因此我们先讲完。正如之前提到的 Person 对象的例子，对象内的属性（包括基本数据类型 <code>int age</code> 和引用的另一个对象 <code>String name</code>），这些属性所占的内容大小，就是对象内容的大小。在该例子中，<code>int</code> 类型的 age 占 4 个字节（即 32 位），引用另一个对象时，存储的是对象的地址，地址是一个 <code>int</code> 类型的指针，因此 <code>String</code> 类型的 name 存储在 Person 对象中也占 4 个字节（即 32 位），两个属性加起来一共占 8 个字节。因此计算对象内容的大小，实际上就是分两部分，基本数据类型一类，占内容大小加起来，引用别的对象占一类，引用一个就是 4 字节（int 的大小），引用 N 个对象就占 N*4 个字节。下面列举了 8 种基本数据类型的大小。<br>| 基本数据类型 | 大小（字节） |<br>|–|–|<br>|byte    |1|<br>|char|    2（表示一个 UTF-16be 编码单元，生僻字用两个char）|<br>|short|    2|<br>|int    |4|<br>|float|    4|<br>|long|    8|<br>|double    |8|<br>|boolean|    通常是1|</p>\n<p>此外还要注意的一点是，如果 A 类继承自 B 类，那么计算 A 类的对象内容大小时，继承来的 B 类的属性也是要算在内的。比如计算 ArrayList 对象大小的时候，它的父类 AbstractList 中的属性，也是要计算在内的。</p>\n<h3 id=\"对象头\"><a href=\"#对象头\" class=\"headerlink\" title=\"对象头\"></a>对象头</h3><p>对象头（Header）比较复杂，它包含着对象的“冗余信息”，这些信息或实现并发锁，或帮助垃圾分类，或包含类的信息。从整体上看，对象头包含三部分的信息，分别是</p>\n<ul>\n<li>标记字段</li>\n<li>地址</li>\n<li>数组长度</li>\n</ul>\n<h4 id=\"标记字段\"><a href=\"#标记字段\" class=\"headerlink\" title=\"标记字段\"></a>标记字段</h4><p>标记字段（Mark Word）是对象头中最复杂的内容，需要对照上面绘制的图来看。由于内存空间寸土寸金，在希望对象能够记录更多信息的同时，还要尽可能地压缩空间，在这种背景之下，32 位虚拟机的对象标记字段长 4 字节，64 位虚拟机的对象标记字段长 8 字节（现在基本都是 64 位了吧），并且都有着动态定义的数据结构，以便在极小的空间内存储尽量多的数据。32 位和 64 位的存储长度不同，仅仅是因为地址指针长度引起的变化，在存储的内容类型方面没有区别。（具体的标记字段信息可见文末的备注）。以我当下的理解，标记字段主要实现了三个事情：</p>\n<ul>\n<li>对并发情况下的 synchronized 支持</li>\n<li>GC 垃圾回收</li>\n<li>保存 hashcode</li>\n</ul>\n<p>标记字段共有五种状态，分别是对应于 synchronized 的四种状态（无锁、偏向锁、轻量级锁和重量级锁），以及一种 GC 状态，这五种状态通过 2 位标志位实现（无锁和偏向锁的标志位相同）。因此，了解标记字段的具体信息，实际上就是在了解 synchronized 锁和垃圾回收的原理。这两部分都有点难，本文暂时不讨论了。</p>\n<h4 id=\"地址信息\"><a href=\"#地址信息\" class=\"headerlink\" title=\"地址信息\"></a>地址信息</h4><p>对象头中有一部分是地址信息，它实际上是一个类型指针，指向了该对象类型的地址。例如 person 对象的对象头中的地址信息，指向了 Person 类的地址（类在方法区）。在这种设计下，可以通过对象找到类，比如在 <code>main()</code> 方法中实例化一个 Person 对象 person，在内存中寻址的过程为：</p>\n<ul>\n<li>main() 方法的 Java 栈中记录着 person 对象的地址；</li>\n<li>根据这个地址在堆中找到了 person 对象；</li>\n<li>person 对象的头部又记录着 Person 类的地址，根据这个地址在方法区中找到了 Person 类；</li>\n</ul>\n<p>（实际上，在对象的头部中保留类的地址信息，通过对象找到类的位置，这种设计是 HotSpot 虚拟机的设计，也有别的虚拟机不这么设计，对象头中并不包含类的地址，不通过对象找类。）地址信息的大小并不是固定的，这跟系统位数有关，32 位的虚拟机，指针是 32 位长，地址信息只需要 32 （即 4 字节），但是对于 64 位的虚拟机，指针是 64 位长，因此地址信息也需要扩增到 64 位（即 8 字节）。</p>\n<p>32 位的虚拟机，理论上只能寻址到 4 GB 的内存空间（2^32 byte = 4 GB），而 64 位的虚拟机能寻址到更多地址。这样的提升是有代价的，一方面内存占用量变大了，原来只需要 4 个字节存储一个地址，现在需要 8 个字节了（如果不需要比 4GB 更多的内存，用这么大的空间是没有意义的），另一方面寻址时操作位数更长的指针，主内存和各级缓存移动数据时，占用的带宽也会增加。Java 虚拟机为了处理这个问题，提出了指针压缩。</p>\n<p>指针压缩的简易原理是这样的：32 位的指针，当然只能找到 4 GB 个内存位置，如果我有一块更大的内存区域，比如 10 GB，32 位的指针就不能指向这 10 GB 中的所有位置，但实际上并不需要找到这块内存中的所有位置，它只需要找到要操作的开始位置就可以了。这意味着 32 位的指针可以引用 40 亿个对象，而不是 40 亿个字节。Java 对象的大小如果一定是 8 字节的整数倍（这个后文有讲），那么就可以使原来只能寻址 4 GB 的内存扩大 8 倍，到 32 GB 的内存。</p>\n<p>因此对于分配内存低于 4 GB 的虚拟机，默认开启指针压缩，指针大小就是 32 位长，对于分配内存在 4 - 32 GB 之间的虚拟机，可以开启指针压缩算法，使指针大小依旧维持在 32 位长，但是对于更大的内存，无法开启指针压缩，指针大小必须是 64 位长。（因此分配内存并不是越大越好，32 GB 处会有一个门槛）指针压缩并非毫无缺陷，这毕竟是多出来的算法，会增加 JVM 的计算量。</p>\n<p><strong>总结</strong>：对象头中的地址信息大小，跟系统位数以及是否开启指针压缩有关，32 位系统、开启了指针压缩的 64 位系统的地址信息长 4 字节，普通 64 位系统的地址信息长 8 字节。</p>\n<h4 id=\"数组大小\"><a href=\"#数组大小\" class=\"headerlink\" title=\"数组大小\"></a>数组大小</h4><p>数组大小并不是必须的，数组才有，非数组没有。因为数组是 new 出来的，需要在堆上分配内存，在这个意义上讲，数组就是对象的一种。数组的长度是需要记录下来的，长度为 4 字节。<code>int</code> 也是 4 字节，这就很容易让人联想在一起。Java 中 <code>int</code> 是有符号整型数，是有负值的，int 的最大值是 <code>2^31 - 1</code>，用二进制表示为 <code>01111111111111111111111111111111</code>。数组的理论最大长度，也应该是 int 的最大值。</p>\n<p>实际的使用中可能会小一点。例如 ArrayList 内部维护的数组，它的最大长度是 <code>Integer.MAX_VALUE - 8</code>，注释称这是因为虚拟机的限制。又例如 HashMap 内部维护的数组，它的最大程度是 <code>1 &lt;&lt; 30</code>，这是 1 位运算之后能获得到的最大值（二进制为 <code>01000000000000000000000000000000</code>）。</p>\n<h2 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h2><p>在计算完对象头和对象内容的大小之后，二者加起来并不一定是最终占内存的大小，还要考虑内存对齐的问题。所有对象的字节大小，必须是 8 的整数倍，如果对象头+对象内容算出来是 15 字节，那么最终对象大小为 16 字节，如果是 20 字节，那么最终对象大小是 24 字节，总之如果不满 8 的整数倍，都填充到 8 的整数倍，填充的部分叫做对齐填充（Padding），实际上就是占位符。对齐填充的原因在于，HotSpot 虚拟机的自动内存管理系统，要求对象的起始地址必须是 8 字节的整数倍（这样寻址更高效，而且实现了指针压缩），因此对象的大小也就必须是 8 字节的整数倍。</p>\n<p>三种情况（32 位虚拟机、64 位虚拟机、64 位虚拟机开启指针压缩）下，对象头的具体存储内容：</p>\n<h3 id=\"32-位虚拟机\"><a href=\"#32-位虚拟机\" class=\"headerlink\" title=\"32 位虚拟机\"></a>32 位虚拟机</h3><p><img src=\"https://img-blog.csdnimg.cn/20200410183206589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"64-位虚拟机\"><a href=\"#64-位虚拟机\" class=\"headerlink\" title=\"64 位虚拟机\"></a>64 位虚拟机</h3><p><img src=\"https://img-blog.csdnimg.cn/20200410183255167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"64-位虚拟机开启指针压缩\"><a href=\"#64-位虚拟机开启指针压缩\" class=\"headerlink\" title=\"64 位虚拟机开启指针压缩\"></a>64 位虚拟机开启指针压缩</h3><p><img src=\"https://img-blog.csdnimg.cn/2020041018341691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"举例计算对象大小\"><a href=\"#举例计算对象大小\" class=\"headerlink\" title=\"举例计算对象大小\"></a>举例计算对象大小</h2><p>最后用一个例子检验上文中的内容，计算一个 HashMap 对象的大小。HashMap 类不是数组，在 64 位开启指针压缩的情况下，对象头只包括 8 字节的标记字段和 4 字节的地址指针，总共 12 字节。</p>\n<p>HashMap 类中分别有下列属性：</p>\n<ul>\n<li>entrySet （对象）</li>\n<li>hashSeed （int）</li>\n<li>loadFactor （float）</li>\n<li>modCount （int）</li>\n<li>size （int）</li>\n<li>table （数组，当对象处理）</li>\n<li>threshold （int）</li>\n</ul>\n<p>检查 HashMap 的所有父类，在 AbstractMap 中发现了两个新的属性：</p>\n<ul>\n<li>keySet （对象）</li>\n<li>values （对象）</li>\n</ul>\n<p>算下来一共是 9 个属性，每个属性很巧都是 4 字节，一共是 9×4 = 36 字节，因此 HashMap 的对象内容为 36 字节。HashMap 对象的对象头 12 字节 + 对象内容 36 字节总共是 48 字节，是 8 字节的倍数，无需对齐填充。因此一个 HashMap 对象的大小是 48 字节。</p>\n","categories":["Java"],"tags":["Java","对象"]},{"title":"关于RNN理论和实践的一些总结","url":"/Deep-Learning/f43bcaebe5e9/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本篇文章主要总结我在学习过程中遇到的RNN、其相关变种，并对相关结构进行说明和结构图展示。内容包括RNN、RecNN、多层、双向、RNNCell等等，同时包括在计算框架（TensorFlow及PyTorch）API层面的一些理解记录。本篇文章不进行深入推导和底层原理介绍，仅做总结记录，感兴趣者可自行根据内容详细查阅资料。</p>\n<blockquote>\n<p>RNN（递归神经网络）包括Recurrent Neural Network和Recursive Neural Network两种，分别为时间递归神经网络和结构递归神经网络。</p>\n</blockquote>\n<p>计算框架版本：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1>在进行后面内容的陈述之前，先来简单结合计算框架说明一下vanilla RNN、LSTM、GRU之间的区别。虽然将vanilla RNN、LSTM、GRU这个三个分开讲进行对比，但是不要忘记它们都是RNN，所以在宏观角度都是如下结构：<br><img src=\"https://img-blog.csdnimg.cn/20201208151624631.png#pic_center\" alt=\"在这里插入图片描述\"><br>而它们区别在于中间的那个隐藏状态计算单元，这里贴出它们的计算单元的细节，从左到右分别是vanilla RNN、LSTM、GRU。<br><img src=\"https://img-blog.csdnimg.cn/20201208152622203.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>看了隐藏单元之后，你有没有发现LSTM和其他两个的输入多了一个cell state，LSTM的门道就在这，cell state 就是实现LSTM的关键（ps：GRU其实也有分hidden state和cell state，不过在GRU中它们两个是相同的）。细节我不去深究，感兴趣的自行查看论文：</li>\n<li><a href=\"https://arxiv.org/pdf/1409.2329.pdf\">RNN</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf\">LSTM</a></li>\n<li><a href=\"https://arxiv.org/pdf/1406.1078v3.pdf\">GRU</a></li>\n</ul>\n<p>我这里就简单的结合<a href=\"https://www.tensorflow.org/api/stable\">TensorFlow</a>和<a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch</a>说明一下cell state和hidden state，首先看下面两个计算框架的调用（详细参数自行查阅文档，这里只是为了说明state）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># TensorFlow中的LSTM调用</span></span><br><span class=\"line\">whole_seq_output, final_memory_state, final_carry_state =</span><br><span class=\"line\">\t\t\ttf.keras.layers.LSTM(<span class=\"number\">4</span>, return_sequences=<span class=\"literal\">True</span>, return_state=<span class=\"literal\">True</span>)(inputs)</span><br><span class=\"line\"><span class=\"comment\"># Pytorch中的LSTM调用</span></span><br><span class=\"line\">output, (hn, cn) = torch.nn.LSTM(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)(<span class=\"built_in\">input</span>, (h0, c0))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># TensorFlow中的GRU调用</span></span><br><span class=\"line\">whole_sequence_output, final_state =</span><br><span class=\"line\">\t\t\ttf.keras.layers.GRU(<span class=\"number\">4</span>, return_sequences=<span class=\"literal\">True</span>, return_state=<span class=\"literal\">True</span>)(inputs)</span><br><span class=\"line\"><span class=\"comment\"># Pytorch中的GRU调用</span></span><br><span class=\"line\">output, hn = torch.nn.GRU(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)(<span class=\"built_in\">input</span>, h0)</span><br></pre></td></tr></table></figure>\n<p>以TensorFlow举例（PyTorch默认都返回），当return_state参数设置为True时，将会返回隐藏层状态，即cell_state。在LSTM 的网络结构中，直接根据当前input 数据，得到的输出称为 hidden state，还有一种数据是不仅仅依赖于当前输入数据，而是一种伴随整个网络过程中用来记忆，遗忘，选择并最终影响hidden state结果的东西，称为 cell state。cell state默认是不输出的，它仅对输出 hidden state 产生影响。通常情况，我们不需要访问cell state，但当需要对 cell state 的初始值进行设定时，就需要将其返回。所以在上面的TensorFlow对LSTM的调用中，final_memory_state是最后一个timestep的状态，final_carry_state是最后一个timestep的cell state。既然见到LSTM和GRU，那下面就贴一张它们的状态更新公式图以作记录：<br><img src=\"https://img-blog.csdnimg.cn/20201208161136131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>后面简要阐述的所有RNN及其变种，都是代指vanilla RNN、LSTM、GRU三个，只不过为了方便描述，以RNN作为总称进行说明。</p>\n<blockquote>\n<p>TensorFlow中，RNN类是作为如第一张结构图那些的宏观结构，所以它有一个cell参数，你可以根据实际需要传入SimpleRNNCell、LSTMCell和GRUCell（这三个你就可以理解成上面讲的计算单元），它们三个可以单独使用，在一些地方特别管用。</p>\n</blockquote>\n<blockquote>\n<p>PyTorch中大致是一样的，不过RNN类则是标准的RNN实现的，而不是像Tensorflow那样的架构，PyTorch同样有RNNCell、LSTMCell和GRUCell</p>\n</blockquote>\n<h1 id=\"标准RNN\"><a href=\"#标准RNN\" class=\"headerlink\" title=\"标准RNN\"></a>标准RNN</h1><p>RNN忽略单元细节的具体结构图如下。从图中就能够很清楚的看到，上一时刻的隐藏层是如何影响当前时刻的隐藏层的（注意这里Output的数量画少了，看起来不够形象，应该是 $X=[x_1,x_2,…,x_m]$和 $O=[o_1,o_2,…,o_m]$）。这里的Output是对应时间步的状态，而 $s$ 是隐藏状态，一般在实践中用它来初始化RNN。<br><img src=\"https://img-blog.csdnimg.cn/20201208112157265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然，可以换一种方式画结构图，如下图所示，按照RNN时间线展开。注意了，隐藏层 $s_t$ 不仅取决于 $x_t$ 还取决与 $s_{t-1}$。<br><img src=\"https://img-blog.csdnimg.cn/2020120811351449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从上面总结公式如下：<br>$$o_t=g(V_{s_t}) \\quad\\quad (1)$$    $$s_t=f(U_{x_t}+W_{s_{t-1}}) \\quad\\quad (2)$$<br>式（1）是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。$V$是输出层的权重矩阵，$g$是激活函数。式（2）是隐藏层的计算公式，它是循环层。$U$ 是输入 $x$ 的权重矩阵，$W$ 是上一次的值作为这一次的输入的权重矩阵，$f$ 是激活函数。从宏观意义上来说，循环层和全连接层的区别就是循环层多了一个权重矩阵 $W$。通过循环带入得下式：<br>$$o_t=Vf(U_{x_t}+Wf(U_{x_{t-1}}+Wf(U_{x_{t-2}}+Wf(U_{x_{t-3}}+…))))$$<br>从上面可以看出，循环神经网络的输出值 $o_t$，是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、$x_{t-3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p>\n<h1 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h1><p>论文：<a href=\"http://deeplearning.cs.cmu.edu/F20/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf\">Link</a><br><img src=\"https://img-blog.csdnimg.cn/20201208144157106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从上图可以看出，双向RNN的隐藏层要保存两个值，一个 $A$ 参与正向计算，另一个值 $A’$ 参与反向计算（注意了，正向计算和反向计算不共享权重），最终的输出值取决于 $A$ 和 $A’$ 的计算方式。其计算方法有很多种，这里结合TensorFlow和PyTorch说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># TensorFlow中，需要使用Bidirectional来实现双向RNN，如下所示</span></span><br><span class=\"line\"><span class=\"comment\"># 其中merge_mode就是A和A&#x27;两者的计算方式：&#123;&#x27;sum&#x27;, &#x27;mul&#x27;, &#x27;concat&#x27;, &#x27;ave&#x27;, None&#125;</span></span><br><span class=\"line\">tf.keras.layers.Bidirectional(</span><br><span class=\"line\">    layer, merge_mode=<span class=\"string\">&#x27;concat&#x27;</span>, weights=<span class=\"literal\">None</span>, backward_layer=<span class=\"literal\">None</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># PyTorch则不同，在各RNN的具体实现中，都有一个bidirectional参</span></span><br><span class=\"line\"><span class=\"comment\"># 数来控制是否是双向的，可自行查看PyTorch的API文档，特别说明的是</span></span><br><span class=\"line\"><span class=\"comment\"># PyTorch没有merge_mode，所以双向RNN直接会返回正向和反向的状态，</span></span><br><span class=\"line\"><span class=\"comment\"># 需要你自行进行合并操作</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"Multi-layer-stacked-RNN\"><a href=\"#Multi-layer-stacked-RNN\" class=\"headerlink\" title=\"Multi-layer(stacked) RNN\"></a>Multi-layer(stacked) RNN</h1><p>将多个RNN堆叠成多层RNN，每层RNN的输入为上一层RNN的输出，如下图所示。多层 (Multi-layer) RNN 效果很好，但可能会常用到 skip connections 的方式<br><img src=\"https://img-blog.csdnimg.cn/20201208164736875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"深度循环神经网络\"><a href=\"#深度循环神经网络\" class=\"headerlink\" title=\"深度循环神经网络\"></a>深度循环神经网络</h1><p>前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/2020120816540656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们把第 $i$ 个隐藏层的值表示为 $s_t^{(i)}$、$s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为：<br>$$o_t=g(V^{(i)}s_t^{(i)}+V^{‘(i)}s_t^{‘(i)})$$    $$s_t^{(i)}=f(U^{(i)}s_t^{(i-1)}+W^{(i)}s_{t-1})$$     $$s_t^{‘(i)}=f(U^{‘(i)}s_t^{‘(i-1)}+W^{‘(i)}s_{t+1}^{‘})$$    $$s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1})$$    $$s_t^{‘(1)}=f(U^{‘(1)}x_t+W^{‘(1)}s_{t+1}^{‘})$$</p>\n<h1 id=\"Recursive-Neural-Network\"><a href=\"#Recursive-Neural-Network\" class=\"headerlink\" title=\"Recursive Neural Network\"></a>Recursive Neural Network</h1><p>RNN适用于序列建模，而许多NLP问题需要处理树状结构，因此提出了RecNN的概念。与RNN将前序句子编码成状态向量类似，RecNN将每个树节点编码成状态向量。RecNN中的每棵子树都由一个向量表示，其值由其子节点的向量表示递归确定。<br><img src=\"https://img-blog.csdnimg.cn/20201208170829765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>RecNN接受的输入为一个有n个单词的句子的语法分析树，每个单词都表示为一个向量，语法分析树表示为一系列的生成式规则。举个例子，The boy saw her duck的分析树如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201208170959474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>对应的生成式规则（无标签+有标签）如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201208171104105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>RecNN的输出为句子的内部状态向量（inside state vectors），每一个状态向量都对应一个树节点。具体RecNN细节自行详细查阅资料。</p>\n<h1 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h1><p>普遍来看, 神经网络都会有梯度消失和梯度爆炸的问题，其根源在于现在的神经网络在训练的时候，大多都是基于BP算法，这种误差向后传递的方式，即多元函数求偏导中，链式法则会产生 vanishing，而 RNN 产生梯度消失的根源是权值矩阵复用。</p>\n<h3 id=\"循环神经网络的训练算法：BPTT\"><a href=\"#循环神经网络的训练算法：BPTT\" class=\"headerlink\" title=\"循环神经网络的训练算法：BPTT\"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>\n<ul>\n<li>前向计算每个神经元的输出值</li>\n<li>反向计算每个神经元的误差项 $\\delta_j$ 值，它是误差函数 $E$ 对神经元 $j$ 的加权输入 $net_j$ 的偏导数</li>\n<li>计算每个权重的梯度</li>\n<li>最后再用随机梯度下降算法更新权重。</li>\n</ul>\n<h3 id=\"RNN的梯度爆炸和消失问题\"><a href=\"#RNN的梯度爆炸和消失问题\" class=\"headerlink\" title=\"RNN的梯度爆炸和消失问题\"></a>RNN的梯度爆炸和消失问题</h3><p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>\n<ul>\n<li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li>\n<li>使用 $relu$ 代替 $sigmoid$ 和 $tanh$ 作为激活函数。</li>\n<li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/\">How to use return_state or return_sequences in Keras</a></li>\n<li><a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a></li>\n<li><a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a></li>\n<li><a href=\"https://www.jianshu.com/p/5f39b91bff11\">RNN summarize</a></li>\n<li><a href=\"https://www.jianshu.com/p/f08eb58cf16b\">从动图中理解 RNN，LSTM 和 GRU</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/135320350\">RNN、lstm、gru详解</a></li>\n<li><a href=\"https://www.jianshu.com/p/403665b55cd4\">用 Recursive Neural Networks 得到分析树</a></li>\n<li><a href=\"https://zybuluo.com/hanbingtao/note/541458\">循环神经网络</a></li>\n<li><a href=\"https://www.cnblogs.com/chenjieyouge/p/12556237.html\">双向 和 多重 RNN</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","RNN","LSTM","GRU"]},{"title":"关于Transformer几个内部细节的总结","url":"/Deep-Learning/58cb665371b1/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？\"><a href=\"#为何在获取输入词向量之后需要对矩阵乘以embeddding-size的开方？\" class=\"headerlink\" title=\"为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？\"></a>为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方？</h1><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">embeddings &#x3D; tf.keras.layers.Embedding(input_dim&#x3D;vocab_size, output_dim&#x3D;embedding_dim,</span><br><span class=\"line\">                                           dtype&#x3D;d_type, name&#x3D;&quot;&#123;&#125;_embeddings&quot;.format(name))(inputs)</span><br><span class=\"line\">embeddings *&#x3D; tf.math.sqrt(x&#x3D;tf.cast(x&#x3D;embedding_dim, dtype&#x3D;d_type), name&#x3D;&quot;&#123;&#125;_sqrt&quot;.format(name))</span><br></pre></td></tr></table></figure>\n<p>我上面使用的是Keras的Embedding，其初始化方式是xavier init，而这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>\n<h1 id=\"Transformer为何使用多头注意力机制？\"><a href=\"#Transformer为何使用多头注意力机制？\" class=\"headerlink\" title=\"Transformer为何使用多头注意力机制？\"></a>Transformer为何使用多头注意力机制？</h1><ul>\n<li><a href=\"https://www.zhihu.com/question/341222779\">问题</a></li>\n</ul>\n<p>先上一份原论文中对Multi-head attention的解释：</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p>\n</blockquote>\n<ul>\n<li>如何解释这个不同的注意力表示子空间？</li>\n<li>以及表示子空间信息的差异性是如何来的？</li>\n</ul>\n<p>对于这两个小问题，我从这个回答中找到了思路：<a href=\"https://www.zhihu.com/question/341222779/answer/814111138\">为什么Transformer 需要进行 Multi-head Attention？</a>。</p>\n<p>首先，有大量的paper表明，Transformer或Bert的特定层是有独特的功能的，底层更偏向于关注语法，顶层更偏向于关注语义，其实这也符合我们的直觉（语法与token位置相关性较大，语义则是更高层的抽象表示）。这反应了结构中不同层所学习的表示空间不同，从某种程度上，又可以理解为在同一层Transformer关注的方面是相同的，那么对该方面而言，不同的头关注点应该也是一样的，而对于这里的“一样”，一种解释是关注的pattern相同，但内容不同，这也就是解释了第一个小问题，不同的头大体上的pattern是一样的，而差异性来源与分离的不同内容，为了更好的说明这一点，从论文：<a href=\"https://arxiv.org/pdf/1906.05714.pdf\">A Multiscale Visualization of Attention in the Transformer Model</a>中截取的一张可视化的图可以形象的说明：<br><img src=\"https://img-blog.csdnimg.cn/20210322213430125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到，相同层的不同头所关注的pattern大致相同，不过细心的小伙伴应该会注意到，同一层中总是有那么一两个头和其他头的pattern不相同，这也是所要解释的第二个小问题。</p>\n<p>我们首先来看看这个过程是怎样的。首先，所有的参数随机初始化，然后用相同的方法前传，在输出端得到相同的损失，用相同的方法后传，更新参数。在这一条逻辑线中，唯一不同的地方在于初始值的不同。设想一下，如果我们把同一层的所有参数（这里的参数都是指的$W^Q_i,W^K_i,W^V_i$ ）初始化成一样的（不同层可以不同），那么在收敛的时候，同一层的所有参数仍然是一样的，自然它们的关注模式也一样。也就是说，相同层中出现的“异类”，从某种解释上说，可以理解为是由于初始化的不一样引起的。</p>\n<p>我还看到了另外一种从实用原因解释的思路，来自苏剑林大神的<a href=\"https://www.zhihu.com/question/446385446/answer/1752279087\">回答</a>，总体上表述的意思是，每个token通常只是注意到有限的若干个token，这说明Attention矩阵通常来说是很“稀疏”的，所以只用一个头的话，当特征维度较大时，计算量就大，这个时候用某种方式对 $Q,K$ 进行分割，然后计算之后在以某种方式整合，虽然这种方式计算量和不分割差不多，但是从某种程度上引入了enhance或noise，类似模型融合，效果表现上也应该更好。</p>\n<p>最后总结陈词就是，Multi-Head其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种enhance或noise而已。</p>\n<h1 id=\"计算Attention时，点乘Attention和加法Attention的区别？\"><a href=\"#计算Attention时，点乘Attention和加法Attention的区别？\" class=\"headerlink\" title=\"计算Attention时，点乘Attention和加法Attention的区别？\"></a>计算Attention时，点乘Attention和加法Attention的区别？</h1><p>Attention发展到现在衍生出了特别多的种类，从功能效果上来说，Attention可以分为基于内容的注意力机制(content-based attention)、基于位置的注意力机制(location-based attention)和混合注意力机制(hybrid attention)，我之前针对Attention做了一个总结，感兴趣的小伙伴可以看一下：<a href=\"https://zhuanlan.zhihu.com/p/338193410\">NLP中遇到的各类Attention结构汇总以及代码复现</a>。<br>$$score(h_j,s_i)=\\left \\langle v,tanh(W_1h_j+W_2s_i) \\right \\rangle$$   $$score(h_j,s_i)=\\left \\langle W_1h_j,W_2s_i \\right \\rangle$$</p>\n<p>而我们这里讨论的点乘Attention和加法Attention属于基于内容的注意力机制(content-based attention)，我们从计算速度上和效果上来比较它们两者间的区别。</p>\n<ul>\n<li>从计算效率上，为了方便感官上的呈现，看下面的两者计算注意力分数的代码，从这里其实很容易看得出来，整体计算量上，它们两者是相似的，在GPU运行上应该没有很明显的差别。<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">加法：score &#x3D; self.V(tf.nn.tanh(self.W1(key) + self.W2(tf.expand_dims(query, 1))))</span><br><span class=\"line\">点乘：score &#x3D; tf.matmul(query, key, transpose_b&#x3D;True) &#x2F; tf.math.sqrt(dk)</span><br></pre></td></tr></table></figure></li>\n<li>从表现效果来讲，论文：Massive Exploration of Neural Machine Translation Architectures里面对此做了对比实验（这篇Paper里基于Seq2Seq做了许多对比实验，很值得阅读，也可以阅读我之前的<a href=\"https://zhuanlan.zhihu.com/p/328801239\">论文阅读笔记：对NMT架构的超参数首次进行大规模消融实验分析</a>），Paper的实验结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20210323225750351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从结果上可以看得出，加法式注意机制略微但始终优于乘法式注意力机制。当然优于Transformer结构中使用了的多头注意力机制，在将表示分割成不同个头进行运算时，使用点乘会更加灵活方便计算，但实际应用在Transformer上的差别，后面实验对比再体会体会。<h1 id=\"transformer中为什么使用不同的K-和-Q？\"><a href=\"#transformer中为什么使用不同的K-和-Q？\" class=\"headerlink\" title=\"transformer中为什么使用不同的K 和 Q？\"></a>transformer中为什么使用不同的K 和 Q？</h1></li>\n<li><a href=\"https://www.zhihu.com/question/319339652\">问题</a></li>\n</ul>\n<p>我们来看看原论文中的公式：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$<br>这就是我们非常熟悉的Self-Attention的计算公式，在这里Q和K是相同的，但是在真正的应用中，通常会分别给K和Q乘上参数矩阵，这样公式会变成如下这样（其中两个参数矩阵进行共享的话，就是reformer的做法）：<br>$$Attention(Q,K,V)=softmax(\\frac{Q(W_qW^T_k)K^T}{\\sqrt{d_k}})V$$<br>在这里，我们把目光放在Softmax上，当Q和K乘上不同的参数矩阵时，根据softmax函数的性质，在给定一组数组成的向量，Softmax先将这组数的差距拉大（由于exp函数），然后归一化，它实质做的是一个soft版本的argmax操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。这样做保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</p>\n<p>如果令Q和K相同，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，这样self-attention就退化成一个point-wise线性映射，对于注意力上的表现力不够强。</p>\n<h1 id=\"Transformer中的Attention为什么scaled\"><a href=\"#Transformer中的Attention为什么scaled\" class=\"headerlink\" title=\"Transformer中的Attention为什么scaled?\"></a>Transformer中的Attention为什么scaled?</h1><ul>\n<li><a href=\"https://www.zhihu.com/question/339723385\">问题</a></li>\n</ul>\n<p>在上一个问题中，我们有提到计算Attention分数时，是通过Softmax进行归一化的，而Softmax有可以当作argmax的一种平滑近似，与argmax操作中暴力地选出一个最大值（产生一个one-hot向量）不同，softmax将这种输出作了一定的平滑，即将one-hot输出中最大值对应的1按输入元素值的大小分配给其他位置。而当喂入的数组内部数量级相差较大时，“1分出去的部分”就会越来越少，当数量级相差到一定程度，softmax将几乎全部的概率分布都分配给了最大值对应的标签，其效果也就被削减了。不仅如此，在输入的数量级很大时，还会导致softmax的梯度消失为0，造成参数更新困难，这也是为什么需要对输入先进行缩放的原因。</p>\n<p>其次，为什么原文中的计算公式对输入缩放的大小是 $\\sqrt{d_k}$？原论文中是这样解释的：</p>\n<blockquote>\n<p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, $q\\cdot k=\\sum^{d_k}_{i=1}q_ik_i$, has mean 0 and variance dk.</p>\n</blockquote>\n<p>假设向量 $q$ 和 $k$ 的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积 $q\\cdot k$ 的均值是0，方差是 $d_k$ 。而方差越大也就说明，点积的数量级越大（以越大的概率取大值）。那么一个自然的做法就是将点积除以 $\\sqrt{d_k}$ ，将方差控制为1，也就有效地控制了前面提到的梯度消失的问题。</p>\n<h1 id=\"在计算注意力分数的时候如何对padding做mask操作？\"><a href=\"#在计算注意力分数的时候如何对padding做mask操作？\" class=\"headerlink\" title=\"在计算注意力分数的时候如何对padding做mask操作？\"></a>在计算注意力分数的时候如何对padding做mask操作？</h1><p>mask是将一些不要用的值掩盖掉，使其不产生作用，有两种mask，第一种是padding mask，在所有scaled dot-product attention都用到，第二种是sequence mask，在decoder的self-attention里面用到。</p>\n<ul>\n<li>padding mask：因为一个批量输入中，所有序列的长度是不同的，为了符合模型的输入方式，会用padding的方式来填充（比如填0），使所有序列的长度一致。但填充部分是没有意义的，所以在计算注意力的时候，不需要也不应该有注意力分配到这些填充的值上面。所以解决方式就是在填充的位置赋予一个很小的负值/负无穷（-np.inf）的值，经过softmax后的得分为0，即没有注意力分配到这个上面。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">seq &#x3D; tf.cast(x&#x3D;tf.math.equal(seq, 0), dtype&#x3D;tf.float32)[:, tf.newaxis, tf.newaxis, :]</span><br></pre></td></tr></table></figure>\n<ul>\n<li>look_ahead_mask：我们知道在计算Q和K的时候，我们目的是通过self-attention获得某个token和它相关联token之间的关联程度，即我们Q和K相乘时，得到一个shape的最后两维为(…seq_len,seq_len)的相关性矩阵，但是，在我们将这个相关性矩阵送进Softmax进行计算之前，我们需要为了防止穿越mask所计算的那个token之后的其他token，为了更方便理解，可以看下面的图：<br><img src=\"https://img-blog.csdnimg.cn/20210325220004803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"为什么-Transformer-需要-positional-encoding-？\"><a href=\"#为什么-Transformer-需要-positional-encoding-？\" class=\"headerlink\" title=\"为什么 Transformer 需要 positional encoding ？\"></a>为什么 Transformer 需要 positional encoding ？</h1></li>\n<li><a href=\"https://www.zhihu.com/question/347678607/answer/835053468\">https://www.zhihu.com/question/347678607/answer/835053468</a></li>\n</ul>\n<p>在没有 Position embedding 的 Transformer 模型并不能捕捉序列的顺序，交换单词位置后 attention map 的对应位置数值也会进行交换，并不会产生数值变化，即没有词序信息。所以这时候想要将词序信息加入到模型中。</p>\n<p>Transformer 使用的解决方案是三角函数实现相对位置信息的表示。原回答已经对论文中的公式做了细致的解释，实质上就是对不同维度使用不同频率的正/余弦公式进而生成不同位置的高维位置向量。不过这里有一点需要解释，为什么奇偶维度之间需要作出区分，分别使用 sin 和 cos 呢？个人觉得主要是因为三角函数的积化和差公式。不过也正如tensor2tensor 的作者对其简洁版本公式的解释那样，奇偶区分可以通过全连接层帮助重排坐标，所以可以直接简单地分为两段(前 256 维使用 sin，后 256 维使用 cos)。</p>\n<p>那么我们如何加入到模型中去呢？fairseq 和 Transformer 都是直接把词向量和位置向量直接相加的方式，让人感到疑惑的是为什么不进行拼接？</p>\n<blockquote>\n<p>[W1 W2][e; p] = W1e + W2p，W(e+p)=We+Wp，就是说求和相当于拼接的两个权重矩阵共享(W1=W2=W)，所以拼接总是不会比相加差的。但是由于参数量的增加，其学习难度也会进一步上升。</p>\n</blockquote>\n<p>无论拼接还是相加，最终都要经过多头注意力的各个头入口处的线性变换，进行特征重新组合与降维，其实每一维都变成了之前所有维向量的线性组合。所以这个决定看上去是根据效果决定的，参数少效果好的相加自然成了模型的选择。</p>\n<h1 id=\"transformer-为什么使用-layer-normalization，而不是其他的归一化方法？\"><a href=\"#transformer-为什么使用-layer-normalization，而不是其他的归一化方法？\" class=\"headerlink\" title=\"transformer 为什么使用 layer normalization，而不是其他的归一化方法？\"></a>transformer 为什么使用 layer normalization，而不是其他的归一化方法？</h1><ul>\n<li><a href=\"https://www.zhihu.com/question/395811291/answer/1251829041\">回答</a></li>\n</ul>\n<p>回答中有一个观点我觉得挺好，就是简单来说，深度学习中的正则化方法就是“通过把一部分不重复的复杂信息损失掉，以此来降低拟合难度以及过拟合的风险，从而加速了模型的收敛”，Normalization目的就是为了让分布稳定下来（降低各维度数据的方差）。所以我们选择不同的Normalization方法，就是针对问题选择在对应维度上进行损失信息。</p>\n<p>而对于NLP任务中，同一batch之间的样本（即句子或者句子对之间）关联比较重要，本身我们就是为了通过大量样本对比学习句子中的语义结构，所以做batch Normalization效果不是很好，选择Layer Normalization对样本内部进行损失信息，反而能降低方差。在论文：<a href=\"https://arxiv.org/pdf/2003.07845.pdf\">Rethinking Batch Normalization in Transformers</a>中，作者对于Transformer中BN表现不好的原因做了一定的empirical和theoretical的分析，其中主要问题是在前向传播和反向传播中，batch统计量和其贡献的梯度都会呈现一定的不稳定性（如下图，在使用BN的Transformer训练过程中，每个batch的均值与方差一直震荡，偏离全局的running statistics）<img src=\"https://img-blog.csdnimg.cn/20210325225151514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>选择什么样的归一化方法，取决于你关注数据的那部分信息，如果某个维度信息的差异性很重要，需要被拟合，那就别在那个维度上进心Normalization。</p>\n","categories":["Deep-Learning"],"tags":["Transformer","NLP"]},{"title":"利器：TTS-Frontend中英Text-to-Phoneme-Converter，附代码","url":"/Deep-Learning/43c556e88dfe/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>NLP的语音合成中，有一种关键技术是将文字拆解成音素，再去语音库里匹配相同音素的语音片段，来实现文字转换语音。音素是给定语言的语音，如果与另一个音素交换，则会改变单词的含义，同时，音素是绝对的，并不是特定于任何语言，但只能参考特定语言讨论音素。由于音素的特性，非常适合用于语音合成领域。</p>\n<blockquote>\n<p>音素（phone），是语音中的最小的单位，依据音节里的发音动作来分析，一个动作构成一个音素。音素分为元音、辅音两大类。</p>\n</blockquote>\n<p>说白了，音素其实就是人在说话时，能发出最最最最短小、简洁的不能再分割的发音，不同的音素就是不同的短发音，可以组成不同的长发音，再组成词句形成语言。<br><img src=\"https://img-blog.csdnimg.cn/20201214120124692.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>本篇文章就来讲讲中文和英文中，如何将文本转换为音素序列。</p>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>众所周知，语音合成系统通常包含前端和后端两个模块。前端模块主要是对输入文本进行分析，提取后端模块所需要的语言学信息。前端模块一般包含文本正则化、分词、词性预测、多音字消歧、韵律预测等子模块。后端模块根据前端分析结果，通过一定的方法生成语音波形。后端模块一般分为基于统计参数建模的语音合成（Statistical Parameter Speech Synthesis，SPSS，以下简称参数合成），以及基于单元挑选和波形拼接的语音合成（以下简称拼接合成）两条技术主线。</p>\n<p>而“端到端”架构的语音合成系统的出现，能够直接从字符文本合成语音，打破了各个传统组件之间的壁垒，使得我们可以从<code>&lt;文本，声谱&gt;</code>配对的数据集上，完全随机从头开始训练。最具代表性的端到端语音合成系统，就是2017年初，Google 提出的端到端的语音合成系统——Tacotron</p>\n<p>从通俗一点的角度来讲，语音合成过程，需要处理两部分内容，分别是文本(Text)处理和音频(speech)处理，如下图是端到端的语音合成系统整体技术架构选型。我们文章要讲的就是属于TTS Frontend范畴的Text-to-Phoneme。<br><img src=\"https://img-blog.csdnimg.cn/20201214143030869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"文本规范化\"><a href=\"#文本规范化\" class=\"headerlink\" title=\"文本规范化\"></a>文本规范化</h1><p>对文本进行预处理，主要是去掉无用字符，全半角字符转化等，对于中文而言，有时候普通话文本中会出现简略词、日期、公式、号码等文本信息，这就需要通过文本规范化，对这些文本块进行处理以正确发音，比如：</p>\n<ul>\n<li>“小明体重是 128 斤”中的“128”应该规范为“一百二十八”，而“G128 次列车”中的“128” 应该规范为“一 二 八”</li>\n<li>“2016-05-15”、“2016 年 5 月 15 号”、“2016/05/15”可以统一为一致的发音</li>\n</ul>\n<p>对于英文而言，也需要将年份、货币、数字、字母等文本信息，转换为完整单词，比如：</p>\n<ul>\n<li>类别为年份（NYER）： 2011 → twenty eleven</li>\n<li>类别为货币(MONEY): £100 → one hundred pounds</li>\n<li>类别为非单词，需要拟音(ASWD): IKEA → apply letter-to-sound</li>\n<li>类别为数字(NUM) : 100 NUM → one hundred</li>\n<li>类别为字母(LSEQ) : DVD → dee vee dee</li>\n</ul>\n<p>这些文本规范化预处理不放在深度学习模型去学习，而是通过各种规则的正则表达式进行转换，所以涉及到的代码工作量还是比较大的，所以我将写好的代码更新至GitHub项目中，方便需要者使用（TensorFlow和PyTorch版本同步更新）：<a href=\"https://github.com/DengBoCong/nlp-paper/tree/master/paper-code/data\">NLP相关Paper笔记和代码复现</a>，这里粘贴一个作为举例，方便大家知道是啥。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_clean_number</span>(<span class=\"params\">text: <span class=\"built_in\">str</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    对句子中的数字相关进行统一单词转换</span></span><br><span class=\"line\"><span class=\"string\">    :param text: 单个句子文本</span></span><br><span class=\"line\"><span class=\"string\">    :return: 转换后的句子文本</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    comma_number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;([0-9][0-9\\,]+[0-9])&quot;</span>)</span><br><span class=\"line\">    decimal_number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;([0-9]+\\.[0-9]+)&quot;</span>)</span><br><span class=\"line\">    pounds_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;£([0-9\\,]*[0-9]+)&quot;</span>)</span><br><span class=\"line\">    dollars_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;\\$([0-9\\.\\,]*[0-9]+)&quot;</span>)</span><br><span class=\"line\">    ordinal_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;[0-9]+(st|nd|rd|th)&quot;</span>)</span><br><span class=\"line\">    number_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&quot;[0-9]+&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    text = re.sub(comma_number_re, <span class=\"keyword\">lambda</span> m: m.group(<span class=\"number\">1</span>).replace(<span class=\"string\">&#x27;,&#x27;</span>, <span class=\"string\">&#x27;&#x27;</span>), text)</span><br><span class=\"line\">    text = re.sub(pounds_re, <span class=\"string\">r&quot;\\1 pounds&quot;</span>, text)</span><br><span class=\"line\">    text = re.sub(dollars_re, _dollars_to_word, text)</span><br><span class=\"line\">    text = re.sub(decimal_number_re, <span class=\"keyword\">lambda</span> m: m.group(<span class=\"number\">1</span>).replace(<span class=\"string\">&#x27;.&#x27;</span>, <span class=\"string\">&#x27; point &#x27;</span>), text)</span><br><span class=\"line\">    text = re.sub(ordinal_re, <span class=\"keyword\">lambda</span> m: inflect.engine().number_to_words(m.group(<span class=\"number\">0</span>)), text)</span><br><span class=\"line\">    text = re.sub(number_re, _number_to_word, text)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> text</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"英文Text-to-Phoneme\"><a href=\"#英文Text-to-Phoneme\" class=\"headerlink\" title=\"英文Text-to-Phoneme\"></a>英文Text-to-Phoneme</h1><p>我们先来讲讲英文的Text-to-Phoneme，想要将英文单词转换成音素，想必需要了解Arpabet，下面是WiKi上的解释：</p>\n<blockquote>\n<p>ARPABET（也称为ARPAbet）是高级研究计划局（ARPA）在1970年代语音理解研究项目中开发的一组语音转录代码。 它代表具有不同ASCII字符序列的通用美国英语的音素和同音素。 </p>\n</blockquote>\n<p>可以理解为通过字母组合定义发音规则，英文发音是由元音辅音等组成的，同时，在元音后紧接着用数字表示压力， 辅助符号在1和2个字母的代码中相同， 在2个字母的符号中，段之间用空格隔开，大概如下这样：<br><img src=\"https://img-blog.csdnimg.cn/20201214160312984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>我们将文本转换为ARPABET并不需要我们了解很多语言学的东西，我们只需要选择现有合适的发音字典就可以了，使用比较多的还是CMU的发音词典，<a href=\"http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=off\">官方网站</a>，想了解原理的可以看<a href=\"http://www.cs.cmu.edu/~awb/papers/ESCA98_lts.pdf\">这篇论文</a>。可以先在官网体验一下是啥效果，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201214161122665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>自行从官网下载发音字典（也可以去我的github下载，链接在文章顶部），音素集，39个音素，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">Phoneme Example Translation</span><br><span class=\"line\">      ------- ------- -----------</span><br><span class=\"line\">      AA\todd     AA D</span><br><span class=\"line\">      AE\tat\tAE T</span><br><span class=\"line\">      AH\thut\tHH AH T</span><br><span class=\"line\">      AO\tought\tAO T</span><br><span class=\"line\">      AW\tcow\tK AW</span><br><span class=\"line\">      AY\thide\tHH AY D</span><br><span class=\"line\">      B \tbe\tB IY</span><br><span class=\"line\">      CH\tcheese\tCH IY Z</span><br><span class=\"line\">      D \tdee\tD IY</span><br><span class=\"line\">      DH\tthee\tDH IY</span><br><span class=\"line\">      EH\tEd\tEH D</span><br><span class=\"line\">      ER\thurt\tHH ER T</span><br><span class=\"line\">      EY\tate\tEY T</span><br><span class=\"line\">      F \tfee\tF IY</span><br><span class=\"line\">      G \tgreen\tG R IY N</span><br><span class=\"line\">      HH\the\tHH IY</span><br><span class=\"line\">      IH\tit\tIH T</span><br><span class=\"line\">      IY\teat\tIY T</span><br><span class=\"line\">      JH\tgee\tJH IY</span><br><span class=\"line\">      K \tkey\tK IY</span><br><span class=\"line\">      L \tlee\tL IY</span><br><span class=\"line\">      M \tme\tM IY</span><br><span class=\"line\">      N \tknee\tN IY</span><br><span class=\"line\">      NG\tping\tP IH NG</span><br><span class=\"line\">      OW\toat\tOW T</span><br><span class=\"line\">      OY\ttoy\tT OY</span><br><span class=\"line\">      P \tpee\tP IY</span><br><span class=\"line\">      R \tread\tR IY D</span><br><span class=\"line\">      S \tsea\tS IY</span><br><span class=\"line\">      SH\tshe\tSH IY</span><br><span class=\"line\">      T \ttea\tT IY</span><br><span class=\"line\">      TH\ttheta\tTH EY T AH</span><br><span class=\"line\">      UH\thood\tHH UH D</span><br><span class=\"line\">      UW\ttwo\tT UW</span><br><span class=\"line\">      V \tvee\tV IY</span><br><span class=\"line\">      W \twe\tW IY</span><br><span class=\"line\">      Y \t<span class=\"keyword\">yield</span>\tY IY L D</span><br><span class=\"line\">      Z \tzee\tZ IY</span><br><span class=\"line\">      ZH\tseizure\tS IY ZH ER</span><br></pre></td></tr></table></figure>\n<p>有了音素集之后就可以使用脚本将文本转换为音素了，我的转换脚本如下:（完整代码同GitHub可找到）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">text_to_phonemes_converter</span>(<span class=\"params\">text: <span class=\"built_in\">str</span>, cmu_dict_path: <span class=\"built_in\">str</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    将句子按照CMU音素字典进行分词切分</span></span><br><span class=\"line\"><span class=\"string\">    :param text: 单个句子文本</span></span><br><span class=\"line\"><span class=\"string\">    :param cmu_dict_path: cmu音素字典路径</span></span><br><span class=\"line\"><span class=\"string\">    :return: 按照音素分词好的数组</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    _, symbols_set = get_phoneme_dict_symbols()</span><br><span class=\"line\"></span><br><span class=\"line\">    alt_re = re.<span class=\"built_in\">compile</span>(<span class=\"string\">r&#x27;\\([0-9]+\\)&#x27;</span>)</span><br><span class=\"line\">    cmu_dict = &#123;&#125;</span><br><span class=\"line\">    text = _clean_text(text)</span><br><span class=\"line\">    text = re.sub(<span class=\"string\">r&quot;([?.!,])&quot;</span>, <span class=\"string\">r&quot; \\1&quot;</span>, text)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 文件是从官网下载的，所以文件编码格式要用latin-1</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(cmu_dict_path, <span class=\"string\">&#x27;r&#x27;</span>, encoding=<span class=\"string\">&#x27;latin-1&#x27;</span>) <span class=\"keyword\">as</span> cmu_file:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> cmu_file:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) <span class=\"keyword\">and</span> (line[<span class=\"number\">0</span>] &gt;= <span class=\"string\">&quot;A&quot;</span> <span class=\"keyword\">and</span> line[<span class=\"number\">0</span>] &lt;= <span class=\"string\">&quot;Z&quot;</span> <span class=\"keyword\">or</span> line[<span class=\"number\">0</span>] == <span class=\"string\">&quot;&#x27;&quot;</span>):</span><br><span class=\"line\">                parts = line.split(<span class=\"string\">&#x27;  &#x27;</span>)</span><br><span class=\"line\">                word = re.sub(alt_re, <span class=\"string\">&#x27;&#x27;</span>, parts[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># 这里要将非cmu音素的干扰排除</span></span><br><span class=\"line\">                pronunciation = <span class=\"string\">&quot; &quot;</span></span><br><span class=\"line\">                temps = parts[<span class=\"number\">1</span>].strip().split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">for</span> temp <span class=\"keyword\">in</span> temps:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> temp <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> symbols_set:</span><br><span class=\"line\">                        pronunciation = <span class=\"literal\">None</span></span><br><span class=\"line\">                        <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> pronunciation:</span><br><span class=\"line\">                    pronunciation = <span class=\"string\">&#x27; &#x27;</span>.join(temps)</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> word <span class=\"keyword\">in</span> cmu_dict:</span><br><span class=\"line\">                        cmu_dict[word].append(pronunciation)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        cmu_dict[word] = [pronunciation]</span><br><span class=\"line\"></span><br><span class=\"line\">    cmu_result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> text.split(<span class=\"string\">&#x27; &#x27;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 因为同一个单词，它的发音音素可能不一样，所以存在多个</span></span><br><span class=\"line\">        <span class=\"comment\"># 音素分词，我这里就单纯的取第一个，后面再改进和优化</span></span><br><span class=\"line\">        cmu_word = cmu_dict.get(word.upper(), [word])[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> cmu_word != word:</span><br><span class=\"line\">            cmu_result.append(<span class=\"string\">&quot;&#123;&quot;</span> + cmu_word + <span class=\"string\">&quot;&#125;&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            cmu_result.append(cmu_word)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot; &quot;</span>.join(cmu_result)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"中文Text-to-Phoneme\"><a href=\"#中文Text-to-Phoneme\" class=\"headerlink\" title=\"中文Text-to-Phoneme\"></a>中文Text-to-Phoneme</h1><p>对于中文，其实我们再熟悉不过了，中文的音素其实就是汉语拼音的最小单元，包括声母，韵母，但是其中还会有一些整体认读音节，更详细的拼音标注文本分析，可以参考<a href=\"https://mtts.readthedocs.io/zh_CN/latest/text_analyse.html\">MTTS文本分析</a>。同一个字在不同分词情况下的发音不同，所以导致数量级比较大，比较典型的可以参见清华大学的<a href=\"https://www.openslr.org/resources.php\">thchs30</a>中文数据集，里面提供了分词好的音素字典，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">SIL sil</span><br><span class=\"line\">&lt;SPOKEN_NOISE&gt; sil</span><br><span class=\"line\">啊 aa a1</span><br><span class=\"line\">啊 aa a2</span><br><span class=\"line\">啊 aa a4</span><br><span class=\"line\">啊 aa a5</span><br><span class=\"line\">啊啊啊 aa a2 aa a2 aa a2</span><br><span class=\"line\">啊啊啊 aa a5 aa a5 aa a5</span><br><span class=\"line\">阿 aa a1</span><br><span class=\"line\">阿 ee e1</span><br><span class=\"line\">阿尔 aa a1 ee er3</span><br><span class=\"line\">阿根廷 aa a1 g en1 t ing2</span><br><span class=\"line\">阿九 aa a1 j iu3</span><br><span class=\"line\">阿克 aa a1 k e4</span><br><span class=\"line\">阿拉伯数字 aa a1 l a1 b o2 sh u4 z iy4</span><br><span class=\"line\">阿拉法特 aa a1 l a1 f a3 t e4</span><br></pre></td></tr></table></figure>\n<p>对应脚本在<a href=\"https://github.com/X-CCS/mandarin_tacotron_GL/blob/master/datasets/thchs30.py\">这里</a>，不过其实汉子转拼音还有更方便的方式，就是使用Python 的拼音库 PyPinyin，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pypinyin <span class=\"keyword\">import</span> pinyin</span><br><span class=\"line\">print(pinyin(<span class=\"string\">&#x27;朝阳&#x27;</span>))</span><br><span class=\"line\"><span class=\"comment\"># [[&#x27;zhāo&#x27;], [&#x27;yáng&#x27;]]</span></span><br></pre></td></tr></table></figure>\n<p> PyPinyin可以用于汉字注音、排序、检索等等场合，是基于 hotto/pinyin 这个库开发的，一些站点链接如下：</p>\n<ul>\n<li><a href=\"https://github.com/mozillazg/python-pinyin\">GitHub</a> </li>\n<li><a href=\"https://pypinyin.readthedocs.io/zh_CN/master/\">文档</a></li>\n<li><a href=\"https://pypi.org/project/pypinyin/\">PyPi</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","NLP","TensorFlow","Pytorch","语音合成","音素"]},{"title":"单纯为了实用，创建线程的几种方法","url":"/Java/625ffa0afe53/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得的点个关注和点个赞，感谢支持。</strong><br>原始的创建线程的方法有三种，分别是：</p>\n<ul>\n<li>继承 Thread 类</li>\n<li>实现 Runnable 接口</li>\n<li>实现 Callable 接口</li>\n</ul>\n<p>这篇博文就单纯的针对这三种接口，讲解如何使用，旨在使用，并不深入讲解各种原理乃至讲解线程相关，嘿嘿嘿。</p>\n<h2 id=\"继承-Thread-类\"><a href=\"#继承-Thread-类\" class=\"headerlink\" title=\"继承 Thread 类\"></a>继承 Thread 类</h2><p>这是一种最为原始的方式，继承 Thread 类，重写 run 方法以实现线程功能。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;hello.&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码是指，我创建了一个名为 <code>HelloThread</code> 的类，该类由于继承 <code>Thread</code> 类因而是一个线程类，它重写了父类的 <code>run</code> 方法，打印了一句话：【<code>hello.</code>】。使用时也异常简单：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Thread thread = <span class=\"keyword\">new</span> HelloThread();</span><br><span class=\"line\">thread.start();</span><br></pre></td></tr></table></figure>\n<p>首先实例化一个线程出来，然后启动线程，当线程启动之后，控制台上打印了一句话：【<code>hello.</code>】。一般情况下不要使用这种方式来创建线程，因为通过继承来实现实在是太臃肿了，很多场景下我们只需要让线程跑起来，实现某个功能（即重写 run 方法），但是继承会实现 Thread 类的全部信息，性能消耗太大。而且 Java 是单继承的，继承了 Thread 类就不能继承其他类了。</p>\n<h2 id=\"实现-Runnable-接口\"><a href=\"#实现-Runnable-接口\" class=\"headerlink\" title=\"实现 Runnable 接口\"></a>实现 Runnable 接口</h2><p><code>Runnable</code> 接口是一切线程创建的根源，其实上面【继承 <code>Thread</code> 类】的途径，也是间接使用了本途径来创建线程的。比较传统的实现 <code>Runnable</code> 接口的方式是，创建一个类，该类 <code>implements Runnable</code> 来实现 <code>Runnable</code> 接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloRunnable</span> <span class=\"keyword\">implements</span> <span class=\"title\">Runnable</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>但是创建一个类，未免太大张旗鼓了些，还要新建一个类，设置好类名，实现接口，之后再实例化，兴师动众。其实实例化对象并不需要创建一个类出来，实现接口就行，用匿名内部类。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Runnable helloRunnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>此外，Runnable 接口是一个函数式接口，只定义了 run 方法，可以使用 lambda 表达式的方式来实例化，那就更简单了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Runnable HelloRunnable = () -&gt; &#123;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>上面三种实现，都只是写了外壳，里面没有写具体的实现过程，具体的实现是要重写 <code>Runnable</code> 接口的 <code>run</code> 方法的。我写了三种实现 <code>Runnable</code> 接口的代码，第一种最容易懂，后面两种如果有困惑，看一看 lambda 表达式就能理解了。实现了 <code>Runnable</code> 接口之后，把它作为参数，放进 Thread 的构造方法里就可以了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Thread thread = <span class=\"keyword\">new</span> Thread(runnable);</span><br><span class=\"line\">thread.start();</span><br></pre></td></tr></table></figure>\n<p>这样就可以了。要不再完整地走一遍？</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 实现 Runnable 接口，重写 run 方法 （这里使用匿名内部类的方式，即上面的第二种）</span></span><br><span class=\"line\">Runnable runnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;by runnable&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 创建线程并开启</span></span><br><span class=\"line\">Thread thread = <span class=\"keyword\">new</span> Thread(runnable);</span><br><span class=\"line\">thread.start();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 控制台上会打印出这样一句话：by runnable</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"实现-Callable-接口\"><a href=\"#实现-Callable-接口\" class=\"headerlink\" title=\"实现 Callable 接口\"></a>实现 Callable 接口</h2><p>以上两种方式，都没有任何的返回值，线程执行动作，执行完就结束了，无声无息。实现 <code>Callable</code> 接口的目的，就是为了让线程执行完之后，能返回信息。简单对比一下，<code>Runnable</code> 接口和 <code>Callable</code> 接口，在代码上的区别：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 实现 Runnable 接口</span></span><br><span class=\"line\">Runnable runnable = <span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// ...</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 实现 Callable 接口</span></span><br><span class=\"line\">Callable callable = <span class=\"keyword\">new</span> Callable() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Object <span class=\"title\">call</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// ...</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>你会发现，实现两个接口都只需要重写一个方法：</p>\n<ul>\n<li>实现 Runnable 接口需要重写【没有返回值】的 run 方法</li>\n<li>实现 Callable 接口需要实现【返回一个对象】的 call 方法。</li>\n</ul>\n<p>其他的地方，在用法上仿佛没有什么不同。实际上，Callable 接口还支持泛型，你可以指定返回值的数据类型：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 指定返回 String 类型</span></span><br><span class=\"line\">Callable&lt;String&gt; callable = <span class=\"keyword\">new</span> Callable&lt;String&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">call</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// ...</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>传统的线程设计，是没有返回值的概念的，因此没办法用线程类来获得返回值。JUC 包设计了一个新的接口：<code>Future</code>，来接收线程的返回值（和其他的功能）。<code>Future</code> 类是一个接口，无法直接实例化，因此又设计了一个名为 <code>FutureTask</code> 的类，该类实现了<code>Future</code> 接口和 <code>Runnable</code> 接口，打通了【线程功能】和【返回值功能】。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">FutureTask futureTask = <span class=\"keyword\">new</span> FutureTask(callable);</span><br><span class=\"line\">Thread thread = <span class=\"keyword\">new</span> Thread(futureTask);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>上面这两行代码，是将刚才写好的 <code>callable</code> 对象，放进 <code>futureTask</code> 中，辗转放进线程中。你可以感受到，<code>FutureTask</code> 类是一个中介，它也支持泛型（不过上面这两行代码没写泛型）。<code>FutureTask</code> 类有一个 <code>get</code> 方法，用于获取 <code>callable</code> 的返回值。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Object returnStr = futureTask.get();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>（如果指定了 <code>FutureTask</code> 的泛型，上面还可以更确切地指定数据类型，例如把上面代码的 <code>Object</code> 改成 <code>String</code>）</p>\n<p>这个 <code>get</code> 方法需要处理两类异常：<code>InterruptedException</code> 和 <code>ExecutionException</code>。</p>\n","categories":["Java"],"tags":["Java","线程"]},{"title":"多种方式轻松搞定SpringBoot部署Docker","url":"/Spring-Boot/f90f7de884de/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得点个关注和点个赞哦，非常感谢支持</strong><br>在部署应用程序时，Spring Boot的灵活打包选项提供了很多选择。可以将Spring Boot应用程序部署到各种云平台，容器映像（例如Docker）或虚拟机/真实机上。这里我们就来探讨SpringBoot如果部署到Docker中。</p>\n<p>我们都知道，在对 Kubernetes 微服务实践过程中，接触最多的肯定莫过于 Docker 镜像。Kubernetes是啥，我这里简单说明一下，Kubernetes 微服务简单说就是一群镜像间的排列组合与相互间调的关系，故而如何编译镜像会使服务性能更优，使镜像构建、推送、拉取速度更快，使其占用网络资源更少。更详细的可以自行查阅，这里就不做更详细的解释。</p>\n<p>@[TOC]</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>这一篇文章，想着是主要把探讨的内容放在SpringBoot和Docker的结合上，所以不想用过多的篇幅讲解Docker安装以及SpringBoot的普通应用构建上，不过这些都是我们对本篇文章进行讲解之前的准备工作，所以我在这里贴出我之前写过的文章，如果没有安装Docker以及不知道怎么构建SpringBoot的普通应用的朋友，可以先跳转过去看，个人认为写的挺详细的。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/103864834\">Docker安装教程</a>：这篇文章把Ubuntu、CentOS、Windows主流系统的安装方法详细的讲解了一遍，包括如何配置镜像加速等内容。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/103915632\">Docker新手宝典（必备）</a>：因为后面要将的内容要构建DockerFile，所以如果还不知道DockerFile是啥的朋友，可以看一下这篇文章。<br><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a>：这篇文章看完，就可以构建一个简单的hello world应用了。</p>\n<ul>\n<li>一个简单的SpringBoot2.x程序，里面就单纯的创建一个Controller控制器，可以访问<a href=\"http://localhost/index%EF%BC%8C%E5%A6%82%E4%B8%8B\">http://localhost/index，如下</a><br><img src=\"https://img-blog.csdnimg.cn/20200327105143301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>Docker我使用的是19版本</li>\n<li>我的服务器用的是CentOS7系统<h2 id=\"手动使用DockerFile构建\"><a href=\"#手动使用DockerFile构建\" class=\"headerlink\" title=\"手动使用DockerFile构建\"></a>手动使用DockerFile构建</h2>这里解释一下Dockfile，Dockfile是一种被Docker程序解释的脚本，Dockerfile由一条一条的指令组成，每条指令对应Linux下面的一条命令。Docker程序将这些Dockerfile指令翻译真正的Linux命令。Dockerfile有自己书写格式和支持的命令，Docker程序解决这些命令间的依赖关系，类似于Makefile。Docker程序将读取Dockerfile，根据指令生成定制的image。相比image这种黑盒子，Dockerfile这种显而易见的脚本更容易被使用者接受，它明确的表明image是怎么产生的。有了Dockerfile，当我们需要定制自己额外的需求时，只需在Dockerfile上添加或者修改指令，重新生成image即可，省去了敲命令的麻烦。</li>\n</ul>\n<p>首先，我们把上面的SpringBoot应用打包，使用Maven指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">mvn <span class=\"keyword\">package</span></span><br></pre></td></tr></table></figure>\n<p>当然，你如果使用的是Idea作为dev的话，可以通过Maven工具打包，不需要输入指令，如下<br><img src=\"https://img-blog.csdnimg.cn/20200327105533786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这样就会将项目打包好了jar包，我这里打包的jar包改名为<code>example.jar</code>，更直观一点，然后接着我们到目标服务器上（注意了，服务器上要已经安装好了Docker），然后随便找个目录创建DockerFile文件，因为我用的是CentOS7，习惯在<code>/var/tmp</code>下创建，创建Dockerfile文件指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">touch Dockerfile</span><br></pre></td></tr></table></figure>\n<p>注意力，Dockerfile必须和Jar包在同一路径下，所以你在上传jar包的时候，要注意了。下面贴出DockerFile的内容，只是用最基本的构建指令，更复杂的可以熟悉之后，自行查阅DockerFile指令进行构建</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">FROM java:<span class=\"number\">8</span></span><br><span class=\"line\">EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\">VOLUME /slm</span><br><span class=\"line\">ADD example.jar boot-docker.jar</span><br><span class=\"line\">RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\">ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br></pre></td></tr></table></figure>\n<p>这里稍微解释一下基本语法</p>\n<ul>\n<li>FROM 基础镜像必要，代表你的项目将构建在这个基础上面</li>\n<li>EXPOSE 允许指定端口转发</li>\n<li>VOLUME 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。</li>\n<li>ADD 将文件从路径 复制添加到容器内部路径 支持远程url 如果是远程url权限将会是600，我这里因为直接上传了，所以就服务器本机就可以了</li>\n<li>ENV 可以用于为docker容器设置环境变量</li>\n<li>ENTRYPOINT 指定 Docker image 运行成 instance (也就是 Docker container) 时，要执行的命令或者文件。</li>\n<li>CMD 和 ENTRYPOINT 都能用来指定开始运行的程序，而且这两个命令都有两种不用的语法：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">CMD [<span class=\"string\">&quot;ls&quot;</span>,<span class=\"string\">&#x27;&#x27;</span>-l<span class=\"string\">&quot;]</span></span><br><span class=\"line\"><span class=\"string\">CMD ls -l</span></span><br></pre></td></tr></table></figure>\n<p>编写好了DockerFile之后，我们开始构建镜像，指令如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker build -t boot-docker .</span><br></pre></td></tr></table></figure>\n<p><code>-t boot-docker</code> 代表你要构建的名字</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Sending build context to Docker daemon  <span class=\"number\">16.</span>81MB</span><br><span class=\"line\">Step <span class=\"number\">1</span>/<span class=\"number\">7</span> : FROM java:<span class=\"number\">8</span></span><br><span class=\"line\"> ---&gt; d23bdf5b1b1b</span><br><span class=\"line\">Step <span class=\"number\">2</span>/<span class=\"number\">7</span> : EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b2445bf62da8</span><br><span class=\"line\">Step <span class=\"number\">3</span>/<span class=\"number\">7</span> : VOLUME /slm</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b73d0b73b868</span><br><span class=\"line\">Step <span class=\"number\">4</span>/<span class=\"number\">7</span> : ADD example.jar boot-docker.jar</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 2b4868aafca9</span><br><span class=\"line\">Step <span class=\"number\">5</span>/<span class=\"number\">7</span> : RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 816b59f199af</span><br><span class=\"line\">Step <span class=\"number\">6</span>/<span class=\"number\">7</span> : ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 784f033b9dd6</span><br><span class=\"line\">Step <span class=\"number\">7</span>/<span class=\"number\">7</span> : ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 92a0da91ea19</span><br><span class=\"line\">Successfully built 92a0da91ea19</span><br><span class=\"line\">Successfully tagged bootdocker:latest</span><br></pre></td></tr></table></figure>\n<p>我们可以看到已经构建完成，<code>Successfully built 92a0da91ea19</code>这句话指明了刚刚构建的镜像ID现在我们可以根据这个ID来进行操作。输入run命令来启动。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker run -itd -p <span class=\"number\">8080</span>:<span class=\"number\">8080</span> --name example 92a0da91ea19</span><br></pre></td></tr></table></figure>\n<ul>\n<li>-d 表示后台运行</li>\n<li>-p映射端口</li>\n<li>–name 容器名称</li>\n</ul>\n<p>已经运行成功访问接口。注意这里因为映射到了宿主机的端口所以访问的是宿主机的IP加端口，比如<a href=\"http://ip:8080/index\">http://ip:8080/index</a></p>\n<h2 id=\"使用Maven构建\"><a href=\"#使用Maven构建\" class=\"headerlink\" title=\"使用Maven构建\"></a>使用Maven构建</h2><p>上面说了使用Dockerfile构建，现在使用Maven来构建，我们还是使用上面的DockerFile内容，我们在项目的目录下创建DockerFile，把上面DockerFIle内容复制过去，结构如下<br><img src=\"https://img-blog.csdnimg.cn/20200327123355916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在pom中加入docker构建依赖</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t&lt;!--新增的docker maven插件--&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;com.spotify&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.4.13&lt;/version&gt;</span><br><span class=\"line\">\t&lt;!--execution 节点中配置当执行 mvn <span class=\"keyword\">package</span> 的时候，顺便也执行一下 docker:build--&gt;</span><br><span class=\"line\">\t&lt;executions&gt;</span><br><span class=\"line\">        &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;build-image&lt;/id&gt;</span><br><span class=\"line\">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">                &lt;goal&gt;build&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">        &lt;/execution&gt;</span><br><span class=\"line\">    &lt;/executions&gt;</span><br><span class=\"line\">\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t&lt;!--Docker 的主机地址--&gt;</span><br><span class=\"line\">\t\t&lt;dockerHost&gt;http:<span class=\"comment\">//192.168.66.131:2375&lt;/dockerHost&gt;</span></span><br><span class=\"line\">\t\t&lt;!--镜像名字--&gt;</span><br><span class=\"line\">\t\t&lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt;</span><br><span class=\"line\">\t\t&lt;!--DokcerFile文件地址--&gt;</span><br><span class=\"line\">\t\t&lt;dockerDirectory&gt;$&#123;project.basedir&#125;&lt;/dockerDirectory&gt;</span><br><span class=\"line\">\t\t&lt;!--镜像的 tags--&gt;</span><br><span class=\"line\">\t\t&lt;imageTags&gt;</span><br><span class=\"line\">            &lt;imageTag&gt;$&#123;project.version&#125;&lt;/imageTag&gt;</span><br><span class=\"line\">        &lt;/imageTags&gt;</span><br><span class=\"line\">\t\t&lt;resources&gt;</span><br><span class=\"line\">\t\t\t&lt;resource&gt;</span><br><span class=\"line\">\t\t\t\t&lt;targetPath&gt;/&lt;/targetPath&gt;</span><br><span class=\"line\">\t\t\t\t&lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt;</span><br><span class=\"line\">\t\t\t\t&lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt;</span><br><span class=\"line\">\t\t\t&lt;/resource&gt;</span><br><span class=\"line\">\t\t&lt;/resources&gt;</span><br><span class=\"line\">\t&lt;/configuration&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure>\n<p>注意dockerDirectory还是要设置Dockerfile文件的路径，然后如果你不配置<code>dockerHost</code>的话，要把项目复制到Linux主机中，执行解压命令</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">unzip boot-docker.zip</span><br><span class=\"line\">cd boot-docker</span><br></pre></td></tr></table></figure>\n<p>执行命令：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">mvn <span class=\"keyword\">package</span> docker:build</span><br></pre></td></tr></table></figure>\n<p>而如果配置了<code>dockerHost</code>的话<br><img src=\"https://img-blog.csdnimg.cn/20200327105533786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>上面两种方式都开始构建build</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">[INFO] Building image boot-docker/boot-docker</span><br><span class=\"line\">Step <span class=\"number\">1</span>/<span class=\"number\">7</span> : FROM java:<span class=\"number\">8</span></span><br><span class=\"line\"> ---&gt; d23bdf5b1b1b</span><br><span class=\"line\">Step <span class=\"number\">2</span>/<span class=\"number\">7</span> : EXPOSE <span class=\"number\">8080</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b2445bf62da8</span><br><span class=\"line\">Step <span class=\"number\">3</span>/<span class=\"number\">7</span> : VOLUME /slm</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; b73d0b73b868</span><br><span class=\"line\">Step <span class=\"number\">4</span>/<span class=\"number\">7</span> : ADD boot-docker-<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT.jar boot-docker.jar</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 2b4868aafca9</span><br><span class=\"line\">Step <span class=\"number\">5</span>/<span class=\"number\">7</span> : RUN sh -c <span class=\"string\">&#x27;touch /boot-docker.jar&#x27;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 816b59f199af</span><br><span class=\"line\">Step <span class=\"number\">6</span>/<span class=\"number\">7</span> : ENV JAVA_OPTS=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 784f033b9dd6</span><br><span class=\"line\">Step <span class=\"number\">7</span>/<span class=\"number\">7</span> : ENTRYPOINT [ <span class=\"string\">&quot;sh&quot;</span>, <span class=\"string\">&quot;-c&quot;</span>, <span class=\"string\">&quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /boot-docker.jar&quot;</span> ]</span><br><span class=\"line\"> ---&gt; Using cache</span><br><span class=\"line\"> ---&gt; 92a0da91ea19</span><br><span class=\"line\">ProgressMessage&#123;id=<span class=\"keyword\">null</span>, status=<span class=\"keyword\">null</span>, stream=<span class=\"keyword\">null</span>, error=<span class=\"keyword\">null</span>, progress=<span class=\"keyword\">null</span>, progressDetail=<span class=\"keyword\">null</span>&#125;</span><br><span class=\"line\">Successfully built 92a0da91ea19</span><br><span class=\"line\">Successfully tagged boot-docker/boot-docker:latest</span><br><span class=\"line\">[INFO] Built boot-docker/boot-docker</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time: <span class=\"number\">19.</span>908s</span><br><span class=\"line\">[INFO] Finished at: Wed Jul <span class=\"number\">10</span> <span class=\"number\">16</span>:<span class=\"number\">00</span>:<span class=\"number\">21</span> CST <span class=\"number\">2019</span></span><br><span class=\"line\">[INFO] Final Memory: 35M/86M</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n<p>接着就可以启动容器了</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker run -itd -p <span class=\"number\">8080</span>:<span class=\"number\">8080</span> --name example 92a0da91ea19</span><br></pre></td></tr></table></figure>\n<p>已经运行成功访问接口。注意这里因为映射到了宿主机的端口所以访问的是宿主机的IP加端口，比如<a href=\"http://ip:8080/index\">http://ip:8080/index</a></p>\n<h2 id=\"使用Idea部署\"><a href=\"#使用Idea部署\" class=\"headerlink\" title=\"使用Idea部署\"></a>使用Idea部署</h2><p>沿用在Maven构建中的pom中的配置，此时我们的 IDEA 中多了一个选项，就是 docker，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020032712400069.png#pic_center\" alt=\"在这里插入图片描述\"><br>点击左边的绿色启动按钮，连接上 Docker 容器，连接成功之后，我们就可以看到目前 Docker 中的所有容器和镜像了，当然也包括我们刚刚创建的 Docker 镜像，如下：<br><img src=\"https://img-blog.csdnimg.cn/20200327124116922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>此时，我们选中这个镜像，右键单击，即可基于此镜像创建出一个容器，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200327124154996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们选择 Create container，然后填入容器的一些必要信息，配置一下容器名称，镜像 ID 会自动填上，暴露的端口使用 Specify 即可，然后写上端口的映射关系：<br><img src=\"https://img-blog.csdnimg.cn/20200327124229736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>配置完成后，点击下方的 run 按钮，就可以开始运行了。</p>\n<p>应用容器化是近年来的热点。而且容器技术层出不穷，掌握应用的容器化技术还是很有必要的。今天我们一步一步从零利用 Docker 构建了一个 Spring Boot 容器 。希望对你有所帮助。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Docker","Maven"]},{"title":"如何理解TensorFlow计算图？","url":"/Deep-Learning/ad4aecf69cbb/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于TensorFlow或者Pytorch，可能很多小伙伴已经使用它执行了很多模型任务了，但是回过头来仔细想想，对于这些计算框架中的计算图可能还一知半解，没有好好理解研究过，这篇文章就来捋一捋计算图，这毕竟是Tensorflow和Pytorch这样的深度学习计算框架非常重要的概念。</p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>无论是机器学习也好，还是深度学习也好，都是围绕着数学建模以及数学运算进行的，在这样的背景之下，诞生了许多的计算框架，我们所熟知的TensorFlow和Pytorch就是其中的主流，这些计算框架以计算服务为根本，自然需要一个计算模型。如果接触过开发的小伙伴就能体会到，这些计算框架的编程方式有着很大的差异。无论是编译类型的语言还是脚本语言，都是一步一步的计算变量， 从而得到结果，比如<code>result = input1 + input2</code>，当执行完语句后，就会得到<code>result</code>的值。</p>\n<p>而TensorFlow和Pytorch则不一样，首先需要通过编程构建一个计算图，然后将数据作为输入，通过这个计算图规定的计算操作进行计算，最后得到计算结果。这种符号式编程有着较多的嵌入和优化，性能也随之提升。同时计算图非常适合用来思考数学表达式，举个例子，比如计算 $e=(a+b)*(b+1)$，在这个式子中存在两个加法和一个乘法的运算，为了更加方便我们讨论，我们引入中间变量来给每个运算的输出表示为一个变量，如下：<br>$$c=a+b$$   $$d=b+1$$   $$e=c∗d$$<br>接下来，我们来构建计算图，我们将所有这些操作放入节点中，并同时计算出计算结果，如下：<br><img src=\"https://pic4.zhimg.com/80/v2-afe67feb6df30d1fe6e7a18caa288ee7_720w.jpg\" alt=\"在这里插入图片描述\"><br>我们可以清晰的看到运算表达式中，各个运算操作以及变量间的依赖和调用关系。 接着我们来求边的偏导数，如下：<br><img src=\"https://pic3.zhimg.com/80/v2-dd6e59e2939393d595fb0d145014f9be_720w.jpg\" alt=\"在这里插入图片描述\"><br>通过链式法则，我们逐节点的计算偏导数，在网络backward时候，需要用链式求导法则求出网络最后输出的梯度，然后再对网络进行优化。类似上图的表达形式就是TensorFlow以及Pytorch的基本计算模型。<strong>总结而言，计算图模型由节点(nodes)和线(edges)组成，节点表示操作符Operator，或者称之为算子，线表示计算间的依赖，实线表示有数据传递依赖，传递的数据即张量，虚线通常可以表示控制依赖，即执行先后顺序。</strong></p>\n<p>计算图从本质上来说，是TensorFlow在内存中构建的程序逻辑图，计算图可以被分割成多个块，并且可以并行地运行在多个不同的cpu或gpu上，这被称为并行计算。因此，计算图可以支持大规模的神经网络，如下：<br><img src=\"https://pic4.zhimg.com/80/v2-da6151da56abe898e72115915e76f603_720w.jpg\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Tensorflow中的计算图\"><a href=\"#Tensorflow中的计算图\" class=\"headerlink\" title=\"Tensorflow中的计算图\"></a>Tensorflow中的计算图</h1><p>TensorFlow中的计算图有三种，分别是静态计算图，动态计算图，以及Autograph，目前TensorFlow2默认采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果（在TensorFlow1中，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图）。对于动态图的好处显而易见，它方便调试程序，让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的，当然，这相对于静态图来讲牺牲了些效率，因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信，而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>\n<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码，使用tf.function构建静态图的方式叫做 Autograph。</p>\n<ul>\n<li>静态计算图：一种比较早先使用静态计算图的方法分两步，第一步定义计算图，第二步在会话中执行计算图，如下展示了TensorFlow1.0和TensorFlow2.0的写法（可以调用tf.global_variables_initializer去初始化变量或者通过tf.control_dependencies去执行计算图中没有包含的节点）：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># TensorFlow1.0</span></span><br><span class=\"line\"><span class=\"comment\">#定义计算图</span></span><br><span class=\"line\">g = tf.Graph()</span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    <span class=\"comment\">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class=\"line\">    x = tf.placeholder(name=<span class=\"string\">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)  </span><br><span class=\"line\">    y = tf.placeholder(name=<span class=\"string\">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    z = tf.string_join([x,y],name = <span class=\"string\">&#x27;join&#x27;</span>,separator=<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#执行计算图</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session(graph = g) <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class=\"string\">&quot;hello&quot;</span>,y:<span class=\"string\">&quot;world&quot;</span>&#125;))</span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"comment\"># TensorFlow2.0</span></span><br><span class=\"line\">g = tf.compat.v1.Graph()</span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    x = tf.compat.v1.placeholder(name=<span class=\"string\">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    y = tf.compat.v1.placeholder(name=<span class=\"string\">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class=\"line\">    z = tf.strings.join([x,y],name = <span class=\"string\">&quot;join&quot;</span>,separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.compat.v1.Session(graph = g) <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    <span class=\"comment\"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class=\"line\">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class=\"string\">&quot;hello&quot;</span>,y:<span class=\"string\">&quot;world&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>动态计算图：动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行，因此称之为 Eager Excution。对于上面的操作，我们可以直接如下面代码的第一部分那样直接使用，也可以将使用动态计算图代码的输入和输出关系封装成函数，如下：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 第一部分</span></span><br><span class=\"line\"><span class=\"comment\"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class=\"line\">x = tf.constant(<span class=\"string\">&quot;hello&quot;</span>)</span><br><span class=\"line\">y = tf.constant(<span class=\"string\">&quot;world&quot;</span>)</span><br><span class=\"line\">z = tf.strings.join([x,y],separator=<span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第二部分</span></span><br><span class=\"line\"><span class=\"comment\"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strjoin</span>(<span class=\"params\">x,y</span>):</span></span><br><span class=\"line\">    z =  tf.strings.join([x,y],separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">    tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> z</span><br><span class=\"line\">result = strjoin(tf.constant(<span class=\"string\">&quot;hello&quot;</span>),tf.constant(<span class=\"string\">&quot;world&quot;</span>))</span><br><span class=\"line\">print(result) <span class=\"comment\"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Autograph：动态计算图运行效率相对较低，可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。不需要使用会话了，一切都像原始的Python语法一样自然。<strong>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率</strong>，如下（这就是为什么我们上面第二部分封装成函数的原因）：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用autograph构建静态图</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@tf.function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">strjoin</span>(<span class=\"params\">x,y</span>):</span></span><br><span class=\"line\">    z =  tf.strings.join([x,y],separator = <span class=\"string\">&quot; &quot;</span>)</span><br><span class=\"line\">    tf.print(z) <span class=\"comment\"># hello world</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> z</span><br><span class=\"line\">result = strjoin(tf.constant(<span class=\"string\">&quot;hello&quot;</span>),tf.constant(<span class=\"string\">&quot;world&quot;</span>))</span><br><span class=\"line\">print(result) <span class=\"comment\"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"tf-function\"><a href=\"#tf-function\" class=\"headerlink\" title=\"@tf.function\"></a>@tf.function</h2><p>需要注意的是不是所有的函数都可以通过tf.function进行加速的，有的任务并不值得将函数转化为计算图形式，比如简单的矩阵乘法，然而，对于大量的计算，如对深度神经网络的优化，这一图转换能给性能带来巨大的提升。我们也把这样的图转化叫作tf.AutoGraph，在Tensorflow 2.0中，会自动的对被@tf.function装饰的函数进行AutoGraph优化。下面我们来看看被tf.function装饰的函数第一次执行时都做了什么：</p>\n<ul>\n<li>函数被执行并且被跟踪(tracing)，Eager execution处于关闭状态，所有的Tensorflow函数被当做tf.Operation进行图的创建。</li>\n<li>AutoGraph被唤醒，去检测Python代码可以转为Tensorflow的逻辑，比如while &gt; tf.while, for &gt; tf.while, if &gt; tf.cond, assert &gt; tf.assert。</li>\n<li>通过以上两步，对函数进行建图，为了保证Python代码中每一行的执行顺序，tf.control_dependencies被自动加入到代码中，保证第i行执行完后我们会执行第i+1行。</li>\n<li>返回tf.Graph，根据函数名和输入参数，将这个graph存到一个cache中。</li>\n<li>对于任何一个该函数的调用，我们会重复利用cache中的计算图进行计算。</li>\n</ul>\n<p>我们来看一下Tensorflow 2.0中Eager Execution的代码如何转为tf.function的代码，首先来看一段简单的Tensorflow 2.0代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span>():</span></span><br><span class=\"line\">    a = tf.constant([[<span class=\"number\">10</span>,<span class=\"number\">10</span>],[<span class=\"number\">11.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    x = tf.constant([[<span class=\"number\">1.</span>,<span class=\"number\">0.</span>],[<span class=\"number\">0.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    b = tf.Variable(<span class=\"number\">12.</span>)</span><br><span class=\"line\">    y = tf.matmul(a, x) + b</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\">print(f().numpy())</span><br><span class=\"line\"><span class=\"comment\">#执行结果</span></span><br><span class=\"line\">[[<span class=\"number\">22.</span> <span class=\"number\">22.</span>]</span><br><span class=\"line\"> [<span class=\"number\">23.</span> <span class=\"number\">13.</span>]]</span><br></pre></td></tr></table></figure>\n<p>因为Tensorflow 2.0默认是Eager execution，代码的阅读和执行就和普通的Python代码一样，简单易读。首先我们简单的加上@tf.function装饰一下，为了方便调试，我们加入一个print和一个tf.print，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@tf.function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span>():</span></span><br><span class=\"line\">    a = tf.constant([[<span class=\"number\">10</span>,<span class=\"number\">10</span>],[<span class=\"number\">11.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    x = tf.constant([[<span class=\"number\">1.</span>,<span class=\"number\">0.</span>],[<span class=\"number\">0.</span>,<span class=\"number\">1.</span>]])</span><br><span class=\"line\">    b = tf.Variable(<span class=\"number\">12.</span>)</span><br><span class=\"line\">    y = tf.matmul(a, x) + b</span><br><span class=\"line\">    print(<span class=\"string\">&quot;PRINT: &quot;</span>, y)</span><br><span class=\"line\">    tf.print(<span class=\"string\">&quot;TF-PRINT: &quot;</span>, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\">f()</span><br><span class=\"line\"><span class=\"comment\">#执行结果</span></span><br><span class=\"line\">PRINT:  Tensor(<span class=\"string\">&quot;add:0&quot;</span>, shape=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), dtype=float32)</span><br><span class=\"line\">ValueError: tf.function-decorated function tried to create variables on non-first call.</span><br></pre></td></tr></table></figure>\n<p>这里有个异常，为什么？因为tf.function可能会对一段Python函数进行多次执行来构图，在多次执行的过程中，同样的Variable被创建了多次，产生错误。这其实也是一个很容易混乱的概念，在eager mode下一个Variable是一个Python object，所以会在执行范围外被销毁，但是在tf.function的装饰下，Variable变成了tf.Variable，是在Graph中持续存在的。所以，把一个在eager mode下正常执行的函数转换到Tensorflow图形式，需要一边思考着计算图一边构建程序。</p>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"http://colah.github.io/posts/2015-08-Backprop/\">Calculus on Computational Graphs: Backpropagation</a></li>\n<li><a href=\"https://medium.com/@yaoyaowd/tensorflow-2-0%E4%B8%8A%E6%89%8B6-%E8%A7%A3%E5%89%96tf-function%E7%9A%84%E4%BD%BF%E7%94%A8-b48cef249ca4\">Tensorflow 2.0上手6: 解剖tf.function的使用</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/33378444\">pytorch的计算图</a></li>\n<li><a href=\"http://www.likuli.com/archives/705/\">TensorFlow计算模型——计算图</a></li>\n<li><a href=\"https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/2-2,%E4%B8%89%E7%A7%8D%E8%AE%A1%E7%AE%97%E5%9B%BE.md\">eat_tensorflow2_in_30_days</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","TensorFlow","AutoGraph","计算图"]},{"title":"打造一个能够在线部署的深度学习对话系统--开源更新中","url":"/Code/7a43f279d20b/","content":"<h1 id=\"项目说明\"><a href=\"#项目说明\" class=\"headerlink\" title=\"项目说明\"></a>项目说明</h1><p>一个能够在线部署的全流程对话系统，项目地址：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">nlp-dialogue</a>。本项目的目标是奔着构建一个能够在线部署、执行、应用的全流程对话系统，即包含语料处理、训练、评估、推断、部署、Web服务的从头到尾的UI化系统功能。项目中计划同时包含开放域和面向任务型两种对话系统，模型的思路来源即为针对相关模型进行复现（论文阅读笔记放置在另一个项目：<a href=\"https://github.com/DengBoCong/nlp-paper\">nlp-paper</a>）。本项目中同时使用TensorFlow和Pytorch两种计算框架进行实现，可按需要进行切换，目前实现功能模型如下：</p>\n<ul>\n<li>TensorFlow模型<ul>\n<li>Transformer</li>\n<li>Seq2Seq</li>\n<li>SMN检索式模型</li>\n<li>Scheduled Sampling的Transformer</li>\n<li>GPT2</li>\n<li>Task Dialogue</li>\n</ul>\n</li>\n<li>Pytorch模型<ul>\n<li>Transformer</li>\n<li>Seq2Seq</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"功能效果\"><a href=\"#功能效果\" class=\"headerlink\" title=\"功能效果\"></a>功能效果</h1><p>项目可以在开发终端使用命令行进行测试和使用（执行和目录说明见后面小节），下面的demo演示为使用<code>beam size</code>为<code>3</code>的BeamSearch进行推断，有时因为语料的原因，对话可能略偏杂乱，词汇量不足也会出现<code>&lt;unk&gt;</code>的情况，命令行终端使用如下：<br><img src=\"https://img-blog.csdnimg.cn/20210215120151867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Web服务界面进行聊天使用如下（启动server，详细请看后节执行说明）：<br><img src=\"https://img-blog.csdnimg.cn/2021021512025911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20210215121035158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"语料\"><a href=\"#语料\" class=\"headerlink\" title=\"语料\"></a>语料</h1><p>在本项目中的<a href=\"https://github.com/DengBoCong/nlp-dialogue/tree/main/dialogue/data\">data</a>目录下放着各语料的玩具数据，可用于验证系统执行性，完整语料以及Paper可以在<a href=\"https://github.com/DengBoCong/nlp-paper\">这里</a>查看。语料方面这里知识简单陈列了比较容易找到的，其实还有很多丰富的对话语料，包括逻辑学习语料等，可自行收集：</p>\n<ul>\n<li>LCCC</li>\n<li>CrossWOZ</li>\n<li>小黄鸡</li>\n<li>豆瓣</li>\n<li>Ubuntu</li>\n<li>微博</li>\n<li>青云</li>\n<li>贴吧</li>\n</ul>\n<h1 id=\"执行说明\"><a href=\"#执行说明\" class=\"headerlink\" title=\"执行说明\"></a>执行说明</h1><ul>\n<li>Linux执行run.sh，项目工程目录检查执行check.sh（或check.py）</li>\n<li>根目录下的actuator.py为总执行入口，通过调用如下指令格式执行（执行前注意安装requirements.txt）：<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">python actuator.py --version [Options] --model [Options] ...</span><br></pre></td></tr></table></figure></li>\n<li>通过根目录下的actuator.py进行执行时，<code>--version</code>、<code>--model</code>和<code>--act</code>为必传参数，其中<code>--version</code>为代码版本<code>tf/torch</code>，<code>--model</code>为执行对应的模型<code>transformer/smn...</code>，而act为执行模式（缺省状态下为<code>pre_treat</code>模式），更详细指令参数参见各模型下的<code>actuator.py</code>或config目录下的对应json配置文件。</li>\n<li><code>--act</code>执行模式说明如下：<ul>\n<li>pre_treat模式为文本预处理模式，如果在没有分词结果集以及字典的情况下，需要先运行pre_treat模式</li>\n<li>train模式为训练模式</li>\n<li>evaluate模式为指标评估模式</li>\n<li>chat模式为对话模式，chat模式下运行时，输入ESC即退出对话。</li>\n</ul>\n</li>\n<li>正常执行顺序为pre_treat-&gt;train-&gt;evaluate-&gt;chat</li>\n<li>各模型下单独有一个actuator.py，可以绕开外层耦合进行执行开发，不过执行时注意调整工程目录路径</li>\n</ul>\n<h1 id=\"目录结构说明\"><a href=\"#目录结构说明\" class=\"headerlink\" title=\"目录结构说明\"></a>目录结构说明</h1><ul>\n<li>dialogue下为相关模型的核心代码放置位置，方便日后进行封装打包等<ul>\n<li>checkpoints为检查点保存位置</li>\n<li>config为配置文件保存目录</li>\n<li>data为原始数据储存位置，同时，在模型执行过程中产生的中间数据文件也保存在此目录下</li>\n<li>models为模型保存目录</li>\n<li>tensorflow及pytorch放置模型构建以及各模组执行的核心代码</li>\n<li>preprocess_corpus.py为语料处理脚本，对各语料进行单轮和多轮对话的处理，并规范统一接口调用</li>\n<li>read_data.py用于load_dataset.py的数据加载格式调用</li>\n<li>metrics.py为各项指标脚本</li>\n<li>tools.py为工具脚本，保存有分词器、日志操作、检查点保存/加载脚本等</li>\n</ul>\n</li>\n<li>docs下放置文档说明，包括模型论文阅读笔记</li>\n<li>docker（mobile）用于服务端（移动终端）部署脚本</li>\n<li>server为UI服务界面，使用flask进行构建使用，执行对应的server.py即可</li>\n<li>tools为预留工具目录</li>\n<li>actuator.py（run.sh）为总执行器入口</li>\n<li>check.py（check.sh）为工程目录检查脚本</li>\n</ul>\n<h1 id=\"SMN模型运行说明\"><a href=\"#SMN模型运行说明\" class=\"headerlink\" title=\"SMN模型运行说明\"></a>SMN模型运行说明</h1><p>SMN检索式对话系统使用前需要准备solr环境，solr部署系统环境推荐Linux，工具推荐使用容器部署(推荐Docker)，并准备：</p>\n<ul>\n<li>Solr(8.6.3)</li>\n<li>pysolr(3.9.0)</li>\n</ul>\n<p>以下提供简要说明，更详细可参见文章：<a href=\"https://zhuanlan.zhihu.com/p/300165220\">搞定检索式对话系统的候选response检索–使用pysolr调用Solr</a></p>\n<h2 id=\"Solr环境\"><a href=\"#Solr环境\" class=\"headerlink\" title=\"Solr环境\"></a>Solr环境</h2><p>需要保证solr在线上运行稳定，以及方便后续维护，请使用DockerFile进行部署，DockerFile获取地址：<a href=\"https://github.com/docker-solr/docker-solr\">docker-solr</a></p>\n<p>仅测试模型使用，可使用如下最简构建指令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker pull solr:8.6.3</span><br><span class=\"line\"># 然后启动solr</span><br><span class=\"line\">docker run -itd --name solr -p 8983:8983 solr:8.6.3</span><br><span class=\"line\"># 然后创建core核心选择器，这里取名smn(可选)</span><br><span class=\"line\">docker exec -it --user&#x3D;solr solr bin&#x2F;solr create_core -c smn</span><br></pre></td></tr></table></figure>\n<p>关于solr中分词工具有IK Analyzer、Smartcn、拼音分词器等等，需要下载对应jar，然后在Solr核心配置文件managed-schema中添加配置。</p>\n<p><strong>特别说明</strong>：如果使用TF-IDF，还需要在managed-schema中开启相似度配置。</p>\n<h2 id=\"Python中使用说明\"><a href=\"#Python中使用说明\" class=\"headerlink\" title=\"Python中使用说明\"></a>Python中使用说明</h2><p>线上部署好Solr之后，在Python中使用pysolr进行连接使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install pysolr</span><br></pre></td></tr></table></figure>\n<p>添加索引数据（一般需要先安全检查）方式如下。将回复数据添加索引，responses是一个json,形式如：[{},{},{},…]，里面每个对象构建按照你回复的需求即可：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">solr &#x3D; pysolr.Solr(url&#x3D;solr_server, always_commit&#x3D;True, timeout&#x3D;10)</span><br><span class=\"line\"># 安全检查</span><br><span class=\"line\">solr.ping()</span><br><span class=\"line\">solr.add(docs&#x3D;responses)</span><br></pre></td></tr></table></figure>\n<p>查询方式如下，以TF-IDF查询所有语句query语句方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;!func&#125;sum(product(idf(utterance,key1),tf(utterance,key1),product(idf(utterance,key2),tf(utterance,key2),...)</span><br></pre></td></tr></table></figure>\n<p>使用前需要先将数据添加至Solr，在本SMN模型中使用，先执行pre_treat模式即可。</p>\n<h1 id=\"参考代码和文献\"><a href=\"#参考代码和文献\" class=\"headerlink\" title=\"参考代码和文献\"></a>参考代码和文献</h1><ol>\n<li><a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need</a> | <a href=\"https://zhuanlan.zhihu.com/p/250946855\">阅读笔记</a>：Transformer的开山之作，值得精读 | Ashish et al,2017</li>\n<li><a href=\"https://arxiv.org/pdf/1612.01627v2.pdf\">Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</a> | <a href=\"https://zhuanlan.zhihu.com/p/270554147\">阅读笔记</a>：SMN检索式对话模型，多层多粒度提取信息 | Devlin et al,2018</li>\n<li><a href=\"https://arxiv.org/pdf/1703.03906.pdf\">Massive Exploration of Neural Machine Translation Architectures</a> | <a href=\"https://zhuanlan.zhihu.com/p/328801239\">阅读笔记</a>：展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。 | Denny et al,2017</li>\n<li><a href=\"https://arxiv.org/pdf/1906.07651.pdf\">Scheduled Sampling for Transformers</a> | <a href=\"https://zhuanlan.zhihu.com/p/267146739\">阅读笔记</a>：在Transformer应用Scheduled Sampling | Mihaylova et al,2019</li>\n</ol>\n","categories":["Code"],"tags":["对话系统"]},{"title":"有必要了解的Subword算法模型","url":"/Deep-Learning/38fa61dd1a7b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>Subword算法在预训练语言模型中有着不小的地位，它在分词和字典方面的优化改进带来的影响，间接或直接影响着任务最终的性能效果，因此，作为NLP研究者或开发者，有必要了解下Subword算法的原理。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>在NLP领域，对语料进行预处理的过程中，我们需要进行分词和生成词典。很多时候用多了框架的API，觉得分词和生成字典就是调用的事情，不过事情并没有那么简单，比如其中涉及到的未登录词的问题，就对任务性能影响很大。一种很朴素的做法就是将未见过的词编码成#UNK ，有时为了不让字典太大，只会把出现频次大于某个阈值的词丢到字典里边，剩下所有的词都统一编码成#UNK 。</p>\n<blockquote>\n<p>未登录词：简单来讲就是在验证集或测试集出现了训练集从来没见到过的单词。</p>\n</blockquote>\n<p>总结而言就是，任务（如对话、机翻）的词表是定长的，但是需要实际输入的词汇是开放的(out of vocabulary)。以前的做法是新词汇添加到词典中，但是过大的词典会带来两个问题：</p>\n<ul>\n<li>稀疏问题：某些词汇出现的频率很低，得不到充分的训练</li>\n<li>计算量问题：词典过大，也就意味着embedding过程的计算量会变大</li>\n</ul>\n<p>对于分词和生成字典方面，常见的方法有：</p>\n<ul>\n<li>给低频次再设置一个back-off 表，当出现低频次的时候就去查表</li>\n<li>不做word-level转而使用char-level，既然以词为对象进行建模会有未登录词的问题，那么以单个字母或单个汉字为对象建模不就可以解决了嘛？因为不管是什么词它肯定是由若干个字母组成的。</li>\n</ul>\n<p>第一种方法，简单直接，若干back-off做的很好的话，对低频词的效果会有很大的提升，但是这种方法依赖于back-off表的质量，而且也没法处理非登录词问题。第二种方法，的确可以从源头解决未登录词的问题，但是这种模型粒度太细。</p>\n<p>下面举例word-level和subword-level的一种直观感受：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">训练集的词汇: old older oldest smart smarter smartest</span><br><span class=\"line\">word-level 词典: old older oldest smart smarter smartest 长度为 <span class=\"number\">6</span></span><br><span class=\"line\">subword-level 词典: old smart er est 长度为 <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"Subword算法\"><a href=\"#Subword算法\" class=\"headerlink\" title=\"Subword算法\"></a>Subword算法</h1><p>如何将词分解成subword，依据不同的策略，产生了几种主流的方法: Byte Pair Encoding (BPE)、wordpiece 和 Unigram Language Model等。值得一提的是，这几种算法的处理流程跟语言学没有太大的关系，单纯是统计学的解决思路，Subword模型的主要趋势：</p>\n<ul>\n<li>与单词级别的模型架构相同，但使用的是字符级别的输入</li>\n<li>采用混合架构，输入主要是字符，但是会混入其他信息</li>\n</ul>\n<h3 id=\"Byte-Pair-Encoding（BPE）\"><a href=\"#Byte-Pair-Encoding（BPE）\" class=\"headerlink\" title=\"Byte Pair Encoding（BPE）\"></a>Byte Pair Encoding（BPE）</h3><p>BPE算法流程可参考<a href=\"https://arxiv.org/abs/1508.07909\">论文</a>。Byte Pair Encoding最初是一种压缩算法，其主要是使用一些出现频率高的byte pair来组成新的byte。但它也可以作为一种分词算法(尽管其本质是自下而上的聚类方法)，它以数据中所有（Unicode）字符的单字组词汇开头并且使用最常见的n-gram对来组成一个新的n-gram。例如”loved”,”loving”,”loves”这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>\n<p><strong>这个算法有一些需要注意的地方</strong>：</p>\n<ul>\n<li>有一个目标词汇量大小并在到达时停止训练</li>\n<li>需要确定单词的最长分割片段</li>\n<li>分词过程仅在由某些先前的标记器（通常为MT的Moses标记器）标识的单词内进行。</li>\n<li>自动决定系统的词汇，不再以常规方式过度使用“单词”</li>\n</ul>\n<p><strong>获取subword词表的流程（learn-bpe）</strong>：</p>\n<ul>\n<li>准备足够大的训练语料</li>\n<li>确定期望的subword词表大小</li>\n<li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>\n<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>\n<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>\n</ul>\n<p><strong>示例</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">第一步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第二步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w es t &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d es t &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第三步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w est &lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d est &lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">第四步：&#123;<span class=\"string\">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;n e w est&lt;/w&gt;&#x27;</span>: <span class=\"number\">6</span>, <span class=\"string\">&#x27;w i d est&lt;/w&gt;&#x27;</span>: <span class=\"number\">3</span>&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>实现（论文给出示例，如下）</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201204144643865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p><strong>编码和解码（apply-bpe及其逆过程）</strong>：</p>\n<ul>\n<li>在之前的算法中，我们已经得到了subword的词表，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。</li>\n<li>我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</li>\n</ul>\n<blockquote>\n<p>停止符”</w>“的意义在于表示subword是词后缀。举例来说：”st”字词不加”</w>“可以出现在词首如”st ar”，加了”</w>“表明改字词位于词尾，如”wide st</w>“，二者意义截然不同。</p>\n</blockquote>\n<p><strong>示例</strong>：</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">编码过程：</span><br><span class=\"line\"><span class=\"comment\"># 给定单词序列</span></span><br><span class=\"line\">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 假设已有排好序的subword词表</span></span><br><span class=\"line\">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 迭代结果</span></span><br><span class=\"line\">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class=\"line\">&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]</span><br><span class=\"line\">&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">解码过程：</span><br><span class=\"line\"><span class=\"comment\"># 编码序列</span></span><br><span class=\"line\">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class=\"line\"><span class=\"comment\"># 解码序列</span></span><br><span class=\"line\">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure>\n<p>更直观的以单词“where”为例，首先按照字符拆分开，然后查找词表文件，逐对合并，优先合并频率靠前的字符对。85 319 9 15 表示在该字符对在词表文件中的评率排名。<br><img src=\"https://img-blog.csdnimg.cn/20201204143517536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>最终where</w>可以在词表文件中被找到，因此where的bpe分词结果为where</w>，对于其他并不能像where一样能在词表文件中找到整个词的词来说，bpe分词结果以最终查询结束时的分词结果为准。</p>\n<h3 id=\"Wordpiece-Sentecepiece-model\"><a href=\"#Wordpiece-Sentecepiece-model\" class=\"headerlink\" title=\"Wordpiece\\Sentecepiece model\"></a>Wordpiece\\Sentecepiece model</h3><p>Google NMT使用了借鉴了上面的方法，其V1使用的是<a href=\"https://arxiv.org/pdf/1609.08144.pdf\">wordpiece mode</a>，V2使用的是sentecepiece model。它并没有采取字符集别的n-gram计数方法，而是使用贪心近似来最大化语言模型日志可能性来选择片段。添加n-gram信息，是为了最大限度地减少perplexity，Wordpiece模型标记化内部的单词，Sentencepiece模型则对原始文本进行处理。</p>\n<p>wordpiece作为BERT使用的分词方式，其生成词表的方式和BPE非常相近，都是用合并token的方式来生成新的token，最大的区别在于选择合并哪两个token。BPE选择频率最高的相邻字符对进行合并，而wordpiece是基于概率生成的。</p>\n<p>BERT模型使用的是wordpiece的变体，对于一些常见词如1910s、at、fairfax等词直接使用，对于其他词则根据wordpieces来构建。所以，需要注意方在其他任务中使用bert时，必须处理这个问题。</p>\n<blockquote>\n<p>Choose the new word unit out of all possible ones that increase the likelihood on the training data the most when added to the mode.</p>\n</blockquote>\n<p>从字面上可能有些难以理解，列一下公式就比较清楚了。在做分词的时候假设词和词之间是独立的，所以句子的likelihood等于句子中每个词概率的乘积：<br>$$logP(Sentence)=\\sum_{i=1}^nlogP(t_i) \\quad if\\ Sentence\\ has\\ token\\ t_1…t_n$$<br>如果把相邻的 $x$ 和 $y$ 两个token合并生成一个新的token叫做 $z$，那么整个句子likelihood的变化可以用下面的式子来表达：<br>$$logP(t_z)-(logP(t_x) + logP(t_y))=log\\frac{P(t_z)}{P(t_x\\cdot P(t_y))}$$<br>这不就是两个token之间的互信息嘛！所以wordpiece和BPE的核心区别就在于wordpiece是按token间的互信息来进行合并而BPE是按照token一同出现的频率来合并的。</p>\n<p>wordpiece算法中subword词表的学习跟BPE也差不多：</p>\n<ul>\n<li>准备语料，分解成最小单元，比如英文中26个字母加上各种符号，作为原始词表</li>\n<li>利用上述语料训练语言模型</li>\n<li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li>\n<li>重复上步骤，直至词表大小达到设定值或概率增量低于某一阈值</li>\n</ul>\n<h3 id=\"unigram-language-model\"><a href=\"#unigram-language-model\" class=\"headerlink\" title=\"unigram language model\"></a>unigram language model</h3><p>语言模型作为NLP的大厦根基，也是unigram分词的基础。在wordpiece算法中，其实已经用到了language modeling，在选择token进行合并的时候目标就是能提高句子的likelihood。而<a href=\"https://arxiv.org/pdf/1804.10959.pdf\">unigram</a>分词则更进一步，直接以最大化句子的likelihood为目标来直接构建整个词表。</p>\n<p>首先，了解一下怎么样在给定词表的条件下最大化句子的likelihood。 给定词表及对应概率值: {“你”:0.18, “们”:0.16, “好”:0.18, “你们”:0.15}，对句子”你们好“进行分词:</p>\n<ul>\n<li>划分为”你” “们” “好” 的概率为 0.18<em>0.16</em>0.18=0.005184</li>\n<li>划分为”你们” “好” 的概率为 0.15*0.18=0.027</li>\n</ul>\n<p>明显看出后一种分词方式要比前一种好，当然在真实的案例下词表可能有几万个token，直接罗列各种组合的概率显然不可能，所以需要用到Viterbi算法。因此在给定词表的情况下，可以 </p>\n<ul>\n<li>计算每个token对应的概率</li>\n<li>找到一个句子最好的分词方式</li>\n</ul>\n<p>但是在词表没有确定的情况下，同时要优化词表和词表里每个token的概率很难做到。unigram分词使用逐步迭代的方式来求解，具体步骤如下：</p>\n<ul>\n<li>首先初始化一个很大的词表</li>\n<li>重复以下步骤直到词表数量减少到预先设定的阈值：<ul>\n<li>保持词表不变，用EM算法来求解每个token的概率</li>\n<li>对于每一个token，计算如果把这个token从词表中移除而导致的likelihood减少值，作为这个token的loss</li>\n<li>按loss从大到小排序，保留前n%（原文中为80%）的token。</li>\n</ul>\n</li>\n</ul>\n<p>初始化词表可以用不同的方法，一个比较直接的办法就是用所有长度为1的token加上高频出现的ngram来作为起始词表。</p>\n<h1 id=\"其他Subword模型\"><a href=\"#其他Subword模型\" class=\"headerlink\" title=\"其他Subword模型\"></a>其他Subword模型</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1610.03017.pdf\">Fully Character-Level Neural Machine Translation without Explicit Segmentation</a><br><img src=\"https://img-blog.csdnimg.cn/2020120416231516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"http://proceedings.mlr.press/v32/santos14.pdf\">Character-level to build word-level</a>：该网络结构主要是对字符进行卷积以生成单词嵌入，同时使用固定窗口对PoS标记的字嵌入进行操作。<br><img src=\"https://img-blog.csdnimg.cn/20201204162518375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1508.06615.pdf\">Character-Aware Neural Language Models</a>：这是一个更加复杂的方法，其主要动机在于：提供一种功能强大，功能强大的语言模型，可以在各种语言中有效。其可编码子词相关性：eventful, eventfully, uneventful。同时解决先前模型的罕见字问题，使用更少的参数获得可比较的表现力。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201204163241225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Hybrid NMT(<a href=\"https://arxiv.org/pdf/1604.00788.pdf)%EF%BC%9A%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E9%9D%9E%E5%B8%B8%E5%87%BA%E8%89%B2%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9C%A8%E5%8D%95%E8%AF%8D%E7%BA%A7%E5%88%AB%E8%BF%9B%E8%A1%8C%E7%BF%BB%E8%AF%91%EF%BC%8C%E4%BD%86%E6%98%AF%E5%9C%A8%E6%9C%89%E9%9C%80%E8%A6%81%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%AF%E4%BB%A5%E5%BE%88%E6%96%B9%E4%BE%BF%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E7%BA%A7%E5%88%AB%E7%9A%84%E8%BE%93%E5%85%A5%E3%80%82\">https://arxiv.org/pdf/1604.00788.pdf)：这是一个非常出色的框架，主要是在单词级别进行翻译，但是在有需要的时候可以很方便的使用字符级别的输入。</a><br><img src=\"https://img-blog.csdnimg.cn/20201204163135592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1606.02601.pdf\">chars for word embeddings</a>：该模型的目标与word2vec相同，但是使用的时字符集别的输入。它使用了双向的LSTM结构尝试捕获形态并且能够推断出词根。<br><img src=\"https://img-blog.csdnimg.cn/20201204163452177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1607.04606.pdf\">Enriching Word Vectors with Subword Information</a>：它是word2vec的升级版，对于具有大量形态学的稀有词和语言有更好的表征，它也可以说是带有字符n-gram的w2v skip-gram模型的扩展。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/86965595\">深入理解NLP Subword算法：BPE、WordPiece、ULM</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/112444056\">NLP中的subword算法及实现</a></li>\n<li><a href=\"https://www.jianshu.com/p/4b5e84c48628\">CS224N(12)-子词模型</a></li>\n<li><a href=\"https://github.com/rsennrich/subword-nmt\">subword-nmt</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/69414965\">CS224N笔记(十二):Subword模型</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding\">Byte pair encoding</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["词袋","深度学习","NLP","Subword","BPE"]},{"title":"深度学习序列数据处理利器-tokenizer，结合TensorFlow和PyTorch","url":"/Deep-Learning/14c91d9dedcf/","content":"<blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>这里我们来好好探讨一下深度学习中，专门用于序列数据处理的Tokenizer，它可以帮助我们快速的建立词汇表字典，并提供了各种方法，针对文本和序列之间的转换，极大的方便的使用。TensorFlow中有keras实现的Tokenizer，而PyTorch本身是没有Tokenizer，但是我们可以通过引入torchtext或torchnlp库达到同样的效果，本文将对这几种工具的分词器部分的使用进行说明讲解。使用到的计算框架版本如下：</p>\n<ul>\n<li>TensorFlow：2.3.1</li>\n<li>PyTorch：1.7.0</li>\n<li>torchtext：0.8.0</li>\n<li>torchnlp：0.5.0</li>\n</ul>\n<p>这里我想多提一点，可能有助于后面内容的理解。我研究了这些分词器的源码，其实内部实现并不是很复杂，除了torchtext特殊一点（所以它也是使用起来最复杂的一个），torchtext不仅仅是分词器的作用，它还同时做了词嵌入，所以你使用它的时候可以选择用何种预训练词嵌入模型，如果词汇数量够多，你甚至可以直接使用内置的词向量嵌入数据，复杂但是功能齐全.</p>\n<p>而torchnlp和Tokenizer则是通过维护一个内部词汇表，不过区别在于，torchnlp内部表有两个，分别是index_to_token和token_to_index，一个是list，一个是dict，词汇的分布则是按照token出现的顺序进行编码的。而Tokenizer同样也有两个，一个是word_index和index_word，都是dict，不过它们维护的词汇分布则是按照词汇的频率由高到低，相同频率则按照出现顺序进行编码的。</p>\n<p>本文主要讲解使用方法，不讲解内部原理，有兴趣的可以去看看它们的源码实现，逻辑性还是很清晰的，看着很舒服。</p>\n<h1 id=\"TensorFlow中的Tokenizer\"><a href=\"#TensorFlow中的Tokenizer\" class=\"headerlink\" title=\"TensorFlow中的Tokenizer\"></a>TensorFlow中的Tokenizer</h1><p>其实相对而言，使用Keras的Tokenizer比较顺畅，一种丝滑的感觉（封装的比较完整），使用它我们可以对文本进行预处理，序列化，向量化等。Tokenizer基于矢量化语料库、单词数、TF-IDF等，将每个文本转换为整数序列（每个整数是字典中标记的索引）或转换成矢量（其中每个标记的系数可以是二进制的）。</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">Tokenizer</a><br>​</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">www.tensorflow.org</span><br><span class=\"line\">tf.keras.preprocessing.text.Tokenizer(</span><br><span class=\"line\">    num_words=<span class=\"literal\">None</span>, filters=<span class=\"string\">&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`&#123;|&#125;~\\t\\n&#x27;</span>, lower=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    split=<span class=\"string\">&#x27; &#x27;</span>, char_level=<span class=\"literal\">False</span>, oov_token=<span class=\"literal\">None</span>, document_count=<span class=\"number\">0</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tnum_words：根据单词频率，保留的最大单词数。仅保留最常见的num_words-<span class=\"number\">1</span>个单词，也就是保留前num_words-<span class=\"number\">1</span>个频率高的单词，不会影响内部词汇表的大小，但是会限制text和sequence转换的词汇量大小。</span><br><span class=\"line\">\tfilters：一个字符串（注意，不是正则表达式字符串哦），其中每个元素都是将从文本中过滤掉的字符。默认值为所有标点符号，加上制表符和换行符，再减去<span class=\"string\">&#x27;字符。</span></span><br><span class=\"line\"><span class=\"string\">\tlower：bool类型，是否将文本转换为小写。</span></span><br><span class=\"line\"><span class=\"string\">\tsplit：字符串，分隔词的分隔符，用于split()方法。</span></span><br><span class=\"line\"><span class=\"string\">\tchar_level：如果为True，则每个字符都将被视为token。</span></span><br><span class=\"line\"><span class=\"string\">\toov_token：如果给定的话，它将被添加到word_index中，并在text_to_sequence调用期间用于替换词汇外的单词，字典中的编码一直都是第一个。</span></span><br></pre></td></tr></table></figure>\n<p>默认情况下，Tokenizer将删除所有标点符号，从而将文本转换为以空格分隔的单词序列（单词可能包含<code>&#39;</code>字符）。 然后将这些序列分为token列表，然后将它们编码索引或向量化。注意了，<code>0</code>是一个保留索引，不会分配给任何单词。下面通过调用代码以及输出效果直观的展示用法，也能体会深刻，具体方法的参数和含义，自行查看官方文档：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">text = [</span><br><span class=\"line\">  <span class=\"string\">&quot;你 去 那儿 竟然 不喊 我 生气 了&quot;</span>,</span><br><span class=\"line\">  <span class=\"string\">&quot;道歉 ！ ！ 再有 时间 找 你 去&quot;</span></span><br><span class=\"line\"> ]</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=<span class=\"string\">&#x27;&lt;UNK&gt;&#x27;</span>, num_words=<span class=\"literal\">None</span>)</span><br><span class=\"line\">tokenizer.fit_on_texts(text) <span class=\"comment\">#text可以是字符串列表，字符串生成器（以提高内存效率）或字符串的列表的列表。</span></span><br><span class=\"line\">print(tokenizer.word_counts)</span><br><span class=\"line\"><span class=\"comment\">#OrderedDict([(&#x27;你&#x27;, 2), (&#x27;去&#x27;, 2), (&#x27;那儿&#x27;, 1), (&#x27;竟然&#x27;, 1), (&#x27;不喊&#x27;, 1), (&#x27;我&#x27;, 1), (&#x27;生气&#x27;, 1), (&#x27;了&#x27;, 1), (&#x27;道歉&#x27;, 1), (&#x27;！&#x27;, 2), (&#x27;再有&#x27;, 1), (&#x27;时间&#x27;, 1), (&#x27;找&#x27;, 1)])</span></span><br><span class=\"line\"><span class=\"comment\"># 单词计数，按次数排序</span></span><br><span class=\"line\">print(tokenizer.word_index)</span><br><span class=\"line\"><span class=\"comment\">#&#123;&#x27;&lt;UNK&gt;&#x27;: 1, &#x27;你&#x27;: 2, &#x27;去&#x27;: 3, &#x27;！&#x27;: 4, &#x27;那儿&#x27;: 5, &#x27;竟然&#x27;: 6, &#x27;不喊&#x27;: 7, &#x27;我&#x27;: 8, &#x27;生气&#x27;: 9, &#x27;了&#x27;: 10, &#x27;道歉&#x27;: 11, &#x27;再有&#x27;: 12, &#x27;时间&#x27;: 13, &#x27;找&#x27;: 14&#125;</span></span><br><span class=\"line\">print(tokenizer.index_word)</span><br><span class=\"line\"><span class=\"comment\">#&#123;1: &#x27;&lt;UNK&gt;&#x27;, 2: &#x27;你&#x27;, 3: &#x27;去&#x27;, 4: &#x27;！&#x27;, 5: &#x27;那儿&#x27;, 6: &#x27;竟然&#x27;, 7: &#x27;不喊&#x27;, 8: &#x27;我&#x27;, 9: &#x27;生气&#x27;, 10: &#x27;了&#x27;, 11: &#x27;道歉&#x27;, 12: &#x27;再有&#x27;, 13: &#x27;时间&#x27;, 14: &#x27;找&#x27;&#125;</span></span><br><span class=\"line\">print(tokenizer.num_words)</span><br><span class=\"line\"><span class=\"comment\">#None，它不会影响内部词汇表的大小，但是会限制text和sequence转换的词汇量大小</span></span><br><span class=\"line\">print(tokenizer.document_count)</span><br><span class=\"line\"><span class=\"comment\">#2，输出的是传入序列的数量</span></span><br><span class=\"line\">print(tokenizer.index_docs)</span><br><span class=\"line\"><span class=\"comment\">#defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;8: 1, 7: 1, 2: 2, 9: 1, 10: 1, 6: 1, 5: 1, 3: 2, 13: 1, 12: 1, 11: 1, 4: 1, 14: 1&#125;)</span></span><br><span class=\"line\"><span class=\"comment\">#每个词汇的次数，以索引为key</span></span><br><span class=\"line\">print(tokenizer.word_docs)</span><br><span class=\"line\"><span class=\"comment\"># defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;我&#x27;: 1, &#x27;不喊&#x27;: 1, &#x27;你&#x27;: 2, &#x27;生气&#x27;: 1, &#x27;了&#x27;: 1, &#x27;竟然&#x27;: 1, &#x27;那儿&#x27;: 1, &#x27;去&#x27;: 2, &#x27;时间&#x27;: 1, &#x27;再有&#x27;: 1, &#x27;道歉&#x27;: 1, &#x27;！&#x27;: 1, &#x27;找&#x27;: 1&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 每个词汇的次数，以单词为key</span></span><br><span class=\"line\">print(tokenizer.texts_to_sequences(text))</span><br><span class=\"line\"><span class=\"comment\"># [[2, 3, 5, 6, 7, 8, 9, 10], [11, 4, 4, 12, 13, 14, 2, 3]]</span></span><br><span class=\"line\"><span class=\"comment\"># 如果设置了num_words为5，则只能转换前4个频率最高的单词，其余为&lt;UNK&gt;，输出如下：</span></span><br><span class=\"line\"><span class=\"comment\">#[[2, 3, 1, 1, 1, 1, 1, 1], [1, 4, 4, 1, 1, 1, 2, 3]]</span></span><br><span class=\"line\">print(tokenizer.texts_to_matrix(text, <span class=\"string\">&quot;binary&quot;</span>))</span><br><span class=\"line\"><span class=\"comment\"># &quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># [[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]</span></span><br><span class=\"line\"><span class=\"comment\"># [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]</span></span><br><span class=\"line\"><span class=\"comment\"># binary：one-hot编码，count:计数编码，tfidf：词频-逆文档频率编码，freq：频率</span></span><br><span class=\"line\"></span><br><span class=\"line\">sequence = [[<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>], [<span class=\"number\">11</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">print(tokenizer.sequences_to_matrix(sequence))</span><br><span class=\"line\"><span class=\"comment\">#[[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]</span></span><br><span class=\"line\"><span class=\"comment\"># [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]</span></span><br><span class=\"line\"><span class=\"comment\">#&quot;binary&quot;, &quot;count&quot;, &quot;tfidf&quot;, &quot;freq&quot;</span></span><br><span class=\"line\">print(tokenizer.sequences_to_texts(sequence))</span><br><span class=\"line\"><span class=\"comment\">#[&#x27;你 去 那儿 竟然 不喊 我 生气 了&#x27;, &#x27;道歉 ！ ！ 再有 时间 找 你 去&#x27;]</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"torchnlp\"><a href=\"#torchnlp\" class=\"headerlink\" title=\"torchnlp\"></a>torchnlp</h1><p>PyTorch-NLP是Python中的自然语言处理（NLP）库。 它是根据最新的研究成果而构建的，从一开始就旨在支持快速原型设计。 PyTorch-NLP带有预训练的嵌入，采样器，数据集加载器，度量，神经网络模块和文本编码器。编码方法里面很多，这里使用比较典型的<code>StaticTokenizerEncoder</code>进行说明。</p>\n<p><a href=\"https://pytorchnlp.readthedocs.io/en/latest/index.html\">Welcome to Pytorch-NLP’s documentation!</a><br>​</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">pytorchnlp.readthedocs.io</span><br><span class=\"line\">torchnlp.encoders.text.StaticTokenizerEncoder(</span><br><span class=\"line\">\tsample, min_occurrences=<span class=\"number\">1</span>, append_sos=<span class=\"literal\">False</span>, append_eos=<span class=\"literal\">False</span>, tokenize=&lt;function _tokenize&gt;, </span><br><span class=\"line\">\tdetokenize=&lt;function _detokenize&gt;, reserved_tokens=[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;/s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;copy&gt;&#x27;</span>], </span><br><span class=\"line\">\tsos_index=<span class=\"number\">3</span>, eos_index=<span class=\"number\">2</span>, unknown_index=<span class=\"number\">1</span>, padding_index=<span class=\"number\">0</span>, **kwargs</span><br><span class=\"line\">)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsample: 用于构建编码字典的数据样本</span><br><span class=\"line\">\tmin_occurrences (<span class=\"built_in\">int</span>, optional): 要添加到编码字典中的token的最小出现次数。</span><br><span class=\"line\">tokenize (<span class=\"built_in\">callable</span>): 序列的分词方法</span><br><span class=\"line\">detokenize (<span class=\"built_in\">callable</span>): 序列的token合并方法</span><br><span class=\"line\">append_sos (<span class=\"built_in\">bool</span>, optional): 如果为<span class=\"literal\">True</span>，则在编码向量中插入SOS token。</span><br><span class=\"line\">append_eos (<span class=\"built_in\">bool</span>, optional): 如果为<span class=\"literal\">True</span>，则在编码向量中插入EOS token。</span><br><span class=\"line\">reserved_tokens (<span class=\"built_in\">list</span> of <span class=\"built_in\">str</span>, optional): 将保留标记列表插入字典开头。</span><br><span class=\"line\">sos_index (<span class=\"built_in\">int</span>, optional): sos token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">eos_index (<span class=\"built_in\">int</span>, optional): eos token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">unknown_index (<span class=\"built_in\">int</span>, optional): unk token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">padding_index (<span class=\"built_in\">int</span>, optional): pad token用于编码序列的开头，即token所在的索引。</span><br><span class=\"line\">batch (<span class=\"built_in\">list</span> of torch.Tensor): 编码序列的batch大小</span><br><span class=\"line\">lengths (torch.Tensor): 编码序列中，序列的长度列表</span><br><span class=\"line\">dim (<span class=\"built_in\">int</span>, optional): 指定分隔的序列维度</span><br></pre></td></tr></table></figure>\n<p>传给<code>StaticTokenizerEncoder</code>的sample是一个序列列表，这个和在Tokenizer中的是差不多的，<code>tokenize</code>和Tokenizer中的<code>split</code>是类似的功能，只不过<code>tokenize</code>传入的是方法，<code>StaticTokenizerEncoder</code>内部有一个初始化的token列表，长这样：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;/s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;s&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;copy&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>然后添加进来的序列就在其末尾进行顺序的补入，还有要说的就是，如果上面关于sos、eos等等的参数没有特别指定，就直接使用这个初始化列表里面的token。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">texts = [</span><br><span class=\"line\">  <span class=\"string\">&quot;你 去 那儿 竟然 不喊 我 生气 了&quot;</span>,</span><br><span class=\"line\">  <span class=\"string\">&quot;道歉 ！ ！ 再有 时间 找 你 去&quot;</span></span><br><span class=\"line\"> ]</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = StaticTokenizerEncoder(sample=texts, tokenize=<span class=\"keyword\">lambda</span> x: x.split())</span><br><span class=\"line\">print(tokenizer.index_to_token)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;copy&gt;&#x27;, &#x27;你&#x27;, &#x27;去&#x27;, &#x27;那儿&#x27;, &#x27;竟然&#x27;, &#x27;不喊&#x27;, &#x27;我&#x27;, &#x27;生气&#x27;, &#x27;了&#x27;, &#x27;道歉&#x27;, &#x27;！&#x27;, &#x27;再有&#x27;, &#x27;时间&#x27;, &#x27;找&#x27;]</span></span><br><span class=\"line\"><span class=\"comment\"># 编码就是按照它的list索引进行的</span></span><br><span class=\"line\">print(tokenizer.token_to_index)</span><br><span class=\"line\"><span class=\"comment\"># &#123;&#x27;&lt;pad&gt;&#x27;: 0, &#x27;&lt;unk&gt;&#x27;: 1, &#x27;&lt;/s&gt;&#x27;: 2, &#x27;&lt;s&gt;&#x27;: 3, &#x27;&lt;copy&gt;&#x27;: 4, &#x27;你&#x27;: 5, &#x27;去&#x27;: 6, &#x27;那儿&#x27;: 7, &#x27;竟然&#x27;: 8, &#x27;不喊&#x27;: 9, &#x27;我&#x27;: 10, &#x27;生气&#x27;: 11, &#x27;了&#x27;: 12, &#x27;道歉&#x27;: 13, &#x27;！&#x27;: 14, &#x27;再有&#x27;: 15, &#x27;时间&#x27;: 16, &#x27;找&#x27;: 17&#125;</span></span><br><span class=\"line\">print(tokenizer.vocab)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;copy&gt;&#x27;, &#x27;你&#x27;, &#x27;去&#x27;, &#x27;那儿&#x27;, &#x27;竟然&#x27;, &#x27;不喊&#x27;, &#x27;我&#x27;, &#x27;生气&#x27;, &#x27;了&#x27;, &#x27;道歉&#x27;, &#x27;！&#x27;, &#x27;再有&#x27;, &#x27;时间&#x27;, &#x27;找&#x27;]</span></span><br><span class=\"line\">print(tokenizer.vocab_size)</span><br><span class=\"line\"><span class=\"comment\"># 18</span></span><br><span class=\"line\">print([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts])</span><br><span class=\"line\"><span class=\"comment\"># [tensor([ 5,  6,  7,  8,  9, 10, 11, 12]), tensor([13, 14, 14, 15, 16, 17,  5,  6])]</span></span><br><span class=\"line\"><span class=\"comment\"># 你会发现返回的是一个torch.tensor的列表，你如果想要整理成一个array的Tensor，使用如下方法</span></span><br><span class=\"line\">print(stack_and_pad_tensors([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts]))</span><br><span class=\"line\"><span class=\"comment\"># BatchedSequences(tensor=tensor([[ 5,  6,  7,  8,  9, 10, 11, 12],</span></span><br><span class=\"line\"><span class=\"comment\">#         [13, 14, 14, 15, 16, 17,  5,  6]]), lengths=tensor([8, 8]))</span></span><br><span class=\"line\">print(stack_and_pad_tensors([tokenizer.encode(text) <span class=\"keyword\">for</span> text <span class=\"keyword\">in</span> texts])[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 5,  6,  7,  8,  9, 10, 11, 12],</span></span><br><span class=\"line\"><span class=\"comment\">#         [13, 14, 14, 15, 16, 17,  5,  6]])</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"torchtext\"><a href=\"#torchtext\" class=\"headerlink\" title=\"torchtext\"></a>torchtext</h1><p>torchtext库是PyTorch项目的一部分，和torchvision等一样，和torch核心库分离开，从torchtext这个名字我们也能大概猜到该库是pytorch圈中用来预处理文本数据集的库，torchtext在文本数据预处理方面特别强大。使用它来对文本数据进行预处理简直不要太方便，提供了非常多的API使用。当然本文主要是说明文本序列转换方面的，具体torchtext的完整使用，就不做赘述，想要深入了解的可以参考其官方文档。</p>\n<p><a href=\"https://pytorch.org/text/stable/index.html\">TorchText</a><br>​<br>我这里简单的概括一下它在数据预处理方面的简单流程，让你有个简单的了解。<br><img src=\"https://pic4.zhimg.com/80/v2-327ae85e5c64443a7faa8a721b283307_720w.jpg\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Train/Validation/Test数据集分割</li>\n<li>文件数据导入（File Loading）</li>\n<li>分词（Tokenization） 文本字符串切分为词语列表</li>\n<li>构建词典(Vocab) 根据训练的预料数据集构建词典</li>\n<li>数字映射(Numericalize/Indexify) 根据词典，将数据从词语映射成数字，方便机器学习</li>\n<li>导入预训练好的词向量(word vector)</li>\n<li>分批(Batch) 数据集太大的话，不能一次性让机器读取，否则机器会内存崩溃。解决办法就是将大的数据集分成更小份的数据集，分批处理</li>\n<li>向量映射（Embedding Lookup） 根据预处理好的词向量数据集，将5的结果中每个词语对应的索引值变成 词语向量</li>\n</ul>\n<p>除了第一步和最后一步需要我们使用其他库或者自己编写方法外，其他所有的步骤，torchtext都提供了API。言归正传，说会文本数据转换。如果你使用上面的预处理流程，就可以得到关于贴合原数据的向量序列。这里我就不说这种方法，我这里说一下，使用现有字典或预训练模型进行转换的方法。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">torchtext.data.functional.sentencepiece_numericalizer(sp_model)</span><br><span class=\"line\">sp_id_generator = sentencepiece_numericalizer(sp_model)</span><br><span class=\"line\">list_a = [<span class=\"string\">&quot;sentencepiece encode as pieces&quot;</span>, <span class=\"string\">&quot;examples to   try!&quot;</span>]</span><br><span class=\"line\"><span class=\"built_in\">list</span>(sp_id_generator(list_a))</span><br><span class=\"line\"><span class=\"comment\">#[[9858, 9249, 1629, 1305, 1809, 53, 842], [2347, 13, 9, 150, 37]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">torchtext.data.functional.numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=<span class=\"literal\">None</span>)</span><br><span class=\"line\">vocab = &#123;<span class=\"string\">&#x27;Sentencepiece&#x27;</span> : <span class=\"number\">0</span>, <span class=\"string\">&#x27;encode&#x27;</span> : <span class=\"number\">1</span>, <span class=\"string\">&#x27;as&#x27;</span> : <span class=\"number\">2</span>, <span class=\"string\">&#x27;pieces&#x27;</span> : <span class=\"number\">3</span>&#125;</span><br><span class=\"line\">ids_iter = numericalize_tokens_from_iterator(vocab,</span><br><span class=\"line\">                             simple_space_split([<span class=\"string\">&quot;Sentencepiece as pieces&quot;</span>,</span><br><span class=\"line\">                                              <span class=\"string\">&quot;as pieces&quot;</span>]))</span><br><span class=\"line\"><span class=\"keyword\">for</span> ids <span class=\"keyword\">in</span> ids_iter:</span><br><span class=\"line\">    print([num <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> ids])</span><br><span class=\"line\"><span class=\"comment\">#[0, 2, 3]</span></span><br><span class=\"line\"><span class=\"comment\">#[2, 3]</span></span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["词袋","深度学习","TensorFlow","PyTorch","Tokenizer","分词器"]},{"title":"聊一聊Spring-Boot中跨域场景","url":"/Spring-Boot/c3c6ba065761/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>跨域问题我相信大多数人都遇见过，这里我做一个简单的总结，大体上将跨域问题进行一个简单的介绍，以及针对SpringBoot进行跨域解决方案的说明。如果觉得写得好有所收获，记得点个赞及点个关注哦。</p>\n<h2 id=\"介绍跨域\"><a href=\"#介绍跨域\" class=\"headerlink\" title=\"介绍跨域\"></a>介绍跨域</h2><p>跨域有个的英文简称，叫做CORS，其全称叫做跨域资源共享(CORS) ，是一种机制。跨域的基本原理就是使用额外的 HTTP 头来告诉浏览器，让运行在一个 origin (domain) 上的 Web 应用被准许访问来自不同源服务器上的指定的资源。当一个资源从与该资源本身所在的服务器「不同的域、协议或端口」请求一个资源时，资源会发起一个「跨域 HTTP 请求」。</p>\n<p>这里要强调一下的是，很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。同源策略是由 Netscape 提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持 JavaScript 的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。换而言之，如果协议、域名或者端口不相同，那么应用与服务之间的请求就是跨域请求，这个时候就需要进行特殊的处理，才能通过浏览器的安全策略。</p>\n<p>同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是 JSONP，JSONP 虽然能解决跨域但是有一个很大的局限性，那就是只支持 GET 请求，不支持其他类型的请求，在 RESTful 时代这几乎就没什么用。而这里说的 CORS（跨域源资源共享）是一个 W3C 标准，它是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是 JSONP 模式的现代版。在 Spring 框架中，对于 CORS 也提供了相应的解决方案，在 Spring Boot 中，这一方案得倒了简化，无论是单纯的跨域，还是结合 Spring Security 之后的跨域，都变得非常容易了。</p>\n<h2 id=\"事前准备\"><a href=\"#事前准备\" class=\"headerlink\" title=\"事前准备\"></a>事前准备</h2><p>首先创建两个普通的 Spring Boot 项目，这个就不用我多说，第一个命名为 provider 提供服务，第二个命名为 consumer 消费服务，第一个配置端口为 8080，第二个配置配置为 8081，然后在 provider 上提供两个 hello 接口，一个 get，一个 post，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloController</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@GetMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@PostMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;post hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 consumer 的 <code>resources/static</code> 目录下创建一个 html 文件，发送一个简单的 ajax 请求，如下：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div id=<span class=\"string\">&quot;app&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;input type=<span class=\"string\">&quot;button&quot;</span> onclick=<span class=\"string\">&quot;btnClick()&quot;</span> value=<span class=\"string\">&quot;get_button&quot;</span>&gt;</span><br><span class=\"line\">&lt;input type=<span class=\"string\">&quot;button&quot;</span> onclick=<span class=\"string\">&quot;btnClick2()&quot;</span> value=<span class=\"string\">&quot;post_button&quot;</span>&gt;</span><br><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">btnClick</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">        $.get(<span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span>, <span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">msg</span>) </span>&#123;</span><br><span class=\"line\">            $(<span class=\"string\">&quot;#app&quot;</span>).html(msg);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">btnClick2</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">        $.post(<span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span>, <span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">msg</span>) </span>&#123;</span><br><span class=\"line\">            $(<span class=\"string\">&quot;#app&quot;</span>).html(msg);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/script&gt;</span><br></pre></td></tr></table></figure>\n<p>然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Access to XMLHttpRequest at <span class=\"string\">&#x27;http://localhost:8080/hello&#x27;</span> from origin <span class=\"string\">&#x27;http://localhost:8081&#x27;</span> has been blocked by CORS policy: No <span class=\"string\">&#x27;Access-Control-Allow-Origin&#x27;</span> header is present on the requested resource.</span><br></pre></td></tr></table></figure>\n<p>可以看到，由于同源策略的限制，请求无法发送成功。使用 CORS 可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在 provider 中如何配置。首先可以通过 <code>@CrossOrigin</code> 注解配置某一个方法接受某一个域的请求，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloController</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@CrossOrigin(value = &quot;http://localhost:8081&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@GetMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@CrossOrigin(value = &quot;http://localhost:8081&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@PostMapping(&quot;/hello&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">hello2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;post hello&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个注解表示这两个接口接受来自 <code>http://localhost:8081</code> 地址的请求，配置完成后，重启 provider ，再次发送请求，浏览器控制台就不会报错了，consumer 也能拿到数据了。此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200620000112985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>这个表示服务端愿意接收来自 <code>http://localhost:8081</code> 的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。provider 上，每一个方法上都去加注解未免太麻烦了，有的小伙伴想到可以讲注解直接加在 Controller 上，不过每个 Controller 都要加还是麻烦，在 Spring Boot 中，还可以通过全局配置一次性解决这个问题，全局配置只需要在 SpringMVC 的配置类中重写 <code>addCorsMappings</code> 方法即可，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebMvcConfig</span> <span class=\"keyword\">implements</span> <span class=\"title\">WebMvcConfigurer</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addCorsMappings</span><span class=\"params\">(CorsRegistry registry)</span> </span>&#123;</span><br><span class=\"line\">        registry.addMapping(<span class=\"string\">&quot;/**&quot;</span>)</span><br><span class=\"line\">        .allowedOrigins(<span class=\"string\">&quot;http://localhost:8081&quot;</span>)</span><br><span class=\"line\">        .allowedMethods(<span class=\"string\">&quot;*&quot;</span>)</span><br><span class=\"line\">        .allowedHeaders(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>/**</code> 表示本应用的所有方法都会去处理跨域请求，<code>allowedMethods</code> 表示允许通过的请求数，<code>allowedHeaders</code> 则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。</p>\n<h2 id=\"SpringSecurity中解决跨域问题\"><a href=\"#SpringSecurity中解决跨域问题\" class=\"headerlink\" title=\"SpringSecurity中解决跨域问题\"></a>SpringSecurity中解决跨域问题</h2><p>如果使用了 Spring Security，上面的跨域配置会失效，因为请求被 Spring Security 拦截了。当引入了 Spring Security 的时候，我们有两种办法开启 Spring Security 对跨域的支持。</p>\n<h4 id=\"方式一\"><a href=\"#方式一\" class=\"headerlink\" title=\"方式一\"></a>方式一</h4><p>方式一就是在前面的基础上，添加 Spring Security 对于 CORS 的支持，只需要添加如下配置即可：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .authorizeRequests()</span><br><span class=\"line\">                .anyRequest().authenticated()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .formLogin()</span><br><span class=\"line\">                .permitAll()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .httpBasic()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf()</span><br><span class=\"line\">                .disable();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>一个 <code>.cors</code> 就开启了 Spring Security 对 CORS 的支持。</p>\n<h4 id=\"方式二\"><a href=\"#方式二\" class=\"headerlink\" title=\"方式二\"></a>方式二</h4><p>方式二则是去除前面的跨域配置，直接在 Spring Security 中做全局配置，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .authorizeRequests()</span><br><span class=\"line\">                .anyRequest().authenticated()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .formLogin()</span><br><span class=\"line\">                .permitAll()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .httpBasic()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors()</span><br><span class=\"line\">                .configurationSource(corsConfigurationSource())</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf()</span><br><span class=\"line\">                .disable();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\">CorsConfigurationSource <span class=\"title\">corsConfigurationSource</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        UrlBasedCorsConfigurationSource source = <span class=\"keyword\">new</span> UrlBasedCorsConfigurationSource();</span><br><span class=\"line\">        CorsConfiguration configuration = <span class=\"keyword\">new</span> CorsConfiguration();</span><br><span class=\"line\">        configuration.setAllowCredentials(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        configuration.setAllowedOrigins(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setAllowedMethods(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setAllowedHeaders(Arrays.asList(<span class=\"string\">&quot;*&quot;</span>));</span><br><span class=\"line\">        configuration.setMaxAge(Duration.ofHours(<span class=\"number\">1</span>));</span><br><span class=\"line\">        source.registerCorsConfiguration(<span class=\"string\">&quot;/**&quot;</span>,configuration);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> source;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过 CorsConfigurationSource 实例对跨域信息作出详细配置，例如允许的请求来源、允许的请求方法、允许通过的请求头、探测请求的有效期、需要处理的路径等等。使用这种方式就可以去掉前面的跨域配置了。</p>\n<h2 id=\"OAuth2跨域处理\"><a href=\"#OAuth2跨域处理\" class=\"headerlink\" title=\"OAuth2跨域处理\"></a>OAuth2跨域处理</h2><p>还有一种情况就是 OAuth2 允许跨域，如果用户要访问 OAuth2 端点，例如<code>/oauth/token</code> ，出现了跨域该怎么配置呢？这个其实很简单，我们只要在请求中携带一个Token就可以了，主要是配置一个 CorsFilter，我这里就把核心配置类列出来：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GlobalCorsConfiguration</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> CorsFilter <span class=\"title\">corsFilter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        CorsConfiguration corsConfiguration = <span class=\"keyword\">new</span> CorsConfiguration();</span><br><span class=\"line\">        corsConfiguration.setAllowCredentials(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedOrigin(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedHeader(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        corsConfiguration.addAllowedMethod(<span class=\"string\">&quot;*&quot;</span>);</span><br><span class=\"line\">        UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = <span class=\"keyword\">new</span> UrlBasedCorsConfigurationSource();</span><br><span class=\"line\">        urlBasedCorsConfigurationSource.registerCorsConfiguration(<span class=\"string\">&quot;/**&quot;</span>, corsConfiguration);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> CorsFilter(urlBasedCorsConfigurationSource);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后在 SecurityConfig 中开启跨域支持：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@Order(Ordered.HIGHEST_PRECEDENCE)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        http</span><br><span class=\"line\">                .requestMatchers().antMatchers(HttpMethod.OPTIONS, <span class=\"string\">&quot;/oauth/**&quot;</span>)</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .csrf().disable().formLogin()</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .cors();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","categories":["Spring-Boot"],"tags":["Sprint Boot","跨域","SCORS"]},{"title":"论文阅读笔记：An-End-to-End-Trainable-Neural-Network-Model-with...","url":"/Paper-Reading/66c85cb16080/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog<br>原文链接：<a href=\"https://arxiv.org/pdf/1708.05956.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>我们提出了面向任务的对话系统的新型端到端可训练神经网络模型，该模型能够跟踪对话状态，基于知识（KB）的API调用，并将结构化的KB查询结果合并到系统响应中，从而成功完成面向任务的对话。通过在对话历史上的进行belief tracking和KB结果处理，进而模型产生结构良好的系统响应。我们使用从第二个Dialog State Tracking Challenge（DSTC2）语料库转换而来的数据集在饭店搜索域中评估模型。实验结果表明，在给定对话历史记录的情况下，该模型可以很好地跟踪对话状态。此外，我们的模型在产生适当的系统响应方面表现出令人鼓舞的结果，其使用基于每个响应的准确性评估指标优于以前的端到端可训练神经网络模型。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>端到端可训练神经网络模型可以直接针对最终系统目标函数（例如任务成功率）进行优化，从而缓解了可信分配和在线适应的挑战。在这项工作中，我们提出了面向任务的对话的端到端可训练神经网络模型，该模型将统一网络应用于belief tracking，基于知识（KB）操作和响应创建。该模型能够跟踪对话状态，与KB交互以及将结构化KB查询结果合并到系统响应中，从而成功完成面向任务的对话框。我们表明，在给出对话历史记录的情况下，我们提出的模型可以有效地跟踪状态。与先前的端到端可训练神经网络模型相比，我们的模型还证明了在提供适当的系统响应和进行面向任务的对话方面的有更好的性能。</p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><ul>\n<li>Dialog State Tracking<ul>\n<li>在口语对话系统中，对话状态跟踪或belief tracking是指在可能的对话状态上保持分布的任务，这些状态直接确定系统的动作。</li>\n<li>使用诸如CRF或RNN之类的序列模型进行判别的方法可以灵活地探索任意特征并实现最新的DST性能，从而解决了生成模型的局限性。</li>\n</ul>\n</li>\n<li>End-to-End Task-Oriented Dialog Models<ul>\n<li>我们的模型使用统一的网络进行信念跟踪，知识操作和响应生成，以充分探索可以在不同任务之间共享的知识</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Proposed-Method\"><a href=\"#Proposed-Method\" class=\"headerlink\" title=\"Proposed Method\"></a>Proposed Method</h1><p>我们将面向任务的对话建模为一个多任务序列学习问题，其组件用于编码用户输入，跟踪信念状态，发出API调用，处理KB结果以及生成系统响应。模型架构如下图所示，对话框中的多轮序列使用LSTM递归神经网络进行编码， 根据对话历史记录，会话状态保持在LSTM状态。LSTM状态向量用于生成：（1）通过从非词法化系统响应候选列表中选择句结构；（2）信念跟踪器中每个插槽值的概率分布；（3）指向检索到的KB结果中与用户查询匹配的实体。通过用预测的插槽值和实体属性值替换去词化的token来生成最终的系统响应。<br><img src=\"https://img-blog.csdnimg.cn/20200929090815736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Utterance Encoding：这里的话语编码是指将单词序列编码为连续的密集向量。我们使用双向LSTM将用户输入编码为句向量，其中用户输入第 $k$ 轮对话共 $T_k$ 个 单词表示为$U_k=(w_1,w_2,…,w_{T_k})$。用户的句向量 $U_k$ 表示为 $U_k=[\\overrightarrow{h_{T_k}^{U_k}},\\overleftarrow{h_{1}^{U_k}}]$，$\\overrightarrow{h_{T_k}^{U_k}}$和 $\\overleftarrow{h_{1}^{U_k}}$ 是第 $k$ 轮最后的前向和反向的句级LSTM状态。</li>\n<li> Belief Tracking：信念跟踪（或对话状态跟踪）通过沿对话顺序积累信息来维持和调整对话状态（例如用户的目标）。在第 $k$ 轮从用户输入中收集新信息后，神经对话模型会更新每种插槽类型 $m∈M$ 的候选值的概率分布 $P(S_{k}^{m})$。在回合 $k$，对话级LSTM（LSTMD）更新其隐藏状态 $s_k$，并在接收到用户输入编码 $U_k$ 和 K_B$ 指示器 $I_k$（将在下面的部分中进行说明）之后，使用它来推断用户目标的任何更新。<br>$$s_k=LSTM_D(s_{k-1}, [U_k, I_k])$$$$P(S_{k}^{m}|U_{\\leq k}, I_{\\leq k}) = SlotDist_m(s_k)$$<br>其中，$SlotDist_m(s_k)$是在插槽类型 $m∈M$ 上具有softmax激活功能的多层感知器（MLP）</li>\n<li>Issuing API Calls：基于对话状态，模型可以基于信念跟踪输出发出API调用以查询KB。该模型首先生成一个简单的API调用命令模板。 通过使用信念追踪器针对每个目标插槽的最佳假设替换命令模板中的插槽类型token，产生最终的API调用命令。</li>\n<li>KB Results Processing：一旦神经对话模型接收到KB查询结果，它将通过从返回的列表中选择实体来向用户建议选项。KB搜索或数据库查询的输出通常具有定义明确的结构，并且实体属性与实体索引相关联。在对话的第 $k$ 轮，将二进制KB指示器 $I_k$ 传递给神经对话模型。该指标由上一次API调用中检索到的实体数和当前实体指针决定。当系统处于向用户建议实体的状态时，如果接收到零值 $I_k$，则该模型很可能会通知用户与当前查询匹配的实体不可用，否则，如果 $I_k$ 有值，该模型可能会根据实体指针 $P(E_k)$ 的更新概率分布从检索结果中选择一个实体：<br>$$P(E_k|U_{\\leq k}, I_{\\leq k}) = EntityPointerDist(s_k)$$<br>其中，其中 $EntityPointerDist$ 是具有softmax激活的MLP。</li>\n<li>System Response Generation：在对话的第 $k$ 轮，从非词化响应候选列表中选择句结构 $R_k$。最终的系统响应是通过用预测的插槽值和实体属性值替换非词性化token来产生的。</li>\n<li>Model Training：我们通过找到参数集 $θ$ 来训练神经对话模型，该参数集 $θ$ 最小化了目标标签，实体指针和去词化系统响应的预测分布和真实分布的交叉熵：<br>$$\\underset{\\Theta}{min}\\sum_{k=1}^{K}-[\\sum_{m=1}^{M}\\lambda_{S^m}logP(S_{k}^{m*}|U_{\\leq k},I_{\\leq k};\\Theta )$$$$+\\lambda_ElogP(E_{k}^{<em>}|U_{\\leq k},I_{\\leq k};\\Theta )$$$$+\\lambda_RlogP(R_{k}^{</em>}|U_{\\leq k}, I_{\\leq k};\\Theta )]$$<br>其中，其中 $λ<em>s$ 是每个系统输出成本的线性插值权重。$S</em>{k}^{m∗}$，$E_K^*$和$R_k^*$ 是第 $k$ 轮每个任务的真实标签。</li>\n<li>Alternative Model Designs：直观地，如果模型被明确告知目标值估计并且知道其先前对用户做出的响应，则该模型可能会提供更好的响应。因此，我们设计并评估了一些替代模型架构，以验证这种假设：<ul>\n<li>具有先前发出的非词化系统响应的模型连接回对话级LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,R_{k-1}])$</li>\n<li>具有先前发出的插槽标签的模型连接回对话级LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,S_{k-1}^{1},…,R_{k-1}^{M}])$</li>\n<li>具有先前发出的响应和插槽标签的模型都已连接回对话框LSTM状态：$s_k=LSTM_D(s_{k-1},[U_k,I_k,R_{k-1},S_{k-1}^{1},…,R_{k-1}^{M}])$</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>我们使用DSTC2中的数据进行模型评估。 在这项研究中，我们通过保留对话状态注释并添加系统命令（API调用）来结合原始的DSTC2语料库和此转换版本。下表汇总了该数据集的统计信息。<br><img src=\"https://img-blog.csdnimg.cn/20200929093437880.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用Adam优化方法进行批量为32的小批量模型训练，在模型训练期间，具有dropout的正则化应用于非循环连接[26]，dropout率为0.5。我们将梯度的最大范数设置为5，以防止梯度爆炸。对话层LSTM和话语层LSTM的隐藏层大小分别设置为200和150，大小为300的单词嵌入是随机初始化的，我们还尝试使用在Google新闻数据集上受过训练的预训练词向量来初始化词嵌入。</p>\n<p>下表显示了使用不同用户话语编码方法和不同单词嵌入初始化的模型的评估结果：<br><img src=\"https://img-blog.csdnimg.cn/20200929104233676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了不同的递归模型架构的评估结果：<br><img src=\"https://img-blog.csdnimg.cn/20200929104439793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表中的结果所示，我们的系统可实现与最新系统相当的信念跟踪性能：<br><img src=\"https://img-blog.csdnimg.cn/20200929104545966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>即使使用相同的评估度量，我们的模型在设计上的设置也与下表中的其他已发布模型相比略有不同<br><img src=\"https://img-blog.csdnimg.cn/20200929104800976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><p>在这项工作中，我们为面向任务的对话系统提出了一种新颖的端到端可训练神经网络模型。该模型能够跟踪对话的状态，通过发出API调用与知识交互，并将结构化的查询结果合并到系统响应中，从而成功完成面向任务的对话框。在餐厅搜索域的评估中，使用来自第二个“Dialog State Tracking Challenge”语料库的转换数据集，我们提出的模型显示了在对话轮次序列上跟踪对话状态的鲁棒性能。该模型还展示了在生成适当的系统响应方面的有不错的性能，优于以前的端到端可训练神经网络模型。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","对话系统","End-to-End","Belief-Tracking","神经网络"]},{"title":"论文阅读笔记：Attention-Is-All-You-Need","url":"/Paper-Reading/14883e8ec3b5/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Attention Is All You Need<br>原文链接：<a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>序列转导模型基于复杂的递归或卷积神经网络，包括编码器和解码器，表现最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即Transformer，它完全基于注意力机制，完全消除了重复和卷积。在两个机器翻译任务上进行的实验表明，这些模型在质量上具有优势，同时具有更高的可并行性，并且所需的训练时间大大减少。我们的模型在WMT 2014英语到德语的翻译任务上达到了28.4 BLEU，比包括集成学习在内的现有最佳结果提高了2 BLEU。在2014年WMT英语到法语翻译任务中，我们的模型在八个GPU上进行了3.5天的训练后，创造了新的单模型最新BLEU分数41.8，比文献中最好的模型的训练成本更小。我们展示了Transformer通过将其成功应用于具有大量训练数据和有限训练数据的英语解析，将其很好地概括了其他任务。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>在Transformer出现之前，RNN、LSTM、GRU等在序列模型和转导问题的方法中占据了稳固的地位，比如语言模型、机器翻译等，人们一直在努力扩大循环语言模型和编码器-解码器体系结构的界限。递归模型通常沿输入和输出序列的符号位置考虑计算。将位置与计算时间中的步骤对齐，它们根据先前的隐藏状态ht-1和位置t的输入生成一系列隐藏状态ht。这种固有的顺序性导致了没办法并行化进行训练，这在较长的序列长度上变得至关重要。最近的工作通过分解技巧和条件计算大大提高了计算效率，同时在后者的情况下还提高了模型性能，但是，顺序计算的基本约束仍然存在。注意力机制已成为各种任务中引人注目的序列建模和转导模型不可或缺的一部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。在这项工作中，我们提出了一种Transformer，一种避免重复的模型体系结构，而是完全依赖于注意力机制来绘制输入和输出之间的全局依存关系。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>减少顺序计算的目标也构成了扩展神经GPU，ByteNet和ConvS2S的基础，它们全部使用卷积神经网络作为基本构建块，并行计算所有输入和输出的隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数在位置之间的距离中增加，对于ConvS2S线性增长，而对于ByteNet则对数增长，这使得学习远处位置之间的依存关系变得更加困难。在Transformer中，此操作被减少为恒定的操作次数，尽管以平均注意力加权位置为代价，导致有效分辨率降低，但是我们用多头注意力抵消了这种代价。</p>\n<p>Self-attention（有时称为d intra-attention）是一种与单个序列的不同位置相关的注意力机制，目的是计算序列的表示形式。Self-attention已成功用于各种任务中，包括阅读理解，抽象摘要和学习与任务无关的句子表示。Transformer是第一个完全依靠Self-attention来计算其输入和输出表示的转导模型，而无需使用序列对齐的RNN或卷积。</p>\n<h1 id=\"Model-Architecture\"><a href=\"#Model-Architecture\" class=\"headerlink\" title=\"Model Architecture\"></a>Model Architecture</h1><p>Transformer依旧是遵循encoder-decoder结构，其模型的每一步都是自回归的，在生成下一个模型时，会将先前生成的符号用作附加输入。在此基础上，使用堆叠式Self-attention和point-wise，并在encoder和decoder中使用全连接层，结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20200917160031494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Encoder-and-Decoder-Stacks\"><a href=\"#Encoder-and-Decoder-Stacks\" class=\"headerlink\" title=\"Encoder and Decoder Stacks\"></a>Encoder and Decoder Stacks</h2><ul>\n<li><p>Encoder</p>\n<ul>\n<li>编码器由$N = 6$个相同层的堆栈组成，每层有两个子层，分别是Self-attention机制和位置完全连接的前馈网络</li>\n<li>每个子层周围都使用残差连接并进行归一化，也就是说每个子层的输出为$LayerNorm(x+Sublayer(x))$</li>\n<li>为了促进这些残差连连接，模型中的所有子层以及嵌入层均产生尺寸为dmodel = 512的输出</li>\n</ul>\n</li>\n<li><p>Decoder</p>\n<ul>\n<li>解码器还由N = 6个相同层的堆栈组成</li>\n<li>除了每个编码器层中的两个子层之外，解码器还插入一个第三子层，该子层对编码器堆栈的输出执行多头注意力</li>\n<li>对编码器堆栈的输出执行多头注意力时，要注意使用mask，保证预测只能依赖于小于当前位置的已知输出。</li>\n<li>每个子层周围都使用残差连接并进行归一化</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Attention\"><a href=\"#Attention\" class=\"headerlink\" title=\"Attention\"></a>Attention</h2><p>注意力方法可以描述为将query和一组key-value映射到输出，其中query，key，value和输出都是向量。输出是计算value的加权总和，其中分配给每个value的权重是通过query与相应key的方法来计算的。<br><img src=\"https://img-blog.csdnimg.cn/20200917162420181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Scaled-Dot-Product-Attention\"><a href=\"#Scaled-Dot-Product-Attention\" class=\"headerlink\" title=\"Scaled Dot-Product Attention\"></a>Scaled Dot-Product Attention</h3><p>它的输入是$d_k$维的queries和keys组成，使用所有key和query做点积，并除以$\\sqrt{d_k}$，然后应用softmax函数获得value的权重，公式如下：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$$</p>\n<ul>\n<li>常用注意力方法<ul>\n<li>相加（在更大的$d_k$下，效果更好）</li>\n<li>点积（更快一些）</li>\n<li>所以为了在较大的$d_k$下，点积也能工作的好，在公式中才使用了$\\frac{1}{\\sqrt{d_k}}$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Multi-Head-Attention\"><a href=\"#Multi-Head-Attention\" class=\"headerlink\" title=\"Multi-Head Attention\"></a>Multi-Head Attention</h3><p>多头注意力使模型可以共同关注来自不同位置的不同表示子空间的信息，最后取平均：<br>$$<br>MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^{O}\\<br>head_1=Attention(QW_{i}^{Q},K_{i}^{K},V_{i}^{V})<br>$$<br>论文中使用$h=8$注意力层，其中$d_k=d_v=\\frac{d_{model}}{h}=64$</p>\n<h3 id=\"Applications-of-Attention-in-our-Model\"><a href=\"#Applications-of-Attention-in-our-Model\" class=\"headerlink\" title=\"Applications of Attention in our Model\"></a>Applications of Attention in our Model</h3><p>Transformer以三种不同方式使用多头注意力:</p>\n<ul>\n<li>在“encoder-decoder注意”层中，queries来自先前的decoder层，而keys和values来自encoder的输出，这允许解码器中的每个位置都参与输入序列中的所有位置。</li>\n<li>encoder包含self-attention层。 在 self-attention层中，所有key，value和query都来自同一位置，在这种情况下，是编码器中前一层的输出。</li>\n<li>类似地，decoder中的self-attention层允许decoder中的每个位置都参与decoder中直至并包括该位置的所有位置。我们需要阻止decoder中的向左信息流，以保留自回归属性。</li>\n</ul>\n<h2 id=\"Position-wise-Feed-Forward-Networks\"><a href=\"#Position-wise-Feed-Forward-Networks\" class=\"headerlink\" title=\"Position-wise Feed-Forward Networks\"></a>Position-wise Feed-Forward Networks</h2><p>除了关注子层之外，我们的encoder和decoder中的每个层还包含一个完全连接的前馈网络，该网络分别应用于每个位置。 这由两个线性变换组成，两个线性变换之间有ReLU激活。<br>$$<br>FNN(x)=max(0,xW_1+b_1)W_2+b_2<br>$$<br>虽然线性变换在不同位置上相同，但是它们使用不同的参数</p>\n<h2 id=\"Embeddings-and-Softmax\"><a href=\"#Embeddings-and-Softmax\" class=\"headerlink\" title=\"Embeddings and Softmax\"></a>Embeddings and Softmax</h2><p>与其他序列转导模型类似，使用学习的嵌入将输入标记和输出标记转换为维dmodel的向量。我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一个token概率</p>\n<h2 id=\"Positional-Encoding\"><a href=\"#Positional-Encoding\" class=\"headerlink\" title=\"Positional Encoding\"></a>Positional Encoding</h2><p>位置编码的维数dmodel与嵌入的维数相同，因此可以将两者相加，位置编码有很多选择，可以学习和固定。在这项工作中，我们使用不同频率的正弦和余弦函数，其中pos是位置，i是维度。<br>$$<br>PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})<br>PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>$$<br>也就是说，位置编码的每个维度对应于一个正弦曲线，波长形成从2π到10000·2π的几何级数。当然还有其他的方法，不过选择正弦曲线版本是因为它可以使模型外推到比训练期间遇到的序列长的序列长度</p>\n<h1 id=\"Why-Self-Attention\"><a href=\"#Why-Self-Attention\" class=\"headerlink\" title=\"Why Self-Attention\"></a>Why Self-Attention</h1><p>考虑一下三点：</p>\n<ul>\n<li>每层的总计算复杂度</li>\n<li>可以并行化的计算量，以所需的最少顺序操作数衡量</li>\n<li>网络中远程依赖关系之间的路径长度，在许多序列转导任务中，学习远程依赖性是一项关键挑战。影响学习这种依赖性的能力的一个关键因素是网络中前向和后向信号必须经过的路径长度。输入和输出序列中位置的任意组合之间的这些路径越短，学习远程依赖关系就越容易</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200917175632580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>作为附带的好处，自我关注可以产生更多可解释的模型</p>\n<h1 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h1><h2 id=\"Training-Data-and-Batching\"><a href=\"#Training-Data-and-Batching\" class=\"headerlink\" title=\"Training Data and Batching\"></a>Training Data and Batching</h2><p>我们对标准WMT 2014英语-德语数据集进行了培训，该数据集包含约450万个句子对。句子是使用字节对编码的，字节对编码具有大约37000个token的共享源目标词汇。</p>\n<h2 id=\"Hardware-and-Schedule\"><a href=\"#Hardware-and-Schedule\" class=\"headerlink\" title=\"Hardware and Schedule\"></a>Hardware and Schedule</h2><p>大型模型接受了300,000步（3.5天）的训练。</p>\n<h2 id=\"Optimizer\"><a href=\"#Optimizer\" class=\"headerlink\" title=\"Optimizer\"></a>Optimizer</h2><p>我们使用Adam优化器，其中β1= 0.9，β2= 0.98和$\\xi $= 10-9。 根据公式，我们在训练过程中改变了学习率：<br>$$lrate=d_{model}^{-0.5}\\cdot min(step_num^{-0.5},step_num\\cdot warmup_steps^{-1.5})$$<br>这对应于第一个warmup_steps训练步骤的线性增加学习率，此后与步骤数的平方根的平方成反比地降低学习率，我们使用的warmup_steps=4000。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><ul>\n<li>Residual Dropout</li>\n<li>Label Smoothing</li>\n</ul>\n<h1 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h1><p><img src=\"https://img-blog.csdnimg.cn/20200917203338187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在这项工作中，我们介绍了Transformer，这是完全基于注意力的第一个序列转导模型，用多头自注意力代替了编码器-解码器体系结构中最常用的循环层。对于翻译任务，与基于循环层或卷积层的体系结构相比，可以比在体系结构上更快地训练Transformer。 在WMT 2014英语到德语和WMT 2014英语到法语的翻译任务中，我们都达到了最新水平。 在前一项任务中，我们最好的模型甚至胜过所有先前报告。我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。 我们计划将Transformer扩展到涉及文本以外的涉及输入和输出形式的问题，并研究局部受限的注意机制，以有效处理大型输入和输出，例如图像，音频和视频。 使生成减少连续性是我们的另一个研究目标。</p>\n","categories":["Paper-Reading"],"tags":["Transformer","Attention","注意力机制","Paper"]},{"title":"论文阅读笔记：Covariate-Shift：基于机器学习分类器的回顾和分析","url":"/Paper-Reading/12f6d260a557/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Covariate Shift: A Review and Analysis on Classifiers<br>原文链接：<a href=\"https://ieeexplore.ieee.org/abstract/document/8978471\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>我们都知道在机器学习模型中，训练数据和测试数据是不同的阶段，并且，通常是是假定训练数据和测试数据点遵循相同的分布。但是实际上，模型的输入和输出的联合分布在训练数据和测试数据之间是不同的，这称为dataset shift。dataset shift的一种简单情况就是covariate shift，covariate shift仅输入分布发生变化，而在给定输入的输出条件分布保持不变。本文主要概述了现有covariate shift检测和自适应方法及其应用，同时基于包含合成数据和真实数据的四种数据集，提供了各种covariate shift自适应技术在分类算法上的实验效果分析。实验结果标明，使用Importance Reweighting（重要性重加权）方法和feature-dropping方法能够让机器学习模型在covariate shift问题的表现上有明显提高。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>熟悉机器学习的小伙伴应该都知道，Supervised learning中涉及的步骤包括从真实来源收集数据，数据整合，数据转换，对已知数据进行训练和验证算法，最后将其应用于未知测试数据。所以为了提高这些Supervised learning算法的性能，数据质量起着重要作用。 可以基于各种角度来分析数据质量，例如数据复杂性，缺失值，噪声，数据不平衡，离群值，缩放值等。而在决定机器学习模型的性能中，起着重要作用的一种数据质量度量是dataset shift。它是下面提到的三种shifts的总称：</p>\n<ul>\n<li>Covariate Shift：Change in the independent variables</li>\n<li>Prior probability shift: Change in the target variable</li>\n<li>Concept Shift: Change in the relationship between the independent and the target variable</li>\n</ul>\n<p>在假设测试和训练数据中存在的点或实例属于相同的特征空间和相同的分布的假设下，常用的机器学习模型可以很好地工作，但是，当分布发生变化时，需要使用新的训练数据从头开始重建基础统计模型。</p>\n<p>Covariate Shift一词描述为学习阶段和泛化阶段之间（训练数据和测试数据）输入变量“ X”的分布变化。虽然Covariate Shift是dataset shifts中研究最多的shifts类型，但没有很合适的确切定义。比如在机器学习的角度来看，这种预测性建模通常称为transfer learning，也有一些相似的名称，但是概念上的差异很小，例如population drift，concept drift， dataset shift。以下是文献中存在的几种Covariate Shift定义：</p>\n<ul>\n<li>令 $x$ 为解释变量或协变量，$q_1(x)$是评估预测时的概率密度，$q_0(x)$ 表示观察数据中的概率密度，则$q_0(x)\\neq q_1(x)$ 的情况称为分布的Covariate Shift。</li>\n<li>产生特征向量 $x$ 及其相关类别标签 $y$ 的数据分布由于潜在变量 $t$ 而变化，因此当 $P(y|x,t_1)\\neq P(y|x,t_2)$ 时，可以说发生了Covariate Shift。</li>\n</ul>\n<h1 id=\"Covariate-Shift检测和自适应算法\"><a href=\"#Covariate-Shift检测和自适应算法\" class=\"headerlink\" title=\"Covariate Shift检测和自适应算法\"></a>Covariate Shift检测和自适应算法</h1><p>可以通过使用以下公式给出的重要性权重来消除因Covariate Shift而导致的预测误差：<br>$$W(X)=\\frac{p_{test}(X)}{p_{train}(X)} \\tag{1}$$<br>其中 $p_{test}(X)$ 和 $p_{train}(X)$ 分别是在测试和训练数据集中找到输入 $X$ 的概率。公式（1）来自这样的直觉，即如果特定训练实例出现在测试集中的概率很高，则它必须获得更高的权重。$W(X)$ 给出每个训练输入点的重要性值，将其与这些点相乘将得出更准确的预测。但是，此值是先验未知的，因此需要从数据样本中估算其值，因此，接下来分别列出一些在该领域中引入的最重要的重要性估计方法。</p>\n<h2 id=\"Kernel-Density-Estimation-KDE\"><a href=\"#Kernel-Density-Estimation-KDE\" class=\"headerlink\" title=\"Kernel Density Estimation (KDE)\"></a>Kernel Density Estimation (KDE)</h2><p>KDE是一种非参数方法，用于获得随机变量的概率密度函数的近似值，公式（2）是高斯核方程，公式（3）是KDE的方程<br>$$K(x,x^{‘})=exp(\\frac{-||x-x^{‘}||^2}{2\\sigma^2})\\tag{2}$$    $$\\hat{p}(x)=\\frac{1}{n(2\\pi \\sigma^2)^{\\frac{d}{2}}}\\sum_{i=1}^nK_{\\sigma}(x-x_i)\\tag{3}$$<br>其中，$x$ 和 $x^{‘}$ 是两个内核样本，$\\sigma$ 是内核宽度。KDE给出的近似值的精度完全由上式中选定的 $\\sigma$ 值确定。$\\sigma$ 的最佳值可以通过交叉验证获得，因此，训练和测试数据点可用于通过等式（2）分别获得 $\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ ，并且重要性可以估计为：<br>$$W(X)=\\frac{\\hat{p}<em>{test}(X)}{\\hat{p}</em>{train}(X)}$$<br>但是，上面讨论的方法受到维数的限制，而且支持可靠逼近所需的数据量通常随维数呈指数增长，这在数据样本数量有限的情况下非常复杂。因此，KDE无法用于高维数据，一种解决方法是直接找到 $W(X)$ 而无需计算 $p_{test}(X)$ 和 $p_{train}(X)$ 。</p>\n<h2 id=\"Discriminative-Learning\"><a href=\"#Discriminative-Learning\" class=\"headerlink\" title=\"Discriminative Learning\"></a>Discriminative Learning</h2><p>概率分类器也可以用来直接估计重要性，从训练集中提取的样本标记为 $\\mu = 0$，从测试集中提取的样本标记为 $\\mu = 1$。则概率密度可以表示为如下：<br>$$p_{tr}(X)=p(X|\\mu=0) \\ and \\  p_{te}(X)=p(X|\\mu=1)$$<br>使用贝叶斯定理，重要性权重 $W(X)$ 可写为：<br>$$W(X)=\\frac{p_{tr}}{p_{te}}=\\frac{p(\\mu=0)p(\\mu=1|X) }{p(\\mu=1)p(\\mu=0|X) }$$<br>其中 $\\frac{p(\\mu=0)}{p(\\mu=1)}\\approx \\frac{n_{tr}}{n_{te}}$ 可以容易得到。可以通过使用Logistic回归，随机森林，SVM等分类器区分 ${x_i}<em>{i=1}^{n</em>{tr}}$ 和 ${x_j}<em>{j=1}^{n</em>{te}}$ 来近似估计概率 $p(\\mu|X)$。在此还需要注意的是，可以将训练样本与测试样本分离的概率用作检测数据集中是否存在Covariate Shift的度量，在本文中称为<em>判别测试</em>。但是，训练这些模型有时会很耗时，因此，已经引入了有效的概率分类方法，例如LSPC （最小二乘概率分类器）和IWLSPC（结合了重要性加权LSPC和重要性重加权LSPC）。</p>\n<h2 id=\"Kernel-Mean-Matching\"><a href=\"#Kernel-Mean-Matching\" class=\"headerlink\" title=\"Kernel Mean Matching\"></a>Kernel Mean Matching</h2><p>KMM直接能获得 $W(X)$ 而无需计算 $p_{test}(X)$ 和 $p_{train}(X)$ ，KMM的基本思想是找到ܹ $W(X)$ ，从而使再现核Hilbert（RKHS）空间中的训练点和测试点的方法接近。等式（2）中的高斯核是计算通用RKHS核的示例，并且已证明下式给出的优化问题的解给得出真实的重要性值：<br>$$min_{w_i}[\\frac{1}{2}\\sum_{i,i^{‘}=1}^{n_{tr}}w_iw_{i^{‘}}K_{\\sigma(x_i^{tr},x_{i^{‘}}^{tr})}-sum_{i=1}^{n_{tr}}w_iK_i]\\tag{4}$$<br>其中，$(\\frac{1}{n_{tr}})|\\sum_{i=1}^{n_{tr}}w_i-n_{tr}\\leq \\epsilon| \\ and\\  0\\leq w_1,w_2,w_3,…,w_{n_{tr}}\\leq B$，且$K_i=\\frac{n_{tr}}{n_{te}}\\sum_{j=1}^{n_{te}}K_\\sigma(x_i^{tr},x_j^{te})$</p>\n<p>KMM的性能完全取决于调整参数$B$，$\\epsilon$ 和 $\\sigma$ 的值，因此，诸如交叉验证之类的常规模型选择方法无法找到最佳值。KMM的一种变体解决方案是，$\\sigma$ 选择一个样本间的中值距离。实验证明KMM优于natural plug-in估算器</p>\n<h2 id=\"Kullback-Leblier-Importance-Estimation-Procedure-KLIEP\"><a href=\"#Kullback-Leblier-Importance-Estimation-Procedure-KLIEP\" class=\"headerlink\" title=\"Kullback Leblier Importance Estimation Procedure(KLIEP)\"></a>Kullback Leblier Importance Estimation Procedure(KLIEP)</h2><p>通过交叉验证完成的算法（例如KMM）可能会因Covariate Shift下的偏差导致模型选择失败，因此在CV上使用了重要性加权版本IWCV（重要性加权交叉验证）。但是，在IWCV中，模型选择需要通过重要性估计步骤内的无监督学习来完成，这是一个主要缺点。KLEIP找到重要性估计$\\hat{w}(x)$，以使以使真实测试输入密度 $p_{te}(x)$ 和 $\\hat{p}<em>{te(x)}$ 之间的Kullback-Leibler方差最小，其中$\\hat{p}</em>{te(x)}=\\hat{w}(x)p_{tr}(x)$，这样无需显式建模 $p_{te}(x)$ 和 $p_{tr}(x)$ 即可完成此操作。</p>\n<h2 id=\"Least-Squares-Importance-Fitting-LSIF-Unconstrained-Least-Squares-Importance-Fitting-uLSIF\"><a href=\"#Least-Squares-Importance-Fitting-LSIF-Unconstrained-Least-Squares-Importance-Fitting-uLSIF\" class=\"headerlink\" title=\"Least Squares Importance Fitting (LSIF), Unconstrained Least Squares Importance Fitting (uLSIF)\"></a>Least Squares Importance Fitting (LSIF), Unconstrained Least Squares Importance Fitting (uLSIF)</h2><p>KLIEP使用Kullback-Leibler散度找出两个函数之间的密度差异，LSIF使用平方损失代替$\\hat{w}(x)$建模为KLIEP。交叉验证用于查找诸如正则化参数和内核宽度 $\\sigma$ 之类的调整参数的最佳值。但是，由于数字误差的累积，LSIF有时会给出错误的结果，为了解决这个问题，已经提出了一种近似形式的LSIF，称为uLSIF，它可以通过简单地求解线性方程组来进行解的计算，因此，uLSIF在数值上是稳定的。</p>\n<h1 id=\"Covariate-Shift自适应实际应用\"><a href=\"#Covariate-Shift自适应实际应用\" class=\"headerlink\" title=\"Covariate Shift自适应实际应用\"></a>Covariate Shift自适应实际应用</h1><ul>\n<li>半监督speaker识别：与会话有关的变化，录音场景中的变化以及身体情感变化等</li>\n<li>人脸识别判断年龄：由于环境中光照条件的变化，训练和测试数据倾向于具有不同的分布。</li>\n<li>基于脑电图的脑机接口：脑信号的非平稳性质</li>\n<li>…</li>\n</ul>\n<h1 id=\"实验结果：分类算法的性能分析\"><a href=\"#实验结果：分类算法的性能分析\" class=\"headerlink\" title=\"实验结果：分类算法的性能分析\"></a>实验结果：分类算法的性能分析</h1><p>实验中使用的数据集是一个数据集来自Kaggle仓库包含三个合成数据集，实验的分类算法如下：</p>\n<ul>\n<li>线性判别分析</li>\n<li>K邻近算法</li>\n<li>决策树分类器</li>\n<li>朴素贝叶斯分类器</li>\n</ul>\n<h2 id=\"训练和测试数据为不同分布\"><a href=\"#训练和测试数据为不同分布\" class=\"headerlink\" title=\"训练和测试数据为不同分布\"></a>训练和测试数据为不同分布</h2><ul>\n<li>处理技术：Discriminative Learning</li>\n</ul>\n<p>实验用数据集（称为数据集-I）具有1000个训练样本和1000个测试样本，其中训练样本的分布是正态的，而测试样本的分布是二项式的，训练样本包含特征变量 $X$，$Y$。训练集是通过选择1000个遵循均匀分布且方差= 1和均值= 25（在下面的等式中表示为“data”）的随机样本而形成的：<br>$$X=11\\times data-6\\tag{5a}$$    $$Y=X^2+10\\times X -5\\tag{5b}$$<br>类似地，测试集由遵循二项式分布的1000个随机数组成，被选择的概率为 $p = 0.8$，值的范围为1到20。特征$X$，$Y$使用上式计算。 $\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ 的分布如下：<br><img src=\"https://img-blog.csdnimg.cn/20201226121106135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实验中，LDA，KNN和决策树分类器的准确性得分有所提高，而朴素贝叶斯分类器的准确性得分下降。</p>\n<h2 id=\"训练和测试数据不同的均指和方差\"><a href=\"#训练和测试数据不同的均指和方差\" class=\"headerlink\" title=\"训练和测试数据不同的均指和方差\"></a>训练和测试数据不同的均指和方差</h2><ul>\n<li>处理技术：KDE</li>\n</ul>\n<p>第二个实验数据集（称为数据集-II）具有相同分布但均值和方差不同，训练和测试集分别有1000个样本，且训练和测试样本的分布都均匀。训练集包含两个特征 $X$，$Y$。将生成1000个均值为25和方差为1的随机值。类似地，创建具有正态分布的测试数据，并由$X$，$Y$两列组成。选择了均值为80，方差为1的1000个随机数。$\\hat{p}<em>{test}(X)$ 和 $\\hat{p}</em>{train}(X)$ 的分布如下：<br><img src=\"https://img-blog.csdnimg.cn/20201226125001315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表实验结果显示，所有分类算法的性能都有提高：<br><img src=\"https://img-blog.csdnimg.cn/20201226125124775.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"训练和测试数据具有相同的分布，且属性数量增加\"><a href=\"#训练和测试数据具有相同的分布，且属性数量增加\" class=\"headerlink\" title=\"训练和测试数据具有相同的分布，且属性数量增加\"></a>训练和测试数据具有相同的分布，且属性数量增加</h2><ul>\n<li>处理技术：KLIEP</li>\n</ul>\n<p>对于第三个实验，生成了两个具有正态分布的数据集。训练和测试集大约有500个样本和3个属性$X$，$Y$和$Z$，其中$X$，$Y$是输入属性，$Z$是预测标签，$Z$的计算公式如下：<br>$$Z=sin(Y\\times \\pi)+X\\tag{6}$$<br><img src=\"https://img-blog.csdnimg.cn/20201226125627102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实验中，决策树分类器的性能提高约30%，其他三个分类器减少近20%。</p>\n<h2 id=\"真实数据\"><a href=\"#真实数据\" class=\"headerlink\" title=\"真实数据\"></a>真实数据</h2><p>此实验的数据集是从Kaggle仓库中提取的“俄罗斯住房市场”数据集（Dataset-IV）进行的。<br><img src=\"https://img-blog.csdnimg.cn/20201226130159555.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>机器学习算法的性能是将其用于现实世界场景中要考虑的重要因素， 它在很大程度上取决于数据集和数据的分布。当将诸如决策树或神经网络之类的机器学习模型在一个场景下训练并利用其来提高另一种场景下的泛化时，则发生的域自适应称为转移学习。但是在监督学习算法中，要确保模型在训练和测试场景中都能正常工作，重要的是要确保训练样本和测试样本的分布相同。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","AutoGraph","计算图","Covariate Shift"]},{"title":"论文阅读笔记：CrossWOZ：A-Large-Scale-Chinese-Cross-Domain-Task...","url":"/Paper-Reading/0e5d72d373dc/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset<br>原文链接：<a href=\"https://arxiv.org/pdf/2002.11893.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>最近在搜集一些对话数据集，众所周知，生成对话数据集是一件费钱又费时的工作，所以一般只有大机构才能做出高质量且庞大的数据集，所以看到好的数据集，那还不赶紧收藏一波。<br><a href=\"https://github.com/thu-coai/CrossWOZ\">CrossWOZ代码和数据</a></p>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>为了推进多域（跨域）对话建模并缓解中文面向任务的数据集的不足的问题，我们提出了CrossWOZ，这是第一个大规模的中文跨域“人机交互”任务导向的数据集。CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。大约60％的对话具有跨域用户目标，这些目标有利于域间依赖性，并有助于对话中跨域自然过渡。我们还为pipeline的面向任务的对话系统提供了一个用户模拟器和一些基准模型，这将有助于研究人员在该语料库上比较和评估他们的模型。CrossWOZ的规模庞大和丰富注释使它适合研究跨域对话建模中的各种任务，例如对话状态跟踪，策略学习，用户模拟等。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>许多语料库已经推进了面向任务的对话系统的研究，不过大部分都是单领域对话，比如ATIS、DSTC 2、Frames、KVRET、WOZ 2.0和M2M等。这些数据集的大小，语言变化或任务复杂性仍然受到限制。在现实生活中的对话中，人们自然会在不同领域或场景之间进行转换，同时仍保持连贯的上下文，因此，现实对话比仅在单个域中模拟的对话要复杂得多。提及到多领域对话语料，最多被提及的就是MultiWOZ，但是，这个数据集状态注释有很多噪音，并且缺少用户端对话行为。<strong>跨域的依赖关系可以简单地体现为对不同的域施加相同的预先规定的约束，例如要求将旅馆和景点都定位在城镇中心。</strong>，如下是一个对话示例：<br><img src=\"https://img-blog.csdnimg.cn/20200927172302406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>跨领域对话的数据样例</strong><br><img src=\"https://img-blog.csdnimg.cn/20200927172857102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本数据集的特点：</p>\n<ul>\n<li> 用户在某个领域的选择可能会影响到与之相关的领域的选择，在跨领域上下文理解更有挑战。</li>\n<li> 第一个大规模中文跨领域任务导向数据集。</li>\n<li> 在用户端和系统端都有详细的对话状态记录，标注信息全面。</li>\n</ul>\n<p><strong>与其他数据集的对比</strong><br><img src=\"https://img-blog.csdnimg.cn/20200927173339629.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>对话数据的收集方式有三类：</p>\n<ul>\n<li>human-to-human</li>\n<li>human-to-machine</li>\n<li>machine-to-machine</li>\n</ul>\n<h1 id=\"Data-Collection\"><a href=\"#Data-Collection\" class=\"headerlink\" title=\"Data Collection\"></a>Data Collection</h1><p>语料库是模拟旅行者寻找旅游信息并计划其在北京旅行的场景，domains包括酒店，景点，餐厅，地铁和出租车。 数据收集过程总结如下：</p>\n<ol>\n<li>基础数据库的构建：通过爬虫从网络上获取了北京市的酒店/旅游景点/饭店以及地铁和出租车信息。</li>\n<li>目标构建：论文通过算法自动生成标注人员的对话目标。</li>\n<li>对话收集：要求工人进行少量对话，并向他们提供有关对话质量的反馈。</li>\n<li>数据标注：每个对话都包含结构化的目标，任务描述，用户状态，系统状态，对话行为和对话。</li>\n</ol>\n<p>基础数据库信息示例如下：<br><img src=\"https://img-blog.csdnimg.cn/20200927180441535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>用户目标示例如下：<br><img src=\"https://img-blog.csdnimg.cn/20200927180519346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Statistics\"><a href=\"#Statistics\" class=\"headerlink\" title=\"Statistics\"></a>Statistics</h1><p><img src=\"https://img-blog.csdnimg.cn/20200927182041653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200927182214247.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200927182241872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Benchmark-and-Analysis\"><a href=\"#Benchmark-and-Analysis\" class=\"headerlink\" title=\"Benchmark and Analysis\"></a>Benchmark and Analysis</h1><p><img src=\"https://img-blog.csdnimg.cn/20200927182351883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了第一个大规模的中文跨域任务导向对话数据集，CrossWOZ。CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域。此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。大约60％的对话具有跨域用户目标，这鼓励了相关域之间的自然过渡。得益于丰富的对话状态注释以及用户端和系统端的对话行为，该语料库为研究跨域对话建模的各种任务（例如对话状态跟踪，策略学习等）提供了新的测试平台。我们的实验表明，跨域约束对于所有这些任务都是具有挑战性的。相关领域之间的转换对建模尤其具有挑战性。 除了基于语料库的组件级评估外，我们还使用用户模拟器执行了系统级评估，这需要针对pipeline跨域对话系统的所有组件提供更强大的模型。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","面向任务","CrossWOZ"]},{"title":"论文阅读笔记：Latent-Intention-Dialogue-Models","url":"/Paper-Reading/f52747614156/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Latent Intention Dialogue Models<br>原文链接：<a href=\"https://arxiv.org/pdf/1705.10229.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>开发能够做出自主决策并通过自然语言进行交流的对话代理是机器学习研究的长期目标之一。传统方法要么依靠手工制作一个小的状态动作集来应用不可扩展的强化学习，要么构建确定性模型来学习无法捕获自然对话可变性的对话语句。论文提出了一种隐意图对话模型（Latent Intention Dialogue Model, LIDM），通过离散的隐变量来学习对话意图，这些隐变量可以看作引导对话生成的动作决策，进而运用强化学习可以提升性能。实际上在任务型对话中，这个隐含的意图可以理解为是action。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>本文的贡献有两个方面：首先，我们表明神经变分推理框架能够从数据中发现离散的，可解释的意图，从而形成对话主体的决策基础。其次，代理能够基于相同框架内的外部奖励来修改其对话策略，这很重要，因为它为构建自主对话代理程序提供了垫脚石，该对话代理程序可以通过与用户交互来不断提高自身水平。实验结果证明了我们的潜在意图模型的有效性，该模型在基于语料库的自动评估和人工评估方面均达到了最新水平。</p>\n<h1 id=\"Latent-Intention-Dialogue-Model-for-Goal-oriented-Dialogue\"><a href=\"#Latent-Intention-Dialogue-Model-for-Goal-oriented-Dialogue\" class=\"headerlink\" title=\"Latent Intention Dialogue Model for Goal-oriented Dialogue\"></a>Latent Intention Dialogue Model for Goal-oriented Dialogue</h1><blockquote>\n<p>Knowledge graph（KG）和Knowledge base（KB）几乎可以看做同义词，只不过Knowledge base是知识库，而Knowledge graph则是基于知识库的图结构。</p>\n</blockquote>\n<p>LIDM基于(Wen et al,2017，这篇文章也有<a href=\"https://dengbocong.blog.csdn.net/article/details/108912048\">笔记</a>)中描述的端到端系统架构。目标导向型对话通过人机对话交互，帮助用户完成特定的任务。给定一个用户输入 $u_t$ 和知识库(KB)，模型需要将输入解析为可执行的命令Q在知识库搜索。基于返回的搜索结果，模型需要以自然语言的形式给出回复 ，整体模型结构如下<br><img src=\"https://img-blog.csdnimg.cn/2020100809581517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>将分为三个模块：Representation Construction，Policy Network和Generator。</p>\n<h2 id=\"Representation-Construction\"><a href=\"#Representation-Construction\" class=\"headerlink\" title=\"Representation Construction\"></a>Representation Construction</h2><p>表征构建部分主要是为了捕捉用户的意图和获取知识库查询结果。$S_t$ 是对话状态向量，$u_t$ 是用户输入经过BiLSTM的最后隐层状态；belief vector 是特定槽-值队的概率分布拼接，通过预训练的RNN-CNN belief tracker抽取。$m_{t-1}$ 是上一轮的机器回复，$b_{t-1}$ 是上一轮的belief vector。基于belief vector，查询Q与知识库交互，并返回表示匹配程度的one-hot vector $x_t$。</p>\n<ul>\n<li>将utterance通过BiLSTM取两个方向最后一个时刻的隐层状态concat后得到表征 $u$<br>$$u_t=biLSTM_\\Theta(u_t)$$</li>\n<li>分别对utterance句子 $u_t$ 和系统上一时刻的回复 $m_{t-1}$ 先用CNN，尔后通过一个RNN做轮与轮之间的belief tracker<br>$$b_t=RNN-CNN(u_t,m_{t-1},b_{t-1})$$</li>\n<li>通过当前轮得到的 $b$ 生成一个查询语句Q(<strong>基于belief向量，通过取每个槽位的最大值的并集来形成查询Q</strong>)，查询KB后能够得到一个结果表征 $x$（<strong>表示KB中的匹配程度</strong>）</li>\n<li>将$u,b,x$进行concatenation后得到state表征$s$<br>$$s_t=u_t⊕b_t⊕x_t$$</li>\n</ul>\n<h2 id=\"Policy-Network-and-Generator\"><a href=\"#Policy-Network-and-Generator\" class=\"headerlink\" title=\"Policy Network and Generator\"></a>Policy Network and Generator</h2><p>基于之前构建的对话状态，策略网络通过一个MLP输出latent intention（实际上就是一个action） $z_t$。<br>$$\\pi_\\Theta (z_t|s_t)=softmax(W_2^T\\cdot tanh(W_1^Ts_t+b_1)+b_2)$$  $$z_t^{(n)}\\sim \\pi_\\Theta(z_t|s_t)$$<br>和以往的隐变量是一个多维值连续的向量不同，这里的 $z_t$ 是离散的多个类别，类别数是人工设定的，$\\pi_t$ 输出的是 $z_t$ 的softmax概率。从强化学习的角度来看，一个隐意图 $z_t^{(n)}$ 可以看作是一个动作。从策略网络采样得到一个隐意图 $z_t^{(n)}$ 加上之前的状态向量$S_t$，计算出控制向量 $d_t$，作为条件控制LSTM 语言模型生成回复。<br>$$d_t=W_4^Tz_t⊕[sigmoid(W_3^Tz_t+b_3)\\cdot W_5^Ts_t]$$  $$p_\\Theta(m_t|s_t,z_t)=\\coprod_{j}p(w_{j+1}^t|w_j^t,h_{j-1}^t,d_t)$$<br>上面两式子中的 $z_t$ 是 $z_t^{(n)}$的one-hot形式，$w_j^t$是生成的第 $t$ 轮对话的最后一个词，$h_{j-1}^t$ 论文说是解码器的最后一个隐层状态（下标是 $j-1$有点奇怪）。从而，LIDM模型的公式化表示就是：<br>$$p_\\Theta(m_t|s_t)=\\sum_{z_t}p_\\Theta(m_z|z_t,s_t)\\pi_\\Theta(z_t|s_t)$$</p>\n<h2 id=\"Inference\"><a href=\"#Inference\" class=\"headerlink\" title=\"Inference\"></a>Inference</h2><p>LIDM模型的公式化式很难直接求解，因为有隐变量的存在，套路是将其转化为优化下界。变分推断部分，LIDM通过识别网络 $q_{\\phi}(z_t|s_t,m_t)$ 来逼近隐变量的后验分布 $p(z_t|s_t,m_t)$, 训练时优化变分下界。<br>$$L =\\mathbb{E}_{q\\phi (z_t)}[logp_\\Theta (m_t|z_t,s_t)]-\\lambda D_{KL}(q_\\phi (z_t)||\\pi_\\Theta (z_t|s_t))$$  $$\\leq log\\sum_{z_t}p_\\Theta (m_t|z_t,s_t)\\pi_\\Theta (z_t|s_t)$$  $$=logp_\\Theta (m_t|s_t)$$<br>其中识别网络 $q_{\\phi}(z_t|s_t,m_t)$<br>$$q_{\\phi}(z_t|s_t,m_t)=Multi(o_t)=softmax(W_6o_t)$$   $$o_t=MLP_\\phi(b_t,x_t,u_t,m_t)$$   $$u_t=biLSTM_\\phi(u_t),m_t=biLSTM_\\phi(m_t)$$<br>而值得注意的是，目标函数中包含三个不同的模型，因此各自的参数也各不相同，需要分开进行优化</p>\n<ul>\n<li>对于生成回复模型（即p(m|z,s)部分）的参数优化<br><img src=\"https://img-blog.csdnimg.cn/20201008121203569.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>对于policy network的参数优化<br><img src=\"https://img-blog.csdnimg.cn/20201008121241452.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>对于inference network的参数优化（这里我觉得文中定义的r(m,z,s)似乎少了一项）<br><img src=\"https://img-blog.csdnimg.cn/20201008121319185.png#pic_center\" alt=\"在这里插入图片描述\"><h2 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h2>变分推断的隐变量 $z$ 是从一个分布采样得到，存在high variance的缺点，并且容易陷入LSTM语言模型的disconnection 现象中。论文通过半监督学习提升训练的稳定性和阻止disconnction。针对这些问题，论文提出用Semi-Supervision和Reinforcement learning缓解这一问题。<h3 id=\"Semi-Supervision\"><a href=\"#Semi-Supervision\" class=\"headerlink\" title=\"Semi-Supervision\"></a>Semi-Supervision</h3>论文将隐意图的推断看作是无监督的聚类任务，因此运用标准的聚类算法处理一部分语料，并生成标签 $\\hat{z}_t$，并把它当作一个观测变量。训练的数据分成两部分：未标注的样本集 $(m_t,s_t)\\in\\mathbb{U}$ 和聚类生成标注样本集 $(m_t,s_t,\\hat{z})\\in\\mathbb{L}$ ，分别用不同的目标函数：</li>\n<li>将未能够聚类的样例U，继续利用上面的方法进行训练；<br>$$L_1 =\\sum_{(m_t,s_t)\\in U}\\mathbb{E}_{q\\phi (z_t|s_t,m_t)}[logp_\\Theta (m_t|z_t,s_t)]-\\lambda D_{KL}(q_\\phi (z_t|s_t,m_t)||\\pi_\\Theta (z_t|s_t))$$</li>\n<li>将能够聚类得到了标签的样例则利用下式进行优化<br>$$L_2 =\\sum_{(m_t,\\hat{z}_t,s_t)\\in L}log[p_\\Theta(m_t|\\hat{z}_t,s_t)\\pi_\\Theta(\\hat{z}_t|s_t)q_\\phi (\\hat{z}_t|s_t,m_t)]$$</li>\n<li>二者形成一个joint objective，$\\alpha$是监督样本和无监督样本之间的权衡因子。<br>$$L=\\alpha L_1+L_2$$</li>\n</ul>\n<h3 id=\"Reinforcement-Learning\"><a href=\"#Reinforcement-Learning\" class=\"headerlink\" title=\"Reinforcement Learning\"></a>Reinforcement Learning</h3><p>策略网络 $\\pi_\\Theta(z_t|s_t)$ 是从潜在的数据分布学习到的，但不一定是最优的决策。论文提出用强化学习对策略网络进行微调参数，对未标注的数据集 $\\mathbb{U}$ 训练过程如下： 从策略网络采样得到动作 $z_t^{(n)}$，得到反馈 $r_t^{(n)}$，进行梯度更新：<br>$$\\frac{\\partial \\jmath}{\\partial \\theta }\\approx \\frac{1}{N}\\sum_{n}r_t^{(n)}\\frac{\\partial log\\pi_\\theta(z_t^{(n)}|s_t)}{\\partial \\theta’}$$<br><img src=\"https://img-blog.csdnimg.cn/20201008155355324.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验用的数据集是<strong>CamRest676 corpus</strong>，是一个预订餐馆的对话语料。用户可以用三个信息槽（food, pricerange, area）来限制搜索的范围，一旦完成预订，系统可以返回另外三个信息（address, phone, postcode）。语料共有676个对话，将近2750轮。对话隐意图的数量I设置为50，70和100。实验结果用task success rate和BLEU作为指标。<br><img src=\"https://img-blog.csdnimg.cn/20201008155700274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Ground Truth: human-authored response</li>\n<li>NDM：the vanilla neural dialogue model</li>\n<li>NDM+Att: NDM plus an attention mechanism on the belief tracker</li>\n<li>NDM+Att+SS: the attentive NDM with self-supervised sub-task neuron</li>\n</ul>\n<p>可以看到，LIDM模型在BLEU指标上有良好的结果，但在任务成功率上表现不佳。将LIDM训练的策略网络作为initial policy，经过Reinforcement Learning微调参数后，任务成功率得到大幅度的提升，并且BLEU指标没有下降太多。</p>\n<p>论文还在Succss、Compression、Naturalness三个指标上进行人工打分，得到的结果如下：<br><img src=\"https://img-blog.csdnimg.cn/202010081559391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Comprehension和Naturalness的最高分是5分。可以看到，其实三个系统的任务成功率其实相差不是很大，不过在Comprehension和Naturalness上LIDM均超过了NDM。</p>\n<p>从下面的示例对话可以感受下隐意图变量控制生成的回复。不同的隐意图会生成不同的回复，但也可以看到一些隐意图（如intention 0）可以生成风格非常不同的回复，作者认为这是变分推断的variance性质造成的。</p>\n<p>另外一个有趣的现象是，经过RL训练后的LIDM，会采取更“贪婪”的追求任务成功的行为，具体表现在Table 5，一旦在数据库搜索到相关结果，会抢先用户提问之前将该餐馆的所有信息都返回给用户，这也造成对话的轮数更简短。这也说明增强学习训练效果的一个直观体现。<br><img src=\"https://img-blog.csdnimg.cn/20201008160057779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201008160114977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了一个通过离散潜在变量模型学习对话意图的框架，并介绍了用于目标导向对话模型的潜在意图对话模型（LIDM）。我们已经表明，LIDM可以从基础数据分布中发现有效的初始策略，并且能够使用强化学习基于外部奖励来修改其策略。我们认为，这是构建自主对话代理的有希望的一步，因为学习到的离散潜在变量接口使代理能够使用几种不同的范例进行学习。实验表明，所提出的LIDM能够与人类受试者进行交流，并且优于以前发表的结果。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","对话系统","Paper","Latent-Intention"]},{"title":"论文阅读笔记：Layer-Normalization","url":"/Paper-Reading/18004f87a32e/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Layer Normalization<br>原文链接：<a href=\"https://arxiv.org/pdf/1607.06450.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>训练最新的深度神经网络在计算上是昂贵的，减少训练时间的一种方法是归一化神经元，最近引入的一种称为<strong>批归一化</strong>的技术使用训练案例的小批量上神经元的总输入分布来计算均值和方差，然后使用均值和方差对每个训练案例中该神经元的总输入进行归一化，这大大减少了前馈神经网络的训练时间。但是，批归一化的效果取决于小批量的大小，如何将其应用于递归神经网络尚不明显。在本文中，我们通过在单个训练案例上计算从层的所有总输入到神经元的归一化的均值和方差，将批归一化转换为层归一化。像批归一化一样，我们还为每个神经元提供了自己的自适应偏差和增益，这些偏差和增益在归一化之后且在非线性之前应用。与批归一化不同，层归一化在训练和测试期间执行完全相同的计算。通过在每个时间步分别计算归一化统计量，将其应用于递归神经网络也很简单。层归一化在动态稳定循环网络中的隐藏状态方面非常有效。从经验上讲，我们表明与以前发布的技术相比，层归一化可以大大减少训练时间。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>已经证明，使用某种形式的随机梯度下降训练的深度神经网络在计算机视觉和语音处理的各种监督学习任务上的性能大大优于以前的方法。但是最先进的深度神经网络通常需要几天的训练，可以通过在不同机器上为训练案例的不同子集计算梯度或在许多机器上拆分神经网络本身来加快学习速度，但这可能需要大量的交互且复杂 软件，随着并行度的增加，这将导致收益迅速减少。正交方法是修改在神经网络的前向传递中执行的计算，以使学习更轻松。最近，提出了批归一化，以通过在深度神经网络中包括其他标准化阶段来减少训练时间。<strong>归一化使用训练数据中的平均值和标准差对每个求和的输入进行标准化</strong>，即使使用简单的SGD，使用批归一化训练的前馈神经网络也能更快收敛，除了改善训练时间外，批统计中的随机性还可以作为训练期间的正则化器。</p>\n<p><strong>尽管简单，但批归一化仍需要对求和的输入统计信息求平均值，在深度固定的前馈网络中，直接为每个隐藏层分别存储统计信息很简单，但是，递归神经网络（RNN）中递归神经元的总输入通常随序列长度而变化，因此对RNN应用批归一化似乎需要针对不同时间步长进行不同统计。此外，批归一化不能应用于在线学习任务或批必须很小的超大型分布式模型</strong>。</p>\n<p>本文介绍了层归一化，这是一种提高各种神经网络模型训练速度的简单归一化方法，与批量归一化不同，该方法从隐藏层内神经元的总输入直接估算归一化统计数据，因此归一化不会在训练案例之间引入任何新的依存关系，我们表明，层归一化对RNN效果很好，并改善了几种现有RNN模型的训练时间和泛化性能。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>前馈神经网络是从输入模式$x$到输出向量$y$的非线性映射，考虑深度前馈神经网络中的第$l$个隐藏层，并将$l$表示为该层中神经元的总输入的向量表示，通过权重矩阵$W^l$和自下而上的输入$h^l$的线性投影来计算总和输入，如下所示：<br>$$a_{i}^{l}={w_{i}^{l}}^{T}h^l $$ $$h_{i}^{l+1}=f(a_{i}^{l}+b_{i}^{l})$$<br>其中f(.)是逐个元素的非线性函数，而$w_{i}^{l}$是第$i$个隐藏单元的传入权重，$b_{i}^{l}$是标量偏差参数，使用基于梯度的优化算法学习神经网络中的参数，并通过反向传播计算梯度。</p>\n<p>深度学习的挑战之一是，相对于一层中权重的梯度高度依赖于上一层中神经元的输出，特别是如果这些输出以高度相关的方式变化时。因此提出了批归一化，以减少这种不希望的“Internal Covariate Shift”。该方法对训练案例中每个隐藏单元的求和输入进行归一化。 具体来说，对于第$l$层中的第$i$个求和输入，批归一化方法根据求和输入在数据分布下的方差重新缩放<br>$$\\bar{a}<em>{i}^{l}=\\frac{g</em>{i}^{l}}{\\sigma_{i}^{l}}(a_{i}^{l}-\\mu_{i}^{l})$$ $$\\mu_{i}^{l}=\\underset{X\\sim P(x)}{\\mathbb{E}}[a_{i}^{l}]$$  $$\\sigma_{i}^{l}=\\sqrt{\\underset{X\\sim P(x)}{\\mathbb{E}}[(a_{i}^{l}-\\mu_{i}^{l})^2]}$$<br>其中$\\bar{a}_{i}^{l}$是第$l$层中第$i$个隐藏单元的归一化总和输入，$g_i$是在非线性激活函数之前缩放归一化激活的增益参数，注意期望是在整个训练数据分布下。 在上面的等式中精准计算期望值通常是不切实际的，因为这将需要使用当前的权重集前向传递至整个训练数据集。 取而代之的是，使用当前小批量的经验样本估算出$µ$和$σ$，这限制了小批量生产的规模，并且很难应用于递归神经网络。</p>\n<p>Internal Covariate Shift一个较规范的定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p>\n<p>关于Batch Normalization的相关介绍，可以看这边<a href=\"https://zhuanlan.zhihu.com/p/34879333\">知乎文章</a>，里面写的蛮清晰的。</p>\n<h1 id=\"Layer-normalization\"><a href=\"#Layer-normalization\" class=\"headerlink\" title=\"Layer normalization\"></a>Layer normalization</h1><p>请注意，一层输出的变化将趋向于导致对下一层求和的输入发生高度相关的变化，尤其是对于ReLU单元，其输出可以变化$l$。这表明可以通过固定每一层内求和输入的均值和方差来减少“covariate shift”问题。因此，我们计算与以下相同层中所有隐藏单元的层归一化统计量：<br>$$\\mu^{l}=\\frac{1}{H}\\sum_{i=1}^{H}a_{i}^{l}$$ $$\\sigma^{l}=\\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_{i}^{l}-\\mu^l)^2}$$<br>其中H表示层中隐藏单元的数量，和batch Normalization不同的是，这是在层归一化下，层中所有隐藏单元共享相同的归一化项$μ$和$σ$，但是不同的训练案例具有不同的归一化项。</p>\n<p> 与批归一化不同，层归一化对小批处理的大小没有任何限制，它可以在批大小为1的在线方式中使用。</p>\n<h2 id=\"Layer-normalized-recurrent-neural-networks\"><a href=\"#Layer-normalized-recurrent-neural-networks\" class=\"headerlink\" title=\"Layer normalized recurrent neural networks\"></a>Layer normalized recurrent neural networks</h2><p>在NLP任务中，对于不同的训练案例，通常有不同的句子长度。这在RNN中很容易处理，因为每个时间步使用相同的权重。但是，当我们以明显的方式将批归一化应用于RNN时，我们需要为序列中的每个时间步计算并存储单独的统计信息。如果测试序列比任何训练序列都要长，这是有问题的。层归一化不存在此类问题，因为其归一化项仅取决于当前时间步对层的求和输入。 在所有时间步中，它也只有一组增益和偏置参数共享。</p>\n<p>在标准RNN中，根据当前输入$x^t$和隐藏状态$h^{t-1}$的计算递归层中的总输入，计算得出$a^t=W_{hh}h^{t-1}+W_{xh}x^t$<br>$$h^t=f[\\frac{g}{\\sigma^{t}}\\bigodot (a^t-\\mu^t)+b]$$ $$\\mu^t=\\frac{1}{H}\\sum_{i=1}^{H}a_{i}^{t}$$  $$\\sigma^t=\\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_{i}^{t}-\\mu^t)^2}$$</p>\n<p>$W_{hh}$是递归隐藏层的隐藏权重，$W_{xh}$是自底向上的输入的隐藏权重，$\\bigodot$是两向量之间逐元素点积，$b$和$g$定义为与$h^t$维度相同的偏置和增益参数。</p>\n<p>在标准RNN中，递归单元的总输入的平均幅度影响了每个时间步长增长或收缩，从而导致梯度爆炸或消失。在层归一化的RNN中，归一化项使将所有求和的输入重新缩放为层不变，这将导致更稳定的隐藏到隐藏的变化。</p>\n<h1 id=\"Related-work\"><a href=\"#Related-work\" class=\"headerlink\" title=\"Related work\"></a>Related work</h1><p>在权重归一化中，使用输入权重的L2范数代替方差来归一化对神经的求和输入，使用预期统计量应用权重归一化或批归一化等效于对原始前馈神经网络进行不同的参数化，但是，我们提出的层归一化方法不是对原始神经网络的重新参数化。因此，层归一化模型具有与其他方法不同的不变性，我们将在以下部分中研究该方法。</p>\n<h1 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h1><ul>\n<li>权重不变和数据转换</li>\n<li>训练期间参数的几何空间：paper证明归一化标量$σ$可以隐式降低学习率并使学习更稳定</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200924224142904.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Experimental-results\"><a href=\"#Experimental-results\" class=\"headerlink\" title=\"Experimental results\"></a>Experimental results</h1><p><img src=\"https://img-blog.csdnimg.cn/20200924224048429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224237130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224246129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200924224300892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们介绍了层归一化以加快神经网络的训练。我们提供了理论分析，将层归一化与批归一化和权重归一化的不变性进行了比较。我们显示了层归一化对于每个训练案例特征平移和缩放都是不变的。根据经验，我们证明了递归神经网络从拟议的方法中受益最大，特别是对于长序列和小型小批处理。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Paper","Layer-Normalization","归一化","白化"]},{"title":"论文阅读笔记：MultiWOZ-2.2","url":"/Paper-Reading/9f6b46ae549c/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines<br>原文链接：<a href=\"https://arxiv.org/pdf/2007.12720.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>最近在搜集一些对话数据集，众所周知，生成对话数据集是一件费钱又费时的工作，所以一般只有大机构才能做出高质量且庞大的数据集，所以看到好的数据集，那还不赶紧收藏一波。</p>\n<ul>\n<li><a href=\"https://github.com/budzianowski/multiwoz\">代码链接</a></li>\n<li><a href=\"http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\">数据集链接</a></li>\n<li><a href=\"https://arxiv.org/pdf/1810.00278.pdf\">MultiWOZ论文地址</a></li>\n<li><a href=\"https://arxiv.org/pdf/1907.01669.pdf\">MultiWOZ 2.1论文地址</a></li>\n</ul>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>MultiWOZ是一个著名的面向任务的对话数据集，其中包含10,000多个跨越8个域的带注释对话，而被广泛用作对话状态跟踪的基准。但是，最近的工作报告说，对话状态注释中存在大量噪音。MultiWOZ 2.1中识别并修复了许多错误的注释和用户话语，从而改进了该数据集的版本。本篇论文工作介绍了MultiWOZ 2.2，它是该数据集的又一个改进版本。首先，我们在MultiWOZ 2.1之上的17.3％话语中识别并修复对话状态注释错误。其次，我们通过不允许带有大量可能值的槽（例如，餐厅名称，预订时间）来重新定义数据集。此外，我们为这些插槽引入了插槽跨域注释，以在最近的模型中将它们标准化，该模型以前使用自定义字符串匹配启发法来生成它们。我们还会在更正后的数据集上对一些最新的对话状态跟踪模型进行基准测试，以方便将来进行比较。最后，我们讨论了对话数据收集的最佳做法，可以帮助避免注释错误。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>最近，数据驱动技术已针对不同的对话系统模块实现了最先进的性能，但是，由于训练上述模块需要广泛的注释，因此收集高质量的注释对话数据集仍然是研究人员的一个挑战。很多公共数据集，如DSTC2、WOZ、SimulatedDialogue、MultiWOZ、TaskMaster、SGD等等，对促进这一领域的研究非常有用。在这些数据集中，MultiWOZ是用于对话状态跟踪的最广泛使用的基准。它包含超过10,000个对话，涉及8个领域，分别是：餐厅，酒店，景点，出租车，火车，医院，公共汽车和警察。MultiWOZ 2.2做出了如下三点贡献：</p>\n<ul>\n<li>我们修复MultiWOZ 2.1中的注释错误，不一致和本体问题，并发布其改进版本。</li>\n<li>我们为用户和系统话语添加了插槽跨度注释，以在未来的模型中对其进行标准化。我们还为每个用户的语句注解活跃用户的意图和请求槽。</li>\n<li>我们在更正后的数据集中对一些最新的对话状态跟踪模型进行基准测试，以便于将来的工作进行比较。</li>\n</ul>\n<h1 id=\"Annotation-Errors\"><a href=\"#Annotation-Errors\" class=\"headerlink\" title=\"Annotation Errors\"></a>Annotation Errors</h1><p>Wizard-of-Oz（人机交互）：在此设置中，两个人群工作人员配对在一起，一个充当用户，另一个充当对话代理。每个对话由一组指定用户目标的唯一指令驱动，这些指令与扮演用户角色的人群共享。在每个用户转过身后，扮演对话代理（向导）角色的人群工人会注释更新后的对话状态。更新状态后，该工具会显示将对话状态与向导匹配的一组实体，然后向导使用该实体来生成响应并将其发送给用户。</p>\n<p>以下两类主要错误，但在MultiWOZ 2.1中未进行纠正：</p>\n<ul>\n<li>Hallucinated Values：幻觉值以对话状态存在，而未在对话历史记录中指定，我们观察到了四种不同类型的此类错误，它们在下图中显示并在下面进行了描述。<ul>\n<li>过早的加入，代理在以后的讲话中已经提到了这些值。</li>\n<li>对话中甚至在未来的讲话中都根本没有提到这些值。</li>\n<li>由于印刷错误，在对话历史记录中找不到这些值。</li>\n<li>隐式时间处理，这特别涉及以时间为值的插槽</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927233324153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Inconsistent State Updates：我们还遇到了MultiWOZ 2.1中的注释，这些注释在语义上是正确的，但是没有遵循一致的注释准则。对话状态中出现不一致的原因有三个：<ul>\n<li>多种来源，可以通过各种来源在对话状态中引入插槽值。</li>\n<li>值的歧义，比如“18:00”和“6 pm”</li>\n<li>跟踪策略不一致</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927234238301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Ontology-Issues\"><a href=\"#Ontology-Issues\" class=\"headerlink\" title=\"Ontology Issues\"></a>Ontology Issues</h1><p>为了解决不完整的问题，MultiWOZ 2.1通过列出整个数据集中对话状态中存在的所有值来重建了本体，但是仍然存在一些未解决的问题。首先，对于某些插槽，列出了共享相同语义的多个值。其次，我们观察到本体中有多个插槽值，这些值无法与数据库中的任何实体相关联。</p>\n<h1 id=\"Correction-Procedure\"><a href=\"#Correction-Procedure\" class=\"headerlink\" title=\"Correction Procedure\"></a>Correction Procedure</h1><p>为了避免上述问题，我们主张在数据收集之前定义本体，这不仅可以作为注释者的指南，而且还可以防止数据集中的注释不一致以及印刷和注释错误导致的本体破坏。本节描述了我们对新本体的定义，我们将其称为策略，然后是对状态和动作注释的更正。最后，我们还显示了修改的统计信息。</p>\n<ul>\n<li>策略定义：策略将不同的插槽分为两类-非分类和分类。具有大量可能值的动态插槽被归为“非分类”，与本体不同，架构不提供此类插槽的预定义值列表，他们的值是从对话历史中提取的。相反有限的插槽被归为“分类”，与本体类似，策略列出了此类插槽的所有可能值。此外，在注释期间，必须从模式中定义的预定义候选列表中选择对话状态和用户或系统操作中这些插槽的值，这有助于实现注释的完整性和一致性。如下图：</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200927235925309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>分类插槽：分类插槽所有可能值的列表是从MultiWOZ 2.1随附的相应数据库中构建的。</li>\n<li>非分类插槽：从对话历史记录中提取非分类插槽的值，我们使用一种自定义的字符串匹配方法，该方法考虑了可能的拼写错误和替代表达式来定位语义上与注释相似的所有值。如果有多个匹配项，我们选择最近提及的值并注释其跨度。下图是2.1和2.2的区别</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200928000339675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>用户和系统动作：用户和系统动作注释提供了相应话语的语义表示。为了保持对话行为和domain之间的联系，这里用到了本文二作之前的一个做法，将同一domain下的对话对话行为组合起来放到了frames里。</li>\n<li>数据：<br><img src=\"https://img-blog.csdnimg.cn/20200928145728211.png#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"Additional-annotations\"><a href=\"#Additional-annotations\" class=\"headerlink\" title=\"Additional annotations\"></a>Additional annotations</h1>除了跨域注释外，我们还为每个用户回合添加了活动的用户意图和请求槽。预测活动用户的意图和请求槽是两个新的子任务，可用于评估模型性能并促进对话状态跟踪。主动意图或API的预测对于支持数百个API的大规模对话系统的效率也至关重要。</li>\n<li>Active：active intent指定了用户话语中表达的所有意图。</li>\n<li>Requested Slots：Requested slots指定了用户想系统请求的槽位，也就是所谓的提问槽。</li>\n</ul>\n<h1 id=\"Dialogue-State-Tracking-Benchmarks\"><a href=\"#Dialogue-State-Tracking-Benchmarks\" class=\"headerlink\" title=\"Dialogue State Tracking Benchmarks\"></a>Dialogue State Tracking Benchmarks</h1><p>最新数据驱动对话系统状态跟踪模型主要采用两种方法：span-based和candidate-based，本文baseline包括：SGDbaseline、TRADE、DS-DST。</p>\n<ul>\n<li><p>各个模型在这三个数据及上的joint acc<br><img src=\"https://img-blog.csdnimg.cn/20200928150723774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p>在categorical和non-categorical两类slot上单独计算joint acc<br><img src=\"https://img-blog.csdnimg.cn/2020092815080387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h1><p>Wizard-of-Oz范式虽然是个强有力的收集自然语言对话的技术，但是他也存在很大的噪音。作者在这一节就提出了几点，来最小化标注错误。</p>\n</li>\n<li><p>在标注数据前，应该先定义一个本体或者schema。对于categorical的槽位，这个schema需要定义好明确的slot，每个slot都有一个固定的可能的value的集合。然后标注接口，需要强制性的保证这些槽值的正确性。对于non-categorical的槽位，标注接口要限制标注的value必须是出现在对话历史中的值。</p>\n</li>\n<li><p>在标注任务之后，可以进行简单的检查，以识别错误的标注。</p>\n</li>\n</ul>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>MultiWOZ 2.1是MultiWOZ 2.0数据集的改进版本，被广泛用作对话状态跟踪的基准。我们找出在MultiWOZ 2.1中未解决的注释错误，不一致和与本体相关的问题，并发布更正的版本– MultiWOZ 2.2。我们添加了新的策略，标准化的插槽值，更正的注释错误和标准化的跨域注释。此外，我们为每个用户回合注释了活动意图和请求槽位，除了修复现有操作外，并添加了丢失的用户和系统操作。我们在新数据集上对一些最新模型进行了基准测试：实验结果表明，MultiWOZ 2.1和MultiWOZ 2.2的模型性能相似。我们希望清理后的数据集有助于在模型之间进行更公平的比较，并促进该领域的研究。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","MultiWOZ"]},{"title":"论文阅读笔记：Neural-Belief-Tracker--数据驱动的对话状态跟踪","url":"/Paper-Reading/e6ce0f001fde/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Neural Belief Tracker: Data-Driven Dialogue State Tracking<br>原文链接：<a href=\"https://arxiv.org/pdf/1606.03777.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>belief tracker是现代口语对话系统的核心组件之一，它可以在对话的每个步骤中估算用户的目标，但是，大多数当前方法很难扩展到更大，更复杂的对话域。这是由于它们对以下方面的依赖：a）需要大量带注释的训练数据的口语理解模型； 或b）手工制作的词典，用于捕获用户语言的某些语言变化。我们提出了一种新颖的Neural Belief Tracking (NBT) 框架，该框架通过基于表示学习的最新进展来克服这些问题。NBT通过推理对预先训练的单词向量进行建模，学习将其组合为用户话语和对话上下文的分布表示形式。我们对两个数据集的评估表明，该方法超越了过去的局限性，与依赖于手工制作的语义词典的最新模型的性能相匹配，并且在不提供此类词典的情况下其性能优于后者。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>下图中的示例显示了三轮对话中每个用户语句后的真实状态，从该示例可以看出，DST模型依赖于标识用户话语中的本体。<br><img src=\"https://img-blog.csdnimg.cn/20201006114135584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图给出了一个针对三个槽值对的字典的示例（传统的做法是建语义词典），我们称其为<strong>非词化</strong>(delexicalisation)的这种方法显然无法扩展到更大，更复杂的对话域。如意大利语和德语这种词汇和形态丰富的语言。<br><img src=\"https://img-blog.csdnimg.cn/20201006114443735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在本文中，我们介绍了两个新模型，统称为 Neural Belief Tracker (NBT)系列，这些模型将SLU和DST结合在一起，可以有效地学习处理变化，而无需任何手工资源。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><ul>\n<li>Separate SLU</li>\n<li>Joint SLU/DST</li>\n</ul>\n<p>本文提出的工作的主要动机是克服影响以前的信念跟踪模型的限制。NBT模型通过以下方式有效地从可用数据中学习</p>\n<ul>\n<li>利用预训练词向量中的语义信息来解决词汇/形态上的歧义</li>\n<li>最大化本体值之间共享的参数数量</li>\n<li>具有学习领域特定释义和其他变体的灵活性，这使得依靠精确匹配和去词缀化作为一种可靠的策略是不可行的</li>\n</ul>\n<h1 id=\"Neural-Belief-Tracker\"><a href=\"#Neural-Belief-Tracker\" class=\"headerlink\" title=\"Neural Belief Tracker\"></a>Neural Belief Tracker</h1><p>下图展示了该模式下的信息流<br><img src=\"https://img-blog.csdnimg.cn/20201006152514188.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>给定三个模型输入，NBT层次结构的第一层执行表示学习，从而为用户话语生成矢量表示$(r)$，当前的候选插槽值对表示$(c)$，系统对话动作表示为$(t_q, t_s, t_v)$。随后，学习到的向量表示通过上下文建模和语义解码子模块进行交互，以获得中间交互向量$d_r,d_c,d$，这些用作最终决策模块的输入，该模块决定用户是否表达了由候选插槽值对表示的意图。</p>\n<p>目的是根据用户的输入（User Utterance，由ASR得到的结果，当然也可以直接是用户的文本输入）和系统上一轮的回复（System Output），遍历Domain Ontology（说白了就是某个领域内slot-value对可能取值）中每一个(slot,value)对，以判断用户真实意图中包含该slot-value对的概率大小。例如上图中的Domain Ontology存在三个可能的slot-value对，分别是(food, Indian), (food, Persian), (food, Czech)。而本论文的目的便是需要分别遍历这三个可能取值，假设当前遍历到了(food, Persian)这个取值，通过表征模型可以得到它的表征c，再通过图中所示的流程，最后可以得到一个结果y，这个结果便表明了(food, Persian)这个slot-value对属于用户真实意图的可能性大小。</p>\n<h3 id=\"Representation-Learning\"><a href=\"#Representation-Learning\" class=\"headerlink\" title=\"Representation Learning\"></a>Representation Learning</h3><p>这里分别使用了两个模型来得到文本的表征：NBT-DNN和NBT-CNN，所有的表征学习都是建立在词向量上，论文说用专注于语义的词向量，效果会比普通的词向量好，可以看作是《同义词林》的“词向量版”。</p>\n<p>模型的输入包括系统的前一个对话动作，用户的输入 $u$ 和一个候选的slot-value对。输入 $u$ 的词向量分别是 $u_1,…,u_k$。 $V_i^n$ 是n个词向量的拼接。<br>$$V_i^n=u_i⊕…⊕u_{i+n-1}$$</p>\n<p>先看看NBT-DNN，结构如下图所示。计算累积n-gram特征向量 $r_1, r_2, r_3$，分别对应unigrams(1-gram)，bigrams(2-gram)和trigrams(3-gram)；再经过全连接层和非线性映射得到 $r_n^{‘}$，s代表不同的slot；最后求和得到用户输入的一个表征向量 $r$。<img src=\"https://img-blog.csdnimg.cn/20201006162453533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>$$r_n=\\sum_{i=1}^{k_u-n+1}V_i^n$$  $$r_n^{‘}=\\sigma (W_n^sr_n+b_n^s)$$ $$r=r_1^{‘}+r_2^{‘}+r_3^{‘}$$</p>\n<p>实际上，模型应该能学到哪些utterance是更重要的，如更侧重于形容词、名词的检测。因此，论文利用了NLU上得到成功应用的CNN架构实现第二个版本NBT-CNN。CNN结构也很熟悉，词向量的输入，过卷积层，抽n-gram特征，然后是非线性激活函数，max-pooling，求和。$F_n^s \\in R^{L\\times nD}$代表卷积过滤器，$m_n=[V_1^n;V_2^n,…;V_{k-n+1}^n]$ 是n-grams的各个拼接词向量。<br>$$R_n=F_n^sm_n$$  $$r_n^{‘}=maxpool(ReLU(R_n+b_n^s))$$</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201006223101275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>实际上就是一个简单的CNN模型，分别取了filter-size为1,2,3这三种，output size都是L=300。</p>\n<h3 id=\"Semantic-Decoding\"><a href=\"#Semantic-Decoding\" class=\"headerlink\" title=\"Semantic Decoding\"></a>Semantic Decoding</h3><p>这个模块对表征 $r$ 检测是否包含候选slot-value对 $c$，处理方法也比较简单。$(c_s, c_v)$ 分别是slot和value的词向量表示，投影映射成与 $r$ 相同维度的向量，点积求相似度 $d$。<br>$$c=\\sigma(W_c^s(c_s+c_v)+b_c^s)$$   $$d=r⊕c$$<br>这个模块主要是计算slot-value对和用户句子的关系，简单而言的话，slot的词向量（如果有多个词则简单相加）和value的词向量（如果有多个词则简单相加），通过一个全连接层和非线性映射后得到表征c（该表征将slot和value的信息融合成一个向量），与句子表征r进行element-wise的乘积，得到d（依然是一个向量）。</p>\n<h3 id=\"Context-Modelling\"><a href=\"#Context-Modelling\" class=\"headerlink\" title=\"Context Modelling\"></a>Context Modelling</h3><p>当用户询问时，仅从当前用户的输入还不足以抽取意图，<em>belief tracker</em>应该考虑对话的上下文，特别是上一句系统的动作。论文提出了两种动作：<em>System Request</em>和<em>System Confirm</em>。</p>\n<ul>\n<li>系统请求（System Request）：系统上一轮在向用户请求一个具体的信息，比如”what price range would you like?”，此时用户需要给出一个具体的信息，此时用t(q)表示”price range”这个slot；</li>\n<li>系统确认（System Confirm）：系统上一轮在让用户在确认一个具体的信息，比如”‘how about Turkish food?’”，此时用户一般只需要回答是与不是即可，此时用(t(s),t(v))表示（food, Turkish）这个slot-value对。</li>\n</ul>\n<p>第一种情景是，系统对一个特定的slot发出提问，用户一般会给出具体的value。第二种是系统询问用户，某个slot-value是否正确，用户一般只会回答对或错。这两个场景应分别计算。$t_q$ 是request的参数，$(t_s, t_v)$是confirm的参数。$t_q,t_s,t_v$ 都是slot/value的词向量，多个词时直接求和得到。通过系统动作，候选对 $(c_s,c_v)$ 作为一个门，控制输入表征$r$的信息输出（般情况下系统要么是请求，要么是确认，那么此时t(q)为0向量或者(t(s),t(v))是零向量。）：<br>$$m_r=(c_s \\cdot t_q)r$$  $$m_c=(c_s \\cdot t_s)(c_v \\cdot t_v)r$$</p>\n<p>该机制有点类似于将候选槽值与系统请求某个槽的信息或确认某个槽值对，计算一个相似度（上面公式都是点乘），然后通过这个相似度对用户的句子表征进行一个类似于门的控制（主要是scale作用）。</p>\n<p>Binary Decision Maker：最后的二分类决策层。$\\phi_{dim}(x)=\\sigma(W_x+b)$ 将输入 $x$ 映射到维度为size的向量，softmax二分类，完成slot-value对的存在检测：<br>$$y=\\phi_2(\\phi_{100}(d)+\\phi_{100}(m_r)+\\phi_{100}(m_c))$$</p>\n<h1 id=\"Belief-State-Update-Mechanism\"><a href=\"#Belief-State-Update-Mechanism\" class=\"headerlink\" title=\"Belief State Update Mechanism\"></a>Belief State Update Mechanism</h1><p>论文提出了一种简单的belief state的更新机制：先估计当前轮对话的slot-value，再更新历史记录。在嘈杂的环境中，取ASR输出的前N个最佳结果（N-best list）进行分析。对于第 $t$ 轮对话，$sys^{t-1}$ 表示前一个系统动作，$h^t$ 是ASR输出的结果假设，$h_i^t$ 是N-best list中的第 $i$ 个，$s$ 是slot，$v$ 是value，NBT模型需要估计 $(s,v)$ 在用户的口语输入中的概率：<br>$$\\mathbb{P}(s,v|h^t, sys^{t-1})=\\sum_{i=1}^{N}p_i^t\\mathbb{P}(s,v|h_i^t,sys^t)$$<br>对于当前和历史对话的belief state更新，引入一个权重系数 $\\lambda$：<br>$$\\mathbb{P}(s,v|h^{1:t},sys^{1:t-1})=\\lambda\\mathbb{P}(s,v|h^t,sys^{t-1})+(1-\\lambda)\\mathbb{P}(s,v|h^{1:t-1},sys^{1:t-2})$$<br>然后对于slot $s$检测到的values，取概率最大的作为当前的goal value。</p>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>实验仍然是task-oriented的对话任务，数据集有两个：DSTC2和WOZ 2.0。<br><img src=\"https://img-blog.csdnimg.cn/20201006232123896.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到，NBT-DNN和NBT-CNN都能超过基于语义词典的模型，当然NBT-CNN多了不同n-grams特征的权重学习，会更好一点。论文还做了不同词向量对结果影响的实验。<br><img src=\"https://img-blog.csdnimg.cn/20201006232145637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看出，专门针对语义任务的词向量Paragram-SL999对实验效果提升明显，这也很显然，先验知识更丰富，对下游的任务当然效果更佳。</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>在本文中，我们提出了一种新颖的神经信念跟踪（NBT）框架，旨在克服当前在现实世界中的对话域中部署对话系统的障碍。NBT模型提供了将口语理解和对话状态跟踪相结合的已知优势，而无需依赖手工制作的语义词典来实现最新的性能。我们的评估证明了这些好处：NBT模型与使用此类词典的模型的性能相匹配，并且在这些词典不可用时性能大大优于它们。最后，我们证明了NBT模型的性能随着底层单词向量的语义质量而提高。据我们所知，我们第一个超越内在评估并证明语义专业化可以提高下游任务性能。在未来的工作中，我们打算探索NBT在多域对话系统中的应用，以及在英语以外要求复杂形态变化处理的语言中的应用。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","对话系统","对话状态跟踪","Neural-Belief-Tracker"]},{"title":"论文阅读笔记：SMN检索式多轮对话系统","url":"/Paper-Reading/7820bcc68547/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots<br>原文链接：<a href=\"https://arxiv.org/pdf/1612.01627v2.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>本文的SMN模型结构可以说影响了很多后续相关的论文，所解决的是基于检索的聊天机器人中多回合对话的回复选择。在之前的工作，基于检索的聊天机器人的做法是将context里所有的utterances都连接在一起，将这个长长的context做处理然后和response作匹配，这样做可能会丢失语句间的关系或重要的上下文信息。Sequential Matching Network（SMN）模型就是为了解决这些问题而来的。</p>\n<p><strong>SMN首先在多个粒度级别上将上下文中每个utterance都和response做匹配，然后通过卷积和池化操作从每对中提取重要的匹配信息作为向量，接着通过递归神经网络（RNN）按时间顺序累积矢量，该神经网络对utterance之间的关系进行建模，最后使用RNN的隐藏状态计算最终匹配分数。</strong></p>\n<p>构建对话机器人的现有方法中，可以分为 generation-based（生成式）和retrieval-based（检索式），相对于生成式而言，检索式拥有的信息更加丰富，且运行流畅的特点。选择response的关键在于输入response匹配。与单回合对话不同，多回合对话需要在响应和对话上下文之间进行匹配，在该上下文中，不仅需要考虑response和输入信息之间的匹配，而且还需要考虑前一回合中response和utterances之间的匹配。总结而言该领域的任务存在如下的challenges：</p>\n<ul>\n<li>如何根据上下文识别重要信息（单词，短语和句子），这对于选择正确的response并利用相关信息进行匹配至关重要</li>\n<li>如何对上下文中的utterances之间的关系进行建模。</li>\n</ul>\n<p>下图展示了challenges的例子：第二句的“hold a drum class”和第三句的“drum”相关性很强，所以回复是高度依赖于上下文以及语句之间的关系的。<br><img src=\"https://img-blog.csdnimg.cn/20201030203732642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在上面粗体中描述了SMN的结构流程，具体来说，对于每个utterance-response对而言，模型通过word embeddings和带有门控递归单元（GRU）的递归神经网络的隐藏状态来构造word-word相似度矩阵和sequence-sequence相似度矩阵。这两个矩阵分别在word级别和segment（单词子序列）级别上捕获成对的重要匹配信息，并且通过交替对矩阵进行卷积和池化操作，将信息提取和融合为匹配向量。</p>\n<p>通过这种方式，可以在response的充分监督下识别上下文中多个粒度级别的重要信息，并以最小的损失进行匹配，然后将匹配向量喂给另一个GRU，以形成context和response的匹配分数，这个GRU会根据上下文中utterances的时间顺序在其隐藏状态下累积匹配。它以匹配的方式对utterances之间的关系和依存关系进行建模，并以utterances的顺序来监督配对匹配的累积。context和response的匹配程度是由具有GRU隐藏状态的logit模型计算的。</p>\n<h1 id=\"具体结构及实现\"><a href=\"#具体结构及实现\" class=\"headerlink\" title=\"具体结构及实现\"></a>具体结构及实现</h1><h3 id=\"问题符号化\"><a href=\"#问题符号化\" class=\"headerlink\" title=\"问题符号化\"></a>问题符号化</h3><p>数据集表示为 $D={(y_i,s_i,r_i)}<em>{i=1}^N$，其中$s_i={u_{i,1},…,u</em>{i,n_i}}$表示对话上下文，${u_{i,k}}_{k=1}^{n_i}$表示utterances，$r_i$表示response的候选，$y_i\\in{0,1}$表示标签（当$y_i=1$ 时意味着 $r_i$ 是 $s_i$ 合适的回复，否则 $y_i=0$），目标是学习 $D$ 的匹配模型 $g(.,.)$，对于任何context-response pair $(s,r)$，$g(s,r)$ 衡量 $s$ 和 $r$ 之间的匹配程度。</p>\n<h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><p><img src=\"https://img-blog.csdnimg.cn/20201030215924927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>按照上面的模型结构图，SMN首先将context-response分解为几个utterance-response pair匹配，然后通过递归神经网络将所有pairs匹配累积为基于上下文的匹配。模型分为三层：</p>\n<ul>\n<li>在word级别和segment级别将候选response与上下文中的每个utterance匹配，然后通过卷积提取两个层上的重要匹配信息，池化并编码为匹配向量。</li>\n<li>将上一层得到的匹配向量喂入第二层，在该层中，它们按照上下文中utterances的时间顺序在具有GRU的递归神经网络的隐藏状态下进行累积。</li>\n<li>将上一层的隐藏状态用于计算最终匹配分数</li>\n</ul>\n<p><strong>这样的模型结构有以下的优势：</strong></p>\n<ul>\n<li>一个候选response可以在一开始就匹配上下文中的每个utterances，因此可以充分提取每个utterance-response pair中的匹配信息，并以最小的损失将其携带到最终匹配分数</li>\n<li>由于从每个utterance中提取信息是在不同的粒度级别上进行的，并且在response的充分监督下进行，因此可以很好地识别和提取对每个utterance中的response选择有用的语义结构。</li>\n<li>匹配和utterance关系是耦合而不是分开建模的，因此，作为一种knowledge，utterance关系（例如顺序）可以监督匹配分数的形成。</li>\n</ul>\n<p>接下来针对三层的细节展开描述。</p>\n<h3 id=\"Utterance-Response匹配\"><a href=\"#Utterance-Response匹配\" class=\"headerlink\" title=\"Utterance-Response匹配\"></a>Utterance-Response匹配</h3><p>给定上下文 $s$ 中的语句 $u$ 和候选响应 $r$，然后对$u$ 和 $r$ 进行Embedding得到对应的表示 $U=[e_{u,1},…,e_{u,n_u}]$ 和 $R=[e_{r,1},…,e_{r,n_r}]$ ，其中 $e_{u,i},e_{r,i}\\in \\mathbb{R}^d$ 分别是$u$ 和 $r$ 的第 $i$ 个单词的嵌入。然后使用 $U\\in \\mathbb{R}^{d\\times n_u}$ 和 $R\\in \\mathbb{R}^{d\\times n_r}$ 构造word-word相似度矩阵 $M_1\\in \\mathbb{R}^{n_u\\times n_r}$，以及sequence-sequence相似度矩阵 $M_2\\in \\mathbb{R}^{n_u\\times n_r}$，这两个矩阵作为卷积神经网络（CNN）的两个input channels，CNN从矩阵中提取重要的匹配信息，并将该信息编码为匹配向量 $v$。具体来说，$\\forall i,j$，$M_1$ 第 $(i,j)$ 个元素被定义为（公式1）：<br>$$e_{1,i,j}=e_{u,i}^T\\cdot e_{r,j}$$<br>$M_1$ 在单词级别上模拟 $u$ 和 $r$ 之间的匹配。为了构造$M_2$ ，我们首先使用GRU将 $U$ 和 $R$ 转换为隐藏向量。假设 $H_u=[h_{u,1},…,h_{u,n_u}]$ 是 $U$ 的隐藏向量，则 $\\forall i,h_{u,i}\\in \\mathbb{R}^m$ 定义为（公式2）：<br>$$z_i=\\sigma (W_{ze_{u,i}}+U_zh_{u,i-1})$$  $$r_i=\\sigma (W_{re_{u,i}}+U_rh_{u,i-1})$$  $$\\tilde{h}<em>{u,i}=tanh(W</em>{he_{u,i}}+U_{h}(r_i\\odot h_{u,i-1}))$$  $$h_{u,i}=z_i\\odot\\tilde{h}<em>{u,i}+(1-z_i)\\odot h_{u,i-1}$$<br>其中 $h</em>{u,0}=0$，$z_i$ 和 $r_i$ 分别是update gate和reset gate，$\\sigma(\\cdot)$是一个sigmoid函数，$W_z,W_h,W_r,U_z,U_r,U_h$ 都是参数，同样，我们有 $H_r=[h_{r,1},…,h_{r,n_r}]$ 作为 $R$ 的隐藏向量，然后 $\\forall i,j$，$M_2$ 第 $(i,j)$ 个元素被定义为（公式3）：<br>$$e_{2,i,j}=e_{u,i}^TAh_{r,j}$$<br>其中 $A\\in \\mathbb{R}^{m\\times m}$ 是一个线性变换， $\\forall i$，GRU对直到位置 $i$ 的单词之间的顺序关系和依赖关系进行建模，并对text segment进行编码，直到第 $i$ 个单词为隐藏矢量为止，$M_2$ 在segment级别上建模$u$ 和 $r$ 之间的匹配。</p>\n<p>然后，CNN将 $M_1$ 和 $M_2$ 处理为  $v$。$\\forall f=1,2$，CNN视 $M_f$ 为输入通道，交替进行卷积和最大池化操作。假设$\\forall f=1,2$， $z^{(l,f)}=[z_{i,j}^{(l,f)}]<em>{I^{(l,f)}\\times J^{(l,f)}}$ 表示 $l$ 层上类型为 $f$ 的特征图的输出，其中 $z^{(0,f)}=M_f$。在卷积层上，我们使用窗口大小为 $r_w^{(l,f)}\\times r_h^{(l,f)}$ 的2D卷积运算，并将 $z_{i,j}^{(l,f)}$ 定义为（公式4）：<br>$$z</em>{i,j}^{(l,f)}=\\sigma(\\sum_{f^{‘}=0}^{F_{l-1}}\\sum_{s=0}^{r_{w}^{(l,f)}}\\sum_{t=0}^{r_{h}^{(l,f)}}W_{s,t}^{(l,f)}\\cdot z_{i+s,j+t}^{(l-1,f^{‘})}+b^{l,k})$$<br>其中，$\\sigma(\\cdot)$是一个ReLU，$W^{(l,f)}\\in \\mathbb{R}^{r_w^{(l,f)}\\times r_h^{(l,f)}}$ 和 $b^{l,k}$ 是参数，$F_{l-1}$ 是第 $(l-1)$ 层上的特征图的数量，最大池化操作基于卷积操作，可以表示为（公式5）：<br>$$z_{i,j}^{(l,f)}=\\underset{p_w^{(l,f)}&gt;s\\geq0}{max} \\underset{p_h^{(l,f)}&gt;t\\geq0}{max}z_{i+s,j+t}$$<br>其中 $p_w^{(l,f)}$ 和 $p_h^{(l,f)}$ 分别是2D池的宽度和高度，最终特征图的输出被串联并映射到低维空间，并通过线性变换作为匹配向量 $v\\in \\mathbb{R^q}$</p>\n<h3 id=\"Accumulation匹配\"><a href=\"#Accumulation匹配\" class=\"headerlink\" title=\"Accumulation匹配\"></a>Accumulation匹配</h3><p>假设 $[v_1,…,v_n]$ 是第一层的输出（对应 $n$ pairs），在第二层，GRU取 $[v_1,…,v_n]$ 作为输入，并将匹配序列编码为其隐藏状态 $H_m=[h_1^{‘},…,h_n^{‘}]\\in  \\mathbb{R^{q\\times n}}$。 其详细参数设置与公式（2）相似，这个层有两个函数：</p>\n<ul>\n<li>它在上下文中建模utterances的依存关系和时间关系</li>\n<li>它利用时间关系来监督对accumulation的累积，作为基于上下文的匹配</li>\n</ul>\n<h3 id=\"Prediction和Learning匹配\"><a href=\"#Prediction和Learning匹配\" class=\"headerlink\" title=\"Prediction和Learning匹配\"></a>Prediction和Learning匹配</h3><p>存在 $[h_1^{‘},…,h_n^{‘}]$，我们将 $g(s,r)$ 定义为（公式6）：<br>$$g(s,r)=softmax(W_2L[h_1^{‘},…,h_n^{‘}]+b_2)$$<br>其中，$W_2$ 和 $b_2$ 是参数，我们考虑  $L[h_1^{‘},…,h_n^{‘}]$ 的三个参数化：</p>\n<ul>\n<li>仅使用最后一个隐藏状态，  $L[h_1^{‘},…,h_n^{‘}]=h_n^{‘}$。</li>\n<li>隐藏状态线性组合，  $L[h_1^{‘},…,h_n^{‘}]=\\sum_{i=1}^{n}w_ih_i^{‘}$，其中 $w_i\\in  \\mathbb{R}$</li>\n<li>我们遵循并运用注意力机制来组合隐藏状态，则 $L[h_1^{‘},…,h_n^{‘}]$ 被定义为（公式7）：<br>$$t_i=tanh(W_{1,1}h_{u_i,n_u}+W_{1,2}h_i^{‘}+b_1)$$  $$a_i=\\frac{exp(t_i^Tt_s)}{\\sum_i(exp(t_i^Tt_s))}$$  $$L[h_1^{‘},…,h_n^{‘}]=\\sum_{i=1}^{n}a_ih_i^{‘}$$<br>其中， $W_{1,1} \\in \\mathbb{R^{q\\times m}}$ ，$W_{1,2}\\in  \\mathbb{R^{q\\times q}}$和 $b_1\\in  \\mathbb{R^{q}}$ 是参数， $h_i^{‘}$ 和 $h_{u_i,n_u}$ 分别是第 $i$ 个匹配向量和第 $i$ 个utterance的最终隐藏状态， $t_s\\in \\mathbb{R^{q}}$ 是一个虚拟上下文向量，它是随机初始化的，并在训练中共同学习。我们用 $L[h_1^{‘},…,h_n^{‘}]$ 的三个参数化来表示我们的模型，分别是 $SMN_{last}$，$SMN_{static}$ 和 $SMN_{dynamic}$，并在实验中进行比较。</li>\n</ul>\n<p>我们通过用 $D$ 最小化交叉熵来学习 $g(\\cdot,\\cdot)$。令 $\\theta$ 表示 $SMN$ 的参数，则学习的目标函数 $L(D,\\theta)$ 可表示为（公式8）：<br>$$-\\sum_{i=1}^{N}[y_ilog(g(s_i,r_i))+(1-y_i)log(1-g(s_i,r_i))]$$</p>\n<h3 id=\"检索候选Response\"><a href=\"#检索候选Response\" class=\"headerlink\" title=\"检索候选Response\"></a>检索候选Response</h3><p>作者利用启发式方法从索引中获取候选response，将前一轮的utterances ${u_1,…,u_{n-1}}$ 和 $u_n$ 进行计算，根据他们的<strong>tf-idf</strong>得分，从 ${u_1,…,u_{n-1}}$ 中提取前 $5$ 个关键字，然后，我们将扩展后的message用于索引，并使用索引的内联检索算法来检索候选response。最后，我们使用 $g(s,r)$ 对候选进行排名，并返回第一个作为对上下文的response。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><ul>\n<li>Ubuntu语料</li>\n<li>豆瓣多轮语料：下图给出三组的统计数据。<br><img src=\"https://img-blog.csdnimg.cn/20201031102216402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p><strong>下图显示两个数据集的评估结果：</strong><br><img src=\"https://img-blog.csdnimg.cn/20201031103318145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用Ubuntu语料库中的示例对第二层中的相似性矩阵和GRU的gates进行可视化，以进一步阐明我们的模型如何在上下文中识别重要信息，以及如何使用前文所述的GRU的门机制选择重要的匹配向量，示例及可视化图如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\tu1: how can unzip many rar (_number_ for example ) files at once; </span><br><span class=\"line\">\tu2: sure you can do that in bash; </span><br><span class=\"line\">\tu3: okay how? </span><br><span class=\"line\">\tu4: are the files all in the same directory? </span><br><span class=\"line\">\tu5: yes they all are; </span><br><span class=\"line\">\tr: then the command glebihan should extract them all from&#x2F;to that directory</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/2020103110442392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>下面是消融（Ablation）实验的结果：</strong><br><img src=\"https://img-blog.csdnimg.cn/20201031104859530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>跨context长度的效果：</strong>显示了豆瓣语料库上不同长度间隔的MAP的比较<br><img src=\"https://img-blog.csdnimg.cn/20201031104816446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>最大context长度：</strong>展示了在最大context长度方面，SMN在Ubuntu Corpus和Douban Corpus上的性能<br><img src=\"https://img-blog.csdnimg.cn/20201031105902650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>错误分析：</strong></p>\n<ul>\n<li>逻辑一致性。 SMN在语义级别上对context和response进行建模，但很少关注逻辑一致性</li>\n<li>检索后没有正确的候选</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文提出了一个新的基于上下文的模型，用于基于检索的聊天机器人中的多轮响应选择，论文还详尽的介绍了豆瓣对话语料库，并且做了实验去研究应该在context中取多少个utterance，即取多少轮对话，实验证明轮数取10的时候效果最好，很值得学习的论文。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","SMN","多轮对话"]},{"title":"论文阅读笔记：Recent-Advances-and-Challenges-in-Task...","url":"/Paper-Reading/a78371b14017/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Recent Advances and Challenges in Task-oriented Dialog Systems<br>原文链接：<a href=\"https://arxiv.org/pdf/2003.07490.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>由于在人机交互和自然语言处理中的重要性和价值，面向任务的对话系统在学术界和工业界都受到越来越多的关注。在本文中，我们调查了面向任务的对话系统的最新进展和挑战。我们还讨论了面向任务的对话系统的三个关键主题：（1）提高数据效率以促进在资源匮乏的环境中进行对话建模；（2）为对话策略学习建模多回合模型以实现更好的任务完成性能；（3）将领域本体知识整合到对话模型中。此外，我们回顾了对话评估和一些常用语料库的最新进展。我们认为，尽管这项调查不完整，但可以为面向任务的对话系统的未来研究提供启发。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>通常，面向任务的对话系统是建立在结构化本体之上的，该本体定义了任务的领域知识。有关面向任务的对话系统的现有研究可以大致分为两类：pipeline和end-to-end。建立pipeline系统通常需要大规模的标记对话数据来训练每个组件，模块化的结构使系统比端到端的系统更具解释性和稳定性，因此，大多数现实世界的商业系统都是以这种方式构建的。而端到端的结构像是黑匣子，这更加不可控。如下图所示，对于pipeline和end-to-end方法中的每个单独组件，我们列出了一些关键问题，在这些问题中提出了典型的作品。<br><img src=\"https://img-blog.csdnimg.cn/20200928160614477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在pipeline方法中，最近的研究更多地集中在对话框状态跟踪和对话框策略组件上，这也称为“对话框管理”。基于域本体，通过预测每个槽的值，DST任务可以视为分类任务（受限制与训练数据，OOV问题），对话策略学习任务通常被认为是强化学习任务。然而，与其他众所周知的RL任务不同，对话策略的训练需要真实的人作为环境，这是非常昂贵的。面向任务的对话系统中的三个关键问题：</p>\n<ul>\n<li>数据效率：资源匮乏的问题是主要的挑战之一。</li>\n<li>多回合策略：提出了许多解决方案以解决多轮交互式训练中的这些问题，以更好地进行策略学习，包括基于模型的计划，奖励估计和端到端策略学习。</li>\n<li>本体整合：面向任务的对话系统必须查询知识库（KB）以检索一些实体以生成响应，由于没有显式的状态表示形式，因此这种简化使构造查询变得困难。</li>\n</ul>\n<h1 id=\"Modules-and-Approaches\"><a href=\"#Modules-and-Approaches\" class=\"headerlink\" title=\"Modules and Approaches\"></a>Modules and Approaches</h1><p>有关面向任务的对话系统的现有研究可以大致分为两类：pipeline和end-to-end。在pipeline方法中，该模型通常由几个组件组成，包括自然语言理解（NLU），对话状态跟踪（DST），对话策略和自然语言生成（NLG），如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/2020092820241314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>值得注意的是，尽管NLU-DST-Policy-NLG框架是pipeline系统的典型配置，但还有其他一些配置。有一些研究合并了一些典型的组件，例如单词级DST和单词级策略。在端到端方法中，对话系统在端到端方式，无需指定每个单独的组件。</p>\n<ul>\n<li>NLU：主要是识别对话动作，其由意图和插槽值组成，即由意图识别和槽值提取组成，示例如下。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/2020092820284111.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>DST：对话状态跟踪器通过将整个对话上下文作为输入来估算每个时间步的用户目标。在时间 $t$ 的对话状态可以看作是直到 $t$ 之前的对话回合的抽象表示。</li>\n<li>对话策略：以对话状态为条件，对话策略会产生下一个系统动作。如下图所示，在特定的时间步 $t$ 处，用户在 $a_t$ 处执行操作，收到奖励 $R_t$，状态更新为 $S_t$。<br><img src=\"https://img-blog.csdnimg.cn/20200928204116814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>NLG：该任务将对话用作输入并生成自然语言响应。为了改善用户体验，所产生的话语应该（1）充分传达对话行为的语义以完成任务，并且（2）与人类语言类似，是自然的，特定的，信息丰富的。</li>\n<li>End-to-end方法：面向任务的对话系统的端到端方法受到开放域对话系统研究的启发，如下图。<br><img src=\"https://img-blog.csdnimg.cn/20200928205223851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1>大多数评估研究都遵循PARADISE框架，一种是对话成本，它衡量对话中产生的成本，例如对话回合数。另一个是任务成功，评估系统是否成功解决了用户问题。评估面向任务的对话系统的方法可以大致分为以下三种：</li>\n<li>Automatic Evaluation</li>\n<li>Simulated Evaluation</li>\n<li>Human Evaluation</li>\n</ul>\n<h1 id=\"Corpora\"><a href=\"#Corpora\" class=\"headerlink\" title=\"Corpora\"></a>Corpora</h1><p>收集了具有不同域和注释粒度的大量语料库，以促进对面向任务的对话系统的研究。如下图所示：</p>\n<ul>\n<li>informable slot 一般是由用户告知系统的，用来约束对话的一些条件，系统为了完成任务必须满足这些约束。</li>\n<li>requestable slot 一般是用户向系统咨询的，可以来做选择的一些slot。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200928210217414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h1><ul>\n<li>数据效率：资源匮乏的问题是主要的挑战之一。回顾了为缓解此问题而提出的一些最新方法。我们首先回顾一下迁移学习方法，这些方法可以从大规模数据中获取先验知识，或者从其他任务中采用经过训练的模型。然后，我们介绍了一些无监督的方法，这些方法可以通过启发式规则在资源很少的情况下直接学习而几乎没有注释。此外，我们还回顾了最近在构建数据驱动的用户模拟器方面的工作。</li>\n<li>多回合策略：提出了许多解决方案以解决多轮交互式训练中的这些问题，以更好地进行策略学习，包括基于模型的计划，奖励估计和端到端策略学习。面向任务的对话系统的对话管理的最新研究主要集中在以下主题上：（1）带有带有用于自由槽位的值解码器的DST；（2）进行对话计划以提高策略学习中的样本效率（3）用户目标估计，以预测任务成功和用户满意度。</li>\n<li>本体整合：面向任务的对话系统必须查询知识库（KB）以检索一些实体以生成响应，由于没有显式的状态表示形式，因此这种简化使构造查询变得困难。我们介绍有关（1）对话任务模式集成（2）面向任务的对话模型中的知识库集成的一些最新进展。</li>\n</ul>\n<h1 id=\"Discussion-and-Future-Trends\"><a href=\"#Discussion-and-Future-Trends\" class=\"headerlink\" title=\"Discussion and Future Trends\"></a>Discussion and Future Trends</h1><p>在本文中，我们回顾了面向任务的对话系统的最新进展，并讨论了三个关键主题：数据效率、多回合策略、本体知识整合。最后，我们讨论面向任务的对话系统的一些未来趋势：</p>\n<ul>\n<li>对话系统的预训练方法</li>\n<li>领域适应，跨领域应用</li>\n<li>鲁棒性</li>\n<li>End-to-end模型</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","NLP","对话系统","面向任务","综述"]},{"title":"论文阅读笔记：Scheduled-Sampling-for-Transformers","url":"/Paper-Reading/8fd93ce36699/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Scheduled Sampling for Transformers<br>原文链接：<a href=\"https://arxiv.org/pdf/1906.07651.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Scheduled sampling(计划采样)是一种避免Exposure Bias的技术，它包括向模型喂入Teacher-Forcing的embeddings和训练时间上一步中的模型预测的混合，该技术已用于通过递归神经网络（RNN）改善模型性能。在Transformer模型中，与RNN不同，新单词的生成会涉及到到目前为止生成的完整句子，而不仅是最后一个单词，致使应用Scheduled sampling技术并非易事。文中提出了一些结构上的更改，以允许通过两次遍历解码策略将Scheduled sampling应用于Transformer架构。</p>\n<blockquote>\n<p>由于训练和预测的时候decode行为的不一致， 导致预测单词（predict words）在训练和预测的时候是从不同的分布中推断出来的。而这种不一致导致训练模型和预测模型直接的Gap，就叫做 Exposure Bias。</p>\n</blockquote>\n<p>本文的创新和贡献：</p>\n<ul>\n<li>提出了一种在Transformer模型中使用Scheduled sampling的新策略，即在训练阶段内经过decoder两次</li>\n<li>比较了使用模型代替标准目标时以模型预测为条件的几种方法</li>\n<li>在两个语言对的机器翻译任务中使用Transformer测试了Scheduled sampling，并获得了接近Teacher-Forcing基线的结果（某些模型的改进幅度高达1个BLEU点）。</li>\n<li>线性衰减，指数衰减和反sigmoid衰减</li>\n</ul>\n<h1 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h1><p>众所周知，Transformer是一个 autoregressive模型，其中，每个单词的生成都取决于序列中所有先前的单词，而不仅是最后生成的单词。单词的顺序是通过将位置嵌入与相应的单词嵌入相加来实现的，而在解码器中使用位置屏蔽可确保每个单词的生成仅取决于序列中的前一个单词，而不取决于随后的单词。由于这些原因，想要将Scheduled sampling应用在Transformer中比较困难，所以需要对Transformer的结构进行一定的修改，更改后的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020202136686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如果你熟悉Transformer你会发现，其实encoder和decoder的结构都没有变，只是多了一个decoder</p>\n<h2 id=\"Transformer的Two-decoder\"><a href=\"#Transformer的Two-decoder\" class=\"headerlink\" title=\"Transformer的Two-decoder\"></a>Transformer的Two-decoder</h2><p>流程分为以下几步</p>\n<ul>\n<li>首次通过decoder，获取模型预测</li>\n<li>将标准序列与预测序列混合，对于序列中的每个位置，我们以给定的概率选择是使用标准token还是使用来自模型的预测</li>\n<li>将标准序列与预测序列的混合再次输入decoder，产生最终预测</li>\n</ul>\n<p>注意：重要的是要提到两个decoder是相同的，并且共享相同的参数</p>\n<h2 id=\"Embedding-Mix\"><a href=\"#Embedding-Mix\" class=\"headerlink\" title=\"Embedding Mix\"></a>Embedding Mix</h2><p>对于序列中的每个位置，第一遍解码器都会给出每个词汇词的得分。以下是使用模型预测时使用这些分数的几种方法</p>\n<ul>\n<li>完全不混合嵌入并通过模型预测传递argmax，即使用来自解码器的得分最高的词汇词嵌入。</li>\n<li>混合top-k嵌入，即使用得分最高的5个词汇词嵌入的加权平均值。</li>\n<li>通过将嵌入结合softmax与temperature进行传递，使用较高的temperature参数可使argmax更好地近似，公式如下：其中 $\\bar{e}<em>{i-1}$ 是在当前位置使用的向量，通过所有词汇词的嵌入量之和，以及分数 $s</em>{i-1}$ 的softmax加权获得。<br>$$\\bar{e}<em>{i-1}=\\sum_ye(y)\\frac{exp(as_{i-1}(y))}{\\sum</em>{y^{‘}}exp(as_{i-1}(y^{‘}))}$$</li>\n<li>使用argmax的另一种方法是从softmax分布中采样嵌入，公式如下：其中，$U ∼ Uniform(0, 1)$，$G=-log(-logU)$<br>$$\\bar{e}<em>{i-1}=\\sum_ye(y)\\frac{exp(a(s_{i-1}(y))+G_y)}{\\sum_{y^{‘}}exp(a(s</em>{i-1}(y^{‘})+G_{y^{‘}}))}$$</li>\n<li>通过嵌入的sparsemax</li>\n</ul>\n<h2 id=\"权重更新\"><a href=\"#权重更新\" class=\"headerlink\" title=\"权重更新\"></a>权重更新</h2><p>基于第二个解码器遍历的输出来计算交叉熵损失。对于将所有词汇单词相加的情况（Softmax，Gumbel softmax，Sparsemax），尝试两种更新模型权重的方法，如下：</p>\n<ul>\n<li>仅根据目标与模型预测之间的混合，通过最终做出预测的解码器进行反向传播。</li>\n<li>通过第二遍以及第一遍解码器进行反向传播，从而预测模型输出</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p>实验在如下两个数据集中进行：</p>\n<ul>\n<li>IWSLT 2017 German−English </li>\n<li>KFTT Japanese−English</li>\n</ul>\n<p>使用字节对编码（BPE）进行联合分割，超参数如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020211057568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>本论文的数据使用线性衰减最适合：$t(i)=max{\\epsilon, k-ci}$，其中，$0 \\leq \\epsilon &lt;1$ 是模型中要使用的最小Teacher-Forcing概率，$k$ 和 $c$ 提供衰减的偏移量和斜率。此函数确定训练步骤 $i$ 的Teacher-Forcing比 $t$，即在序列中每个位置进行Teacher-Forcing的概率，实验结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20201020211943651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>仅使用模型预测的得分最高的单词的Scheduled sampling效果不佳，使用混合嵌入（top-k，softmax，Gumbel softmax或sparsemax）并且仅使用第二个解码器通过的反向传播模型，在验证集上的性能略好于基准。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>这篇论文阐述了在Transformer上使用Scheduled Sampling的思路，对于几种Scheduled策略也进行了实验，说明了效果，值得借鉴。总体来说，实现思路不是很复杂，不过中间的可控性不高，并且可能需要找到符合数据集的一种更佳方式，可能泛化上不是很好。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Transformer","Paper","Scheduled-Sampling"]},{"title":"论文阅读笔记：Self-Attention-with-Relative-Position-Representation","url":"/Paper-Reading/5ad967c3667a/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Self-Attention with Relative Position Representations<br>原文链接：<a href=\"https://arxiv.org/pdf/1803.02155.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>了解Transformer的都知道，与递归和卷积神经网络相反，它没有在其结构中显式地建模相对或绝对位置信息，而是它需要在其输入中添加绝对位置的表示，这是一种完全依赖于注意力机制的方法。在本篇论文中，提出了一种替代方法，扩展了自注意机制，可以有效地考虑相对位置或序列元素之间距离的表示。本文描述了该方法的有效实现，并将其转换为可感知到任意图标记输入的相对位置感知自注意力机制的实例，即提出了一种将相对位置表示形式并入Transformer自注意机制的有效方法，残差连接有助于将位置信息传播到更高的层。</p>\n<p>循环神经网络（RNN）通常根据时间 $t$ 的输入和先前的隐藏状态 $h_{t-1}$ 计算隐藏状态 $h_t$，直接通过其顺序结构沿时间维度捕获相对位置和绝对位置。非循环模型不必一定要顺序考虑输入元素，因此可能需要显式编码位置信息才能使用序列顺序。</p>\n<p>一种常见的方法是使用与输入元素结合的位置编码，以将位置信息公开给模型。这些位置编码可以是位置的确定性函数或学习的表示形式。比如，卷积神经网络捕获每个卷积内核大小内的相对位置，已被证明仍然受益于位置编码。</p>\n<h3 id=\"Relation-aware自注意力\"><a href=\"#Relation-aware自注意力\" class=\"headerlink\" title=\"Relation-aware自注意力\"></a>Relation-aware自注意力</h3><h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文提出了自注意力的扩展，可用于合并序列的相对位置信息，从而提高了机器翻译的性能。论文中的这个思路可以借鉴参考，通过对自注意力的改造，就不需要进行硬位置编码了，但是论文中貌似没有比较硬位置编码和该方法的效果。</p>\n","categories":["Paper-Reading"],"tags":["Transformer","Paper","Self-Attention","Relative-Position-Representation","RPR","相对位置编码"]},{"title":"论文阅读笔记：Seq2Seq模型解码重复和不停止原因分析","url":"/Paper-Reading/f5a757ac2f8f/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Paper 1：<a href=\"https://arxiv.org/pdf/2002.02492.pdf\">Consistency of a Recurrent Language Model With Respect to Incomplete Decoding</a><br>Paper 2：<a href=\"https://arxiv.org/pdf/2012.14660.pdf\">A Theoretical Analysis of the Repetition Problem in Text Generation</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>Seq2Seq模型在深度学习中已经屡见不鲜了，而且在各种任务上表现很出色，但是其中存在一个相信很多人都遇到的问题，也就是Seq2Seq在解码过程中，某些token反复出现，而且结束解码符号一直都不出现。针对这两个问题，上述两篇论文分别从“解码不停止”以及“重复解码”这两个方面进行分析陈述（其实这两个问题是同理的），而本篇文章针对两篇论文进行学习和总结。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>用最大似然估计（MLE）训练的神经序列模型已经成为各种自然语言应用中的标准做法，但是其被证明存在length bias 和 degenerate repetition 问题（可能是与最大似然目标的局部归一化有关，即真实输出分布和算法学习到的分布不一致）。</p>\n<h1 id=\"“解码不停止”\"><a href=\"#“解码不停止”\" class=\"headerlink\" title=\"“解码不停止”\"></a>“解码不停止”</h1><p>为了分析“解码不停止”问题，论文首先定义了解码算法的不一致性，即<strong>该算法可以产生一个概率为零的无限长序列</strong>。对于一致的递归语言模型与不完整的解码算法配合使用后，会导致不一致的序列分布的观点，论文中对此提出了两种解决不一致问题的方法</p>\n<ul>\n<li>consistent sampling：以确保在解码期间不会将终止token排除在选择之外</li>\n<li>self-terminating recurrent language model：该模型保证最终将终止token排在所有其他token之上，从而确保在不完全解码下的一致性</li>\n</ul>\n<p>所以在进一步分析解码算法之前，需要先建立recurrent language model，方便后面分析，即条件语言模型在每个时间步的计算如下：<br>$$p_{\\theta}(y_t=v|y_{&lt;t},C)=\\frac{exp(u_v^Th_t+c_v)}{\\sum_{v^{‘}\\in V}exp(u_{v^{‘}}^Th_t+c_{v^{‘}})}$$<br>其中，$h_t=f_{\\theta}(y_t,h_{t-1}),h_0=g_{\\theta}(C),u,c,\\theta$ 都是参数，$C$ 是上下文分布 $p(C)$ 的token集合中的元素。由此可以计算一条完整的序列的概率 $Y=(y_1,…,y_T)$ ：<br>$$p_{\\theta}(Y|C)=\\prod_{t=1}^{T}p_{\\theta}(y_t|y_{&lt;t},C)$$</p>\n<p>其中，$y_{&lt;t}=(y_1,…,y_{t-1})$。有了这些，我们可以开始来讨论解码算法，解码算法大致可以分为两类：</p>\n<ul>\n<li>Stochastic decoding：随机性解码算法</li>\n<li>Deterministic decoding：确定性解码算法</li>\n</ul>\n<h1 id=\"Stochastic-decoding\"><a href=\"#Stochastic-decoding\" class=\"headerlink\" title=\"Stochastic decoding\"></a>Stochastic decoding</h1><p>随机性解码算法，通俗点理解就是哪怕输入文本固定了，解码出来的输出文本也不是固定的。对于Seq2Seq来说，我们很多时候希望得到确定性的结果，所以多数场景下我们都是用Beam Search。但是由于Beam Search的生成结果可能会出现过于单一的现象，又或者有时候我们希望增加输出的多样性，这时候就需要随机性解码算法，论文中提到三种随机性解码算法</p>\n<ul>\n<li>原生随机解码：原生随机解码算法其实就是从文字表面意思理解的逻辑，即每步按概率随机采样一个token，直到采样到终止token才停止，这看着就和它的名字一样随意。</li>\n<li><a href=\"https://arxiv.org/pdf/1805.04833.pdf\">top-k随机解码</a>：其在原生随机采样上加以完善，也就是限定随机采样的范围，即只在每步最高的前k个token中，重新进行归一化之后再随机采样，如下，其中 $V_k=arg\\underset{v^{‘}}{top-k}p_{\\theta}(v^{‘}|y_{&lt;t},C)$：<br>$$q(v)∝\\left{\\begin{matrix}  p_{\\theta}(v|y_{&lt;t},C),\\ if\\ v\\in V_k \\ 0,\\ otherwise \\end{matrix}\\right.$$</li>\n<li><a href=\"https://arxiv.org/pdf/1904.09751.pdf\">Nucleus随机解码</a>：跟top-k随机解码类似，也是对采样空间做了个截断，截断方式是：固定 $p\\in (0,1)$，然后只保留概率最高的、概率和刚好超过 $p$ 的若干个token，所以它也叫top-p采样。公式和上面相似，不过在于另 $v\\in V_{\\mu}$，其中$V_{\\mu}={v_1,…,v_{k_{\\mu}}}$，则<br>$$k_{\\mu}=min{k|\\sum_{i=1}^kp_{\\theta}(v_i|y_{&lt;t},C)&gt;\\mu}$$</li>\n</ul>\n<h1 id=\"Deterministic-decoding\"><a href=\"#Deterministic-decoding\" class=\"headerlink\" title=\"Deterministic decoding\"></a>Deterministic decoding</h1><p>确定性解码算法就是当输入文本固定之后，解码出来的输出文本也是固定的，这类算法就包含比较常见的Greedy Search 和Beam Search，事实上Greedy Search是 Beam Search 的一个特例，所以只需要讨论Beam Search，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210206150514589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>正如上图所示，每个序列直到出现终止token就停止，最后从这k个已经完成终止的序列中选最优的那个输出。一般有两种选择，一是输出总得分最大的，二是输出平均得分最大的（处以各自token数），有时候还会根据实际需求加入长度惩罚等。</p>\n<h1 id=\"解码算法的一致性\"><a href=\"#解码算法的一致性\" class=\"headerlink\" title=\"解码算法的一致性\"></a>解码算法的一致性</h1><p>正如我们前面定义的解码算法的不一致性，从Seq2Seq的模型设计和上面介绍的解码算法来看，并没有任何的理论保证解码过程一定能停下来，也就是说并没有东西保证一定会出现终止token标记，这只能靠模型自己学出来，而当模型学得不够好时，就会出现“根本停不下来”的现象了。针对这问题，论文提出了对应的策略，接下来要提到的对对策内容理解来自文章：<a href=\"https://kexue.fm/archives/7500\">如何应对Seq2Seq中的“根本停不下来”问题？</a></p>\n<h2 id=\"有界的隐向量\"><a href=\"#有界的隐向量\" class=\"headerlink\" title=\"有界的隐向量\"></a>有界的隐向量</h2><p>在对最上面我们事先定义好的条件语言模型进行建模的一个经典方法就是：<br>$$p(y_t|y_{\\lt t}, x)=softmax(Wh_t+b),\\quad h_t=f(y_{\\lt t}, x)$$<br>也就是说，先算一个隐向量 $h_t$，然后接一个全连接，然后softmax激活，在这种形式下，原论文提到：</p>\n<blockquote>\n<p>如果对于任意的 $t$，$∥h_t∥$是有上界的，那么原生随机解码就能够“适可而止”。</p>\n</blockquote>\n<p>看上去很强很实用的一个结论是不是？让$∥h_t∥$是有上界是一个很简单的事情，比如加个Layer Norm就可以了，那是不是说加个Layer Norm就可以解决所有的问题呢？并不是。上述结论理论上是对的，推理过程是：因为 $∥h_t∥$ 是有上界的，所以对于任意的 $t$、任意的token，$p(y_t|y_{\\lt t}, x)$ 是有正的下界的（因为 $Wh_t$不会无穷大，所以 $e^{Wh_t}$ 也不会无穷大，归一化后也不会无限接近于0），那也就意味着存在一个正数 $\\epsilon &gt; 0$，总有 $p(\\text{<eos>}|y_{\\lt t}, x)\\geq \\epsilon$，因为概率是一个正数，因此只要你采样足够多步，总有机会采样到<code>&lt;eos&gt;</code>的，所以不会永远停不下来，示意图如下。<br><img src=\"https://img-blog.csdnimg.cn/20210206153804221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>这推理过程是不是有点让人啼笑皆非？没错，是能停，但是要采样足够多步，感觉就像是“只要你买足够多张彩票就一定能中头奖”一样，并没什么确切的实际价值。采样足够多步之后，该循环的、该重复的token可能都已经重复多次了，就算能停下来，得到的输出可能也没有意义了，或许还不如直接按长度截断。</p>\n<h2 id=\"主动添加-lt-eos-gt\"><a href=\"#主动添加-lt-eos-gt\" class=\"headerlink\" title=\"主动添加&lt;eos&gt;\"></a>主动添加<code>&lt;eos&gt;</code></h2><p>注意上述结论还只是对原生随机解码成立，对于top-k随机解码和Nucleus随机解码不一定成立，因为经过截断后<code>&lt;eos&gt;</code>就不一定出现在采样空间中了，当然，我们可以手动把<eos>添加进采样空间，所以就有了如下的结论：</p>\n<blockquote>\n<p>如果对于任意的$t$，$∥h_t∥$是有上界的，并且我们把<code>&lt;eos&gt;</code>也加入到采样空间中，那么top-k随机解码和Nucleus随机解码就能够“适可而止”。</p>\n</blockquote>\n<h2 id=\"自截断设计\"><a href=\"#自截断设计\" class=\"headerlink\" title=\"自截断设计\"></a>自截断设计</h2><p>注意，上面的两个结论都只能用于随机解码，对于确定性解码来说，因为没有了随机性，所以我们没法保证一定能碰到<code>&lt;eos&gt;</code>。为此，原论文提出了一个自截断的设计：想办法让 $p(\\text{<eos>}|y_{\\lt t}, x)$ 有正的下界，而且这个下界随着 $t$ 的增大而增大，最终逐渐趋于1。这种自截断的设计也不复杂，就是定义 $p(\\text{<eos>}|y_{\\lt t}, x) = 1 - \\alpha(h_t)$，其中<br>$$\\alpha(h_0)=\\sigma\\left(w_{\\text{<eos>}}^{\\top} h_0 + b_{\\text{<eos>}}\\right)$$    $$\\alpha(h_t)=\\sigma\\left(w_{\\text{<eos>}}^{\\top} h_t + b_{\\text{<eos>}}\\right)\\left[1 - p(\\text{<eos>}|y_{\\lt {t-1}}, x)\\right]$$<br>这里的 $\\sigma(\\cdot)$ 负责将 $\\mathbb{R}$ 映射到 $[0, 1-\\epsilon]$，比如可以用 $\\sigma(\\cdot)=(1-\\epsilon)\\text{sigmoid}(\\cdot)$。设计好 $p(\\text{<eos>}|y_{\\lt t}, x)$ 后，剩下的token概率还是按照原来的softmax方式计算，然后乘以 $\\alpha(h_t)$ 即可。现在我们有：<br>$$p(\\text{<eos>}|y_{\\lt t}, x)=1 - \\sigma\\left(w_{\\text{<eos>}}^{\\top} h_t + b_{\\text{<eos>}}\\right)\\left[1 - p(\\text{<eos>}|y_{\\lt {t-1}}, x)\\right]\\ =1 - \\prod_{i=0}^t\\sigma\\left(w_{\\text{<eos>}}^{\\top} h_i + b_{\\text{<eos>}}\\right)\\ \\geq 1 - (1 - \\epsilon)^{t+1} $$<br>显然只要 $t &gt; -\\ln 2/\\ln (1-\\epsilon)$，$p(\\text{<eos>}|y_{\\lt t}, x) &gt; 0.5$，也就是说，对于贪心搜索来说必然在 $-\\ln 2/\\ln (1-\\epsilon)$ 步内停止，而对随着 $p(\\text{<eos>}|y_{\\lt t}, x)$ 越来越接近1，显然Beam Search也能在有限步停止。</p>\n<p>示意图如下：<br><img src=\"https://img-blog.csdnimg.cn/20210206153904241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>下图使用论文中讨论的解码算法的递归语言模型的非终止率：<br><img src=\"https://img-blog.csdnimg.cn/20210206154144299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图是Paper中的示例：<br><img src=\"https://img-blog.csdnimg.cn/20210206154256330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示了Paper中自截断设计的优势：<br><img src=\"https://img-blog.csdnimg.cn/20210206154439581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>关于第二篇文章，可以看这一篇解析：<br><a href=\"https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247508015&idx=1&sn=cdd96d721dbca5fc5bb0390536e1a18c&chksm=970f88f9a07801effb8994afae258b8e2b51bf8b499599133f3501d4ad92e6378f0568edd6ce&mpshare=1&scene=23&srcid=02035hH26qrD9MDND1bsXTaP&sharer_sharetime=1612281796520&sharer_shareid=e7514cbb57886566ce01f800bc56a3ad#rd\">AAAI21 | Seq2Seq模型成为“复读机”的原因找到了？</a></p>\n<p>参考文献：</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2002.02492.pdf\">Consistency of a Recurrent Language Model With Respect to Incomplete Decoding</a></li>\n<li><a href=\"https://arxiv.org/pdf/2012.14660.pdf\">A Theoretical Analysis of the Repetition Problem in Text Generation</a></li>\n<li><a href=\"https://kexue.fm/archives/7500\">如何应对Seq2Seq中的“根本停不下来”问题？</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["Paper","Seq2Seq","语言模型","Decoder"]},{"title":"论文阅读笔记：Tacotron和Tacotron2","url":"/Paper-Reading/cea2357273fe/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Tacotron和Tacotron2<br>Tacotron：<a href=\"http://bengio.abracadoudou.com/cv/publications/pdf/wang_2017_arxiv.pdf\">Tacotron: A Fully End-To-End Text-To-Speech Synthesis Model</a><br>Tacotron2：<a href=\"https://arxiv.org/pdf/1712.05884.pdf\">Natural TTS Synthesis By Conditioning Wavenet On Mel Spectrogram Predictions</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文主要是对Tacotron和Tacotron2论文中的关键部分进行阐述和总结，之所以两篇论文放在一起，是因为方便比较模型结构上的不同点，更清晰的了解Tacotron2因为改进了哪些部分，在性能上表现的比Tacotron更好。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>语音合成系统通常包含多个阶段，例如TTS Frontend，Acoustic model和Vocoder，如下图更直观清晰一点：<br><img src=\"https://img-blog.csdnimg.cn/2020121421532563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>构建这些组件通常需要广泛的领域专业知识，并且可能包含脆弱的设计选择。在很多人困扰于繁杂的特征处理的时候，Google推出了Tacotron，一种从文字直接合成语音的端到端的语音合成模型，虽然在效果上相较于传统方法要好，但是相比Wavenet并没有明显的提升（甚至不如Wavenet），不过它更重要的意义在于end-to-end（Wavenet是啥将在后面对比vocoder的时候讲解，顺便提一下Tacotron使用的是Griffin-Lim算法，而Tacotron2使用的是修改版Wavenet）。此外，相较于其他样本级自回归方法合成语音，Tacotron和Tacotron2是在帧级生成语音，因此要快得多。</p>\n<p>在传统的Pipeline的统计参数TTS，通常有一个文本前端提取各种语言特征，持续时间模型，声学特征预测模型和基于复杂信号处理的声码器。而端到端的语音合成模型，只需要对文本语音进行简单的处理，就能喂给模型进行学习，极大的减少的人工干预，对文本的处理只需要进行文本规范化以及分词token转换（论文中使用character，不过就语音合成而言，使用Phoneme字典更佳），关于文本规范化（数字、货币、时间、日期转完整单词序列）以及text-to-phoneme可以参见我的另一篇<a href=\"https://zhuanlan.zhihu.com/p/336872753\">利器：TTS Frontend 中英Text-to-Phoneme Converter，附代码</a>。端到端语音合成系统的优点如下：</p>\n<ul>\n<li>减少对特征工程的需求</li>\n<li>更容易适应新数据（不同语言、说话者等）</li>\n<li>单个模型可能比组合模型更健壮，在组合模型中，每个组件的错误都可能叠加而变得更加复杂</li>\n</ul>\n<p><strong>端到端语音合成模型的困难所在：</strong><br>不同Speaker styles以及不同pronunciations导致的对于给定的输入，模型必须对不同的信号有着更大的健壮性，除此之外Tacotron原本下描述：</p>\n<blockquote>\n<p>TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into audio</p>\n</blockquote>\n<p>上面这句是Tacotron原文中说的，简单来说就是TTS输出是连续的，并且输出序列（音频）通常比输入序列（文本）长得多，导致预测误差迅速累积。想要了解更多关于语音合成的背景知识，可以参考文章<a href=\"https://www.jianshu.com/p/46888767dcef\">Text-to-speech</a></p>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><h2 id=\"Tacotron\"><a href=\"#Tacotron\" class=\"headerlink\" title=\"Tacotron\"></a>Tacotron</h2><p>Tacotron的基础架构是Seq2Seq模型，下图是模型的总体架构，该模型包括编码器，基于注意力的解码器和post-processing net，从高层次上讲，模型将字符作为输入，并生成频谱图，然后将其转换为波形。<br><img src=\"https://img-blog.csdnimg.cn/20201214230823985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>要特别说明的是架构中，raw text经过pre-net后，将会把输出喂给一个叫CBHG的模块以映射为hidden representation，再之后decoder会生成mel-spectrogram frame。所谓CBHG就是作者使用的一种用来从序列中提取高层次特征的模块，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201214231254861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"CBHG内部结构说明\"><a href=\"#CBHG内部结构说明\" class=\"headerlink\" title=\"CBHG内部结构说明\"></a>CBHG内部结构说明</h3><p>CBHG使用了1D卷积、highway、残差链接和双向GRU的组合，输入序列，输出同样也是序列，因此，它从序列中提取表示非常强大。CBHG架构流程如下</p>\n<ul>\n<li>首先使用 $K$ 组1D卷积对输入序列进行卷积，其中第 $k$ 组表示为 $C_k$ ，卷积的宽度为 $k$（即 $k=1,2,…,K$）。 这些卷积层显式地对本地和上下文信息进行建模（类似于对unigram，bigrams以及K-gram的建模）</li>\n<li>然后将卷积输出堆叠在一起，并进行最大化池，以增加局部不变性。注意了，最大化池使用stride为1来保留原始时间分辨率</li>\n<li>接着将处理后的序列传递给一些固定宽度的1D卷积，其输出通过残差连接与原始输入序列相加，同时批量归一化用于所有卷积层</li>\n<li>然后将输出喂到多层highway网络中以提取高级特征。</li>\n<li>最后，在顶部堆叠双向GRU RNN，以从前向和后向上下文中提取顺序特征。</li>\n</ul>\n<p>在Encoder中，输入被CBHG处理之前还需要经过pre-net进行预处理，作者设计pre-net（pre-net是由全连接层+dropout组成的模块）的意图是让它成为一个bottleneck layer来提升模型的泛化能力，以及加快收敛速度。</p>\n<h3 id=\"Decoder结构说明\"><a href=\"#Decoder结构说明\" class=\"headerlink\" title=\"Decoder结构说明\"></a>Decoder结构说明</h3><p>随后就是Decoder了，论文中使用两个decoder</p>\n<ul>\n<li>attention decoder：attention decoder用来生成query vector作为attention的输入，交由注意力模块生成context vector</li>\n<li>output decoder：output decoder则将query vector和context vector组合在一起作为输入。</li>\n</ul>\n<p>作者并没有选择直接用output decoder来生成spectrogram，而是生成了80-band mel-scale spectrogram，也就是我们之前提到的mel-spectrogram，熟悉信号处理的同学应该知道，spectrogram的size通常是很大的，因此直接生成会非常耗时，而mel-spectrogram虽然损失了信息，但是相比spectrogram就小了很多，且由于它是针对人耳来设计的，因此对最终生成的波形的质量不会有很多影响。</p>\n<p>随后使用post-processing network（下面会讲）将seq2seq目标转换为波形，然后使用一个全连接层来预测decoder输出。Decoder中有一个<strong>trick就是在每个decoder step预测多个(r个)frame，这样做可以缩减计算量，且作者发现这样做还可以加速模型的收敛</strong>。</p>\n<blockquote>\n<p>论文提到scheduled sampling在这里使用会损失音频质量</p>\n</blockquote>\n<h3 id=\"post-processing-net和waveform-synthesis\"><a href=\"#post-processing-net和waveform-synthesis\" class=\"headerlink\" title=\"post-processing net和waveform synthesis\"></a>post-processing net和waveform synthesis</h3><p>作者使用比较简单的Griffin-Lim 算法来生成最终的波形，由于decoder生成的是mel-spectrogram，因此需要转换成linear-scale spectrogram才能使用Griffin-Lim算法，这里作者同样使用CBHG来完成这个任务。实际上这里post-processing net中的CBHG是可以被替换成其它模块用来生成其它东西的，比如直接生成waveform，在Tacotron2中，CBHG就被替换为Wavenet来直接生成波形。</p>\n<h3 id=\"模型详细的配置\"><a href=\"#模型详细的配置\" class=\"headerlink\" title=\"模型详细的配置\"></a>模型详细的配置</h3><p><img src=\"https://img-blog.csdnimg.cn/20201214233931169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>对Decoder和post-processing net使用L1损失，并取平均。作者使用32batch，并将序列padding到最大长度。关于padding的说明，Tacotron原文如下：</p>\n<blockquote>\n<p>It’s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames. However, we found that models trained this way don’t know when to stop emitting outputs, causing repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct the zero-padded frames.</p>\n</blockquote>\n<h2 id=\"Tacotron2\"><a href=\"#Tacotron2\" class=\"headerlink\" title=\"Tacotron2\"></a>Tacotron2</h2><p>Tacotron比较明显的缺点就是生成最终波形的Griffin-Lim算法，Tacotron中作者也提到了，这个算法只是一个简单、临时的neural vocoder的替代，因此要改进Tacotron就需要有一个更好更强大的vocoder。</p>\n<p>接下来我们来看看Tacotron2，它的模型大体上分为两个部分：</p>\n<ul>\n<li>具有注意力的循环序列到序列特征预测网络，该网络根据输入字符序列预测梅尔谱帧的序列</li>\n<li>WaveNet的修改版，可生成以预测的梅尔谱帧为条件的time-domain waveform样本</li>\n</ul>\n<p>结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201215095332283.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Tacotron2选择预测a low-level acoustic表示，即mel-frequency spectrograms（Tacotron使用 linear-frequency scale spectrograms），Tacotron2原文描述如下：</p>\n<blockquote>\n<p>This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame.</p>\n</blockquote>\n<p>mel-frequency spectrogram与linear-frequency spectrograms有关，即短时傅立叶变换（STFT）幅度。mel-frequency是通过对STFT的频率轴进行非线性变换而获得的，同时受到人类听觉系统的启发，用较少的维度表示频率内容，原因很好理解，低频中的细节对于音频质量至关重要，而高频中往往包含摩擦音等噪音，因此通常不需要对高频细节建模。</p>\n<p>虽然linear spectrograms会丢弃相位信息（因此是有损的），但是诸如Griffin-Lim之类的算法能够估算此丢弃的信息，从而可以通过短时傅立叶逆变换进行时域转换。而mel spectrogram会丢弃更多信息，因此它的逆问题更具有挑战性，这个时候作者想到了WaveNet。</p>\n<p>除了Wavenet，Tacotron2和Tacotron的主要不同在于：</p>\n<ul>\n<li>不使用CBHG，而是使用普通的LSTM和Convolution layer</li>\n<li>decoder每一步只生成一个frame</li>\n<li>增加post-net，即一个5层CNN来精调mel-spectrogram</li>\n</ul>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h2 id=\"Tacotron-1\"><a href=\"#Tacotron-1\" class=\"headerlink\" title=\"Tacotron\"></a>Tacotron</h2><p>下图展示Decoder step中，使用不同组件学习到attention alignment的效果：<br><img src=\"https://img-blog.csdnimg.cn/20201215001842202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示了post-processing net的实验效果，可以看到有post-processing net的网络效果更好：<br><img src=\"https://img-blog.csdnimg.cn/20201215090803459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>MOS分数对比如下表：<br><img src=\"https://img-blog.csdnimg.cn/20201215105516463.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Tacotron2-1\"><a href=\"#Tacotron2-1\" class=\"headerlink\" title=\"Tacotron2\"></a>Tacotron2</h2><p>下表展示了Tacotron2与各种现有系统的MOS分数比较。Tacotron2的分数已经和人类不相上下了，这在很大程度上要归功于Wavenet。<br><img src=\"https://img-blog.csdnimg.cn/20201215105745330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是对合成的音频的评价：<br><img src=\"https://img-blog.csdnimg.cn/20201215111449854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>文中提到，Wavenet在这个模型中是和剩下的模型分开训练的，Wavenet的输入是mel-spectrogram，输出是waveform，这个时候就需要考虑输入的mel-spectrogram是选择ground truth，还是选用prediction，作者做了相关实验，结果如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201215112349842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到使用模型生成的mel-spectrogram来训练的Wavenet取得了最好的结果，作者认为这是因为这种做法保证了数据的一致性。下表是生成mel-spectrogram和linear spectrogram的区别（结果证明mel-spectrogram是最好的，同时还能够减少计算，加快inference的时间）：<br><img src=\"https://img-blog.csdnimg.cn/20201215112431948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是对WaveNet简化之后的MOS分数情况：<br><img src=\"https://img-blog.csdnimg.cn/20201215112759691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"关于vocoder\"><a href=\"#关于vocoder\" class=\"headerlink\" title=\"关于vocoder\"></a>关于vocoder</h1><p>Tacotron使用的是Griffin-Lim算法，Griffin-Lim是一种声码器，常用于语音合成，用于将语音合成系统生成的声学参数转换成语音波形，这种声码器不需要训练，不需要预知相位谱，而是通过帧与帧之间的关系估计相位信息，从而重建语音波形。更正式一点的解释是Griffin-Lim算法是一种已知幅度谱，未知相位谱，通过迭代生成相位谱，并用已知的幅度谱和计算得出的相位谱，重建语音波形的方法，具体可参考这篇<a href=\"https://zhuanlan.zhihu.com/p/66809424\">Griffin-Lim 声码器介绍</a></p>\n<p>而Tacotron2使用的WaveNet采用了扩大卷积和因果卷积的方法，让信息随着网络深度增加而成倍增加，可以对原始语音数据进行建模。WaveNet是强大的音频生成模型。 它适用于TTS，但由于其样本级自回归特性而速度较慢。不过要注意的是，WaveNet还需要对现有TTS前端的语言功能进行调整，因此不是端对端的：它仅替代声码器和声学模型。具体可参见如下两篇文章，或参见原论文：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/24317897\">谷歌WaveNet如何通过深度学习方法来生成声音？</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/24568596\">谷歌WaveNet 源码详解</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","NLP","TensorFlow","语音合成","Paper","Tacotron"]},{"title":"论文阅读笔记：XLNet--自回归语言模型的复兴","url":"/Paper-Reading/faa42cbf3533/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：XLNet: Generalized Autoregressive Pretraining for Language Understanding<br>原文链接：<a href=\"https://arxiv.org/pdf/1906.08237.pdf\">Link</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>Google发布的XLNet在问答、文本分类、自然语言理解等任务上都大幅超越BERT，XLNet提出一个框架来连接语言建模方法和预训练方法。我们所熟悉的BERT是denoising autoencoding模型，最大的亮点就是能够获取上下文相关的双向特征表示，所以相对于标准语言模型（自回归）的预训练方法相比，基于BERT的预训练方法具有更好的性能，但是这种结构同样使得BERT有着它的缺点：</p>\n<ul>\n<li>生成任务表现不佳：预训练过程和生成过程的不一致，导致在生成任务上效果不佳；</li>\n<li>采取独立性假设：没有考虑预测[MASK]之间的相关性（位置之间的依赖关系），是对语言模型联合概率的有偏估计（不是密度估计）；</li>\n<li>输入噪声[MASK]，造成预训练-精调两阶段之间的差异；</li>\n<li>无法适用在文档级别的NLP任务，只适合于句子和段落级别的任务；</li>\n</ul>\n<p>鉴于这些利弊，作者提出一种广义自回归预训练方法XLNet，该方法：</p>\n<ul>\n<li>enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization orde</li>\n<li>overcomes the limitations of BERT thanks to its autoregressive formulation</li>\n</ul>\n<h1 id=\"前情提要\"><a href=\"#前情提要\" class=\"headerlink\" title=\"前情提要\"></a>前情提要</h1><p>首先在此之前需要了解一下预训练语言模型的相关联系和背景，这里推荐两篇文章，一篇是邱锡鹏老师的关于NLP预训练模型的总结Paper：<a href=\"https://arxiv.org/pdf/2003.08271.pdf\">Pre-trained Models for Natural Language Processing: A Survey</a>，我之前对它有写过阅读笔记：<a href=\"https://zhuanlan.zhihu.com/p/352152573\">论文阅读笔记：超详细的NLP预训练语言模型总结清单！</a>，还有一篇就是：<a href=\"https://zhuanlan.zhihu.com/p/76912493\">nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)</a>，其中总结的也相当的全面精辟到位。</p>\n<p>目前无监督表示学习这一块，自回归（autogression）语言建模和自动编码（autoencoding）无疑是最成功的两个。对于ELMO、GPT等预训练模型都是基于传统的语言模型（自回归语言模型AR），自回归语言模型天然适合处理生成任务，但是无法对双向上下文进行表征，因此人们反而转向自编码思想的研究（如BERT系列模型）。</p>\n<p>那AE就完美了嘛？自编码语言模型（AE）虽然可以实现双向上下文进行表征，但是依旧存在不适于生成任务的问题，就和上面说的BERT的缺点一样，以BERT为代表的系列模型：</p>\n<ul>\n<li>BERT系列模型引入独立性假设，没有考虑预测[MASK]之间的相关性；</li>\n<li>MLM预训练目标的设置造成预训练过程和生成过程不一致；</li>\n<li>预训练时的[MASK]噪声在finetune阶段不会出现，造成两阶段不匹配问题；</li>\n</ul>\n<p>对于AE和AR两种模型在各自的方向优点，有什么办法能构建一个模型使得同时具有AR和AE的优点并且没有它们缺点呢？这也是XLNet诞生的初衷，对于XLNet：</p>\n<ul>\n<li>不再像传统AR模型中那样使用前向或者反向的固定次序作为输入，XLNet引入排列语言模型，采用排列组合的方式，每个位置的上下文可以由来自左边和右边的token组成。在期望中，每个位置都要学会利用来自所有位置的上下文信息，即，捕获双向上下文信息。</li>\n<li>作为一个通用的AR语言模型，XLNet不再使用data corruption，即不再使用特定标识符号[MASK]。因此也就不存在BERT中的预训练和微调的不一致性。同时，自回归在分解预测tokens的联合概率时，天然地使用乘法法则，这消除了BERT中的独立性假设。</li>\n<li>XLNet在预训练中借鉴了Transformer-XL中的segment recurrence机制的相对编码方案，其性能提升在长文本序列上尤为显著。</li>\n<li>由于分解后次序是任意的，而target是不明确的，所以无法直接使用Transformer-XL，论文中提出采用“reparameterize the Transformer(-XL) network”以消除上述的不确定性。</li>\n</ul>\n<h1 id=\"排列语言模型\"><a href=\"#排列语言模型\" class=\"headerlink\" title=\"排列语言模型\"></a>排列语言模型</h1><p>受无序NADE（Neural autoregressive distribution estimation）的想法的启发，提出一个排列组合语言模型，该模型能够保留自回归模型的优点，同时能够捕获双向的上下文信息。例如一个长度为T的序列，其排序组合为T!，如果所有排列组合次序的参数共享，那么模型应该会从左右两个方向的所有位置收集到信息。但是由于遍历 T! 种路径计算量非常大（对于10个词的句子，10!=3628800）。因此实际只能随机的采样 T! 里的部分排列，并求期望；<br><img src=\"https://img-blog.csdnimg.cn/20210401224809488.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>为了更好的理解，看下面这张图：<br><img src=\"https://img-blog.csdnimg.cn/20210401225106603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>假设输入的序列是[1,2,3,4], 排列共有4x3x2=24种，选其中的四种分别为[3,2,4,1],[2,4,3,1],[1,4,2,3],[4,3,1,2]。在预测位置3的单词时，第一种排列看不到任何单词，第二种排列能看到[2,4]，第三种排列能看到[1,2,4]，第四种排列能看到[4]，所以预测位置3的单词时，不仅能看到上文[1,2]，也能看到下文的[4]，所以通过这种方式，XLNet模型能同时编码上下文信息。</p>\n<blockquote>\n<p>PLM的本质就是LM联合概率的多种分解机制的体现，将LM的顺序拆解推广到随机拆解，但是需要保留每个词的原始位置信息（PLM只是语言模型建模方式的因式分解/排列，并不是词的位置信息的重新排列！）</p>\n</blockquote>\n<p>但是有个问题需要注意，上面提出的排列语言模型，在实现过程中，会存在一个问题，举个例子，还是输入序列[1, 2, 3, 4]肯定会有如下的排列[1, 2, 3, 4]，[1,2,4,3]，第一个排列预测位置3，得到如下公式 $P(3|1,2)$，第二个排列预测位置4,得到如下公式 $P(4|1,2)$，这会造成预测出位置3的单词和位置4的单词是一样的，尽管它们所在的位置不同。论文给出具体的公式解释如下：<br><img src=\"https://img-blog.csdnimg.cn/2021040122582989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>那怎么解决没有目标(target)位置信息的问题？那就是下面要讲的Two-Stream Self-Attention。</p>\n<h1 id=\"Two-Stream-Self-Attention\"><a href=\"#Two-Stream-Self-Attention\" class=\"headerlink\" title=\"Two-Stream Self-Attention\"></a>Two-Stream Self-Attention</h1><p>除了上述之外，模型的实现过程中还有两点要求</p>\n<ul>\n<li>在预测当前单词的时候，只能使用当前单词的位置信息，不能使用单词的内容信息。</li>\n<li>在预测其他单词的时候，可以使用当前单词的内容信息</li>\n</ul>\n<p>为了满足同时这两个要求，XLNet提出了双流自注意力机制，结构如下：<br><img src=\"https://img-blog.csdnimg.cn/20210401233224610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<blockquote>\n<p>下文用 $g_{z_t}$ 表示，上下文的内容信息用 $x_{z&lt;t}$ 表示，目标的位置信息 $z_t$ ，目标的内容信息 $x_{z_t}$</p>\n</blockquote>\n<ul>\n<li>content stream：上面图(a)中，$h_{z_t}^{(m)}\\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=h_{z&lt;t}^{(m-1)};\\theta)$，预测其他单词时，使用自己的内容信息 $h_1^{(0)}$，即Content 流主要为 Query 流提供其它词的内容向量，包含位置信息和内容信息</li>\n<li>query stream：上面图(b)中，$g_{z_t}^{(m)}\\leftarrow Attention(Q=g_{z_t}^{(m-1)},KV=h_{z&lt;t}^{(m-1)};\\theta)$，预测当前单词时，不能使用当前单词内容信息 $h_1^{(0)}$，Query 流就为了预测当前词，只包含位置信息，不包含词的内容信息；</li>\n<li>总流程：上图(c)中，首先，第一层的查询流是随机初始化了一个向量即 $g_i^{(0)}=w$，内容流是采用的词向量即 $h_i^{(0)}=e(x_i)$，self-attention的计算过程中两个流的网络权重是共享的，最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。</li>\n</ul>\n<h1 id=\"集成Transformer-XL\"><a href=\"#集成Transformer-XL\" class=\"headerlink\" title=\"集成Transformer-XL\"></a>集成Transformer-XL</h1><p>除了上文提到的优化点，作者还将transformer-xl的两个最重要的技术点应用了进来，即相对位置编码与片段循环机制。我们先看下recurrence mechanism（不采用BPTT方式求导）。</p>\n<ul>\n<li>前一个segment计算的representation被修复并缓存，以便在模型处理下一个新的segment时作为扩展上下文resume；</li>\n<li>最大可能依赖关系长度增加了N倍，其中N表示网络的深度；</li>\n<li>解决了上下文碎片问题，为新段前面的token提供了必要的上下文；</li>\n<li>由于不需要重复计算，Transformer-XL在语言建模任务的评估期间比vanilla Transformer快1800+倍；</li>\n</ul>\n<p>bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。假设给定一对位置 $i$ 和 $j$ ，如果 $i$ 和 $j$ 是同一个片段里的那么我们令这个片段编码 $s_{ij}=s_{+}$，如果不在一个片段里则令这个片段编码为 $s_{ij}=s_{-}$，这个值是在训练的过程中得到的，也是用来计算attention weight时候用到的，在传统的transformer中attention weight=$Softmax(\\frac{Q\\cdot K}{d}V)$，在引入相对位置编码后，首先要计算出 $a_{ij}=(q_i+b)^Ts_{sj}$，其中 $b$也是一个需要训练得到的偏执量，最后把得到的 $a_{ij}$与传统的transformer的weight相加从而得到最终的attention weight。</p>\n<p>关于相对位置编码更详细的描述可以参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/105001610\">Transformer改进之相对位置编码(RPE)</a></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>XLNet预训练阶段和BERT差不多，不过去除了Next Sentence Prediction，作者发现该任务对结果的提升并没有太大的影响。输入的值还是 [A, SEP, B, SEP, CLS]的模式，A与B代表的是两个不同的片段。更详细的实现细节可以参考<a href=\"https://github.com/zihangdai/xlnet\">论文源码</a>。</p>\n<p>XLNet的创新点：</p>\n<ul>\n<li>仍使用自回归语言模型，未解决双向上下文的问题，引入了排列语言模型</li>\n<li>排列语言模型在预测时需要target的位置信息，为此引入了Two-Stream:Content流编码到当前时刻的所有内容，而Query流只能参考之前的历史信息以及当前要预测的位置信息</li>\n<li>为了解决计算量大的问题，采取随机采样语言排列+只预测1个句子后面的 $\\frac{1}{K}$ 的词</li>\n<li>融合Transformer-XL的优点，处理过长文本</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["BERT","XLNet","预训练模型"]},{"title":"论文阅读笔记：一种模型训练模型的开放域问答方法，SOTA！","url":"/Paper-Reading/fee5036f15c4/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p><a href=\"https://openreview.net/pdf?id=NTEz-6wysdb\">DISTILLING KNOWLEDGE FROM READER TO RETRIEVER FOR QUESTION ANSWERING</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>开放域问答系统中的Retriever的训练，往往是有监督的，这需要我们为模型提供大量的样本数据，这篇文章利用知识蒸馏的技术，让Retriever去学习Reader的attention score，这样的方法并不需要标注好的query和documents对，换句话说，阅读理解模型的注意力权重可以提供更好的检索模型训练信号。这个模型定位的任务是开放域的问答任务，系统通过知识蒸馏的方法，从大量的非标注的文本中学习到一个“Retriever”模型，然后使用一个“Reader”模型对从“Retriever”模型检索出的文本，计算出Attention Score，从而得到精确的对标文本。</p>\n<p>接下来我们来详细了解一下这个系统的具体细节，后文将“Retriever”表述为检索模型，将“Reader”表述为阅读模型。</p>\n<blockquote>\n<p>当前主流的问答系统主要分为几类：FAQ检索型、闲聊型、任务型、知识图谱型、阅读理解型等等。他们之间互相有些区别，但本质上都可以被看作是从庞大的信息中找到想要答案的过程，方法上互相之间也有一些借鉴意义。</p>\n</blockquote>\n<p>对检索式模型感兴趣的小伙伴可以看看百度之前提出的SMN和DAM，我对这两个模型的Paper都有阅读笔记：<br><a href=\"https://zhuanlan.zhihu.com/p/306846122\">DAM</a><br><a href=\"https://zhuanlan.zhihu.com/p/270554147\">SMN</a></p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>早先的开放域问答系统是将基于TF-IDF的词频算法用作检索模型，并结合阅读模型进行构建的，典型的就是DrQA模型，众所周知，基于词频的检索模型的一大优点就是简单有效，大体上可以归纳为：</p>\n<ul>\n<li>对于大量文本，可以将所有段落的词频都提前统计出来，并储存为向量的形式</li>\n<li>对于给定问题，通过向量近邻搜索就可以快速查询到最佳候选段落。</li>\n</ul>\n<p>不过文中也指出，使用词频算法的检索模型有着非常明显的局限性，即：</p>\n<ul>\n<li>词频不能完全表示文本的含义，检索出的文本质量也因此受限，从而影响问答系统整体的表现。</li>\n<li>基于词频的检索模型不包含注意力机制，很难给关键程度不同的信息以不同的评分</li>\n</ul>\n<p>总的来说，单纯基于词频的方式会导致信息检索的依据维度过于片面，而且信息相关性无处体现。自然而然的，我们就会想到使用Language Model来进行相关替换，论文中提到相关的迭代工作如下：</p>\n<ul>\n<li>Vector space model</li>\n<li>Neural information retrieval</li>\n<li>End-to-end retrieval</li>\n<li>Unsupervised learning：这也是与论文的工作最接近的方法，即尝试从无监督的数据中学习信息检索系统。</li>\n</ul>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><h2 id=\"Reader\"><a href=\"#Reader\" class=\"headerlink\" title=\"Reader\"></a>Reader</h2><p>我们首先来了解<br><a href=\"https://arxiv.org/pdf/2007.01282.pdf\">Fusion-in-Decoder</a><br>这个模型底层结构是一个sequence-to-sequence模型，由编码器和解码器组成，编码器独立处理不同的文本输入 $(s_k)_{1\\leq k\\leq n_p}$ 名词短语。在基于Wikipedia的开放域问答中，每个输入 $s_k$ 都是question q和 support passage的concatenation，带有特殊的tokens，形如<code>question:</code>，<code>title:</code>和 <code>context:</code>，分别添加在问题、Wikipedia文章的标题和每篇文章的文本前。然后将编码器的输出表示concatenation起来，形成维度为 $(\\sum_kl_k)$ 的全局表示 $X$，其中，$k$ 是第 $k$ 段的长度，$d$ 是模型的嵌入和隐藏表示的大小。然后，解码器将这种表示应用于一个规则的自回归模型、交替的self-attention、cross-attention和feed-forward进行处理。Fusion-in-Decoder模型结构如下：<br><img src=\"https://img-blog.csdnimg.cn/20210516104300834.png\" alt=\"在这里插入图片描述\"></p>\n<p>解码器说白了就是使用Transformer中的那套生成信息的结构，这里说明一下Q、K、V的表示形式：<br>$$Q=W_QH,K=W_KX,V=W_VX$$</p>\n<p>后面计算相关系数和注意力分数具体见Transformer，下面我们来通俗的理解一下Reader：</p>\n<ul>\n<li>阅读模型的打分往往只基于被送入阅读器的段落。想要获得不同段落之间的交互信息，必须将所有候选段落拼接输入阅读器。但由于BERT的复杂度随着序列长度平方级增长，拼接输入并不高效。而Fusion-in-Decoder model中采用生成式（Encoder-Decoder）模型作为阅读模型，他们将不同段落分别输入Encoder获得段落的的表示，然后将这些表示拼接在一起作为Decoder的输入。这样Encoder不需要用平方级的复杂度，Decoder在生成答案的时候也获得了所有段落的信息。</li>\n<li>Reader部分直接沿用了Fusion-in-Decoder模型，通过Fusion-in-Decoder模型检索候选文章输出答案时产生的attention矩阵来指导Retriever进行学习。</li>\n<li>这其实是一件非常直觉又有趣的事情，Retriever本应该作为Reader的老师，告诉Reader应该看哪些文章，并从中得出答案。但相反，由于Reader已经提前知道了答案，如果Reader阅读范围足够广，我们可以通过attention矩阵来映射Reader在生成答案时把注意力放在了哪些文章上，并以此告诫Retriever，下次召回时应该召回类似的文章。</li>\n</ul>\n<h2 id=\"CROSS-ATTENTION-SCORE\"><a href=\"#CROSS-ATTENTION-SCORE\" class=\"headerlink\" title=\"CROSS-ATTENTION SCORE\"></a>CROSS-ATTENTION SCORE</h2><p>根据Fusion-in-Decoder论文提到的，decoder中的cross-attention计算公式如下（上面提到的具体化）：<br>$$Q=W_QH,K=W_KX,V=W_VX$$<br>$$a_{i,j}=Q_i^TK_j,\\ \\ \\bar{a}<em>{i,j}=\\frac{exp(a</em>{i,j})}{\\sum_mexp(a_{i,m})},\\ \\ O_i=W_O\\sum_j\\bar{a}<em>{i,j}V</em>{i,j}$$</p>\n<p>作者假设 $a_{:,j}$ 可以用来度量第 $j$ 个key token对于模型通过value计算下一个特征表示的重要性，并以此作为与该key token对应文档的重要性的代表——the more the tokens in a text segment are attended to, the more relevant the text segment is to answer the question</p>\n<p>因此通过对attention score进行聚合，我们可以得到各个Passage的相关性分数 $(G_{q,pk})1\\leq k \\leq n$，具体来说分数$G$是聚合了decoder中该文档所有key token的pre-attention scores $a_{0,:}$ 得到的。作者比较了不同的mean_max和选取不同层attention score的聚合方法。最后表面简单的对所有层，所有token的attention score作平均效果最佳。<br><img src=\"https://img-blog.csdnimg.cn/20210516111451924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>作者通过一个简单的实验证明了 $G_{q,pk}$ 是一个优秀的Passage相关性的评估指标：100篇由DPR召回的文档，使用DPR分数选择前10篇，召回性能从48.2EM降低到了42.9EM，但如果根据 $G_{q,pk}$ 选择前10篇最重要的文档，召回性能仅从48.2EM降低到了46.8EM。</p>\n<p><strong>结论：在生成式阅读模型中，Encoder和Decoder的交互注意力权重可看作预测时不同段落信息的重要度。</strong></p>\n<h2 id=\"Retriever\"><a href=\"#Retriever\" class=\"headerlink\" title=\"Retriever\"></a>Retriever</h2><p>这里我们简单来了解一下DPR模型，再最上面我们有说过基于词频的检索模型的一些明显的缺点，为了解决这类检索模型的缺点，常见的问答系统先通过基于词频的检索模型初筛出较大量候选段落，再应用基于BERT的检索模型，将问题和段落拼接在一起进行精细排序。</p>\n<p>然而，这类分阶段检索的模型仍然存在一些问题：它始终需要进过基于词频的检索，有信息损失；每次预测都需要将较大量的文本送入BERT进行精排。能否在一开始就用BERT这类效果更好的模型预先编码好所有段落，在检索的时候直接进行向量搜索呢？</p>\n<p>DPR的方法来解决上述缺陷，由于BERT这类模型太过庞大，在预测的时候无法实时对所有段落进行重编码，因而问题和段落需要分开编码。文中采用两个不同的BERT模型分别编码问题和段落，问题和段落编码向量相似度即为检索模型的打分。</p>\n<p>DPR的一大创新点在于线下完成所有段落的编码。训练段落编码器时，将含有标准答案字串的候选段落作为编码器的正例，其他段落作为负例。训练完成后，即可在预测前对所有段落进行编码。预测时只需要编码问题，即可通过向量搜索得到相关段落。然而，DPR在监督信息的获取上是存在一定问题的——这也是基于网络的检索模型训练的一大难点。DPR是利用答案字符串是否出现在段落中的信号来定义编码器的正负例，这个信号中包含了大量的噪声。</p>\n<p>本文中的Retriever的模型结构设计与DPR相似，只是将两个独立的Bert编码器减少到了一个共享参数的特征编码器。损失函数的设计上，由于Retriever不再是拟合一个二分类问题，而是拟合Reader产生的注意力分布，因此需要最小化 $S_{\\theta}(q,p)$ 和  $G_{q,pk}$  之间的KL散度：<br><img src=\"https://img-blog.csdnimg.cn/20210516112922882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>公式中表示Decoder第 $i$ 个token对Encoder第 $j$ 个token，在第 $k$ 层第 $h$ 个注意力头（head）的注意力权重，$G_{q,p}$ 表示的是阅读模型对这个问题，段落对的注意力权重打分，$S_{q,p}$ 表示检索模型对问题是 $q$ 检索出段落 $p$ 的打分，也就是检索模型的输出。这样，我们就可以让检索模型学习阅读模型的注意力信息了。</p>\n<p>文中试验了不同的设置，最终确定：Decoder的第0个token对于Encoder同一段落中所有token的注意力权重平均值是最佳设定。同时作者还比较了MSE、max-margin loss等其他损失函数，最终KL散度的训练效果最佳。<br><img src=\"https://img-blog.csdnimg.cn/20210516113242173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"ITERATIVE-TRAINING\"><a href=\"#ITERATIVE-TRAINING\" class=\"headerlink\" title=\"ITERATIVE TRAINING\"></a>ITERATIVE TRAINING</h2><p>训练的Pipeline分为4步：</p>\n<ul>\n<li>Train the reader R using the set of support documents for each question $D_q^0$</li>\n<li>Compute aggregated attention scores $(G_{q,p})_{q\\in Q,p\\in D_q^0}$ with the reader R.</li>\n<li>Train the retriever E using the scores $(G_{q,p})_{q\\in Q,p\\in D_q^0}$ </li>\n<li>Retrieve top-passages with the new trained retriever E</li>\n</ul>\n<p>总的来说，就是对于每个问题，用检索模型选取前 $k$ 个相关段落，用于训练阅读模型。在相关段落上训练好阅读模型后，对于每个问题的候选段落计算池化之后的注意力权重。利用注意力权重作为检索模型的蒸馏训练信号，训练检索模型。从随机初始化的检索模型开始训练无疑是效率很低的，初始的候选段落便显得尤为重要。作者选取了不同的初始筛选方法（BM25，BERT，DPR）来确定第一步的相关段落集合。<br><img src=\"https://img-blog.csdnimg.cn/20210516114534215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>实验发现从DPR方法选择的初始相关段落可以让模型达到最好的效果（从迭代效果来看，实验能够说明这一套迭代的pipeline是有效的）。由于BERT的预训练目标和相关度排序相差甚远，因而用预训练好的BERT作为检索模型的初始参数并选择最初的相关段落集效果不佳。但作者提出的训练方法可以在4个迭代内让检索模型大幅提升效果。<br><img src=\"https://img-blog.csdnimg.cn/2021051611463813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>之所以这一套QA循环训练系统不需要给到retriever E标注好的数据对，是因为在训练过程中我们只需要初始化好最开始的support documents集合，通过对R的训练指导E的下一步的召回，即可实现自我迭代。</p>\n<p>这里有一个比较关键的地方，在于每轮迭代开始时，重新初始化reader-T5的权重，并保留retriever-Bert的权重继续训练。文章并没有对这样的做法做出解释，猜测有两个作用：</p>\n<ul>\n<li>reader-T5的训练相对于retriever-Bert来说更加容易，尽管我们初始化了reader-T5，但由于retriever-Bert的性能越来越强，召回的文章越来越准确，reader-T5也会更快的收敛，这一定程度上平衡了两个模块的训练进程，防止一方陷入过拟合后导致系统崩溃。</li>\n<li>通过初始化reader-T5，文章通过监控reader-T5和retriever-Bert的注意力/打分对文章排名的差异性来监控retriever-Bert的训练进程，当reader-T5和retriever-Bert对文章排名结果相近且稳定时，认为系统已经训练充分，停止迭代。</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本论文仅使用问题答案样本对就实现了开放域问答的SOTA，这归功于其精彩蒸馏思路将原本两个独立的训练模块Reader和Retriever进行连接，Pipeline的思路减少了模块之间的误差传递，使得整个系统更加一体化。并且文章对于损失函数、初始化方法、attention score的进行了大量的对比实验，组合出了一套完整的训练方案。如果我们把Reader视为精排，Retriever视为召回，这种通过精排模块指导召回模块训练的方法非常值得借鉴。本文从侧面反映了Transformer架构中，attention机制的可解释性，利用模型的attention模块的中间输出，我们可以获取更多其他的有效信息。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","检索式","QA"]},{"title":"论文阅读笔记：使用Transformer进行语音合成","url":"/Paper-Reading/5dff8ecd5ba4/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Neural Speech Synthesis with Transformer Network<br>原文链接：<a href=\"https://arxiv.org/pdf/1809.08895.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>虽然像Tacotron2这样的TTS模型实现了最新的性能，但它们仍然存在两个问题：</p>\n<ul>\n<li>在训练和推理过程中效率低下（巨慢）</li>\n<li>难以使用当前的递归神经网络（RNN）对长期依赖性进行建模</li>\n</ul>\n<p>本文受Transformer启发，使用多头自注意力机制取代Tacotron2中的RNN结构和原始注意力机制。借助多头自注意力机制，可以并行构造编码器和解码器中的隐藏状态，从而提高训练效率，同时，不同时间步的任意两个输入通过自注意力机制直接连接，有效解决了远程依赖问题。使用phoneme（音素）序列作为输入，Transformer TTS网络生成梅尔频谱图，然后通过WaveNet声码器以输出最终的音频结果。</p>\n<blockquote>\n<p>phoneme音素：能区分意义的最小声音单位，比如dog和fog中，d和f只要改变一个就改变了意义。</p>\n</blockquote>\n<ul>\n<li>Tacotron2模型结构<br><img src=\"https://img-blog.csdnimg.cn/20201205235515714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>Transformer模型结构<br><img src=\"https://img-blog.csdnimg.cn/20201206000024694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>这里对一些语音方面的相关概念进行说明，如果不感兴趣想要直接看论文内容，可以直接跳过这一节。</p>\n<ul>\n<li>speech wave：语音波是一种compound wave，即包含各种频率的波。因此在频域上表示语音更为合适。</li>\n<li>pitch音高：声音的尖锐程度，在频域中表现为频率的高低。</li>\n<li>基础频率F0：浊音中存在基础频率，而清音中不存在，F0决定了声音的音高。</li>\n<li>formants共振峰：是一种元音特有的在频域中的现象，因为只有元音有基础频率。每个元音都有两个共振峰，可以用来区分元音，记为F1和F2。F1,F2取决于基础频率，如果基础频率太高，共振峰可能会消失，这种情况下就区分不出来元音，这种现象在各种女高音身上比较常见。</li>\n<li>timbre音色：音色在广义上是指声音不同于其它的特点，在语音中不同的音节都有不同的特点，这可以通过频域观察出来，另外，特别地，对于元音我们可以通过共振峰来分辨音色。</li>\n<li>noise：噪音、辅音(摩擦音)都会有broad spectrum，也就是说我们无法通过共振峰来识别它们。</li>\n<li>envelope包络：在波的时域和频域图中，用来形容图形的整体形状的叫做包络。</li>\n<li>frontend：主要是文字处理，使用NLP技术，从离散到离散，包括基本的分词、text normalization、POS以及特有的Pronunciation标注。</li>\n<li>backend：根据前端结果生成语音，从离散到连续</li>\n<li>segmentation &amp; normalization：去噪、分句、分词以及把缩写、日期、时间、数字还有符号都换成可发音的词，这一步叫spell out。基本都基于规则。</li>\n</ul>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><p>与基于RNN的模型相比，在神经TTS中使用Transformer具有两个优点：</p>\n<ul>\n<li>因为可以并行提供解码器输入序列的帧，它可以通过取代循环连接来进行并行训练。</li>\n<li>自注意力将整个序列的全局上下文注入到每个输入帧中，直接建立了长距离依赖关系</li>\n</ul>\n<p>在本节中，将介绍Transformer TTS模型的体系结构，并分析每个部分的功能，整个模型结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/2020120609460844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Text-to-Phoneme转换器：由于训练数据不足的情况下很难学习语言的所有规律性，并且某些例外情况很少出现，而无法通过神经网络学习。因此，作者建立了一个规则系统并将其实现为文本到音素的转换器，它可以覆盖绝大多数情况。</li>\n<li>Scaled Positional编码：采用的位置编码是Transformer的正弦位置编码：<br>$$PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$    $$PE(pos,2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$<br>其中 $pos$ 是时间步长索引，$2i$ 和 $2i+1$ 是通道索引，$d_{model}$ 是每个帧的维数。不过有一点要注意的是，不像文本训练那样，source和target都是一个语言空间，embedding的编码是相似的。TTS中使用固定的位置嵌入可能会对编码器和解码器的pre-nets都施加严格的约束（将在后面描述），因此作者使用具有可训练权重的位置编码，以便这些位置编码可以自适应地匹配编码器和解码器pre-nets输出的比例，公式如下：<br>$$x_i=prenet(phoneme_i)+\\alpha PE(i)$$<br>其中$\\alpha$是可训练权重。</li>\n<li>Encoder Pre-net：在Tacotron2中，将三层CNN应用于输入文本嵌入，它可以对输入字符序列中的上下文进行建模。在这里的Transformer TTS模型中，将 phoneme序列输入到同一网络中，这称为“encoder pre-net”。每个phoneme具有512维的可训练嵌入，每个卷积层的输出具有512个通道，然后进行batch normalization、ReLU激活以及dropout。此外，由于ReLU的输出范围是 $[0,\\infty]$，而这些位置编码的每个维数都在 $[-1,1]$ 中，所以作者在最终ReLU激活后添加了线性层。在实验中证明，在Nonnegative Embeddings中添加以0为中心的位置信息将影响模型的性能。</li>\n<li>Decoder Pre-net：梅尔频谱图首先被具有ReLU激活的，由两个全连接层（每个层都有256个隐藏单元）组成的神经网络处理，称为“decoder pre-net”，它在TTS系统中起着重要作用。phonemes具有可训练的嵌入，因此其子空间是自适应的，而梅尔频谱图的是固定的。<strong>作者推断decoder pre-net负责将梅尔频谱图投影到与phonemes嵌入相同的子空间中，从而可以计算 $\\left \\langle phoneme, mel\\ frame \\right \\rangle$ 对的相似性，从而使注意力机制发挥作用</strong>。此外，还尝试了2个没有非线性激活的全连接层，但是无法生成合理的注意力矩阵来对齐编码器和解码器的隐藏状态。此外，<strong>作者推测梅尔谱图具有一个紧凑且低维的子空间，其中256个隐藏单元足以映射</strong>。同encoder pre-net一样，还添加了一个附加的线性层，不仅用于中心一致性，而且还获得与位置编码相同的尺寸。</li>\n<li>Encoder：在Tacotron2中，编码器是双向RNN，而这里使用Transformer编码器代替它。与原始的双向RNN相比，多头注意力将注意力分散到几个子空间中，从而可以在多个不同方面建模帧关系，并直接建立任意两个帧之间的长依赖关系，因此每个帧都被视为整个序列的全局上下文。这对于合成音频韵律至关重要，尤其是在句子较长的情况下。</li>\n<li>Decoder：在Tacotron2中，解码器是一个结合location sensitive attention的2层RNN，而这里使用Transformer解码器代替它。</li>\n<li> Mel Linear、Stop Linear和Post-net：与Tacotron2相同，我们分别使用两个不同的线性层来预测梅尔频谱图和停止标记，并使用5层CNN产生残差来完善mel频谱图的重建。值得一提的是，对于停止标记的线性层而言，每个序列的末尾只有一个正样本，表示“停止”，而其他帧则是负样本，这种不平衡可能导致无法停止的推断。在计算二元交叉熵损失时，作者在停止标记的正样本上施加正权重 $(5.0\\sim 8.0)$，从而有效地解决了这个问题。</li>\n</ul>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>实验使用25小时的专业语音对测试Transformer TTS模型，并通过人工测试在MOS和CMOS中评估音频质量。<strong>由于训练样本的长度相差很大，因此，如果以长样本为准扩大batch尺寸将占用很大内存，而如果以短样本为准缩小batch尺寸则会浪费并行计算能力，因此，作者使用动态batch大小，其中最大总的Mel光谱图帧数是固定的，并且一个batch应包含尽可能多的样本。</strong></p>\n<p>Tacotron2使用字符序列作为输入，而本文的模型是根据pre-normalized phoneme序列训练的。自回归WaveNet包含2个QRNN层和20个扩张层，所有残差通道和扩张通道的大小均为 $256$。QRNN最终输出的每一帧均被复制200次，以具有与音频样本相同的空间分辨率且条件为20扩张层。</p>\n<p>下表是MOS和CMOS指标的对比结果：<br><img src=\"https://img-blog.csdnimg.cn/20201206111801454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图是模型生成的梅尔频谱的结果对比：如我们所见，论文模型在重建以红色矩形标记的细节方面做得更好，而Tacotron2在高频区域则省略了细节纹理。<br><img src=\"https://img-blog.csdnimg.cn/20201206112001432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了中心一致的位置编码效果稍好：<br><img src=\"https://img-blog.csdnimg.cn/2020120611221863.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下图表明编码器和解码器的最终位置编码比例不同的对比：<br><img src=\"https://img-blog.csdnimg.cn/20201206114754487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下表显示了具有可缩放比例的模型，其性能略有提高：<br><img src=\"https://img-blog.csdnimg.cn/20201206112337437.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>下面3张表是比较具有不同层数和头注意力数的性能和训练速度。发现减少层数和头注意力数均可以提高训练速度，但另一方面，会在不同程度上损害模型性能。<br><img src=\"https://img-blog.csdnimg.cn/20201206113930677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201206113940804.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>值得再次提及的是batch大小对于训练稳定性至关重要，并且更多的层可以完善生成的mel频谱图的细节，尤其是在高频区域，从而提高模型性能。论文作者对这一模型做了很多的实验，总的来说，训练时期的速度大大提高，加快了2到3倍，生成语音的质量也好于传统RNN结构模型（存疑，复现版本仅仅能做到效果相接近，可能是作者的调参技艺比较高超）。基于Transformer的TTS模型已是现在主流的End-to-End TTS系统的baseline，它的实现必不可少，而且因为Transformer本身优异的结构，也能大大加快实验的速度。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Transformer","NLP","Pytorch","语音合成","Paper"]},{"title":"论文阅读笔记：全局-局部自注意力的对话状态跟踪","url":"/Paper-Reading/9c4b9a5af67b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Global-Locally Self-Attentive Dialogue State Tracker<br>原文链接：<a href=\"https://arxiv.org/pdf/1805.09655.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>对话状态跟踪（在对话上下文中估计用户目标和请求）是面向任务的对话系统的重要组成部分。在本文中，提出了“全局-局部自注意力对话状态追踪”（GLAD），该学习器使用全局本地模块来学习用户话语的表示和以前的系统动作。模型使用全局模块在不同类型（称为插槽）的对话状态的估计量之间共享参数，并使用本地模块学习特定于插槽的特征。DST中的状态（state）通常由一个请求（request）和联合目标（joint goals）的集合组成。请求即请求系统返回所需信息（例如：request(address)），目标即用户想要完成的动作（例如：inform(food=french)），请求和目标可以用槽位-值对（slot-value pair）来表示（(food,french)， (request, address)）。<br><img src=\"https://img-blog.csdnimg.cn/20201019151853610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>作者认为传统的DST极其依赖Spoken Language Understanding（SLU），而依赖SLU极易导致错误的积累，且通常的SLU对比较少见的slot-value对容易分错，为了解决这些问题，本文提出了一种全局-局部自注意力对话状态追踪方法（Global-Locally Self-Attentive Dialogue State Tracker， GLAD），使用全局模块在预测每个slot的预测器之间共享参数，而使用局部模块来学习每个slot的特征表示。通过这种设计，能够使GLAD在更小的训练数据上泛化到更少见的slot-value对。</p>\n<h1 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h1><p>模型整体框架如下，主要包含两个模块：encoder模块和scoring模块。在encoder模块中，可以看到分别针对system action，user utterance和候选slot-value对这三者进行encoder。注意，后面讨论的都是针对某个具体的候选slot-value对。scoring模块包含两个打分器，分别给当前话语和之前轮次系统行为对当前状态预测的贡献打分。<br><img src=\"https://img-blog.csdnimg.cn/20201019232904710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"全局-局部自注意力编码器\"><a href=\"#全局-局部自注意力编码器\" class=\"headerlink\" title=\"全局-局部自注意力编码器\"></a>全局-局部自注意力编码器</h2><p>在针对三部分输入进行encoding的过程中，使用的都是Global-Locally Self-Attentive Encoder，这个encoder的框架如下：<br><img src=\"https://img-blog.csdnimg.cn/20201019233304219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>基本可以分为如下几个步骤：</p>\n<ul>\n<li>将输入分别通过global和local的BiLSTM得到各自的表征，全局与局部双向LSTM类似，输入embedding后的序列，得到编码输出，区别在于全局双向LSTM的参数在不同slot间共享，而每个slot都有各自的局部双向LSTM参数。<br>$$H^g=biLSTM^g(X)\\in \\mathbb{R}^{n \\times d_{rnn}}$$  $$H^s=biLSTM^s(X)\\in \\mathbb{R}^{n \\times d_{rnn}}$$</li>\n<li>将global和local的进行加权融合，两个LSTM的输出通过一个混合层相结合，形成全局-局部编码的最终输出，其中 $\\beta^s$ 是学习到的一个0-1之间的权重值（每个slot的 $\\beta^s$ 不同）。<br>$$H = \\beta^sH^s+(1-\\beta^s)H^g\\in \\mathbb{R}^{n \\times d_{rnn}}$$</li>\n<li>类似的，针对global和local的分别使用attention机制，并最后做融合。其中，对 $H$ 的每个元素 $H_i$ 计算一个标量attention分值，通过softmax标准化，再对 $H_i$ 做加权求和，得到最终的输出。<br>$$a_{i}^{g}=W^gH_i+b^g\\in \\mathbb{R}$$  $$p^g=softmax(a^g)\\in \\mathbb{R}^n$$  $$c^g=\\sum_ip_{i}^gH_i\\in  \\mathbb{R}^{d_{rnn}}$$</li>\n</ul>\n<p>$$a_{i}^{s}=W^sH_i+b^s\\in \\mathbb{R}$$  $$p^s=softmax(a^s)\\in \\mathbb{R}^n$$  $$c^s=\\sum_ip_{i}^sH_i\\in  \\mathbb{R}^{d_{rnn}}$$<br>最终的全局-局部自注意力表示为：<br>$$c=\\beta^sc^s+(1-\\beta^s)c^g\\in\\mathbb{R}^{n \\times d_{rnn}}$$</p>\n<ul>\n<li>注意，encoder会输出两个东西，$H$ 和 $c$ 都会在下面的scoreing module中使用到：</li>\n</ul>\n<h2 id=\"Encoding-module\"><a href=\"#Encoding-module\" class=\"headerlink\" title=\"Encoding module\"></a>Encoding module</h2><p>分别针对三个输入利用上面的encoder进行encoding，其中，用 $U$ 表示当前话语的embedding序列， $A_j$ 表示之前的第 $j$ 个系统行为， $V$ 表示当前考虑的slot-value对，以上述全局-局部自注意力编码器为基础的编码模块的输出为：<br>$$H^{utt},c^{utt}=encode(U)$$  $$H_j^{act},c_j^{act}=encode(A_j)$$  $$H^{val},c^{val}=encode(V)$$</p>\n<h2 id=\"Scoring-Module\"><a href=\"#Scoring-Module\" class=\"headerlink\" title=\"Scoring Module\"></a>Scoring Module</h2><p>分为三个步骤：</p>\n<ul>\n<li>通过类似attention机制进行utterance与slot-value的匹配和打分，具体而言，当前用户话语对于当前slot-value对是否在当前轮次中的贡献是用户直接表述出来的（比如：how about a French restaurant in the centre of town?）。针对这种情况，使用当前slot-value对表示 $c^{val}$ 对 $H^{utt}$ 进行attention并加权求和，用所得结果为该slot-value对打分。<br>$$a_i^{utt}=(H_i^{utt})^Tc^{val}\\in\\mathbb{R}$$  $$p^{utt}=softmax(a^{utt})\\in\\mathbb{R}^m$$  $$q^{utt}=\\sum_ip_i^{utt}H_i^{utt}\\in\\mathbb{R}^{d_{rnn}}$$  $$y^{utt}=Wq^{utt}+b\\in\\mathbb{R}$$</li>\n<li>同样地，先进行system action与utterance之间的融合，尔后再与slot-value进行匹配。具体而言，而当当前用户话语没有呈现足够信息时，对当前轮次状态的推断则需要考虑之前轮次的系统行为（例如用户只回答了“yes”）。针对这种情况，采取与上述attention过程类似的思路对过往轮次系统行为对当前状态推断的贡献打分。<br>$$a_j^{act}=(C_j^{act})^Tc^{utt}\\in\\mathbb{R}$$  $$p^{act}=softmax(a^{act})\\in\\mathbb{R}^{l+1}$$  $$q^{act}=\\sum_ip_j^{act}C_j^{act}\\in\\mathbb{R}^{d_{rnn}}$$  $$y^{act}=(q^{act})^Tc^{val}\\in\\mathbb{R}$$</li>\n<li>最后打分由两部分分数加权求和，并经过sigmoid标准化。<br>$$y=\\sigma(y^{utt}+wy^{act})\\in\\mathbb{R}$$</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><ul>\n<li>展示了GLAD与以前的最新模型相比的性能<br><img src=\"https://img-blog.csdnimg.cn/2020102011100272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>我们在开发套件上进行了分解实验，以分析GLAD不同组件的有效性。<ul>\n<li>时间顺序对于状态跟踪很重要</li>\n<li>Self-attention可实现特定于插槽的强大功能学习</li>\n<li>Global-local共享可改善目标跟踪<br><img src=\"https://img-blog.csdnimg.cn/20201020111233472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这是一个示例，其中self-attention模块专注于话语的相关部分。<br><img src=\"https://img-blog.csdnimg.cn/20201020111654205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示了，在训练数据中显示了GLAD的性能以及这两种共享变体在不同出现次数上的表现。对于具有大量训练数据的槽值对，模型之间没有明显的性能差异，因为有足够的数据可以概括<br><img src=\"https://img-blog.csdnimg.cn/20201020112030255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图展示GLAD的预测示例<br><img src=\"https://img-blog.csdnimg.cn/20201020112258567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1>GLAD的核心是Global-Locally自注意编码器，其全局模块允许在插槽之间共享参数，而本地模块则允许学习特定于插槽的特征。这使GLAD可以在很少的训练数据的情况下对稀有的插槽值对进行概括。global和local的思想值得一些借鉴。</li>\n</ul>\n</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["对话系统","Attention","注意力机制","Paper","状态跟踪"]},{"title":"论文阅读笔记：四种用于学习对话上下文表示的无监督预训练方法的对比和分析","url":"/Paper-Reading/7f6535cceaa6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Pretraining Methods for Dialog Context Representation Learning<br>原文链接：<a href=\"https://arxiv.org/pdf/1906.00414.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>本文考察了各种用于学习对话上下文表示的无监督预训练目标， 提出了两种新颖的对话上下文编码器预训练方法，并研究了四种方法。使用MultiWoz数据集对每个预训练目标进行了微调，并在一组下游对话任务上进行了评估，并观察到了出色的性能改进。 进一步的评估表明，我们的预训练目标不仅可以带来更好的性能，而且可以带来更好的收敛性，并且模型需要的数据更少，并且具有更好的领域通用性。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>目前预训练方法仍处在起步阶段，我们仍然不能完全了解他们的性质。大多数方法都是基于语言模型的，给一个句子，预测当前词，下一个词或者被mask的词。如Word2Vec，Glove，ELMO等。这些方法将自然语言看作是word token的流，需要复杂的模型利用大规模的语料库和庞杂的计算来发现更高级别的依赖关系。BERT模型也是基于语言模型，但是加入了句子对级别的信息，预测两句话是否是连续的。这种方法在预训练时利用了语句之间的关系。但是，在对话上下文建模这种存在多轮的依赖关系的任务上还并没有行之有效的预训练方法，于是本文在这个方面做了一些尝试。本文目的就是研究几个预训练话语级语言表示的方法，本文迈出了建立对话系统预训练方法系统分析框架的第一步。</p>\n<p>评估预训练方法的四个假设：</p>\n<ul>\n<li>预训练能够在整个可用数据集上进行微调，且提升下游任务</li>\n<li>预训练结果需要更好的收敛</li>\n<li>预训练将在有限的数据下表现出色</li>\n<li>预训练有助于领域通用化</li>\n</ul>\n<p>对话与其他文本的区别：</p>\n<ul>\n<li>对话必须是语句之间连贯的，并在多轮上达到一个交际的目的。</li>\n<li>对话在本质上是互动的，说话者之间有反馈，而且说话者轮流进行发言。</li>\n</ul>\n<p>本文的主要贡献：</p>\n<ul>\n<li>针对对话上下文表示研究四个不同的预训练方法，包括两个新的方法</li>\n<li>在四个下游任务上，综合分析预训练对对话上下文表示的影响</li>\n</ul>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>这项工作与NLP系统的辅助多任务学习和带预训练的迁移学习的研究紧密相关。</p>\n<h2 id=\"Training-with-Auxiliary-Tasks\"><a href=\"#Training-with-Auxiliary-Tasks\" class=\"headerlink\" title=\"Training with Auxiliary Tasks\"></a>Training with Auxiliary Tasks</h2><p>结合有用的辅助损失函数来补充主要目标已被证明可以改善深度神经网络模型的性能。一些辅助损失函数专门设计来提高特殊任务的性能。在一些案例中，辅助函数被用来提升模型的泛化能力。经过适当的辅助任务预训练后，模型可以捕获更长的依赖关系。</p>\n<h2 id=\"Transfer-Learning-with-Pretraining\"><a href=\"#Transfer-Learning-with-Pretraining\" class=\"headerlink\" title=\"Transfer Learning with Pretraining\"></a>Transfer Learning with Pretraining</h2><p>基本过程通常是首先在无监督目标的海量文本数据上预训练功能强大的神经编码器。 第二步是使用更小的域内数据集对特定的下游任务微调此预训练模型。ELMo使用BiLSTM网络来训练双向语言模型来同时预测前一个词和后一个词。OpenAI的GPT使用Transformer网络和BERT进行了两个目标的同时训练：掩蔽语言模型和下一句预测。每个模型均已在GLUE基准上展示了最新的结果。这些利用大规模预训练的模型优于仅使用域内数据的系统。用于学习从输入文本中提取话语级别信息的预训练方法的工作很少。BERT中的下一句话预测损失是朝着这个方向迈出的一步。尽管这些预训练方法擅长于对顺序文本进行建模，但它们并未明确考虑对话的独特话语级功能。因此，我们在研究预训练目标时采取了第一步，以提取对话上下文的更好的话语级表示形式。</p>\n<h1 id=\"Pretraining-Objectives\"><a href=\"#Pretraining-Objectives\" class=\"headerlink\" title=\"Pretraining Objectives\"></a>Pretraining Objectives</h1><p>本文定义了一种强有力的表示形式，它可以捕获整个对话历史中的话语级信息以及构成该历史的话语中的话语级信息，在本文的定义下，当表示允许模型在各种下游任务上表现更好时，表示就足够通用了。</p>\n<ul>\n<li>一个任意T轮对话（对话历史）的表示符号：$c = [u_1,…,u_t]$，$u_i$是一个话语。</li>\n<li>对话回复$R = {r_1,…,r_M}$</li>\n</ul>\n<h2 id=\"Next-Utterance-Retrieval（NUR-检索下一句话）\"><a href=\"#Next-Utterance-Retrieval（NUR-检索下一句话）\" class=\"headerlink\" title=\"Next-Utterance Retrieval（NUR-检索下一句话）\"></a>Next-Utterance Retrieval（NUR-检索下一句话）</h2><p>NUR的目的就是在$k$个候选回复中选择正确的下一句话。对于此任务，本文使用分层编码器来生成对话上下文的表示，方法是首先通过双向长期短期记忆网络（biLSTM）独立运行每个话语，然后使用所得的话语表示来生成整个对话上下文的表示。给定$[u_1，… u_{T-1}]$，NUR的任务是从R中选择正确的下一个话语$u_T$。损失运算公式如下：<br>$$<br>\\hat{u_i}=f_u(u_i), i\\in [1,T-1]$$<br>$$r_{gt} = f_r(u_T)$$<br>$$r_{j} = f_r(r_j),r_j\\sim p_n(r)$$<br>$$a_{gt} = (h_{T-1})^{T}r_{gt}$$<br>$$a_{j} = (h_{T-1})^{T}r_{j}$$<br>总损失如下：<br>$$<br>L = -log_p(u_T|u_1,…u_{T-1})=-log(\\frac{exp(a_{gt})}{exp(a_{gt}+\\sum_{j=1}^{K}exp(a_j)})<br>$$</p>\n<h2 id=\"Next-Utterance-Generation（NUG-生成下一句话）\"><a href=\"#Next-Utterance-Generation（NUG-生成下一句话）\" class=\"headerlink\" title=\"Next-Utterance Generation（NUG-生成下一句话）\"></a>Next-Utterance Generation（NUG-生成下一句话）</h2><p>给定对话历史，根据对话历史生成下一句话。预训练时使用分层Encoder-Decoder结构，在进行下游任务时，仅使用Encoder。对话上下文和下一个话语被编码为式8，最小化损失为式9：<br>$$<br>L = -log_p(u_T|u_1,…u_{T-1})\\<br>=-\\sum_{k}^{N}log_p(w_k|w&lt;k,h_{T-1})<br>$$</p>\n<h2 id=\"Masked-Utterance-Retrieval（MUR-根据mask的对话历史检索下一句话）\"><a href=\"#Masked-Utterance-Retrieval（MUR-根据mask的对话历史检索下一句话）\" class=\"headerlink\" title=\"Masked-Utterance Retrieval（MUR-根据mask的对话历史检索下一句话）\"></a>Masked-Utterance Retrieval（MUR-根据mask的对话历史检索下一句话）</h2><p>与NUR相同，给定对话历史，从$K$个候选回复中选择正确的下一句话，区别</p>\n<ul>\n<li>对话历史中的一句话被随机选择的另一句话替换。</li>\n<li>用替换掉的句子的表示作为对话历史的表示。</li>\n</ul>\n<p>替换的语句索引为$t$，且是在对话部分中随机采样的<br>$$<br>t \\sim Uniform[1,T]<br>$$<br>其中<br>$$<br>\\hat{u_i}=f_u(u_i), i\\in [1,T]$$<br>$$r_{gt} = f_r(u_T)$$<br>$$r_{j} = f_r(r_j),r_j\\sim p_n(r)$$<br>$$a_{gt} = (h_T)^{T}r_{gt}$$<br>$$a_{j} = (h_T)^{T}r_{j}<br>$$</p>\n<p>总损失：<br>$$<br>L = -log_p(u_T|u_1,…,q,…u_{T-1})\\<br>=-log(\\frac{exp(a_{gt})}{exp(a_{gt}+\\sum_{j=1}^{K}exp(a_j)})<br>$$</p>\n<h2 id=\"Inconsistency-Identification（InI-识别不一致语句）\"><a href=\"#Inconsistency-Identification（InI-识别不一致语句）\" class=\"headerlink\" title=\"Inconsistency Identification（InI-识别不一致语句）\"></a>Inconsistency Identification（InI-识别不一致语句）</h2><p>识别一段对话历史中不一致的句子。输入是一段对话历史，其中的一句被随机替换掉，模型需要找到被替换的是哪一句。<br>$$<br>L = -log_p(t|u_1,…,q,…u_T)\\<br>=-log(\\frac{exp(a_t)}{\\sum_{j=1}^{T}exp(a_i)})<br>$$<br>其中<br>$$<br>\\hat{u_i}=f_u(u_i)),i\\in [1,T]\\<br>[h_1…h_T]=f_c(\\hat{u_1},…\\hat{u_T})\\<br>a_i=(h_T)^{T}h_i,i\\in[1,T]<br>$$<br>这个任务的目标是建模单个语句的表示和对话上下文的全局表示的一致性。</p>\n<h1 id=\"Downstream-Tasks\"><a href=\"#Downstream-Tasks\" class=\"headerlink\" title=\"Downstream Tasks\"></a>Downstream Tasks</h1><p>本文选择了以下四个下游任务来测试预训练表示的通用性和有效性。实验数据用的是MultiWoz，其中8422个对话用于训练，1000个用于验证，另外1000个用于测试。</p>\n<ul>\n<li>预测对话状态：这是一个多分类任务，给定对话历史，预测当前的对话状态。对话状态由27种实体的1784个值的one-hot向量拼接而成。这个任务度量了系统维护完整且准确的对话上下文状态表示的能力。由于输出有1784维，这就要求预训练的对话历史表示模型必须有足够强的概括性，才能对对话状态进行准确的预测。</li>\n<li>预测对话行为：与上个任务类似，是一种多分类任务。给定对话历史，预测系统下一步可能采取的动作，输出是一个32维的对话行为向量。</li>\n<li>生成下一句话</li>\n<li>检索下一句话</li>\n</ul>\n<h1 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h1><p>每个模型都训练了15个epoch，选择在验证集上表现最好的模型用于测试。实验中所用参数如下：<br><img src=\"https://img-blog.csdnimg.cn/20200913193940733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为了更直接的评估预训练过程中目标设置不同的差异，这里的预训练和fine-tune都是在同一数据集上进行的。在完整数据集上的表现：<br><img src=\"https://img-blog.csdnimg.cn/20200913194136216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>该实验是为了测试预训练是否对下游任务有用。表中的第一行对每个任务的模型进行随机初始化。如表一所示，预训练表示展示出了它的有效性和通用性。通过非监督的预训练，模型产生的对话表示提升了很多下游任务的性能。通用性体现在这些下游任务都受益于相同的预训练模型。</p>\n<p>在对话行为预测（DAP）和下一句话生成（NUG）任务上，以识别不一致语句（InI）为目标的预训练模型效果最好。这可能是因为在序列生成模型中全局上下文表示和局部话语表示同样重要。</p>\n<p>在对话行为预测（DAP）任务上，以识别不一致语句（InI）和根据mask的对话历史检索下一句话（MUR）的得分都远远高于基线和其他方法，这可能是因为这两种方法都是训练来学习每个话语的表示，而不仅仅是一个整体的上下文表示。</p>\n<p>在检索下一句话（NUR）任务上，以生成下一句话（NUG）为目标进行预训练时效果最好，这可能是因为生成下一个话语必须捕获的信息与检索下一个话语所需的信息类似。</p>\n<ul>\n<li>本文设置了实验观察预训练表示对下游任务在收敛性上的影响。实验证明，预训练过的模型能更快地收敛到更好的效果。</li>\n<li>一个好的预训练模型应该在下游任务中仅有少量数据的微调的情况下，也能达到很好地效果。本文做了实验验证在微调数据仅有(1%, 2%, 5%, 10% and 50%)时，在下游任务上的表现。</li>\n<li>该实验模拟了在下游任务中添加新域时的场景，假设在所有领域都存在大量的无监督的未标记数据，而对于下游任务仅有50个（0.1%）域内的标记数据和1000个（2%）新域的标注数据。在域内数据上做测试，实验证明预训练模型产生了更通用的表示，并促进了域的泛化。</li>\n</ul>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><p>在这篇文章中，提到了4种无监督的预训练目标来学习对话的上下文的表示，并在有限的微调数据和域外数据的条件下，证明了预训练模型对于提高下游任务性能方面的有效性。其中根据mask的对话历史检索下一句话和不一致语句识别是本文提出的两种新的目标。</p>\n<p>在本文中，无监督预训练被证明能够有效地学习对话上下文的表示。也就是说在有大量未标记的对话数据时，可以采取本文中的几种方法进行预训练。尤其是在标注数据量比较少的情况下。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","无监督学习","预训练"]},{"title":"论文阅读笔记：对NMT架构的超参数首次进行大规模消融实验分析","url":"/Paper-Reading/19ca8a5a5db8/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Massive Exploration of Neural Machine Translation Architectures<br>原文链接：<a href=\"https://arxiv.org/pdf/1703.03906.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>在计算机视觉中通常会在大型超参数空间中进行扫描，但对于NMT模型而言，这样的探索成本过高，从而限制了研究人员完善的架构和超参数选择。更改超参数成本很大，在这篇论文中，展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。本文工作探索NMT架构的常见变体，并了解哪些架构选择最重要，同时展示所有实验的BLEU分数，perplexities，模型大小和收敛时间，包括每个实验多次运行中计算出的方差数。论文主要贡献如下：</p>\n<ul>\n<li>展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。例如，深层编码器比解码器更难优化，密度残差连接比常规的残差连接具有更好的性能，LSTM优于GRU，并且调整好的BeamSearch对于获得最新的结果至关重要。</li>\n<li>确定了随机初始化和细微的超参数变化对指标（例如BLEU）的影响程度，有助于研究人员从随机噪声中区分出具有统计学意义的结果。</li>\n<li>发布了基于TensorFlow的<a href=\"https://github.com/google/seq2seq/\">开源软件包</a>，该软件包专为实现可再现的先进sequence-to-sequence模型而设计。</li>\n</ul>\n<p>本篇论文使用的sequence-to-sequence结构如下，对应的编号是论文中章节阐述部分。<br><img src=\"https://img-blog.csdnimg.cn/20201203111623761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>编码器方法 $f_{enc}$ 将 $x=(x_1,…,x_m)$ 的源tokens序列作为输入，并产生状态序列 $h=(h_1,…,h_m)$。在base model中， $f_{enc}$ 是双向RNN，状态 $h_i$ 对应于由后向RNN和前向RNN的状态的concatenation，$h_i = [\\overrightarrow{h_i};\\overleftarrow{h_i}]$。解码器 $f_{dec}$ 是RNN，可根据 $h$ 预测目标序列 $y =(y_1,…,y_k)$ 的概率。根据解码器RNN中的循环状态 $s_i$、前一个单词 $y_{&lt;i}$ 和上下文向量 $c_i$ 预测每个目标token $y_i\\in 1,…V$ 的概率。上下文向量 $c_i$ 也称为attention向量，并计算为源状态的加权平均值。<br>$$c_i=\\sum_{j}a_{ij}h_j$$   $$a_{ij}=\\frac{\\hat{a}<em>{ij}}{\\sum_j\\hat{a}_{ij}}$$   $$\\hat{a}</em>{ij}=att(s_i,h_j)$$<br>这里的 $att(s_i,h_j)$用于计算编码器状态 $h_j$ 和解码器状态 $s_i$ 之间的unnormalized alignment score。在base model中，使用 $att(s_i,h_j)= \\left \\langle  W_hh_j,W_ss_i\\right \\rangle$，其中矩阵 $W$ 用于将源状态和目标状态转换为相同大小的表示形式。解码器输出固定大小为 $V$ 的词汇表上的分布。<br>$$P(y_i|y_1,…,y_{i-1},x)=softmax(W[s_i,c_i]+b)$$<br>通过使用随机梯度下降来最小化目标词的负对数似然性来对整个模型进行end-to-end训练。</p>\n<p>在以下每个实验中，除了要研究的一个超参数外，基线模型的超参数都保持恒定。以此避免各种超参数更改的影响。当然，此过程并未考虑到超参数之间的相互作用，所以当作者认为这种相互作用很可能发生时，进行了额外的实验。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>论文为了简洁起见，在下面的表中仅报告均值BLEU，标准差，括号中的最高BLEU和模型大小。</p>\n<h3 id=\"Embedding维数\"><a href=\"#Embedding维数\" class=\"headerlink\" title=\"Embedding维数\"></a>Embedding维数</h3><p>我们期望更大的嵌入维数得到更好的BLEU得分，或者至少降低perplexities，但是作者发现情况并非总是如此，下表显示了2048维嵌入效果总体上最佳，但是提升的幅度很小。<br><img src=\"https://img-blog.csdnimg.cn/20201203194252399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>表中发现甚至很小的128维嵌入效果也出乎意料地出色，而收敛速度几乎快一倍。<br><strong>结论</strong></p>\n<ul>\n<li>嵌入维数的大小对梯度更新没有显着差异，并且无论大小如何，对嵌入矩阵的梯度更新的范数在整个训练过程中大致保持恒定。</li>\n<li>没有观察到较大嵌入维数的过度拟合，并且在整个实验中训练的log<br>perplexity大致相等，这表明该模型没有有效利用额外的参数，可能需要更好的优化技术。</li>\n</ul>\n<h3 id=\"RNN-Cell-Variant\"><a href=\"#RNN-Cell-Variant\" class=\"headerlink\" title=\"RNN Cell Variant\"></a>RNN Cell Variant</h3><p>我们使用诸如GRU和LSTM之类的门控单元的动机是梯度消失问题，而使用vanilla RNN（序列分析）单元，深度网络无法通过多层和时间步长有效地传播信息和梯度。对于基于注意力的模型，作者认为解码器应该几乎能够完全基于当前输入和注意力上下文做出决策。我们始终将解码器状态初始化为零而不是传递编码器状态，这一事实支持了这一假设，这意味着解码器状态不包含有关编码源的信息。<br><img src=\"https://img-blog.csdnimg.cn/20201203195750132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>实验中，LSTM单元始终优于GRU单元，但由于softmax的原因，没有观察到两者之间在训练速度上存在较大差异。</li>\n<li>Vanilla-Dec无法像门控变体单元一样学习。这表明解码器确实在多个时间步长中通过其自己的状态传递信息，而不是仅依赖于注意力机制和当前输入。</li>\n</ul>\n<h3 id=\"Encoder-和-Decoder-的深度\"><a href=\"#Encoder-和-Decoder-的深度\" class=\"headerlink\" title=\"Encoder 和 Decoder 的深度\"></a>Encoder 和 Decoder 的深度</h3><p>一般而言，我们希望更深层的网络能比更浅层的网络表现更好。但先前的研究结果没有支撑这一结论，所以作者探讨了多达8层编码器和解码器在深度方面的影响。对于更深的网络，作者还尝试了两种残差连接的形式，以刺激梯度传播。作者在连续的层之间插入残差连接，当 $h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})$ 是 $l$ 层RNN在 $t$ 步的输入，则：<br>$$x_t^{(l+1)}=h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})+x_t^{(l)}$$<br>其中，$x_t^{(0)}$ 输入token的词嵌入。除此之外，还尝试另一种残差连接的变体，在此变体中，添加了从每一层到所有其他层的skip connections（跳跃连接，作用是在比较深的网络中，解决在训练的过程中 梯度爆炸 和 梯度消失 问题）<br>$$x_t^{(l+1)}=h_t^{(l)}(x_t^{(l)},h_{t-1}^{(l)})+\\sum_{j=0}^lx_t^{(j)}$$<br>下表显示了在有和没有残差连接的情况下改变编码器和解码器深度的结果：<br><img src=\"https://img-blog.csdnimg.cn/20201203201626714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>没有明显的证据表明编码器深度必须超过两层，但发现具有残余连接的更深的模型在训练过程中发散的可能性更大。</li>\n<li>对于解码器而言，如果没有残差连接，不可能训练8层及以上的网络</li>\n<li>在整个深度解码器实验中，密度残差连接始终优于常规残差连接，并且在步数方面收敛得更快，如下图所示：</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201203202337587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"单向与双向编码器\"><a href=\"#单向与双向编码器\" class=\"headerlink\" title=\"单向与双向编码器\"></a>单向与双向编码器</h3><p>在这组实验中，探索了有无源输入的反向序列的单向编码器，因为这是一种常用技巧，它使编码器可以为较早的单词创建更丰富的表示形式。下表显示了双向编码器通常优于单向编码器，但差距不大：<br><img src=\"https://img-blog.csdnimg.cn/20201203202750532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>具有反向源输入的编码器始终优于其非反向源输入的编码器，但比较浅的双向编码器效果差。</li>\n</ul>\n<h3 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h3><p>两种最常用的注意力机制分别是加法式的Bahdanau注意力和乘法式的Luong注意力，公式分别如下：<br>$$score(h_j,s_i)=\\left \\langle v,tanh(W_1h_j+W_2s_i) \\right \\rangle$$    $$score(h_j,s_i)=\\left \\langle W_1h_j,W_2s_i \\right \\rangle$$<br>我们将 $W_1h_j$ 和 $W_2s_i$ 的维数称为“注意维数”，并通过更改层大小将其从128更改为1024。作者还通过使用最后一个编码器状态初始化解码器状态（None-State），或将最后一个解码器状态与每个解码器输入连接来使用无注意机制（None-Input）进行实验，结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20201203203732691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br><strong>结论</strong></p>\n<ul>\n<li>加法式注意机制略微但始终优于乘法式注意力机制，而注意维数的影响很小。</li>\n<li>“Non-Input”表现很差，因为它们在每个时间步都可以访问编码器信息。</li>\n<li>发现在整个训练过程中，基于注意力的模型对解码器状态表现出明显更大的梯度更新。</li>\n</ul>\n<h3 id=\"Beam-Search策略\"><a href=\"#Beam-Search策略\" class=\"headerlink\" title=\"Beam Search策略\"></a>Beam Search策略</h3><p>下表显示了改变Beam Search大小和增加长度归一化惩罚的影响：<br><img src=\"https://img-blog.csdnimg.cn/20201203204652203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p><strong>结论</strong></p>\n<ul>\n<li>即使有长度损失，很大的Beam Search性能也比较小的Beam Search差，所以选择正确的Beam Search大小对于获得最佳结果至关重要。</li>\n<li>发现非常大的beam大小会产生较差的结果，并且存在最佳beam大小。</li>\n</ul>\n<h3 id=\"系统比较\"><a href=\"#系统比较\" class=\"headerlink\" title=\"系统比较\"></a>系统比较</h3><p>最后，作者将在newstest2013验证集上选择的所有实验中性能最佳的模型（具有512维加法式注意力的base model）与下表中其他模型的历史结果进行了比较。<br><img src=\"https://img-blog.csdnimg.cn/20201203205123391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>下表是综合了所有hyperparameter的最佳方案配置：<br><img src=\"https://img-blog.csdnimg.cn/20201203205301354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>在这篇论文中，展示了以NMT架构超参数为例的首次大规模分析，实验为构建和扩展NMT体系结构带来了新颖的见解和实用建议。总结以上的重要结论如下：</p>\n<ul>\n<li>2048维的嵌入维度效果最佳，但提升幅度很小，即使嵌入维度为128也似乎有足够的能力来捕获大多数必要的语义信息。</li>\n<li>LSTM单元始终优于GRU单元</li>\n<li>具有2至4层的双向编码器效果最佳，对于更深的编码器训练起来更加不稳定，但是如果可以很好地优化它们，则有着很好的潜力。</li>\n<li>深度为4层的解码器略胜于浅层解码器。残差连接对于训练具有8层解码器是必需的，而密集的残差连接则提供了额外的鲁棒性。</li>\n<li>加法式注意力机制产生了总体最佳结果。</li>\n<li>表现良好的Beam Search使用长度惩罚是至关重要的，beam大小为5到10，长度惩罚为1.0似乎效果很好。</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","TensorFlow","NMT","seq2seq"]},{"title":"论文阅读笔记：应用了Transformer的Encoder的DAM检索式多轮对话系统","url":"/Paper-Reading/6ed7dab2ef88/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network<br>原文链接：<a href=\"https://www.aclweb.org/anthology/P18-1103.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>聊天机器人中的一项重要任务是响应选择，其目的是在给定对话上下文的情况下，从一组候选响应中选择最匹配的响应。除了在基于检索的聊天机器人中发挥关键作用外，响应选择模型还可以用于对话生成的自动评估以及基于GAN（生成对抗网络）神经对话生成的判别器。之所以使用更丰富的上下文信息，是因为人为产生的响应在很大程度上取决于语义和场景上的不同粒度（单词，词组，句子等）的先前对话段。下图展示跨上下文和响应的段对之间的语义连接。<br><img src=\"https://img-blog.csdnimg.cn/20201122202203122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如图所示，在上下文和响应中，通常存在两种匹配的段对，它们的粒度不同：</p>\n<ul>\n<li>表面文本的相关性，例如单词“packages”-“package”和短语“debian package manager”-“debian package manager”的词法重叠。</li>\n<li>这些片段在语义/功能上有着彼此相关的潜在依赖关系。例如，响应中的“it”一词在上下文中指“dpkg”，以及响应中的短语“its just reassurance”，它潜在地指向“what packages are installed on my system”。</li>\n</ul>\n<p>先前的研究表明，在上下文和响应中以不同的粒度捕获那些匹配的片段对是多回合响应选择的关键，如Multi-view和SMN。然而这些对话系统只考虑了表面的文本关联性（surface text relevance），且多采用RNN结构，这会极大的增加模型的推理代价，因此本文提出了基于注意力机制的结构。本文使用完全基于注意力的依赖项信息来研究将响应与多回合上下文匹配的过程，受到Transformer的影响，本文通过两种方式扩展注意力机制来获取表示和匹配信息：</p>\n<ul>\n<li><strong>self-attention</strong>：只使用堆叠的自注意力构造不同粒度的文本段表示，这样我们可以捕获其词级内的依存关系。self-attention其实就是Transformer的Encoder层（没有用Multi-Head），其中 $Q$ 、 $K$ 、$V$ 都是一样的，要么都是response、要么都是utterance。从word embedding开始堆叠self-attention层，每一层self-attention层抽取一种粒度的表示，不同层就抽取了不一样的表示，从而获得句子多粒度的表示。</li>\n<li><strong>cross-attention</strong>：尝试整个上下文和响应的注意力提取真正匹配的句段对。结构上还是Transformer的Encoder层，只不过输入不一样了。其中 $Q$ 是response（utterance），而 $K$ 、 $V$ 是utterance（response）。这样做的话，其实就像用response去表示utterance，用utterance去表示response，从而为response和utterance做匹配提供更多的信息。</li>\n</ul>\n<p>本文在一个统一的神经网络中介绍了这两种注意力，网络命名为Deep Attention Matching Network（DAM），用于多回合响应选择。在实践中，DAM将上下文或响应中的语句的每个词作为抽象语义段的中心含义，并通过堆叠式的自注意力丰富其表示，从而逐渐围绕中心词生成越来越复杂的段表示 。考虑到文本相关性和依存性信息，上下文和响应中的每个语句都基于不同粒度的句段对进行匹配。这样，DAM通常会捕获上下文之间的匹配信息以及从单词级到句子级的响应，然后使用卷积和最大池化操作提取重要的匹配特征，最后通过单层感知器将其融合为一个匹配分数。更重要的是，由于大多数注意力计算可以完全并行化，因此DAM有望在实践中方便部署。</p>\n<h1 id=\"模型细节\"><a href=\"#模型细节\" class=\"headerlink\" title=\"模型细节\"></a>模型细节</h1><h2 id=\"问题符号化\"><a href=\"#问题符号化\" class=\"headerlink\" title=\"问题符号化\"></a>问题符号化</h2><p>给定一个对话数据集 $D={(c,r,y)z}_{Z=1}^N$，其中 $c={u_0,…u_{n-1}}$ 表示一个对话上下文，其中 ${u_i}_{i=0}^{n-1}$ 作为语句和 $r$ 作为候选响应。$y\\in{0,1}$ 是一个二进制标签，指示 $r$ 是否是对 $c$ 的合适响应。我们的目标是学习与 $D$ 匹配的模型$g(c,r)$，该模型可以测量上下文 $c$ 和候选响应 $r$ 之间的相关性。</p>\n<h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h2><p>下图展示了DAM的模型结构，该模型通过representation-matching-aggregation框架来将响应与多回合上下文进行匹配。对于上下文中的每个语句 $u_i=[w_{u_i,k}]<em>{k=0}^{n</em>{u_i}-1}$ 和它的候选响应 $r=[w_{r,t}]<em>{t=0}^{n_r-1}$（其中，$n_{u_i}$ 和 $n_r$ 代表单词数），DAM首先查找共享的单词嵌入表，并将 $u_i$ 和 $r$ 表示为单词嵌入序列，即 $U_i^0=[e</em>{u_i,0}^0,…,e_{u_i,n_{u_i}-1}^0]$ 和 $R^0=[e_{r,0}^0,…,e_{r,n_r-1}^0]$，其中 $e\\in\\mathbb{R}^d$ 表示 $d$ 维词嵌入。<br><img src=\"https://img-blog.csdnimg.cn/20201122220222537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>接下来，通过表示模块为 $u_i$ 和 $r$ 在不同的粒度上构造语义表示。实现方式是通过 $L$ 个相同的自注意层堆叠，每个第 $l$ 个自注意层都将第 $l-1$ 层的输出作为其输入，从而将输入语义向量合成为更复杂的表示形式。通过这种方式，逐渐构建了 $u_i$ 和 $r$ 的多粒度表示，分别表示为 $[U_i^l]<em>{l=0}^L$ 和 $[R^l]</em>{l=0}^L$。</p>\n<p>给定 $[U_i^0,…,U_i^L]$ 和 $[R^0,…,R^L]$，语句 $u_i$ 和响应 $r$ 随后以segment-segment相似矩阵的方式相互匹配。实际上，对于每个粒度 $l\\in[0…L]$，构造两种匹配矩阵，即self-attention匹配 $M_{self}^{u_i,r,l}$ 和cross-attention匹配 $M_{corss}^{u_i,r,l}$，这样便分别使用文字信息和依存关系信息测量话语和回应之间的相关性。这些匹配分数最终被合并为 $3D$ 的匹配 $Q^1$。$Q$ 的每个维度表示<strong>each utterance in context, each word in utterance and each word in response</strong>。然后，通过使用最大池操作进行卷积，提取跨多回合上下文的段对之间的重要匹配信息和候选响应，然后通过单层感知器将其进一步融合为一个匹配分数，代表候选响应与整体上下文之间的匹配程度。</p>\n<p>注意力，我们使用一个共享组件，即“注意力模块”来实现表示中的self-attention和匹配中的cross-attention。在以下各节中，我们将详细讨论注意力模块的实现以及如何使用它来实现self-attention和cross-attention</p>\n<h2 id=\"注意力模块-Attentive-Module\"><a href=\"#注意力模块-Attentive-Module\" class=\"headerlink\" title=\"注意力模块-Attentive Module\"></a>注意力模块-Attentive Module</h2><p>下图显示了Attentive模块的结构，与Transformer中使用的模块相似。<br><img src=\"https://img-blog.csdnimg.cn/20201122221757924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>注意力模块具有三个输入语句：$Q$，$K$，$V$，即$Q=[e_{i=0}^{n_Q-1}]$，$K=[e_i]<em>{i=0}^{n_K-1}$，$V=[e_i]</em>{i=0}^{n_V-1}$，其中，其中 $n_Q$，$n_K$ 和 $n_V$ 表示每个句子中的单词数，而 $e_i$ 表示维数嵌入，$n_K$ 等于 $n_V$。注意力模块首先通过Scaled Dot-Product Attention将查询语句中的每个单词与关键语句中的单词关联，然后将结果应用于值语句，定义为：<br>$$Att(Q,K)=[softmax(\\frac{Q[i]\\cdot K^T}{\\sqrt{d}})]<em>{i=0}^{n_Q-1}$$  $$V_{att}=Att(Q,K)\\cdot V\\in \\mathbb{R}^{n_Q\\times d}$$<br>其中 $Q[i]$是查询语句 $Q$ 中的第 $i$ 个嵌入，$V_{att}$ 的每一行（表示为 $V</em>{att}[i]$）存储值语句中可能与查询语句中的第 $i$ 个单词相关的单词的语义信息。对于每个 $i$，将 $V_{att}[i]$ 和 $Q[i]$ 加在一起，将它们组合成一个包含其联合含义的新表示形式。然后应用层归一化操作，可防止梯度消失或爆炸。接着将具有RELU的前馈网络FFN应用于标准化结果，以便进一步处理融合嵌入，定义为：<br>$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$<br>其中，$x$ 是与查询语句 $Q$ 形状相同的 $2D$ 张量，$W_1$，$b_1$，$W_2$，$b_2$ 是学习参数。$FFN(x)$ 的结果是一个具有与 $x$ 相同形状的 $2D$ 张量，然后将 $FFN(x)$ 残留连接添加到 $x$，最后将融合结果标准化为最终输出。将整个注意力模块表示为：<br>$$AttentiveModule(Q,K,V)$$</p>\n<p>如上所述，注意力模块可以捕获查询语句和关键语句之间的依存关系，并进一步使用依存关系信息将查询语句和值语句中的元素合成为组成表示形式。我们利用Attentive Module的此属性来构造多粒度语义表示以及与依赖项信息的匹配。</p>\n<h2 id=\"整理表示\"><a href=\"#整理表示\" class=\"headerlink\" title=\"整理表示\"></a>整理表示</h2><p>给定 $U_i^0$ 或 $R^0$ （语句 $u_i$ 或响应 $r$ 的单词级嵌入表示），DAM将 $U_i^0$ 或 $R^0$ 用作输入，并分层堆叠Attentive模块以构造 $u_i$ 和 $r$ 的多粒度表示，即表示为：<br>$$U_i^{l+1}=AttentiveModule(U_i^l,U_i^l,U_i^l)$$  $$R^{l+1}=AttentiveModule(R^l,R^l,R^l)$$<br>其中 $l$ 的范围是 $0$ 到 $L − 1$，表示不同的粒度级别。通过这种方式，每个话语或响应中的单词会反复发挥作用，以合成越来越多的整体表示形式，我们将这些多粒度表示形式表示为 $U_i^0,…,U_i^L$ 和 $R^0,…,R^L$ 之后。</p>\n<h2 id=\"Utterance-Response-匹配\"><a href=\"#Utterance-Response-匹配\" class=\"headerlink\" title=\"Utterance-Response 匹配\"></a>Utterance-Response 匹配</h2><p>给定 $[U_i^l]<em>{l=0}^L$ 和 $[R^l]</em>{l=0}^L$ ，在每个粒度级别 $l$ 上构造了两种segment-segment匹配矩阵，即self-attention匹配 $M_{self}^{u_i,r,l}$ 和cross-attention匹配 $M_{corss}^{u_i,r,l}$。其中， $M_{self}^{u_i,r,l}$ 被定义为：<br>$$M_{self}^{u_i,r,l}={U_i^l[k]^T\\cdot R^l[t]}<em>{n</em>{u_i}\\times n_r}$$<br>其中，矩阵中的每个元素都是 $U_i^l[k]$ 和 $R^l[t]$ 的点积，第 $k$ 个嵌入在 $U_i^l$ 中，第 $t$ 个嵌入在 $R^l$ 中，反映了 $u_i$中 第 $k$ 个分段和 $r$ 中第 $t$ 个分段之间，在 第$l$ 层粒度的相关性。 cross-attention匹配矩阵基于cross-attention，其定义为：<br>$$\\tilde{U}<em>i^l=AttentiveModule(U_i^l,R^l,R^l)$$  $$\\tilde{R}^l=AttentiveModule(R^l,U_i^l,U_i^l)$$  $$M_{corss}^{u_i,r,l}={\\tilde{U}_i^l[k]^T\\cdot \\tilde{R}^l[t]}</em>{n_{u_i}\\times n_r}$$<br>其中，我们使用注意力模块将 $U_i^l$ 和 $R^l$ 相互交叉，分别为它们构造两个新的表示形式，分别写为 $\\tilde{U}_i^l$ 和 $\\tilde{R}^l$。 $\\tilde{U}_i^l$ 和 $\\tilde{R}^l$ 都隐式捕获了跨语句和响应的语义结构信息。这样，那些相互依存的片段对在表示上彼此接近，并且那些潜在相互依存的对之间的点积可以增加，从而提供了依赖于感知的匹配信息。</p>\n<h2 id=\"Aggregation\"><a href=\"#Aggregation\" class=\"headerlink\" title=\"Aggregation\"></a>Aggregation</h2><p>在得到 $M_{self}$ 和 $M_{cross}$ 后，就需要将它们聚合起来，做法是将所有的 $2D$ 匹配矩阵聚合成一个大的 $3D$ 匹配图像 $Q$，具体的聚合方法就是将 $M_{self}$ 和 $M_{cross}$ 所有的矩阵排列起来，所以就增加了一个维度，新的维度（可以称之为深度）大小是 $2(L+1)$ ，具体公式如下：<br>$$Q={Q_{i,k,t}}<em>{n\\times n</em>{u_i}\\times n_r}$$<br>其中 $Q_{i,k,t}$ 可以表示为：<br>$$Q_{i,k,t}=[M_{self}^{u_i,r,l}[k,t]]<em>{l=0}^L\\oplus [M</em>{corss}^{u_i,r,l}[k,t]]_{l=0}^L$$</p>\n<p>聚合成 $3D$ 匹配图像后，采用了两次的 $3D$ 卷积和最大池化去提取特征，在实际试验中，第一次 $3D$ 卷积的输入通道数为 $2(L+1)$ ，输出通道数为 $32$，卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。第二次 $3D$ 卷积的输入通道数为 $32$，输出通道数为 $16$，卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。通过卷积和池化提取到特征后（用 $f_{match}(c,r)$ 表示提取后的特征），后面接一层线性层将维度转化成 $1$，用来表示匹配的分数，具体公式如下：<br>$$g(c,r)=\\sigma(W_3f_{match}(c,r)+b_3)$$</p>\n<p>其中 $W_3$ 和 $b_3$ 是学习参数，$\\sigma$ 是sigmoid函数，如果 $r$ 是 $c$ 的合适候选相应，则给出概率。DAM的损失函数为负对数似然，定义为：<br>$$p(y|c,r)=g(c,r)y+(1-g(c,r))(1-y)$$  $$L(\\cdot)=-\\sum_{(c,r,y)\\in D}log(p(y|c,r))$$</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>实验要求每个比较模型从给定对话上下文 $c$ 的 $n$ 个可用候选中选择 $k$ 个最匹配的响应，然后我们将在 $k$ 个选定的真实响应中的正确回复的召回率作为主要评估指标来计算，记为 $R_n@k=\\frac{\\sum_{i=1}^ky_i}{\\sum_{i=1}^ny_i}$，其中 $y_i$ 是每个候选项的二进制标签。除此之外，我们还使用MAP（Mean Average Precision），MRR (Mean Reciprocal Rank)，Precision-at-one P@1。</p>\n<h2 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h2><p>使用的词汇表和词嵌入大小和SMN模型一致。上下文中语句最大设置为 $9$，每个语句最多 $50$ 个单词，并使用word2vec进行词嵌入。我们使用零填充来处理可变大小的输入，并且将FFN中的参数设置为200，与词嵌入大小相同。 我们测试了$1-7$层自注意力层，其中 $5$ 层自注意力层在验证集上获得了最佳分数。试验中两次的 $3D$ 卷积和最大池化，第一次 $3D$ 卷积的卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。第二次 $3D$ 卷积的卷积核的大小是 $[3,3,3]$，步幅为 $[1,1,1]$，最大池化层的核大小是 $[3,3,3]$，步幅为 $[3,3,3]$。我们使用adam优化器，学习率初始化为 $1e-3$，并在训练过程中逐渐降低，并且批大小为 $256$。</p>\n<p>下表显示DAM的评估结果以及所有比较模型：<br><img src=\"https://img-blog.csdnimg.cn/20201122233518572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>论文还使用Ubuntu语料通过数量分析和可视化分析DAM中的self-attention和cross-attention的工作方式。下图的左侧部分显示了在具有不同语句数量的上下文中，Ubuntu 语料上 $R_{10} @1$ 的变化，右侧提供了在具有不同平均语句长度和self-attention数量的性能比较：<br><img src=\"https://img-blog.csdnimg.cn/20201122233939735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图给出了第 $0$，第 $2$ 和第 $4$ self-attention匹配矩阵，第 $4$ cross-attention匹配矩阵的可视化结果，以及第4层中的self-attention和cross-attention的分布。<br><img src=\"https://img-blog.csdnimg.cn/20201122234258665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>模型存在的不足：</p>\n<ul>\n<li>模糊候选响应：候选响应虽然基本上适合于会话上下文，但是还有一些不适当的细节。</li>\n<li>逻辑错误：在给定的对话上下文，由于逻辑不匹配，候选响应不合适。</li>\n</ul>\n<p>作者认为在训练过程中生成对抗性示例而非随机抽样可能是解决模糊候选和逻辑错误的一个好主意，并且捕获隐藏在对话文本后的逻辑信息也值得在将来进行研究 。</p>\n","categories":["Paper-Reading"],"tags":["Transformer","对话系统","Paper","多轮对话","检索式","DAM"]},{"title":"论文阅读笔记：快速的评估选择适合下游任务的预训练模型","url":"/Paper-Reading/526fc2a02a0d/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：LogME: Practical Assessment of Pre-trained Models for Transfer Learning<br>原文链接：<a href=\"https://arxiv.org/pdf/2102.11005.pdf\">Link</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>在NLP领域，预训练模型（准确的说应该是预训练语言模型）似乎已经成为各大任务必备的模块了，经常有看到文章称后BERT时代或后XXX时代，分析对比了许多主流模型的优缺点，这些相对而言有些停留在理论层面，可是有时候对于手上正在解决的任务，要用到预训练语言模型时，面对烟火缭乱的语言模型，需要如何挑选合适的模型应用到任务上来。</p>\n<p>一个非常直接的方法就是把每一个候选模型针对任务都做一遍微调，因为微调涉及到模型训练，时间至少几个小时起步。有些预训练模型的微调还需要进行超参数搜索，想要决定一个预训练模型的迁移效果就需要将近50个小时！对于没有足够算力的我，苦苦寻觅一个能够高效的选择适合的预训练语言模型的方法，不过资料不好找呀，偶然间我才发现了这篇论文，里面提到的LogME方法值得一试。下图是该方法适配的任务：<br><img src=\"https://img-blog.csdnimg.cn/20210321224833785.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>多提一下，我这里说的是预训练语言模型，即在适用于NLP领域内的模型选择打分，而对于适用于CV的一些打分方案，像LEEP、NCE、H scores感兴趣的小伙伴可以找论文看看。</p>\n<p>本文在LogME方法的相关描述上，组织基于论文作者所在学院的官方公众号上的一篇文章，可<a href=\"https://mp.weixin.qq.com/s/9lJEcwkXAN4jaENNghjpyw\">直戳原文阅读</a>。<strong>原Paper中开源的代码使用Pytorch进行GPU加速，我在本文的最后附上我改成TensorFlow2的代码，方便直接应用在TensorFlow2的相关模型上。</strong></p>\n<h1 id=\"前情提要\"><a href=\"#前情提要\" class=\"headerlink\" title=\"前情提要\"></a>前情提要</h1><p>将上面提到的问题，描述成图模型，就是论文中所画出如下的这样：<br><img src=\"https://img-blog.csdnimg.cn/20210321223707890.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在这个任务中，我们假设有 $M$ 个预训练模型组成的集合 ${\\phi_m}^M_{m=1}$ 和 含有 $n$ 个标签的数据集 ${(x_i,y_i)}^n_{i=1}$，正常情况下，我们是通过微调使用各种评判指标作为衡量模型 $\\phi$ 的表现 $T_m$，而现在我们想要通过一种方法得到 $S_m$，其中 ${S_m}^M_{m=1}$ 能够与 ${T_m}^M_{m=1}$ 有着很好的相关性。</p>\n<p>简单来说就是预训练模型选择问题，就是针对用户给定的数据集，从预训练模型库中选择一个最适合的预训练模型用于迁移学习，核心就是要对每一个预训练模型进行迁移性评估(Transferability Assessment)，为每个模型打分，然后选择出打分最高的预训练模型。</p>\n<h1 id=\"LogME方法\"><a href=\"#LogME方法\" class=\"headerlink\" title=\"LogME方法\"></a>LogME方法</h1><p>LogME的优越性能来自于以下三个方面：</p>\n<h3 id=\"无须梯度计算\"><a href=\"#无须梯度计算\" class=\"headerlink\" title=\"无须梯度计算\"></a>无须梯度计算</h3><p>为了加速预训练模型选择，我们仅将预训练模型 $\\phi$ 视作特征提取器，避免更新预训练模型 $\\phi$ 。这样，只需要将预训练模型在给定数据集上前向传播一遍，就可以得到特征 ${f_i=\\phi(x_i)}^n_{i=1}$ 和标注 ${y_i}^n_{n=1}$。于是，这个问题就转化成了如何衡量特征和标注之间的关系，也就是说，这些特征能够多大程度上用于预测这些标注。</p>\n<p>为此，我们采用一般性的统计方法，用概率密度 $p(y|F)$ 来衡量特征与标注的关系。考虑到微调一般就是在预训练模型的特征提取层之上再加一个线性层，所以我们用一个线性层来建模特征与标注的关系。</p>\n<p>说到这里，很多人会想到，一种直观的方法是通过Logistic Regression或者Linear Regression得到最优权重 $w^*$，然后使用似然函数 $p(y|F,w^*)$ 作为打分标准。但是但是这相当于训练一个模型来建模问题，这样容易导致过拟合问题，而且这些方法也有很多超参数需要选择，这使得它们的时间开销很大且效果不好。</p>\n<h3 id=\"无须超参数调优\"><a href=\"#无须超参数调优\" class=\"headerlink\" title=\"无须超参数调优\"></a>无须超参数调优</h3><p>为了避免超参数进行调优，论文中的方法选用的是统计学中的证据(evidence，也叫marginalized likelihood，即<a href=\"https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%BC%98%E4%BC%BC%E7%84%B6\">边缘似然</a>)来衡量特征与标注的关系。它不使用某个特定的 $w^*$ 的值，而是使用 $w$ 的分布 $p(w)$ 来得到边缘化似然的值 $p(y|F)=\\int p(w)p(y|F,w)dw$。它相当于取遍了所有可能的 $w$ 值，能够更加准确地反映特征与标注的关系，不会有过拟合的问题。其中，$p(w)$ 与 $p(y|F,w)$ 分别由超参数 $\\alpha$ 和 $\\beta$ 决定，但是它们不需要 grid search，可以通过最大化evidence来直接求解。于是，我们就得到了对数最大证据(Log Maximum Evidence, 缩写LogME)标准来作为预训练模型选择的依据，如下图：<br><img src=\"https://img-blog.csdnimg.cn/2021032123393016.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>数学推导不在这里赘述了，感兴趣的小伙伴戳原文阅读，该方法的具体细节在下图中给出了，注意，虽然LogME计算过程中将预训练模型视作特征提取器，但是LogME可以用于衡量被用于迁移学习(微调)的性能：<br><img src=\"https://img-blog.csdnimg.cn/20210321234241726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"算法实现优化\"><a href=\"#算法实现优化\" class=\"headerlink\" title=\"算法实现优化\"></a>算法实现优化</h3><p>值得一提的是，LogME算法涉及到很多矩阵分解、求逆、相乘操作，因此一不小心就容易使得算法的复杂度很高(例如上图第9行，粗糙的实现方式)。我们在深入研究该算法后发现，很多矩阵运算的开销可以通过巧妙的计算优化手段大大降低，因此将计算流程优化为上图第10行，整体的计算复杂度降低了一个阶，从四次方降低为三次方(见下表)，使得该算法在数秒内就能处理常见情况：<br><img src=\"https://img-blog.csdnimg.cn/20210321234436218.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>在实验部分，我们用合成数据、真实数据等多种方式方式，测试了LogME在17个数据集、14个预训练模型上的效果，LogME在这么多数据集、预训练模型上都表现得很好，展现了它优异的性能。</p>\n<p>首先让我们看看，LogME给出的打分标准与人的主观感觉是否一致。我们为分类问题和回归问题分别设计了一个toy实验，使用生成数据来测量LogME的值。从下图中可以看出，不管是分类任务还是回归任务，当特征质量越来越差时，LogME的值也越来越低，说明LogME可以很好地衡量特征与标注的关系，从而作为预训练模型选择的标准：<br><img src=\"https://img-blog.csdnimg.cn/20210321235112104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>接下来，用LogME来进行预训练模型选择。使用若干个常用预训练模型，通过耗时的微调过程得到它们的迁移性指标，然后衡量LogME与迁移性指标的相关性。相关性指标为加权肯达尔系数 $\\tau_w$，它的取值范围是 $[-1,1]$。相关系数为 $\\tau_w$ 意味着如果LogME认为预训练模型 $\\phi_1$ 比 $\\phi_2$ 好，那么 $\\phi_1$ 确实比 $\\phi_2$ 好的概率是 $\\frac{\\tau_w+1}{2}$。也就是说，$\\tau_w$ 越大越好。</p>\n<p>将10个常用预训练模型迁移到9个常见分类数据集中，发现LogME与微调准确率有很高的相关性(见下图)，显著优于之前的LEEP和NCE方法。在这几个数据集中，LogME的相关系数 $\\tau_w$  至少有0.5，大部分情况下有0.7或者0.8，也就意味着使用LogME进行预训练模型选择的准确率高达85%或者90%：<br><img src=\"https://img-blog.csdnimg.cn/20210321235634651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在回归任务的实验中，如下图可以看到LogME与MSE有明显的负相关性，而MSE是越低越好，LogME是越大越好，结果符合预期：<br><img src=\"https://img-blog.csdnimg.cn/2021032123580732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图可以看到，在五个任务上，LogME完美地预测了四个预训练模型的表现的相对大小，在另外两个任务上的表现也不错。<br><img src=\"https://img-blog.csdnimg.cn/20210322000146472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>LogME方法不仅效果好，更难得的是它所需要的时间非常短，可以快速评价预训练模型。如果将直接微调的时间作为基准，LogME只需要0.31‰的时间(注意不是百分号，是千分号)，也就是说加速了3000倍！而之前的方法如LEEP和NCE，虽然耗时更少，但是效果很差，适用范围也很有限，完全不如LogME方法：<br><img src=\"https://img-blog.csdnimg.cn/20210322000302184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"TensorFlow2代码\"><a href=\"#TensorFlow2代码\" class=\"headerlink\" title=\"TensorFlow2代码\"></a>TensorFlow2代码</h1><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\">from numba import njit</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">@njit</span><br><span class=\"line\">def each_evidence(y_, f, fh, v, s, vh, N, D):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    compute the maximum evidence for each class</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    alpha &#x3D; 1.0</span><br><span class=\"line\">    beta &#x3D; 1.0</span><br><span class=\"line\">    lam &#x3D; alpha &#x2F; beta</span><br><span class=\"line\">    tmp &#x3D; (vh @ (f @ y_))</span><br><span class=\"line\">    for _ in range(11):</span><br><span class=\"line\">        gamma &#x3D; (s &#x2F; (s + lam)).sum()</span><br><span class=\"line\">        m &#x3D; v @ (tmp * beta &#x2F; (alpha + beta * s))</span><br><span class=\"line\">        alpha_de &#x3D; (m * m).sum()</span><br><span class=\"line\">        alpha &#x3D; gamma &#x2F; alpha_de</span><br><span class=\"line\">        beta_de &#x3D; ((y_ - fh @ m) ** 2).sum()</span><br><span class=\"line\">        beta &#x3D; (N - gamma) &#x2F; beta_de</span><br><span class=\"line\">        new_lam &#x3D; alpha &#x2F; beta</span><br><span class=\"line\">        if np.abs(new_lam - lam) &#x2F; lam &lt; 0.01:</span><br><span class=\"line\">            break</span><br><span class=\"line\">        lam &#x3D; new_lam</span><br><span class=\"line\">    evidence &#x3D; D &#x2F; 2.0 * np.log(alpha) \\</span><br><span class=\"line\">                + N &#x2F; 2.0 * np.log(beta) \\</span><br><span class=\"line\">                - 0.5 * np.sum(np.log(alpha + beta * s)) \\</span><br><span class=\"line\">                - beta &#x2F; 2.0 * beta_de \\</span><br><span class=\"line\">                - alpha &#x2F; 2.0 * alpha_de \\</span><br><span class=\"line\">                - N &#x2F; 2.0 * np.log(2 * np.pi)</span><br><span class=\"line\">    return evidence &#x2F; N</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># D &#x3D; 20, N &#x3D; 50</span><br><span class=\"line\">f_tmp &#x3D; np.random.randn(20, 50).astype(np.float64)</span><br><span class=\"line\">each_evidence(np.random.randint(0, 2, 50).astype(np.float64), f_tmp, f_tmp.transpose(),</span><br><span class=\"line\">                np.eye(20, dtype&#x3D;np.float64), np.ones(20, dtype&#x3D;np.float64), np.eye(20, dtype&#x3D;np.float64), 50,</span><br><span class=\"line\">                20)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def LogME(f: tf.Tensor, y: tf.Tensor, regression&#x3D;False):</span><br><span class=\"line\">    f &#x3D; f.numpy().astype(np.float64)</span><br><span class=\"line\">    y &#x3D; y.numpy()</span><br><span class=\"line\">    if regression:</span><br><span class=\"line\">        y &#x3D; y.numpy().astype(np.float64)</span><br><span class=\"line\"></span><br><span class=\"line\">    fh &#x3D; f</span><br><span class=\"line\">    f &#x3D; f.transpose()</span><br><span class=\"line\">    D, N &#x3D; f.shape</span><br><span class=\"line\">    v, s, vh &#x3D; np.linalg.svd(f @ fh, full_matrices&#x3D;True)</span><br><span class=\"line\"></span><br><span class=\"line\">    evidences &#x3D; []</span><br><span class=\"line\">    if regression:</span><br><span class=\"line\">        K &#x3D; y.shape[1]</span><br><span class=\"line\">        for i in range(K):</span><br><span class=\"line\">            y_ &#x3D; y[:, i]</span><br><span class=\"line\">            evidence &#x3D; each_evidence(y_, f, fh, v, s, vh, N, D)</span><br><span class=\"line\">            evidences.append(evidence)</span><br><span class=\"line\">    else:</span><br><span class=\"line\">        K &#x3D; int(y.max() + 1)</span><br><span class=\"line\">        for i in range(K):</span><br><span class=\"line\">            y_ &#x3D; (y &#x3D;&#x3D; i).astype(np.float64)</span><br><span class=\"line\">            evidence &#x3D; each_evidence(y_, f, fh, v, s, vh, N, D)</span><br><span class=\"line\">            evidences.append(evidence)</span><br><span class=\"line\">    return np.mean(evidences)</span><br></pre></td></tr></table></figure>","categories":["Paper-Reading"],"tags":["预训练模型","评估方法","微调"]},{"title":"论文阅读笔记：语音应用中Transformer与RNN的比较研究","url":"/Paper-Reading/8e77a991aae6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：A Comparative Study on Transformer vs RNN in Speech Applications<br>原文链接：<a href=\"https://arxiv.org/pdf/1909.06317.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>序列到序列模型已广泛用于端到端语音处理中，例如自动语音识别（ASR），语音翻译（ST）和文本到语音（TTS）。本文着重介绍把Transformer应用在语音领域上并与RNN进行对比。与传统的基于RNN的模型相比，将Transformer应用于语音的主要困难之一是，它需要更复杂的配置（例如优化器，网络结构，数据增强）。在语音应用实验中，论文研究了基于Transformer和RNN的系统的几个方面，例如，根据所有标注数据、训练曲线和多个GPU的可伸缩性来计算单词/字符/回归错误。本文的几个主要贡献：</p>\n<ul>\n<li>将Transformer和RNN进行了大规模的比较研究，尤其是在ASR相关任务方面，它们具有显着的性能提升。</li>\n<li>提供了针对语音应用的Transformer的训练技巧：包括ASR，TTS和ST</li>\n<li>在开放源代码工具包<a href=\"https://github.com/espnet/espnet\">ESPnet</a>中提供了可复制的端到端配置和模型，这些配置和模型已在大量可公开获得的数据集中进行了预训练。</li>\n</ul>\n<h1 id=\"端到端RNN\"><a href=\"#端到端RNN\" class=\"headerlink\" title=\"端到端RNN\"></a>端到端RNN</h1><p>如下图中，说明了实验用于ASR，TTS和ST任务的通用S2S结构。<br><img src=\"https://img-blog.csdnimg.cn/20201123224519278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br> S2S包含两个神经网络：其中编码器如下：<br> $$(1):X_0=EncPre(X)$$   $$(2):X_e=EncBody(X_0)$$<br> 解码器如下：<br> $$(3):Y_0[1:t-1]=DecPre(Y[1:t-1])$$   $$(4):Y_d[t]=DecBody(X_e,Y_0[1:t-1])$$    $$(5):Y_{post}[1:t]=DecPost(Y_d[1:t])$$</p>\n<p>其中 $X$ 是源序列，例如，语音特征序列（对于ASR和ST）或字符序列（对于TTS），$e$ 是EncBody层数，$d$ 是DecBody中的层数，$t$ 是目标帧索引，以上等式中的所有方法均由神经网络实现。对于解码器输入 $Y [1：t − 1]$，我们在训练阶段使用一个真实标注的前缀，而在解码阶段使用一个生成的前缀。在训练过程中，S2S模型学习是将在生成的序列 $Y_{post}$ 和目标序列 $Y$ 之间标量损失值最小化：<br>$$(6):L=Loss(Y_{post},Y)$$<br>本节的其余部分描述了基于RNN的通用模块：“EncBody”和“DecBody”。而将“EncPre”，“DecPre”，“DecPost”和“Loss”视为特定于任务的模块，我们将在后面的部分中介绍。</p>\n<p>等式(2)中的EncBody将源序列 $X_0$ 转换为中间序列 $X_e$，现有的基于RNN的EncBody实现通常采用双向长短记忆（BLSTM）。对于ASR，编码序列 $X_e$ 还可以在进行联合训练和解码中，用基于神经网络的时序类分类（CTC）进行逐帧预测。</p>\n<p>等式(4)中的DecBody()将生成具有编码序列 $X_e$ 和目标前缀 $Y_0 [1：t − 1]$ 的前缀的下一个目标帧。对于序列生成，解码器通常是单向的。 例如，具有注意力机制的单向LSTM通常用于基于RNN的DecBody()实现中。该注意力机制计算逐帧权重，以将编码后的帧 $X_e$ 求和，并作为要以前缀 $Y0 [0：t-1]$ 进行转换的逐帧目标向量，我们称这种注意为“encoder-decoder attention”</p>\n<h1 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h1><p>Transformer包含多个dot-attention层：<br>$$(7):att(X^q,X^k,X^v)=softmax(\\frac{X^qX^{kT}}{\\sqrt{d^{att}}})X^v$$<br>其中 $X^k,X^v\\in \\mathbb{R}^{n^k\\times d^{att}}$ 和 $X^q\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是attention层的输入，$d^{att}$是特征维数，$n^q$是 $X^q$ 的长度，$n^k$ 是 $X^k$ 和 $X^v$ 的长度。我们将 $X^qX^{kT}$ 称为“attention matrix”。多头注意力机制如下：<br>$$(8):MHA(Q, K, V )=[H_1,H_2,…,H_{d^{head}}]W^{head}$$  $$(9):H_h=att(QW_h^q,KW_h^k,VW_h^v)$$<br>其中， $K,V \\in \\mathbb{R}^{n^k\\times d^{att}}$ 和 $Q\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是 $MHA$ 层的输入，$H_h\\in \\mathbb{R}^{n^q\\times d^{att}}$ 是第 $h$ 个attention层的输出$(h =1,…,dhead )$。$W_h^q,W_h^k,W_h^v \\in \\mathbb{R}^{d^{att}\\times d^{att}}$和 $W^{head} \\in \\mathbb{R}^{d^{att}d^{head}\\times d^{att}}$ 是可学习的权重矩阵，而 $d^{head}$ 是该层中的注意力头数。</p>\n<p>我们定义用于等式(2)的基于Transformer的EncBody()，如下所示：<br>$$(10):X_i^{‘}=X_i+MHA_i(X_i,X_i,X_i)$$   $$X_{i+1}=X_i^{‘}+FF_i(X_i^{‘})$$</p>\n<p>其中 $i = 0,…,e − 1$ 是编码器层的索引，而 $FF_i$ 是第 $i$ 个两层前馈网络。<br>$$(12):FF(X[t])=ReLU(X[t]W_1^{ff}+b_1^{ff})W_2^{ff}+b_2^{ff}$$<br>其中 $X[t]\\in \\mathbb{R}^{d^{att}}$ 是输入序列 $X$ 的第 $t$ 帧，$W_1^{ff} \\in \\mathbb{R}^{d^{att}\\times d^{ff}},W_2^{ff} \\in \\mathbb{R}^{d^{ff}\\times d^{att}}$ 是可学习的权重矩阵，$b_1^{ff}\\in \\mathbb{R}^{d^{ff}},b_2^{ff}\\in \\mathbb{R}^{d^{att}}$ 是可学习的偏差向量。等式(10)中的 $MHA_i(X_i,X_i,X_i)$ 为“self-attention”</p>\n<p>用于等式(4)的基于Transformer的DecBody()包含两个attention模块：<br>$$(12):Y_j[t]^{‘}=Y_j[t]+MHA_j^{self}(Y_j[t],Y_j[1:t],Y_j[1:t])$$   $$Y_j[t]^{‘’}=Y_j+MHA_j^{src}(Y_j^{‘},X_e,X_e)$$   $$Y_{j+1}=Y_j^{‘’}+FF_j(Y_j^{‘’})$$<br>其中 $j = 0,…,d − 1$ 是解码器层的索引，我们将 $MHA_j^{src}(Y_j^{‘},X_e,X_e)$ 中的解码器输入与编码器输出之间的attention矩阵称为“encoder-decoder attention”。因为单向解码器对于序列生成很有用，所以它在第 $t$ 个目标帧处的attention矩阵被屏蔽，因此它们不会与 $t$ 之后的帧进行连接。可以使用带有三角形二进制矩阵的元素乘积并行地进行序列mask。 因为它不需要顺序操作，所以它提供了比RNN更快的实现。</p>\n<p>Transformer采用正弦位置编码：<br>$$PE[t]=\\left{\\begin{matrix} sin\\frac{t}{10000^{t/d^{att}}} &amp; 当t是偶数\\ cos\\frac{t}{10000^{t/d^{att}}} &amp; 当t是奇数 \\end{matrix}\\right.$$<br>在EncBody()和DecBody()模块之前，输入序列 $X_0,Y_0$ 与（$PE[1],PE[2],…$）串联在一起。</p>\n<h1 id=\"Transformer应用于ASR\"><a href=\"#Transformer应用于ASR\" class=\"headerlink\" title=\"Transformer应用于ASR\"></a>Transformer应用于ASR</h1><p>S2S从log-mel滤波器组语音特征的输入序列 $X^{fbank}$ 预测字符或SentencePiece的目标序列 $Y$。ASR中的源 $X$ 表示为具有音高特征的83维log-mel滤波器阵列帧的序列。首先，EncPre()通过使用具有256维，步长为2且内核大小为3的两层CNN或类似于VGG的最大池化的两层CNN，将源序列 $X$ 转换为子采样序列 $X_0\\in \\mathbb{R}^{n^{sub}\\times d^{att}}$，其中 $n^{sub}$ 是CNN输出序列的长度。然后，EncBody()将 $X_0$ 转换为CTC和解码器的一系列编码特征 $X_e\\in \\mathbb{R}^{n^{sub}\\times d^{att}}$。</p>\n<p>解码器接收编码序列 $X_e$ 和目标序列 $Y[1:t-1]$：字符或SentencePiece。首先，等式(3)中的DecPre()将，进行词嵌入。接下来，DecBody()和单线性层DecPost()在给定 $X_e$ 和目标序列 $Y[1:t-1]$ 的情况下，预测下一个token，即 $Y_{post}[t]$ 的后验分布。解码器和CTC模块的后验分布预测为：$p_{s2s}(Y|X)$和 $p_{ctc}(Y|X)$，训练阶段两个loss加权求和：<br>$$L^{ASR}=-alogp_{s2s}(Y|X)-(1-a)logp_{ctc}(Y|X)$$</p>\n<blockquote>\n<p>In the decoding stage, the decoder predicts the next token given the speech feature X and the previous predicted tokens using beam search, which combines the scores of S2S, CTC and the RNN language model (LM)</p>\n</blockquote>\n<p>解码阶段除了ctc和s2s以外还需要一个语言模型<br>$$\\hat{Y}=argmax{\\lambda logp_{s2s}(Y|X_e)+(1-\\lambda)logp_{ctc}(Y|X_e)+\\gamma logp_{lm}(Y)}$$</p>\n<h1 id=\"Transformer应用于TTS\"><a href=\"#Transformer应用于TTS\" class=\"headerlink\" title=\"Transformer应用于TTS\"></a>Transformer应用于TTS</h1><p>TTS中编码器的输入是包含EOS符号的token序列，首先，将token序列进行词嵌入，然后将通过权重参数缩放的位置编码添加到向量中，即等式(1)的EncPre()实现。随后，等式(2)中的编码器EncBody()将这个输入序列转换为解码器的编码特征序列。TTS中解码器的输入是一系列编码器特征和log-mel滤波器组特征。在训练中，标记数据log-mel滤波器组特征以teacher forcing方式使用，而推理中，预测特征以自回归方式使用。</p>\n<p>解码器中，作为等式(3)中的DecPre()的TTS实现，Prenet将80维log-mel滤波器组特征的目标序列转换为隐藏特征序列。该网络由两个具有256个单位的线性层，一个ReLU激活函数和一个dropout组成，后面是具有 $d^{att}$ 单位的投影线性层。然后在等式(4)中的解码器DecBody()，其架构与编码器相同，将编码器特征和隐藏特征的序列转换为解码器特征的序列。对 $Y_d$ 的每一帧应用两个线性层以分别计算目标特征和EOS的概率。最后，将Postnet应用于预测目标特征的序列。Postnet是一个五层的CNN，其每一层都是一维卷积，具有256个通道，内核大小为5，然后进行批量归一化，tanh激活函数和dropout。这些模块是等式(5)中DecPost()的TTS实现。</p>\n<p>在TTS训练中，优化了整个网络，以最大程度地减少TTS中的两个损失功能。</p>\n<ul>\n<li>目标特征的L1损失</li>\n<li>EOS概率的二元交叉熵（BCE）损失</li>\n<li>guided attention loss </li>\n</ul>\n<p>为了解决BCE计算中类别不平衡的问题，对正样本使用恒定权重。除此之外，作者表示没有引入任何超参数来平衡这三个损耗值。推理中，模型以自回归方式预测下一帧的目标特征，并且如果EOS的概率变得高于某个阈值（例如0.5），则模型将停止预测。</p>\n<h1 id=\"ASR相关的实验以及调参方法\"><a href=\"#ASR相关的实验以及调参方法\" class=\"headerlink\" title=\"ASR相关的实验以及调参方法\"></a>ASR相关的实验以及调参方法</h1><p><strong>下表中，作者总结了在ASR实验中使用的15个数据集</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201124103745154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>实验配置如下</strong>：<br>除了最大的LibriSpeech（$d^{head}= 8,d^{att} = 512$）外，我们对每个语料库的Transformer采用了相同的架构配置（$e = 12,d = 6,d^{ff} = 2048,d^{head} = 4,d^{att} = 256$）。</p>\n<ul>\n<li>Transformer更快收敛<blockquote>\n<p>Transformer requires a different optimizer configuration from RNN because Transformer’s training iteration is eight times faster and its update is more fine-grained than RNN. </p>\n</blockquote>\n</li>\n<li>参方法主要参考之前论文方法，参数是最后10个epoch的模型的求平均<blockquote>\n<p>To train Transformer, we basically followed the previous literature (e.g., dropout, learning rate, warmup steps). We did not use development sets for early stopping in Transformer. We simply ran 20 – 200 epochs (mostly 100 epochs) and averaged the model parameters stored at the last 10 epochs as the final model.</p>\n</blockquote>\n</li>\n<li>解码阶段Transformer和RNN用相同的配置。<blockquote>\n<p>In the decoding stage, Transformer and RNN share the same configuration for each corpus, for example, beam size (e.g., 20 – 40), CTC weight λ (e.g., 0.3), and LM weight γ (e.g., 0.3 – 1.0)</p>\n</blockquote>\n</li>\n</ul>\n<p>如下表中展示根据每个语料库的字符/单词错误率（CER / WER）的ASR结果，非常好的实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201124105806182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><strong>从图中我们可以得出结论</strong>：</p>\n<ul>\n<li>Transformer更大的minibatch效果更好<blockquote>\n<p>We observed that Transformer trained with a larger minibatch became more accurate while RNN did not.</p>\n</blockquote>\n</li>\n<li>8倍速于RNN<blockquote>\n<p>In this task, Transformer achieved the best accuracy provided by RNN about eight times faster than RNN with a single GPU.</p>\n</blockquote>\n</li>\n</ul>\n<p>下表总结了LibriSpeech ASR基准，因为它是最具竞争力的任务之一。<br><img src=\"https://img-blog.csdnimg.cn/20201124105628908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图显示了在LibriSpeech上使用多个GPU获得的ASR训练曲线：<br><img src=\"https://img-blog.csdnimg.cn/20201124105134691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><strong>训练技巧</strong>：</p>\n<ul>\n<li>扩大minibatch的大小</li>\n<li>使用accumulating gradient strategy</li>\n<li>使用dropout</li>\n<li>使用数据增强方法可以在很大程度上提升性能</li>\n<li>解码阶段RNN和Transformer在相同参数集上性能都比较好 </li>\n</ul>\n<p>下图清楚地表明，在9种语言中，Transformer明显优于RNN：<br><img src=\"https://img-blog.csdnimg.cn/20201124110919724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>TTS中Transformer的配置为：$e=6, d=6, d^{att}=384, d^{ff}=1536, d^{head}=4$，且系统的输入都是字符序列。下面两个图显示了两个语料库中的训练曲线：<br><img src=\"https://img-blog.csdnimg.cn/20201124111444848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下面两个图展示了生成的语音频谱图<br><img src=\"https://img-blog.csdnimg.cn/20201124111649703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20201124111723982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>把Transformer应用在语音领域上与RNN对比的论文，结果也是比较喜人，重点是它在ESPnet上面开源了，模型、代码都给出来了，还给出了各种训练方法和技巧，是一篇实用性很强的文章。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","RNN","Transformer","语音"]},{"title":"论文阅读笔记：超详细的NLP预训练语言模型总结清单！","url":"/Paper-Reading/ad96e3afa777/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Pre-trained Models for Natural Language Processing: A Survey<br>原文链接：<a href=\"https://arxiv.org/pdf/2003.08271.pdf\">Link</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>预训练模型给下游任务带来的效果不言而喻，有了预训练模型，我们可以使用它来加速解决问题的过程。正如论文中所说的那样，<strong>预训练模型（PTMs）的出现将自然语言处理（NLP）带入了一个新时代</strong>。本篇论文基于分类从四个角度对现有PTMs进行系统分类，描述如何使PTMs的知识适应下游任务，然后概述了PTMs未来研究的一些潜在方向，通过本篇综述，来学习了解相关预训练模型。</p>\n<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><ul>\n<li>第一代 PTMs 旨在学习词嵌入，由于下游的任务不再需要这些模型的帮助，因此为了计算效率，它们通常采用浅层模型，如 Skip-Gram 和 GloVe。尽管这些经过预训练的嵌入向量也可以捕捉单词的语义，但它们却不受上下文限制，只是简单地学习「共现词频」。这样的方法明显无法理解更高层次的文本概念，如句法结构、语义角色、指代等等。</li>\n<li>第二代 PTMs 专注于学习上下文的词嵌入，如 CoVe、ELMo、OpenAI GPT 以及 BERT。它们会学习更合理的词表征，这些表征囊括了词的上下文信息，可以用于问答系统、机器翻译等后续任务。另一层面，这些模型还提出了各种语言任务来训练 PTMs ，以便支持更广泛的应用，因此它们也可以称为预训练语言模型。</li>\n</ul>\n<p>下图说明了NLP的通用神经体系结构，词嵌入有两种：Non-contextual Embeddings（非上下文嵌入）和Contextual Embeddings（上下文嵌入）。它们之间的区别在于，单词的嵌入是否根据出现的上下文而动态变化。<br><img src=\"https://img-blog.csdnimg.cn/2021022212223824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>之前 NLP 任务一般会预训练 $e$ 这些不包含上下文信息的词嵌入，我们会针对不同的任务确定不同的上下文信息编码方式，以构建特定的隐藏向量 $h$，从而进一步完成特定任务。但对于预训练语言模型来说，我们的输入也是 $e$ 这些嵌入向量，不同之处在于我们会在大规模语料库上预训练 Contextual Encoder，并期待它在各种情况下都能获得足够好的 $h$，从而直接完成各种 NLP 任务。换而言之，最近的一些 PTMs 将预训练编码的信息，提高了一个层级。</p>\n<p>大多数神经上下文编码器可分为两类：sequence模型和graph-based模型，下图说明了这些模型的体系结构：<br><img src=\"https://img-blog.csdnimg.cn/20210222123920428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>目前Transformer由于其强大的能力，成为了主流的PTMs结构，预训练的优势如下：</p>\n<ul>\n<li>在庞大的文本语料库上进行预训练可以学习通用的语言表示形式并帮助完成下游任务。</li>\n<li>预训练提供了更好的模型初始化，通常可以带来更好的泛化性能并加快目标任务的收敛速度。</li>\n<li>可以将预训练视为一种正则化，以避免对小数据过度拟合。</li>\n</ul>\n<h1 id=\"PTMs概述\"><a href=\"#PTMs概述\" class=\"headerlink\" title=\"PTMs概述\"></a>PTMs概述</h1><h2 id=\"Pre-training-Tasks\"><a href=\"#Pre-training-Tasks\" class=\"headerlink\" title=\"Pre-training Tasks\"></a>Pre-training Tasks</h2><ul>\n<li>Language Modeling (LM)</li>\n<li>Masked Language Modeling (MLM)：被称为完形填空（Cloze）任务，即从输入序列中遮掩一些 token，然后训练模型来通过其余的 token 预测 masked 的 token。<ul>\n<li>Sequence-to-Sequence MLM (Seq2Seq MLM)</li>\n<li>Enhanced Masked Language Modeling (E-MLM)</li>\n</ul>\n</li>\n<li> Permuted Language Modeling (PLM)：给定一个序列，然后从所有可能的排列中随机抽样一个排列。接着将排列序列中的一些 token 选定为目标，同时训练模型以根据其余 token 和目标的正常位置（natural position）来预测这些目标。</li>\n<li>Denoising Autoencoder (DAE)：接受部分损坏的输入，并以恢复这些未失真的原始输入为目标。这类任务会使用标准 Transformer 等模型来重建原始文本，它与 MLM 的不同之处在于，DAE 会给输入额外加一些噪声。</li>\n<li>Contrastive Learning (CTL)：相较于语言建模，CTL 的计算复杂度更低，因而在预训练中是理想的替代训练标准。<ul>\n<li>Deep InfoMax (DIM）</li>\n<li>Replaced Token Detection (RTD）：替换 token 检测（Replaced Token Detection，RTD）与 NCE 相同，但前者会根据上下文语境来预测是否替换 token。</li>\n<li>Next Sentence Prediction (NSP)：顾名思义，NSP 训练模型以区分两个输入句子是否为训练语料库中的连续片段。</li>\n<li>Sentence Order Prediction (SOP)：SOP 使用同一文档中的两个连续片段作为正样本，而相同的两个连续片段互换顺序作为负样本。</li>\n</ul>\n</li>\n</ul>\n<p>下图是这些Pre-training Tasks的损失函数汇总：<br><img src=\"https://img-blog.csdnimg.cn/20210222155841685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"PTMs分类\"><a href=\"#PTMs分类\" class=\"headerlink\" title=\"PTMs分类\"></a>PTMs分类</h2><ul>\n<li>Representation Type：预训练方法（PTMs）使用的词表征类型</li>\n<li>Architectures：预训练方法使用的主干网络</li>\n<li>Pre-Training Task Types：PTMs使用的 预训练任务类型</li>\n<li>Extensions：为特定场景与输入类型所设计的PTMs</li>\n</ul>\n<p>下图详细地展示了各种 PTMs的所属类别，只要看懂了它，基本目前现有的预训练语言模型的整体状态，都能了解了：<br><img src=\"https://img-blog.csdnimg.cn/20210222171337541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表进一步展示了主流预训练方法的更多细节，主流模型、论文、实现，看这张表就足够了：<br><img src=\"https://img-blog.csdnimg.cn/20210222171424152.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"PTMs扩展\"><a href=\"#PTMs扩展\" class=\"headerlink\" title=\"PTMs扩展\"></a>PTMs扩展</h1><h2 id=\"Knowledge-Enriched-PTMs\"><a href=\"#Knowledge-Enriched-PTMs\" class=\"headerlink\" title=\"Knowledge-Enriched PTMs\"></a>Knowledge-Enriched PTMs</h2><p>PTMs通常从通用大型文本语料库中学习通用语言表示，但是缺少特定领域的知识，PTMs中设计一些辅助的预训练任务，将外部知识库中的领域知识整合到PTMs中被证明是有效的</p>\n<ul>\n<li>ERNIE-THU将在知识图谱中预先训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。由于语言表征的预训练过程和知识表征过程有很大的不同，会产生两个独立的向量空间。为解决上述问题，在有实体输入的位置，将实体向量和文本表示通过非线性变换进行融合，以融合词汇、句法和知识信息。</li>\n<li>LIBERT（语言知识的BERT）通过附加的语言约束任务整合了语言知识。</li>\n<li>SentiLR集成了每个单词的情感极性，以将MLM扩展到标签感知MLM（LA-MLM），ABSA任务上都达到SOTA。</li>\n<li>SenseBERT不仅能够预测被mask的token，还能预测它们在给定语境下的实际含义。使用英语词汇数据库 WordNet 作为标注参照系统，预测单词在语境中的实际含义，显著提升词汇消歧能力。</li>\n<li>KnowBERT与实体链接模型以端到端的方式合并实体表示。</li>\n<li>KG-BERT显示输入三元组形式，采取两种方式进行预测：构建三元组识别和关系分类，共同优化知识嵌入和语言建模目标。这些工作通过实体嵌入注入知识图的结构信息。</li>\n<li>K-BERT将从KG提取的相关三元组显式地注入句子中，以获得BERT的扩展树形输入。</li>\n<li>K-Adapter通过针对不同的预训练任务独立地训练不同的适配器来注入多种知识，从而可以不断地注入知识，以解决注入多种知识时可能会出现灾难性遗忘问题。此外，这类PTMs还有WKLM、KEPLER等。</li>\n</ul>\n<h2 id=\"Model-Compression\"><a href=\"#Model-Compression\" class=\"headerlink\" title=\"Model Compression\"></a>Model Compression</h2><p>由于预训练的语言模型通常包含至少数亿个参数，因此很难将它们部署在现实应用程序中的在线服务和资源受限的设备上，模型压缩是减小模型尺寸并提高计算效率的有效方法，论文中提到的5种PTMs的压缩方法为：</p>\n<ul>\n<li>pruning（剪枝）：将模型中影响较小的部分舍弃，如Compressing BERT，还有结构化剪枝 LayerDrop ，其在训练时进行Dropout，预测时再剪掉Layer，不像知识蒸馏需要提前固定student模型的尺寸大小。</li>\n<li>quantization（量化）：将高精度模型用低精度来表示，如Q-BERT和Q8BERT，量化通常需要兼容的硬件。</li>\n<li>parameter sharing （参数共享）：相似模型单元间的参数共享。ALBERT主要是通过矩阵分解和跨层参数共享来做到对参数量的减少。</li>\n<li>module replacing（模块替换）：BERT-of-Theseus根据伯努利分布进行采样，决定使用原始的大模型模块还是小模型，只使用task loss。</li>\n<li>knowledge distillation （知识蒸馏）：通过一些优化目标从大型、知识丰富、fixed的teacher模型学习一个小型的student模型，蒸馏机制主要分为3种类型：<ul>\n<li>从软标签蒸馏：DistilBERT、EnsembleBERT</li>\n<li>从其他知识蒸馏：TinyBERT、BERT-PKD、MobileBERT、 MiniLM、DualTrain</li>\n<li>蒸馏到其他结构：Distilled-BiLSTM</li>\n</ul>\n</li>\n</ul>\n<p>下表是一些代表性的压缩PTMs：<br><img src=\"https://img-blog.csdnimg.cn/20210222181417550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"使PTMs适应下游任务\"><a href=\"#使PTMs适应下游任务\" class=\"headerlink\" title=\"使PTMs适应下游任务\"></a>使PTMs适应下游任务</h1><p>尽管PTMs可以从大型语料库中获取通用语言知识，但是如何有效地将其知识适应下游任务仍然是关键问题。迁移学习旨在使knowledge从源任务（或领域）适应目标任务（或领域），下图给出了迁移学习的示意图：<br><img src=\"https://img-blog.csdnimg.cn/20210222182203577.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>要将PTMs的知识转移到下游NLP任务，我们需要考虑以下问题：</p>\n<ul>\n<li>选择合适的预训练任务：语言模型是PTMs是最为流行的预训练任务，相同的预训练任务有其自身的偏置，并且对不同的任务会产生不同的效果。例如，NSP任务可以使诸如问答（QA）和自然语言推论（NLI）之类的下游任务受益。</li>\n<li>选择合适的模型架构：例如BERT采用的MLM策略和Transformer-Encoder结构，导致其不适合直接处理生成任务。</li>\n<li>选择合适的数据：下游任务的数据应该近似于PTMs的预训练任务，现在已有很多现成的PTMs可以方便地用于各种特定领域或特定语言的下游任务。</li>\n<li>选择合适的layers进行transfer：主要包括Embedding迁移、top layer迁移和all layer迁移。如word2vec和Glove可采用Embedding迁移，BERT可采用top layer迁移，Elmo可采用all layer迁移。</li>\n</ul>\n<p>特征集成还是fine-tune？对于特征集成预训练参数是freeze的，而fine-tune是unfreeze的。特征集成方式却需要特定任务的体系结构，fine-tune方法通常比特征提取方法更为通用和方便，下表给出了适应性PTMs的一些常见组合：<br><img src=\"https://img-blog.csdnimg.cn/20210222182937367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br> Fine-Tuning策略，即通过更好的微调策略进一步激发PTMs性能：</p>\n<ul>\n<li>两阶段 Fine-Tuning策略：如第一阶段对中间任务或语料进行 Fine-Tuning，第二阶段再对目标任务 Fine-Tuning。第一阶段通常可根据特定任务的数据继续进行Fine-Tuning预训练。</li>\n<li>多任务 Fine-Tuning：MTDNN在多任务学习框架下对BERT进行了 Fine-Tuning，这表明多任务学习和预训练是互补的技术。</li>\n<li>采取额外的适配器： Fine-Tuning的主要缺点是其参数效率低，每个下游任务都有自己的 Fine-Tuning参数。因此，更好的解决方案是在固定原始参数的同时，将一些可 Fine-Tuning的适配器注入PTMs。</li>\n<li>逐层阶段：逐渐冻结而不是同时对所有层进行 Fine-Tuning，也是一种有效的 Fine-Tuning策略。</li>\n</ul>\n<h1 id=\"关于PTMs的资源\"><a href=\"#关于PTMs的资源\" class=\"headerlink\" title=\"关于PTMs的资源\"></a>关于PTMs的资源</h1><p>下表提供了一些受欢迎的存储库，包括第三方实现，论文列表，可视化工具以及PTMs的其他相关资源：<br><img src=\"https://img-blog.csdnimg.cn/20210222183621568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>虽然 PTMs已经在很多 NLP 任务中显示出了他们强大的能力，然而由于语言的复杂性，仍存在诸多挑战，论文给出了五个未来 PTMs发展方向的建议。</p>\n<ul>\n<li><strong>PTMs的上限</strong>：目前，PTMs并没有达到其上限。大多数的PTMs可通过使用更长训练步长和更大数据集来提升其性能。目前NLP中的SOTA也可通过加深模型层数来更进一步提升。这将导致更加高昂的训练成本。因此，一个更加务实的方向是在现有的软硬件基础上，设计出更高效的模型结构、自监督预训练任务、优化器和训练技巧等。例如， ELECTRA 就是此方向上很好的一个解决方案。</li>\n<li><strong>面向任务的预训练和模型压缩</strong>：在实践中，不同的目标任务需要 PTMs拥有不同功能。而 PTMs与下游目标任务间的差异通常在于两方面：模型架构与数据分布。尽管较大的PTMs通常情况下会带来更好的性能表现，但在低计算资源下如何使用是一个实际问题。例如，对于 NLP 的 PTM 来说，对于模型压缩的研究只是个开始，Transformer 的全连接架构也使得模型压缩具有挑战性。</li>\n<li><strong>PTMs的架构设计</strong>：对于PTMs，Transformer 已经被证实是一个高效的架构。然而 Transformer 最大的局限在于其计算复杂度（输入序列长度的平方倍）。受限于 GPU 显存大小，目前大多数 PTMs无法处理超过 512 个 token 的序列长度。打破这一限制需要改进 Transformer 的结构设计，例如 Transformer-XL。关于Transformer结构变体可以参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/351742765\">Transformer的9种变体概览</a></li>\n<li><strong>fine-tune中的知识迁移</strong>：finetune是目前将 PTMs的知识转移至下游任务的主要方法，但效率却很低，每个下游任务都需要有特定的finetune参数。一个可以改进的解决方案是固定PTMs的原始参数，并为特定任务添加小型的finetune适配器，这样就可以使用共享的PTMs 服务于多个下游任务。</li>\n<li><strong>PTMs 的解释性与可靠性</strong>：PTMs 的可解释性与可靠性仍然需要从各个方面去探索，它能够帮助我们理解 PTM 的工作机制，为更好的使用及性能改进提供指引。</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","Transformer","NLP","语言模型","BERT","预训练模型"]},{"title":"论文阅读笔记：这篇文章教你在文本分类任务上微调BERT","url":"/Paper-Reading/bb40f25f3748/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：How to Fine-Tune BERT for Text Classification?<br>原文链接：<a href=\"https://arxiv.org/pdf/1905.05583.pdf\">Link</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>预训练语言模型很强，通过微调可以给你的任务模型带来明显的提升，但是针对具体的任务如何进行微调使用，就涉及到了考经验积累的tricks，最近在打文本相关的比赛，正好用预训练模型为基础构建下游任务模型，所以着重的关注一些相关的BERT微调tricks，凑巧看到这篇文章，里面专门介绍 BERT 用于中文文本分类的各种 tricks，所以在此分享一下。这篇文章分别介绍了Fine-Tuning Strategies、Further Pre-training和Multi-Task Fine-Tuning，具体见后文总结介绍。<br>关于预训练语言模型，可以看论文团队的另一篇文章更新的文章：Pre-trained Models for Natural Language Processing: A Survey（<a href=\"https://zhuanlan.zhihu.com/p/352152573\">论文阅读笔记：超详细的NLP预训练语言模型总结清单！</a>）</p>\n<h1 id=\"前情提要\"><a href=\"#前情提要\" class=\"headerlink\" title=\"前情提要\"></a>前情提要</h1><p>首先先确定一下BERT在Text Classification上的一般应用，我们都知道BERT喂入的输入有两个特殊的Token，即[CLS]置于开头，[SEP]用于分隔句子，最后的输出取[CLS]的最后隐藏层状态 $h$ 作为整个序列的表示，然后使用全连接层映射到分类任务上，及：<br>$$p(c|h)=softmax(Wh)$$<br>基于此，论文分别讨论通用微调BERT的方法流程，Fine-Tuning Strategies、Further Pre-training和Multi-Task Fine-Tuning，如下：<br><img src=\"https://img-blog.csdnimg.cn/20210319222831458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>论文分析结果用的实验数据共八个，如下，可以归纳为Sentiment analysis、Question classification、Topic classification、Data preprocessing<br><img src=\"https://img-blog.csdnimg.cn/20210319232645576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<blockquote>\n<p>Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps</p>\n</blockquote>\n<h1 id=\"Fine-Tuning策略\"><a href=\"#Fine-Tuning策略\" class=\"headerlink\" title=\"Fine-Tuning策略\"></a>Fine-Tuning策略</h1><p>我们来带着如下几个问题进行思考：</p>\n<ul>\n<li>BERT的不同层对语义句法信息有着不同的抽取能力，那么那一层更有利于目标任务？</li>\n<li>如何选择优化算法和学习率</li>\n</ul>\n<p>想要微调BERT适配目标任务，主要有三个因素（和上面思考相匹配）：</p>\n<ul>\n<li>BERT最大处理序列长度为512</li>\n<li>BERT-base有12层，需要挑选合适的层用于目标分类任务</li>\n<li>过拟合</li>\n</ul>\n<p>超参：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">batch_size &#x3D; 24; dropout &#x3D; 0.1; learning-rate&#x3D;2e-5; warm-up proportion &#x3D; 0.1; max_epoch &#x3D; 4;</span><br></pre></td></tr></table></figure>\n<h3 id=\"BERT最大处理序列长度为512\"><a href=\"#BERT最大处理序列长度为512\" class=\"headerlink\" title=\"BERT最大处理序列长度为512\"></a>BERT最大处理序列长度为512</h3><p>针对长度超过512的文本，实验如下三种转换策略（ 预留[CLS] 和 [SEP]）：</p>\n<ul>\n<li>head-only：前510 tokens</li>\n<li>tail-only：后510 tokens;</li>\n<li>head+tail：根据经验选择前 128 和后 382 tokens</li>\n<li>分段：首先将输入文本（长度为L）分成k = L/510个小段落，将它们依次输入BERT得到k个文本段落的表示。每个段落的representation是最后一层[CLS]的hidden state，并分别使用mean pooling, max pooling and self-attention来合并所有段落的representation。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20210319235425953.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>实验证实第三种在错误率上有 0.15~0.20% 的优势，也许是因为重要信息在首尾比较多中间比较少？</p>\n<h3 id=\"BERT-base有12层，需要挑选合适的层用于目标分类任务\"><a href=\"#BERT-base有12层，需要挑选合适的层用于目标分类任务\" class=\"headerlink\" title=\"BERT-base有12层，需要挑选合适的层用于目标分类任务\"></a>BERT-base有12层，需要挑选合适的层用于目标分类任务</h3><p>下图是不同层在任务上的测试错误率结果：<br><img src=\"https://img-blog.csdnimg.cn/20210319235624285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>很明显，用最后一层还是比较靠谱的。</p>\n<h3 id=\"过拟合-学习率逐层衰减\"><a href=\"#过拟合-学习率逐层衰减\" class=\"headerlink\" title=\"过拟合-学习率逐层衰减\"></a>过拟合-学习率逐层衰减</h3><p>针对学习率的衰减策略，本文继承了 <a href=\"https://arxiv.org/pdf/1801.06146.pdf\">ULM-Fit</a>）中的方案，叫作 Slanted Triangular，这个方案和 BERT 的原版方案类似，都是带 warmup 的先增后减。通常来说，这类方案对初始学习率的设置并不敏感，但是，在 Fine-Tune阶段使用过大的学习率，会打乱 Pre-train 阶段学习到的句子信息，造成“灾难性遗忘”。这里简单描述一下学习率策略：我们将BERT $L$ 层的参数分别表示为 ${\\theta^1,…,\\theta^L}$，参数更新策略如下：<br>$$\\theta^l_t=\\theta^l_{t-1}-\\eta^l\\cdot \\triangledown_{\\theta^l}J(\\theta)$$<br>其中，$\\eta^l$ 表示第 $l$ 层的学习率，并设 $\\eta^L$ 为初始学习率，而 $\\eta^{k-1}=\\xi\\cdot\\eta^k$，$\\xi$ 就是衰减因子，小于或等于 $1$，如何等于 $1$ 那么就是我们所熟悉的SGD了。关于SGD及其扩展的优化算法可以参考我这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/343564175\">论文阅读笔记：各种Optimizer梯度下降优化算法回顾和总结</a></p>\n<p>这种设置相邻两层的学习率比例，让低层更新幅度更小的方法，系数控制在 0.90 ~ 0.95，整体优化效果不明显，不建议尝试，实验结果如下：<br><img src=\"https://img-blog.csdnimg.cn/20210320000024861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20210320000253814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>上图显示最右边的 4e-4 已经完全无法收敛了，而 1e-4 的 loss 曲线明显不如 2e-5 和 5e-5 的低。根据作者的数据集大小推测，整个 finetune 过程在 1W 步左右，<strong>实测发现，小数据上更多的 epoch 并不能带来效果的提升，不如早一点终止训练省时间</strong>。</p>\n<h1 id=\"Further-Pre-training\"><a href=\"#Further-Pre-training\" class=\"headerlink\" title=\"Further Pre-training\"></a>Further Pre-training</h1><ul>\n<li>BERT在通用域数据下进行预训练，其数据分布与目标域不同，所以可以考虑进一步使用目标域数据对BERT进行预训练，先对基线做进一步的 pretrain，能够帮助 finetune 效果提升。而Further Pre-training有几种方案：</li>\n<li>使用本任务的训练数据进行预训练</li>\n<li>使用有着类似数据分布的相关任务数据进行预训练</li>\n<li>使用跨领域任务的数据进行预训练</li>\n</ul>\n<p>超参：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">batch_size &#x3D; 32; max_length &#x3D; 128; learning_rate &#x3D; 5e-5; warmup_steps &#x3D; 1W; steps &#x3D; 10W;</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用本任务的训练数据进行Further-Pre-training\"><a href=\"#使用本任务的训练数据进行Further-Pre-training\" class=\"headerlink\" title=\"使用本任务的训练数据进行Further Pre-training\"></a>使用本任务的训练数据进行Further Pre-training</h3><p><img src=\"https://img-blog.csdnimg.cn/20210320000741171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上图显示10W步之后达到最优结果，需要注意的是，pretrain 为了提升训练效率，使用的是偏短的 128 个词句子，学习率仍然是带 warmup 的衰减方式，初始值不用像 finetune 那样设置得那么小。整个训练过程为 10W 步，这个值是作者实验测定的，太短或太长都会影响最终模型的准确率。</p>\n<h3 id=\"同领域语料和跨领域语料进行Further-Pre-training\"><a href=\"#同领域语料和跨领域语料进行Further-Pre-training\" class=\"headerlink\" title=\"同领域语料和跨领域语料进行Further Pre-training\"></a>同领域语料和跨领域语料进行Further Pre-training</h3><p><img src=\"https://img-blog.csdnimg.cn/20210320092603113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>上述得到结论，优先使用同领域的无标注数据，其次使用 finetune 的训练数据，什么都没有的话，混一点跨领域数据也可以。</p>\n<h1 id=\"Multi-Task-Fine-Tuning\"><a href=\"#Multi-Task-Fine-Tuning\" class=\"headerlink\" title=\"Multi-Task Fine-Tuning\"></a>Multi-Task Fine-Tuning</h1><ul>\n<li>对目标与中的多个任务同时微调BERT，是否仍然对任务有帮助？</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20210320093338603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在Multi-Task Fine-Tuning中，所有BERT和Embedding都进行共享，只留最后的分类层来适配不同的任务。由实验结果得出Multi-Task Fine-Tuning并不能给Text Classification相关子任务带来很大的提升。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><ul>\n<li>预训练语言模型BERT的最后一层较于其他层来说更加有助于分类</li>\n<li>合适的学习率和层宽有助于BERT的catastrophic forgetting </li>\n<li>使用任务语料或者同域语料进行further pre-training效果更好哦</li>\n<li>是在任务语料太少，混入跨领域数据也可，但是效果并不明显哦</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["NLP","BERT","预训练模型","微调","文本分类"]},{"title":"论文阅读笔记：需要推理的MuTual多轮对话数据集","url":"/Paper-Reading/64d1bd606b34/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：MuTual: A Dataset for Multi-Turn Dialogue Reasoning<br>原文链接：<a href=\"https://arxiv.org/pdf/2004.04494.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>面向非任务的对话系统在给定上下文的情况下，当前系统能够产生相关且流畅的回复，但是由于推理能力较弱，有时会出现逻辑错误。为了促进对话推理研究，发布了多轮对话推理数据集 MuTual，针对性地评测模型在多轮对话中的推理能力。它由基于中国学生英语听力理解考试的8,860个手动注释的对话组成，数据集的<a href=\"https://github.com/Nealcly/MuTual\">GitHub仓库</a>。</p>\n<p>神经对话系统在大型对话语料库上进行训练，并用于预测给定上下文的回复。一般情况下，构建聊天机器人主要有两种解决方案：</p>\n<ul>\n<li>检索式的方法依赖文本匹配技术，在诸多候选回复中，选择匹配分数最高的作为回复；</li>\n<li>生成式的方法使用 Seq2Seq 模型，编码器读取对话历史，解码器直接生成相应回复。</li>\n</ul>\n<p>检索式的方法凭借回复相关性高，流利度高等优势，在工业界取得了更多的应用。不过，虽然在以BERT为代表的预训练模型，在检索式多轮对话任务中，已经基本接近了人类的表现。但实际应用中，当前的对话模型选择出的回复往往相关性较好，但是经常出现常识和逻辑错误等问题。由于现有的大部分检索式对话数据集都没有关注这种对话逻辑问题，导致评价指标也无法直接反映模型对对话逻辑的掌握程度。针对此问题，提出了多轮对话推理数据集 MuTual。</p>\n<p>相比现有的其他检索式聊天数据集，MuTual 要求对话模型具备常识推理能力；相比阅读理解式的推理数据集，MuTual 的输入输出则完全符合标准检索式聊天机器人的流程。因此，MuTual 也是目前最具挑战性的对话式数据集。由于任务不同，目前现有的推理数据集并不能直接帮助指导训练聊天机器人。下表为对话和基于推理的阅读理解的常用数据集：<br><img src=\"https://img-blog.csdnimg.cn/20201109234130781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>MuTual是第一个基于人标签推理的多轮对话数据集，用最佳方法在此数据集上运行的 $R@1$ 为71％，明显不如人类的表现（94％）。</p>\n<h1 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h1><p>原始的听力理解材料和问答对是由语言专家设计的，学生需要根据一段音频从三个选项中选择最佳答案，为了确保学生完全理解音频，大部分问题都需要具备推理能力。原始数据的格式设置为三元组 &lt;Conversation (audio),Question and Choices (text), Answer (image)&gt;<br><img src=\"https://img-blog.csdnimg.cn/20201110090755102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>听力考试要求学生根据一段双人多轮对话，回答额外提出的问题（图1左），并通过学生能否正确答对问题衡量学生是否理解了对话内容。为了更自然的模拟开放领域对话，我们进一步将听力题中额外的问题转化为对话中的回复（图1右）。标注者截选原对话中具备回答问题信息的片段，根据正确选项构造正确的回复（图1右回复 A），根据两个错误选项构造两个错误的回复（回复 C 和回复 D）。</p>\n<p>为了进一步提升难度，引入额外的推理信息，标注者还需根据正确选项构建一个负面的回复（回复 B）。另外，标注者需要保证在无上文信息情况下，所有候选回复在逻辑上皆合理。这样可以让数据集聚焦于检测模型在多轮对话中的推理能力，而非判断单个句子是否具有逻辑性。作者还在标注过程中控制正确和错误的回复与上文的词汇重叠率相似，防止模型可以通过简单的根据文本匹配选出候选回复。<br><img src=\"https://img-blog.csdnimg.cn/20201110091857685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上图是MuTual的详细统计汇总，直观上感觉词汇量比其他数据集要小得多，是由于MuTual是从英语作为外语的听力测试中修改而来的，因此形态和语法的复杂性比其他数据集要简单得多。</p>\n<p>为了评估不同推理类型的分布，我们注释了所涉及的特定推理类型，例如，从测试集中取样并将其分为六类。MuTual 数据集主要包含聊天机器人需要的六种推理能力：态度推理(13%)、数值推理(7%)、意图预测(31%)、多事实推理(24%)和常识等其他推理类型（9%）。</p>\n<ul>\n<li>态度推理（Attitude Reasoning）：这种类型的实例测试模型是否知道说话者对物体的态度。</li>\n<li>数值推理（Algebraic Reasoning）：这种类型的实例测试模型在选择回复时是否具备数值推理能力</li>\n<li>意图预测（Intention Prediction）：此类型测试模型是否可以预测说话者接下来要做什么</li>\n<li>多事实推理（Situational Reasoning）：在这种类型的实例中考虑情况信息（例如，位置，两个讲话者之间的关系），模型应从先前的上下文中挖掘隐式信息。</li>\n<li>常识等其他推理类型（Multi-fact Reasoning and Others）：在这种情况下，正确的响应与上下文中的多个事实有关，这要求模型深刻理解上下文，而不是简单地进行文本匹配。</li>\n</ul>\n<p>这六种类型的推理被认为与真正的聊天机器人最相关。例如，如果机器知道用户的姿势，它可使聊天机器人提出个人建议。意图预测功能使聊天机器人在长时间的对话中能够更智能地做出回复。</p>\n<p>如下图，所有的回复都与上下文相关，但其中只有一个是逻辑正确的。一些错误的回复在极端情况下可能是合理的，但正确的回复是最合适的。<br><img src=\"https://img-blog.csdnimg.cn/20201110092743942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在真实应用场景中，检索式对话模型无法检索所有可能的回复，如果没有检索到合适的回复，系统应具有给予安全回复（safe response）的能力。为了模拟这一场景，我们提出了 MuTual plus。对于每个实例，MuTual plus 随机替换掉 MuTual 中一个候选回复。如果正确回复被替换，安全回复即为新的正确回复。如果错误回复被替换，原正确回复仍为四个回复中最合适的。</p>\n<p>这里说明一下论文中R@1、R@2、MRR等指标的含义</p>\n<blockquote>\n<p>数据集以 Recall@1（正确检索结果出现在检索结果第一位），Recall@2（正确检索结果出现在检索结果前两位），MRR（Mean Reciprocal Rank, 正确检索结果在检索结果中的排名的倒数）作为评价指标。</p>\n</blockquote>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>我们将数据分为训练集，开发集和测试集，比例分别为80％，10％和10％。我们在拆分过程中打包了从同一会话构造的实例，以避免数据泄漏。</p>\n<ul>\n<li><p>不同方法在 MuTual 数据集上的表现对比。<br><img src=\"https://img-blog.csdnimg.cn/20201110094123186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们发现，选择题方法的性能明显优于个人评分方法。一种可能的解释是，多项选择方法将候选回复同时考虑在内，因此他们可以区分safe response是否是最佳选择。相比之下，个人评分方法并不稳健，在训练阶段，safe response容易使其混淆。</p>\n</li>\n<li><p>不同方法在 MuTual plus 数据集上的表现对比，实验调查了模型在训练语料库中从未见过的情况下是否能够很好地处理safe response。<br><img src=\"https://img-blog.csdnimg.cn/2020111010335387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n</ul>\n<p>根据表3，4的结果可以看到，之前的检索式对话模型在 MuTual 上，表现只比随机猜的情况好一点。不过预训练模型也不能取得很好的效果，甚至 RoBERTa 也只能达到71%的 Recall@1。然而未经培训的非母语者可以轻松达到94%。</p>\n<p>下图，我们发现，不同类别的BERT-MC和RoBERTa-MC的趋势相似，RoBERTa-MC在态度推理和多事实推理方面明显优于BERT-MC<br><img src=\"https://img-blog.csdnimg.cn/20201110103744714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>如下图，通过简单的减法步骤即可得出时间差（5:00 pm-6h = 11:00 am），但这对RoBERTa-MC来说是一个巨大的挑战。模型在不同上下文轮数数据的 R@1 对比。#T 表示上下文轮数。#Instances 表示实例的数量。<br><img src=\"https://img-blog.csdnimg.cn/20201110104418149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>进一步研究发现，与其他多轮对话数据集不同的是，在 MuTual 中，模型表现不会随着对话轮数增加而变差，RoBERTa 在两轮和六轮以上的数据上 R@1 相似。这表示推理能力并不依赖复杂的对话历史。如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201110104742960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在推理类型方面，模型在数值推理和意图推测中表现的较差。上图第一个例子中，时差运算只需简单的减法(5:00pm - 6h = 11:00am)，第二个例子需要推理出对话出现在租房场景中，然而对现有的深度学习模型来说依然十分困难。下图展示了前 n 轮对话被删除情况下模型表现显著下滑，证明了解决 MuTual 中的问题需要依赖多轮推理而不是单轮推理。<br><img src=\"https://img-blog.csdnimg.cn/20201110105031354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>MuTual 数据集，用于针对性地评测模型在多轮对话中的推理能力，该数据集有助于将来进行多轮对话推理问题的研究。</p>\n","categories":["Paper-Reading"],"tags":["NLP","对话系统","Paper","数据集","多轮对话","MuTual"]},{"title":"详细SpringBoot教程之Web开发（一）","url":"/Spring-Boot/565b9f12d0a0/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"开发前准备\"><a href=\"#开发前准备\" class=\"headerlink\" title=\"开发前准备\"></a>开发前准备</h2><p>我们接下来准备使用SpringBoot开发一个restful的应用，首先我们通过Idea创建向导帮我们创建SpringBoot应用，勾选我们需要的模块，这里我们还是只勾选一个web场景，后面需要啥再添加。<br><img src=\"https://img-blog.csdnimg.cn/2020022411091976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>前面说过，我们创建的项目中，SpringBoot将会帮我们自动配置好web开发场景中需要的相关配置，只需要在配置文件中指定少量配置就可以运行起来了，我们只需要注意在业务代码中就可以了。</p>\n<p>当然，我们如果想要搞清楚某个场景中，SpringBoot帮我们配置了什么依赖，这个时候就需要我们了解相关的自动配置原理了，前面的博文有相关的讲解，我这里就不深入讲解了。</p>\n<h2 id=\"SpringBoot对静态资源的映射配置规则\"><a href=\"#SpringBoot对静态资源的映射配置规则\" class=\"headerlink\" title=\"SpringBoot对静态资源的映射配置规则\"></a>SpringBoot对静态资源的映射配置规则</h2><p>创建好了项目之后，我们先来讲解一下SpringBoot对于静态资源的相关配置规则，因为我们现在暂时来开发一个前后端没有分离的很彻底的应用（如果前后端分离的很彻底，我们只用SpringBoot来做后端接口），所以必须要知道怎么使用SpringBoot的静态资源。</p>\n<h3 id=\"使用webjars\"><a href=\"#使用webjars\" class=\"headerlink\" title=\"使用webjars\"></a>使用webjars</h3><p>首先SpringBoot的有个叫webjars的东西，所有通过pom.xml引入的静态资源，都在/webjars/**下（这个在静态资源的自动配置类中可以找到），也就是说SpringBoot会自动去classpath:/META-INF/resources/webjars/找资源，webjars以jar包的方式引入静态资源（也就是说，我们都可以通过<a href=\"http://localhost:8080/webjars/xx%E8%AE%BF%E9%97%AE%E5%AF%B9%E5%BA%94%E7%9A%84%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%EF%BC%89\">http://localhost:8080/webjars/xx访问对应的静态资源）</a></p>\n<p>怎么通过pom.xml引入静态资源呢？我们可以去<a href=\"https://www.webjars.org/\">webjars的资源库</a>中找，需要什么静态资源的jar基本都可以在这里找到（指的是框架依赖），里面长这样子。<br><img src=\"https://img-blog.csdnimg.cn/20200224112633172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\">各种静态资源jar都可以通过pom.xml依赖的方式引入 ，我们引入JQuery看看，引入的JQuery的依赖包下边，可以看到webjars内容长这样<br><img src=\"https://img-blog.csdnimg.cn/20200224113123313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们引入了jQuery的静态资源，我们启动项目，来试一下能不能通过对应的路径访问，我这里使用的路径是<a href=\"http://localhost:8080/webjars/jquery/3.4.1/jquery.js\">http://localhost:8080/webjars/jquery/3.4.1/jquery.js</a><br><img src=\"https://img-blog.csdnimg.cn/20200224113521694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>所以发现了没有，我通过SpringBoot来开发相关的静态资源，非常方便，我们只要引入对应的webjars就可以了，SpringBoot不需要我们进行过多的配置，有自己默认配置的规则。</p>\n<h3 id=\"使用resources\"><a href=\"#使用resources\" class=\"headerlink\" title=\"使用resources\"></a>使用resources</h3><p>当然如果我们想要用自己的静态资源，不想使用webjars的东西，怎么做呢？这个时候就需要另外一个映射规则就是/**（也就是说，我们通过<a href=\"http://localhost:8080/xxx.js%E8%AE%BF%E9%97%AE%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%B2%A1%E6%9C%89%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%EF%BC%8C%E5%B0%B1%E4%BC%9A%E9%BB%98%E8%AE%A4%E5%8E%BB%E4%B8%8B%E9%9D%A2%E8%BF%99%E5%87%A0%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%AD%E6%89%BE%EF%BC%89%E7%94%A8%E6%9D%A5%E9%85%8D%E7%BD%AE%E6%98%A0%E5%B0%84%EF%BC%8C%E5%8D%B3%E5%8F%AF%E8%AE%BF%E9%97%AE%E5%BD%93%E5%89%8D%E9%A1%B9%E7%9B%AE%E4%B8%8B%E7%9A%84%E4%BB%BB%E4%BD%95%E8%B5%84%E6%BA%90\">http://localhost:8080/xxx.js访问静态资源，如果我们没有配置相关，就会默认去下面这几个文件夹中找）用来配置映射，即可访问当前项目下的任何资源</a></p>\n<ul>\n<li>classpath:/META-INF/resources/</li>\n<li>classpath:/resources/</li>\n<li>classpath:/static/</li>\n<li> classpath:/public/<br>即通过“/**”访问的静态资源，SpringBoot会去上面这几个找资源，这些也就是SpringBoot默认的静态资源文件夹<br><img src=\"https://img-blog.csdnimg.cn/20200224114201805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里我们可以随便将jquery.js放置在三个文件夹中的一个，然后通过<a href=\"http://localhost:8080/jquery.js%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE%E5%88%B0%E3%80%82\">http://localhost:8080/jquery.js可以访问到。</a><br><img src=\"https://img-blog.csdnimg.cn/20200224114436634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2020022411463312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h2 id=\"配置欢迎页映射\"><a href=\"#配置欢迎页映射\" class=\"headerlink\" title=\"配置欢迎页映射\"></a>配置欢迎页映射</h2><p>静态资源文件夹下的所有index.html页面，会被/<strong>映射；<br>也就是说我们直接访问<a href=\"http://localhost:8080/%E5%B0%B1%E7%9B%B4%E6%8E%A5%E8%AE%BF%E9%97%AE%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%AD%E7%9A%84index.html%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%88%91%E4%BB%AC%E4%BB%A5%E5%89%8DSpringMVC%E4%B8%8B%E9%9D%A2%E7%9A%84index.jsp%EF%BC%8C%E6%AF%94%E5%A6%82%E6%88%91%E4%BB%AC%E7%8E%B0%E5%9C%A8%E9%9A%8F%E6%9C%BA%E9%80%89public%E7%9A%84%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E5%88%9B%E5%BB%BAindex.html\">http://localhost:8080/就直接访问静态资源文件夹中的index.html，相当于我们以前SpringMVC下面的index.jsp，比如我们现在随机选public的静态资源文件夹下创建index.html</a><br><img src=\"https://img-blog.csdnimg.cn/20200224115146775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>同样的所有的</strong>/favicon.ico默认都是在静态资源下找，当然，如果我们不想要使用SpringBoot默认的静态资源文件夹，我们还可以定义静态资源的映射，如下<br><img src=\"https://img-blog.csdnimg.cn/20200224120932331.png#pic_center\" alt=\"在这里插入图片描述\"><br>要注意了，如果自己定义了静态资源映射之后，默认的文件夹就都不生效了，相当于原来的那些访问方式都不生效了。</p>\n<h2 id=\"模板引擎\"><a href=\"#模板引擎\" class=\"headerlink\" title=\"模板引擎\"></a>模板引擎</h2><p>JSP、Velocity、Freemarker、Thymeleaf等等，模板引擎的本质思想是一样的，只是语法不大一致而已。比如说以freemarker为例，如下图。<br><img src=\"https://img-blog.csdnimg.cn/20200224121211766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>SpringBoot推荐的Thymeleaf，不用JSP，因为Thymeleaf语法更加简单，功能更加强大，如何导入呢，其实对于SpringBoot不就是一个引入依赖的事情嘛，很简单，依赖包如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>这里引入的是SpringBoot默认的Thymeleaf依赖版本，如果我们要切换版本怎么切换，如下，加上对应版本号就可以了</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt;</span><br><span class=\"line\">&lt;thymeleaf.version&gt;&lt;/thymeleaf.version&gt;</span><br><span class=\"line\">&lt;thymeleaf-layout-dialect.version&gt;&lt;/thymeleaf-layout-dialect.version&gt;</span><br><span class=\"line\">&lt;/properties&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用Thymeleaf\"><a href=\"#使用Thymeleaf\" class=\"headerlink\" title=\"使用Thymeleaf\"></a>使用Thymeleaf</h2><p>在使用Thymeleaf语法之前，我们先来实验一下，我们只要把HTML页面放在templates目录下就可以了，Thymeleaf就会自动渲染<br><img src=\"https://img-blog.csdnimg.cn/20200224123258894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200224123648165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"正式使用\"><a href=\"#正式使用\" class=\"headerlink\" title=\"正式使用\"></a>正式使用</h3><p>我们先编写一个控制器，用来跳转渲染，控制器中，加入我们待会儿要在HTML通过Thymeleaf获取的一个变量值，如下<br><img src=\"https://img-blog.csdnimg.cn/20200224152855460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在HTML编写代码之前，现在HTML的头标签中导入thymeleaf的名称空间，这样我们在使用Thymeleaf语法的时候，Idea就会提示相关代码。<br><img src=\"https://img-blog.csdnimg.cn/20200224152946227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>编写好之后启动项目，我们通过访问<a href=\"http://localhost:8080/success%E6%9D%A5%E8%AE%BF%E9%97%AEsuccess.html%E9%A1%B5%E9%9D%A2%EF%BC%8C%E8%AE%BF%E9%97%AE%E4%B9%8B%E5%90%8E%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B\">http://localhost:8080/success来访问success.html页面，访问之后结果如下</a><br><img src=\"https://img-blog.csdnimg.cn/20200224220715198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"SpringBoot自动配置MVC\"><a href=\"#SpringBoot自动配置MVC\" class=\"headerlink\" title=\"SpringBoot自动配置MVC\"></a>SpringBoot自动配置MVC</h2><p>SpringBoot集成了SpringMVC，所以里面自然自动配置了SpringMVC运行的相关基本依赖，那么SpringBoot引入了SpringMVC那些依赖呢？这里我大致进行一下介绍。Spring Boot为Spring MVC提供了自动配置，可与大多数应用程序完美配合。其中自动配置在Spring的默认值之上添加了以下功能：</p>\n<ul>\n<li>包含ContentNegotiatingViewResolver和BeanNameViewResolver</li>\n<li>支持服务静态资源，包括对WebJars的支持</li>\n<li>自动注册Converter，GenericConverter和Formatter的beans。</li>\n<li>支持HttpMessageConverters。</li>\n<li>自动注册MessageCodesResolver</li>\n<li>静态index.html支持</li>\n<li>定制Favicon支持</li>\n<li>自动使用ConfigurableWebBindingInitializerbean</li>\n</ul>\n<p>如果要保留这些Spring Boot MVC定制并进行更多的MVC定制（拦截器，格式化程序，视图控制器和其他功能），则可以添加自己@Configuration的type类，WebMvcConfigurer但不添加 @EnableWebMvc（因为@EnableWebMvc会关闭Spring Boot MVC的默认配置，而转向需要读取我们自己添加的配置，稍后会讲）。</p>\n<p>如果你想提供的定制情况RequestMappingHandlerMapping，RequestMappingHandlerAdapter或者ExceptionHandlerExceptionResolver，仍然保持弹簧引导MVC自定义，你可以声明类型的beans WebMvcRegistrations，并用它来提供这些组件的定制实例。</p>\n<p>如果你想对Spring MVC中的完全控制，你可以添加自己的@Configuration注解为@EnableWebMvc，或者添加自己的@Configuration-annotated DelegatingWebMvcConfiguration中的Javadoc中所述@EnableWebMvc。</p>\n<p>后面我们会讨论怎么对SpringMVC进行完全控制。</p>\n<h2 id=\"拓展SpringMVC\"><a href=\"#拓展SpringMVC\" class=\"headerlink\" title=\"拓展SpringMVC\"></a>拓展SpringMVC</h2><p>看到如下配置是不是很熟悉，我们以前在SpringMVC中是这样编写视图控制器和拦截器的，那么我们怎么在SpringBoot中使用呢？<br><img src=\"https://img-blog.csdnimg.cn/20200224160148641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在前面见过，为了方便编写项目的统一配置，和前面见过配置的的地方一样，我们统一把项目的配置放在config包中管理，然后我们在config包下创建一个MyMvcConfig的类，实现WebMvcConfigurer接口，就可以对SpringBoot中关于SpringMVC相关配置进行扩展了。</p>\n<p>WebMvcConfigurer可以用来扩展SpringMVC的功能，需要什么在里面实现什么方法就可以了，比如我们扩展一下视图转换器，如下<br><img src=\"https://img-blog.csdnimg.cn/20200224160934488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这样实现了既保留了使用原有的配置，又能使用我们自己的方式扩展配置</p>\n<h2 id=\"EnableWebMvc\"><a href=\"#EnableWebMvc\" class=\"headerlink\" title=\"@EnableWebMvc\"></a>@EnableWebMvc</h2><p>上面我们可以自己编写类扩展SpringMVC的相关配置，如果我们再在配置类上加上@EnableWebMvc注解，是啥意思呢，也就是说我们将全面接管SpringMVC在SpringBoot中的配置，在SpringBoot中有关SpringMVC所有的默认配置都会失效，需要我们自行配置，比如上面我们默认webjars进行访问静态资源等等，都会失效<br><img src=\"https://img-blog.csdnimg.cn/2020022416164636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>也就是说原本我们不使用@EnableWebMvc，SpringBoot会自动配置SpringMVC的默认依赖，如果我们使用@EnableWebMvc，则@EnableWebMvc就会将WebMVCConfigurationSupport组件导入进来，而导入的WebMVCConfigurationSupport组件里面只有SpringMVC一些最基本的功能。</p>\n<h2 id=\"如何修改SpringBoot的默认配置\"><a href=\"#如何修改SpringBoot的默认配置\" class=\"headerlink\" title=\"如何修改SpringBoot的默认配置\"></a>如何修改SpringBoot的默认配置</h2><p>SpringBoot在自动配置很多组件的时候，先看容器中有没有哦用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有就自动配置，如果有些组件可以有多个（如ViewResolver）将用户配置和自己默认的组合起来</p>\n<p>在SpringBoot中会有非常多的xxxConfigurer帮助我们进行SpringMVC的拓展配置。</p>\n<h2 id=\"写一篇\"><a href=\"#写一篇\" class=\"headerlink\" title=\"写一篇\"></a>写一篇</h2><p>这篇博文我们在进行正式开发前的一些准备，下一篇我们来引入静态资源文件编写相关代码。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Web"]},{"title":"详细SpringBoot教程之Web开发（三）","url":"/Spring-Boot/10c4c12622eb/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"错误处理机制\"><a href=\"#错误处理机制\" class=\"headerlink\" title=\"错误处理机制\"></a>错误处理机制</h2><h3 id=\"SpringBoot默认的错误处理机制\"><a href=\"#SpringBoot默认的错误处理机制\" class=\"headerlink\" title=\"SpringBoot默认的错误处理机制\"></a>SpringBoot默认的错误处理机制</h3><p>我们接上一篇博文的项目，我们现在把拦截器关掉，直接在主配置文件注释掉注入就可以直接关掉了，像这样<br><img src=\"https://img-blog.csdnimg.cn/20200225191208491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后我们现在通过浏览器随便访问一个不存在的连接，会出现下面这样的错误提示，应该不陌生吧，之前肯定是遇到过的。<br><img src=\"https://img-blog.csdnimg.cn/20200225191441571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以查看一下浏览器发送请求的请求头，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020022519272210.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>当然SpringBoot还另外规定了客户端访问无效链接的错误机制，如果我们通过客户端访问会返回一个默认的json数据，像下面这样</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"string\">&quot;timestamp&quot;</span>: 1519637719324,</span><br><span class=\"line\">\t<span class=\"string\">&quot;status&quot;</span>: 404,</span><br><span class=\"line\">\t<span class=\"string\">&quot;error&quot;</span>: <span class=\"string\">&quot;Not Found&quot;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&quot;message&quot;</span>: <span class=\"string\">&quot;No message avaliable&quot;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&quot;path&quot;</span>:  <span class=\"string\">&quot;/dbc/aaa&quot;</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"默认错误处理机制原理\"><a href=\"#默认错误处理机制原理\" class=\"headerlink\" title=\"默认错误处理机制原理\"></a>默认错误处理机制原理</h3><p>老样子，我们想要错误处理机制也有自己的自动配置类（之前说过，SpringBoot功能是通过配置文件来进行控制的，而配置文件一般是通过xxxAutoConfiguration这样的自动配置类来实现的），所以SpringBoot的默认的错误处理机制可以参照自动配置类，也就是ErrorMvcAutoConfiguration。</p>\n<p>ErrorMvcAutoConfiguration通过给给容器中添加了以下组件，来控制错误处理机制</p>\n<ul>\n<li>DefaultErrorAttributes：帮我们在页面共享信息；</li>\n<li>BasicErrorController：处理默认/error请求</li>\n<li>ErrorPageCustomizer：系统出现错误以后来到error请求进行处理；</li>\n<li>DefaultErrorViewResolver：用来控制错误时，启用模板引擎视图，还是默认；</li>\n</ul>\n<p>也就是默认的错误处理流程是，一但系统出现4xx或者5xx之类的错误，ErrorPageCustomizer就会生效（定制错误的响应规则），接着就会来到/error请求，然后就可以通过<strong>BasicErrorController</strong>来进行处理，进行相应时，响应页面去哪个页面是由<strong>DefaultErrorViewResolver</strong>解析得到的；</p>\n<h3 id=\"如何定制错误响应\"><a href=\"#如何定制错误响应\" class=\"headerlink\" title=\"如何定制错误响应\"></a>如何定制错误响应</h3><h4 id=\"如何定制错误页面\"><a href=\"#如何定制错误页面\" class=\"headerlink\" title=\"如何定制错误页面\"></a>如何定制错误页面</h4><p>如果有模板引擎的情况下（我们项目中使用的是Thymeleaf模板引擎），可以通过error/状态码的形式来进行控制，也就是说，我们可以将错误页面命名为“错误状态码.html”，然后放在模板引擎文件夹（即templates目录下）里面的error文件夹下，没有error文件夹就创建一个，发生此状态码的错误就会来到 对应的页面，因为SpringBoot规则中已经默认规定好了。</p>\n<p>更准确的将我们可以使用4xx和5xx作为错误页面的文件名，进而来匹配该种类型的所有错误，匹配的时候遵循精确优先（优先寻找精确的状态码.html），像下面这样<br><img src=\"https://img-blog.csdnimg.cn/20200226184048531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们在默认的错误页面中可以获得如下信息：、</p>\n<ul>\n<li>timestamp：时间戳</li>\n<li>tstatus：状态码</li>\n<li>terror：错误提示</li>\n<li>texception：异常对象</li>\n<li>tmessage：异常消息</li>\n<li>terrors：JSR303数据校验的错误都在这里</li>\n</ul>\n<p>这里要说明一下的是，如果我们项目中没有使用模板引擎（或者模板引擎找不到这个错误页面），就会去静态资源文件夹下找。如果静态资源文件夹中也没有错误页面，就是默认来到SpringBoot默认的错误提示页面。</p>\n<h4 id=\"如何定制错误的json\"><a href=\"#如何定制错误的json\" class=\"headerlink\" title=\"如何定制错误的json\"></a>如何定制错误的json</h4><p>可以自定义异常处理和返回定制json数据，像下面这样，我们可以专门定义一个配置类用来解决错误定制的，我这里命名为MyExceptionHandler，然后通过@ExceptionHandler来定制响应的相关错误，进而返回定制数据<br><img src=\"https://img-blog.csdnimg.cn/20200226184639486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>不过这样处理会有个问题，就是我们确实是定制了返回的json数据，但是本来SpringBoot会根据我们用什么访问请求，然后响应什么，比如我们用浏览器访问，响应html页面，客户端响应json数据，现在这样写，返回的都是接送数据了，所以我们需要换种方式，通过将相应转发到/error进行自适应响应效果处理，像下面这样<br><img src=\"https://img-blog.csdnimg.cn/2020022618532286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"携带定制数据\"><a href=\"#携带定制数据\" class=\"headerlink\" title=\"携带定制数据\"></a>携带定制数据</h4><p>前面我们讲过，出现错误以后，会来到/error请求，会被BasicErrorController处理，响应出去可以获取的数据是由getErrorAttributes得到的（是AbstractErrorController（ErrorController）规定的方法）</p>\n<p>所以，我们可以通过编写一个ErrorController的实现类或者是编写AbstractErrorController的子类，然后放在容器中，我们要知道，页面上能用的数据，或者是json返回能用的数据都是通过errorAttributes.getErrorAttributes得到，也就是说容器中DefaultErrorAttributes.getErrorAttributes()来默认进行数据处理的，我们定制自定义ErrorAttributes，如下<br><img src=\"https://img-blog.csdnimg.cn/20200226185756995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这样，我们的错误响应是自适应的，可以通过定制ErrorAttributes改变需要返回的内容，就不在只有原先的默认属性了，形象的将，我们返回的信息就会是如下</p>\n<ul>\n<li>timestamp：时间戳</li>\n<li>tstatus：状态码</li>\n<li>terror：错误提示</li>\n<li>texception：异常对象</li>\n<li>tmessage：异常消息</li>\n<li>terrors：JSR303数据校验的错误都在这里</li>\n<li>author： dbc</li>\n</ul>\n<h2 id=\"配置嵌入式Servlet容器\"><a href=\"#配置嵌入式Servlet容器\" class=\"headerlink\" title=\"配置嵌入式Servlet容器\"></a>配置嵌入式Servlet容器</h2><p>SpringBoot默认使用Tomcat作为嵌入式的Servlet容器，这我们都是知道的<br><img src=\"https://img-blog.csdnimg.cn/20200226190317180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>那么这个时候我们如果想要换成其他的Servlet容器，我们应该怎么做呢？那么接下来我们就来讲解怎么搞</p>\n<h3 id=\"如何定制和修改Servlet容器的相关配置\"><a href=\"#如何定制和修改Servlet容器的相关配置\" class=\"headerlink\" title=\"如何定制和修改Servlet容器的相关配置\"></a>如何定制和修改Servlet容器的相关配置</h3><p>修改和server有关的配置（ServerProperties即也是使用WebServerFactoryCustomizer），在主配置文件中可以通过这样的形式来设置<br><img src=\"https://img-blog.csdnimg.cn/20200226222617476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>除了在主配置文件中进行相关的设置之外，我们还可以通过注册WebServerFactoryCustomizer<ConfigurableWebServerFactory>组件来在类中自定义配置，也就是嵌入式的Servlet容器的定制器，来修改Servlet容器的配置<br><img src=\"https://img-blog.csdnimg.cn/20200226223525597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"注册Servlet三大组件\"><a href=\"#注册Servlet三大组件\" class=\"headerlink\" title=\"注册Servlet三大组件\"></a>注册Servlet三大组件</h3><p>Servlet三大组件分别是Servlet、Filter、Listener，如果我们原先熟悉SpringMVC开发的应该知道，我们在Webapp下面的web.xml中，经常需要配置这三大组件用来过滤监听相关的请求，而在SpringBoot中，由于SpringBoot默认是以jar包的方式启动嵌入式的Servlet容器来启动SpringBoot的web应用，所以并没有没有web.xml文件。但是我们依旧可以通过SpringBoot特有的相关注册Bean进行注册，分别是</p>\n<ul>\n<li>ServletRegistrationBean</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//注册三大组件</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ServletRegistrationBean <span class=\"title\">myServlet</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    ServletRegistrationBean registrationBean = <span class=\"keyword\">new</span> ServletRegistrationBean(<span class=\"keyword\">new</span> MyServlet(),<span class=\"string\">&quot;/myServlet&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> registrationBean;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>FilterRegistrationBean</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> FilterRegistrationBean <span class=\"title\">myFilter</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    FilterRegistrationBean registrationBean = <span class=\"keyword\">new</span> FilterRegistrationBean();</span><br><span class=\"line\">    registrationBean.setFilter(<span class=\"keyword\">new</span> MyFilter());</span><br><span class=\"line\">    registrationBean.setUrlPatterns(Arrays.asList(<span class=\"string\">&quot;/hello&quot;</span>,<span class=\"string\">&quot;/myServlet&quot;</span>));</span><br><span class=\"line\">    <span class=\"keyword\">return</span> registrationBean;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>ServletListenerRegistrationBean</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ServletListenerRegistrationBean <span class=\"title\">myListener</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = <span class=\"keyword\">new</span> ServletListenerRegistrationBean&lt;&gt;(<span class=\"keyword\">new</span> MyListener());</span><br><span class=\"line\">    <span class=\"keyword\">return</span> registrationBean;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>SpringBoot帮我们自动配置SpringMVC的时候，会自动的注册SpringMVC的前端控制器，即DIspatcherServlet，我们通过查阅DispatcherServletAutoConfiguration发现，会默认拦截，“/”的所有请求，包括静态资源，但是不拦截jsp请求，/*会拦截jsp，可以通过server.servletPath来修改SpringMVC前端控制器默认拦截的请求路径</p>\n<h3 id=\"更换其他嵌入式Servlet容器\"><a href=\"#更换其他嵌入式Servlet容器\" class=\"headerlink\" title=\"更换其他嵌入式Servlet容器\"></a>更换其他嵌入式Servlet容器</h3><p>我们知道，SpringBoot是默认支持Tomcat的，也就是在pom.xml中通过如下依赖引入的<br><img src=\"https://img-blog.csdnimg.cn/20200226224726267.png#pic_center\" alt=\"在这里插入图片描述\"><br>所以，我们不能整个去除web依赖，因为里面还有其他除了Tomcat的依赖，所以我们在web下直接排除Tomcat的依赖就可以了，然后添加其他的嵌入式Servlet容器的依赖，不同的嵌入式Servlet依赖如下</p>\n<h4 id=\"Jetty\"><a href=\"#Jetty\" class=\"headerlink\" title=\"Jetty\"></a>Jetty</h4><p><img src=\"https://img-blog.csdnimg.cn/20200226225112744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"Undertow\"><a href=\"#Undertow\" class=\"headerlink\" title=\"Undertow\"></a>Undertow</h4><p><img src=\"https://img-blog.csdnimg.cn/20200226225326573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"使用外置的Servlet容器\"><a href=\"#使用外置的Servlet容器\" class=\"headerlink\" title=\"使用外置的Servlet容器\"></a>使用外置的Servlet容器</h2><p>在我们之前学习的SpringBoot应用，是通过嵌入式Servlet容器，应用打成可执行的jar，这样做的优点显而易见，就是简单、便携，但是缺点就是默认不支持JSP、优化定制比较复杂（使用定制器ServerProperties、自定义WebServerFactoryCustomizer，自己编写嵌入式Servlet容器的创建工厂ConfigurableWebServerFactory）；<br>但是不着急，我们可以使用外置的Servlet容器，也就是外面安装Tomcat，然后应用war包的方式打包。</p>\n<h3 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h3><ul>\n<li><p>必须创建一个war项目，利用idea创建好目录结构<br><img src=\"https://img-blog.csdnimg.cn/20200226230435665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p>将嵌入式的Tomcat指定为provided；</p>\n</li>\n</ul>\n<figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.boot<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>必须编写一个<strong>SpringBootServletInitializer</strong>的子类，并调用configure方法<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ServletInitializer</span> <span class=\"keyword\">extends</span> <span class=\"title\">SpringBootServletInitializer</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"meta\">@Override</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">protected</span> SpringApplicationBuilder <span class=\"title\">configure</span><span class=\"params\">(SpringApplicationBuilder application)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">//传入SpringBoot应用的主程序</span></span><br><span class=\"line\">      <span class=\"keyword\">return</span> application.sources(SpringBoot04WebJspApplication.class);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>启动服务器就可以使用；</li>\n</ul>\n<p>我这里说一下jar包和war包启动的区别</p>\n<ul>\n<li>jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器；</li>\n<li>war包：启动服务器，<strong>服务器启动SpringBoot应用</strong>通过SpringBootServletInitializer，启动ioc容器；</li>\n</ul>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Web"]},{"title":"详细SpringBoot教程之入门（一）","url":"/Spring-Boot/83b4b122caa4/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"SpringBoot介绍\"><a href=\"#SpringBoot介绍\" class=\"headerlink\" title=\"SpringBoot介绍\"></a>SpringBoot介绍</h2><p>先通过来一些概念，会让我们对技术有着更加清晰的理解。J2EE笨重的开发、繁多的配置、低下的开发效率、复杂的部署流程、第三方技术集成难度大，如果开发过SpringMVC或者单纯使用过SSH、SSM框架的同学，肯定是体验过被很多配置文件支配的恐惧，很难受吧，不过不用担心，我们有了SpringBoot。Spring Boot用来简化Spring应用开发，约定大于配置，去繁从简，just run就能创建一个独立的，产品级别的应用。随着Spring全家桶时代的到来，SpringBoot给我们带来了J2EE一站式解决方案，SpringCloud给我们带来了分布式整体解决方案。从此我们可以使用 SpringBoot 快速的开发基于 Spring 框架的项目，由于围绕 SpringBoot 存在很多开箱即用的 Starter 依赖，使得我们在开发业务代码时能够非常方便的、不需要过多关注框架的配置，而只需要关注业务即可。</p>\n<h2 id=\"SpringBoot优点\"><a href=\"#SpringBoot优点\" class=\"headerlink\" title=\"SpringBoot优点\"></a>SpringBoot优点</h2><ul>\n<li>快速创建独立运行的Spring项目以及与主流框架集成</li>\n<li>使用嵌入式的Servlet容器，应用无需打成WAR包</li>\n<li>starters自动依赖与版本控制</li>\n<li>大量的自动配置，简化开发，也可以修改默认值</li>\n<li>无需配置XML，无代码生成，开箱即用</li>\n<li>准生产环境的运行时应用监控</li>\n<li>与云计算的天然集成</li>\n</ul>\n<p>看到SpringBoot这么多优点，是不是很想立刻上手呢，先不急。这里还是要提一下的，SpringBoot入门很容易，不过想要精通还是有难度的，因为毕竟自动化帮我们把Spring全家桶集成的这么好，我们想要深入精通的话，必须要了解Spring全家桶的技术，当然难呀，不过既然好上手，我们完全可以先学会怎么使用，然后后面熟练了再去研究底层代码原理，这样才有那种豁然开让的感觉。</p>\n<h3 id=\"微服务\"><a href=\"#微服务\" class=\"headerlink\" title=\"微服务\"></a>微服务</h3><p>说到SpringBoot，就不得不提一下微服务，微服务是一种架构风格，可以理解成小型服务，这些服务通过HTTP的方式进行互通。微服务并没有一个官方的定义，想要直接描述微服务比较困难，我们可以通过对比传统WEB应用，来理解什么是微服务，传统的应用成为“单体应用”</p>\n<p>单体应用就是那种，核心分为业务逻辑、适配器以及API或通过UI访问的WEB界面。业务逻辑定义业务流程、业务规则以及领域实体。适配器包括数据库访问组件、消息组件以及访问接口等，所有的这些东西都集成在一起，看着很强大，其实很乱。</p>\n<p>而微服务架构有很多重要的优点，它解决的就是复杂性问题。它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务。这些服务定义了明确的RPC或消息驱动的API边界。微服务架构强化了应用模块化的水平，而这通过单体代码库很难实现。因此，微服务开发的速度要快很多，更容易理解和维护。</p>\n<h2 id=\"实现一个HelloWord应用\"><a href=\"#实现一个HelloWord应用\" class=\"headerlink\" title=\"实现一个HelloWord应用\"></a>实现一个HelloWord应用</h2><p>说了这么多SprigBoot多好多好，不直接上手一个简单的项目说不过去，那我们就来创建运行一个HelloWord应用，体验一些从创建到访问<a href=\"http://localhost:8080/hello%E6%9C%89%E5%A4%9A%E4%B9%88%E7%9A%84%E7%AE%80%E5%8D%95%EF%BC%8C%E4%BD%A0%E5%B0%B1%E4%BC%9A%E6%84%8F%E8%AF%86%E5%88%B0%E5%8E%9F%E5%85%88%E7%9A%84SpringMVC%E4%BB%A5%E5%8F%8ASSH%E4%BB%80%E4%B9%88%E7%9A%84%E6%9C%89%E5%A4%9A%E4%B9%88%E7%B9%81%E7%90%90%E3%80%82\">http://localhost:8080/hello有多么的简单，你就会意识到原先的SpringMVC以及SSH什么的有多么繁琐。</a><br>这里说一下，我们先上手一个简单的应用，然后我会基于这个简单的应用说明一些SpringBoot的配置和使用，正所谓先要尝到甜头，才有学习的动力嘛，废话不多说，上手。</p>\n<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><p>环境啥的其实，也不用多说，因为和我使用的不同版本其实也不会有太大影响，只是可能一些问题是因为版本引起的。</p>\n<ul>\n<li>JDK1.8：1.8的版本很好用，目前大部分公司生产环境的项目也都是1.8</li>\n<li>SpringBoot1.7以上：我这里使用的SpringBoot2.2.4</li>\n<li>Maven3.3以上：我这里使用的是3.5.4</li>\n<li>IDE使用的Idea（用eclipse也可以，不过我使用的是Idea演示）</li>\n</ul>\n<p>这里特别说明一下的是Maven，我们如果使用Idea内置的Maven也可以，不过我喜欢用自己想要的版本，所以就自己给Idea配置我想要的Maven版本，而且还能学到东西，Maven也是我们必须要了解的一项工具。</p>\n<p>我使用的是Idea2018，里面内置的Maven是3.3的，我自己改成了3.5，Maven的下载和安装看我另一篇<a href=\"https://blog.csdn.net/DBC_121/article/details/104384054\">带你了解Maven，并搞定安装和配置</a></p>\n<p>安装好了Maven以后，记得把Maven默认的JDK改成1.8，就是在Maven的安装位置的conf文件夹中，打开setting.xml，在<profile>里面改成如下配置<br><img src=\"https://img-blog.csdnimg.cn/20200221123138800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为了方便，我就直接把内容粘贴出来，你们就不用一个一个打了，如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;profile&gt;</span><br><span class=\"line\">      &lt;id&gt;jdk-1.8&lt;&#x2F;id&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">      &lt;activation&gt;</span><br><span class=\"line\">        &lt;activeByDefault&gt;true&lt;&#x2F;activeByDefault&gt;</span><br><span class=\"line\">        &lt;jdk&gt;1.8&lt;&#x2F;jdk&gt;</span><br><span class=\"line\">      &lt;&#x2F;activation&gt;</span><br><span class=\"line\">      &lt;properties&gt;</span><br><span class=\"line\">        &lt;maven.compiler.source&gt;1.8&lt;&#x2F;maven.compiler.source&gt;</span><br><span class=\"line\">        &lt;maven.compiler.targer&gt;1.8&lt;&#x2F;maven.compiler.targer&gt;</span><br><span class=\"line\">        &lt;maven.compiler.compilerVersion&gt;1.8&lt;&#x2F;maven.compiler.compilerVersion&gt;</span><br><span class=\"line\">      &lt;&#x2F;properties&gt;</span><br><span class=\"line\">    &lt;&#x2F;profile&gt;</span><br></pre></td></tr></table></figure>\n<p>完成了上述操作之后，现在我教你用Idea配置自己的Maven，具体操作如下图：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200221122059420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221122249837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>选择好了之后，点击Apply就可以了。</p>\n<h3 id=\"实现功能\"><a href=\"#实现功能\" class=\"headerlink\" title=\"实现功能\"></a>实现功能</h3><p>一个功能，浏览器发送hello请求，服务器接受请求并处理，相应HelloWorld字符串</p>\n<h3 id=\"具体步骤\"><a href=\"#具体步骤\" class=\"headerlink\" title=\"具体步骤\"></a>具体步骤</h3><p>首先我们使用Idea创建Maven工程<br><img src=\"https://img-blog.csdnimg.cn/20200221122420447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221122524635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221122703163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2020022112400483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后点击Finish就创建成功了，我们的Maven项目也就创建好了，这里要提一下，我们刚创建Maven项目的时候，Idea给我的一些Tip，我们把这个勾选了，以后我们修改pom.xml的时候，就会自动帮我们加载依赖了。<br><img src=\"https://img-blog.csdnimg.cn/20200221125154848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然啦，如果你没有对Maven进行生命配置的话，你会发现Maven加载相关依赖的时候非常的慢，因为Maven中央库在国外，所以自然比较慢，我们可以配置Maven为阿里源，这里就快很多了，具体操作看我另一篇文章<a href=\"https://blog.csdn.net/DBC_121/article/details/104384054\">带你了解Maven，并搞定安装和配置</a>，里面有讲解如何配置阿里源</p>\n<h3 id=\"导入依赖SpringBoot相关的依赖\"><a href=\"#导入依赖SpringBoot相关的依赖\" class=\"headerlink\" title=\"导入依赖SpringBoot相关的依赖\"></a>导入依赖SpringBoot相关的依赖</h3><p>接下来，我们来使用pom加载相关依赖，加载我们的SpringBoot项目，那我们应该是用什么依赖呢，哈哈，不要慌，这里我教你怎么使用官网的例子，首先打开<a href=\"https://start.spring.io/\">Spring官网</a>，然后我们根据我们需要的相关版本，进行勾选生成即可，SpringBoot的版本一般选择最新的release版本就可以了，具体如下图<br><img src=\"https://img-blog.csdnimg.cn/20200221124523916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后我们就可以把生成的pom.xml依赖拷贝到我们项目的pom.xml里面，注意了，不要全部拷贝，基本把我圈出来的换成你自己的，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200221124958346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2020022112545838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>到这里我们就加载完成了一个SpringBoot项目吧，是不是很简单，不过你可能会说，我加载SpringMVC之类的也很简单，不过不要急，我们接着编写一个代码，你就发现我们什么都不用配置。</p>\n<h2 id=\"编写一个主程序，启动SpringBoot应用\"><a href=\"#编写一个主程序，启动SpringBoot应用\" class=\"headerlink\" title=\"编写一个主程序，启动SpringBoot应用\"></a>编写一个主程序，启动SpringBoot应用</h2><p>现在创建一个包，在包下创建一个java类，命名随便，我这里命名为DemoApplication ，如下图，编写的代码如下：<br><img src=\"https://img-blog.csdnimg.cn/20200221152303866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">import org.springframework.boot.SpringApplication;</span><br><span class=\"line\">import org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class=\"line\">import org.springframework.web.bind.annotation.GetMapping;</span><br><span class=\"line\">import org.springframework.web.bind.annotation.RequestParam;</span><br><span class=\"line\">import org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\">@SpringBootApplication</span><br><span class=\"line\">@RestController</span><br><span class=\"line\">public class DemoApplication &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        SpringApplication.run(DemoApplication.class, args);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    @GetMapping(&quot;&#x2F;hello&quot;)</span><br><span class=\"line\">    public String hello(@RequestParam(value &#x3D; &quot;name&quot;, defaultValue &#x3D; &quot;World&quot;) String name) &#123;</span><br><span class=\"line\">        return String.format(&quot;Hello %s!&quot;, name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>代码写好了就直接运行吧，你没听错，啥都不用做，直接运行就好了，运行了之后，直接在浏览器中访问，你就会发现成功了，舒不舒服。<br><img src=\"https://img-blog.csdnimg.cn/20200221152904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"SpringBoot简化部署\"><a href=\"#SpringBoot简化部署\" class=\"headerlink\" title=\"SpringBoot简化部署\"></a>SpringBoot简化部署</h2><p>从创建到运行就是这么方便简单，非常舒服，在进行代码分析讲解之前，我在多说点SpringBoot的甜头，就是打包部署。这个时候会发出疑问，哎，我刚刚创建的时候，好像像Tomcat之类的都没有安装配置，war包也没有打，怎么就运行了呢，更何谈部署。哈哈哈这个时候就能体现SpringBoot的小甜头了，SpringBoot项目可以打成jar包，然后在不需要安装运行环境的情况下，就能直接运行jar包，启动程序，可以移植性非常强。我们可以使用Maven命令打包，也可以直接借助Idea打包，Maven命令自己直接网上查找就可以了，这里我演示使用Idea打包方便，具体操作如下，在Idea侧边栏有Maven Projects，点击打开：<br><img src=\"https://img-blog.csdnimg.cn/2020022115372433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>等待运行结束，在日志那里，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200221153922541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221154021237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>把打包好的jar包粘贴到桌面，然后打开命令行，执行指令就可以运行了。<br><img src=\"https://img-blog.csdnimg.cn/20200221154200410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221154215922.png#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221154238170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>运行成功，然后直接在浏览器访问一样可以成功访问，是不是很有意思。</p>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>已经写了好多啦，入门篇分上下，这里先体验了一下用SpringBoot创建项目的方便，小小的甜头激发一下兴趣，下篇就对上面的代码进行分析，然后比较一下我们以前使用的SpringMVC之类的项目和SpringBoot的区别，这样我们入门更加有概念性。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot"]},{"title":"详细SpringBoot教程之入门（二）","url":"/Spring-Boot/820045d781c4/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"项目打包\"><a href=\"#项目打包\" class=\"headerlink\" title=\"项目打包\"></a>项目打包</h2><p>接上一篇的博文，我们创建了一个简单的SpringBoot应用，并运行而且进行了打包，体验了SpringBoot的“甜头”，那么这里作为开始呢，我想先说说上一篇结尾的打包。</p>\n<p>可能在上一篇学习完打包的同学会有一个疑问，我最开始Tomcat啥的都没有依赖引入，并且打包之后，没有对jar包所在环境进行任何的调整和配置，我为什么执行jar包依旧能毫无报错的执行呢？</p>\n<p>其实这个问题很简单，我们试想一下，应用程序的运行不可能平白无故，在没有任何依赖支持的情况下运行，既然如此，那么一定是我们运行的jar包就已经具备了一切运行所需要的配置依赖，没错，这就是答案。那么我们怎么去看，不急，这里我就告诉你窥探jar包的秘密。首先我们可以使用压缩工具打开我们放在桌面的jar包，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200221161225109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们会看到这三个目录，我们进入第一个目录BOOT-INF中，我们会发现有两个目录，classes这里放的是我们写的代码编译之后的class，而libs里面就是我们秘密的真谛，libs里面放的都是我们项目的依赖，到最底下发现了没有，没错，就是Tomcat依赖包。<br><img src=\"https://img-blog.csdnimg.cn/20200221162256847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>SpringBoot项目在创建的时候，就帮我们引入了Tomcat的依赖包，所以我们才能启动运行程序，如下图，后面我们说到，我们最开始在pom中引入的几个依赖在SpringBoot有特定的名称，叫做starters，我们引入的是web应用的starter，所以会自带帮我们引入我们web开发所需要的相关的基本配置依赖。</p>\n<h2 id=\"pom-xml\"><a href=\"#pom-xml\" class=\"headerlink\" title=\"pom.xml\"></a>pom.xml</h2><p>现在我们来看看项目中引人注目的pom.xml文件，我们看看里面的内容。我们会发现项目里有父项目，点进父项目，如下（只能截取一部分，你可以自行在Idea中打开）<br><img src=\"https://img-blog.csdnimg.cn/20200221163344517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>其实仔细一点的人就会发现，我们以前使用Maven创建SpringMVC之类的项目的时候，我们在导入依赖需要引入版本version，像下面这样</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.hibernate&#x2F;hibernate-core --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.hibernate&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;hibernate-core&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;5.4.12.Final&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>可是在我们的SpringBoot的pom.xml中，我们不需要引入相关依赖，这是因为有上面提到的parent，parent中有所有SpringBoot的依赖，这样，以后我们导入依赖默认是不需要写版本的，如果不在dependencles里面的管理依赖，自然需要我们写版本。</p>\n<h2 id=\"启动器starter\"><a href=\"#启动器starter\" class=\"headerlink\" title=\"启动器starter\"></a>启动器starter</h2><p>上一篇博文里说了，我们只需要导入spring-boot-starter-web依赖，SpringBoot就会自动帮我们导入web开发所需要的基本依赖包，包括Tomcat相关的依赖包，而spring-boot-starter-web就是属于我们所说的SpringBoot启动器，像这样的启动器有很多，我们可以去官方文档中看<a href=\"https://docs.spring.io/spring-boot/docs/2.2.1.RELEASE/reference/html/using-spring-boot.html#using-boot-starter\">传送条</a><br><img src=\"https://img-blog.csdnimg.cn/20200221164333947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如图，点击右边的pom，就会进去该启动器所包含的相关依赖。</p>\n<p>像spring-boot-starter-web这样的启动器正式一点，叫做springboot场景启动器，也就是帮我们导入相关开发场景下（比如spring-boot-starter-web就是web场景下的相关依赖）正常运行所需要的依赖。我们可以在导入不同的场景依赖，比如web开发导入spring-boot-starter-web，数据库jpa访问导入spring-boot-starter-data-jpa，想使用高级消息队列导入spring-boot-starter-amqp等等。</p>\n<p>Springboot将所有的功能场景都抽取出来，做成一个个starter（启动器），只要在项目里面引入这些starter相关场景的所有依赖导入进来，要用什么功能，就倒入什么场景启动器，是不是很方便。</p>\n<h2 id=\"主程序类（主入口类）\"><a href=\"#主程序类（主入口类）\" class=\"headerlink\" title=\"主程序类（主入口类）\"></a>主程序类（主入口类）</h2><p>我们现在来分析一下主程序类中的相关基本注解。</p>\n<h3 id=\"SpringBootApplication\"><a href=\"#SpringBootApplication\" class=\"headerlink\" title=\"@SpringBootApplication\"></a>@SpringBootApplication</h3><p>@SpringBootApplication是Springboot应用标注在某个类上，用来说明这个类是Springboot的入口类，springboot就应该运行在这个类的main方法来启动Springboot应用，我们按住Ctrl键点进去，我们主要看里面的注解<br><img src=\"https://img-blog.csdnimg.cn/20200221165401199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"SpringBootApplication-1\"><a href=\"#SpringBootApplication-1\" class=\"headerlink\" title=\"@SpringBootApplication\"></a>@SpringBootApplication</h3><p>在@SpringBootApplication中，一个比较关键的注解就是@SpringBootApplication，@SpringBootConfiguration是Springboot的配置类，标注在某个类上表示，这个类是一个Springboot的配置，同样按住Ctrl键点进去，我们我们会看到@SpringBootConfiguration中有一个@Configuration，这个注解就是标注各种配置类，配置文件的，如果我们对AOP有一定理解的话，其实就知道配置类也是容器中的一个组件，所以在@Configuration中，我们是通过@Component进行标注的。</p>\n<h3 id=\"EnableAutoConfiguration\"><a href=\"#EnableAutoConfiguration\" class=\"headerlink\" title=\"@EnableAutoConfiguration\"></a>@EnableAutoConfiguration</h3><p>回到@SpringBootApplication中，我会可以看到另一个配置类注解@EnableAutoConfiguration，这个注解是标注着开启自动配置功能，这样才使得我们不需要像以前在Spring项目中那样，配置繁琐的东西，而是Springboot会帮我们自动配置，@EnableAutoConfiguration告诉SoringBoot开启自动配置功能，这样启动配置才能生效，这个注解里面的东西，我们要好好讲一下，因为这样我们就会知道，SpringBoot是怎么帮我们自动加载相关依赖，并放置在哪里的，内容如下。<br><img src=\"https://img-blog.csdnimg.cn/20200221170214590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>@AutoConfigurationPackage自动配置包，Spring的底层注解@import，给容器导入一个组件，导入的组件由AutoConfigurationPackage.Registrar.class进行注册，其实AutoConfigurationPackage就是将主配置类（@SpringBootApplication标注的类）的所在包及下面所有的子包里面所有的组件扫描到Spring容器，如果你以前写过SpringMVC项目之类的会，就会知道，我们在写一些Controller或者Service的时候，我们需要在配置文件中，配置bean扫描controller的包名，而我们在SpringBoot就不需要了</p>\n<h3 id=\"AutoConfigurationImportSelector\"><a href=\"#AutoConfigurationImportSelector\" class=\"headerlink\" title=\"AutoConfigurationImportSelector\"></a>AutoConfigurationImportSelector</h3><p>在@AutoConfigurationPackage下面，我们会发现我们通过import导入了一个类，我们进入到这个类中，然后在下图位置debug一下这个类，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200221171052141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>就会发现，其实AutoConfigurationImportSelector就是给容器中导入非常多的自动配置类，也就是给容器中导入这个场景需要的所有组件，并配置好这些组件，有了自动配置类，就免去了我们手动编写配置注入功能组件等工作了。</p>\n<p>Springboot在启动的时候，从类路径下的META-INF/Spring.factories中获取EnableAutoConfiguration指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置的工作，如下图。<br><img src=\"https://img-blog.csdnimg.cn/20200221171206772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"快速创建SpringBoot项目\"><a href=\"#快速创建SpringBoot项目\" class=\"headerlink\" title=\"快速创建SpringBoot项目\"></a>快速创建SpringBoot项目</h2><p>前面我们创建一个项目是，先创建一个Maven项目，然后导入相关的Springboot依赖，但是其实我们可以在idea中，利用Idea创建项目，帮我们自动创建我们需要的相关依赖的springboot项目<br><img src=\"https://img-blog.csdnimg.cn/202002211717361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221172004325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200221172021952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>通过这一系列的操作，我们也创建出了一个已经导入依赖的SPringboot项目了，当然啦，如果还差什么，我们依旧可以在pom中导入依赖，所以其实没有差别，我个人喜欢直接创建一个Maven项目，需要什么就在官网上导入相关的依赖，这样方便我精简项目，以及项目使用了什么包心知肚明，我们在项目侧边栏lib中可以看到我们项目所有的依赖。<br><img src=\"https://img-blog.csdnimg.cn/2020022117230894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里要特别说明的是，我们使用idea创建的项目里面的resources下面，会帮我们自动创建了三个东西，我这里说明一下。</p>\n<ul>\n<li>static：保存所有的静态资源，什么js、css之类的都在这下面</li>\n<li>templates：保存所有的模板页面，Springboot默认jar包使用的是嵌入式的Tomcat，默认不支持JSP页面，所以在模板这里，我们可以使用模板引擎，比如freemaker、thymeleaf之类的</li>\n<li>application.properties，这个是SpringBoot的配置文件。前面我们知道SpringBoot有很多默认配置，我们就可以在这个文件夹修改默认配置，比如端口之类的。</li>\n</ul>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>这里我们算是对一个简单的HelloWord进行了了解，到此，我们也对SpringBoot庞大的技术世界说了一句HelloWord，那么下一篇博文，我将要讲讲SpringBoot的配置</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot"]},{"title":"详细SpringBoot教程之启动配置原理","url":"/Spring-Boot/a9afbec9ed9f/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"启动配置原理\"><a href=\"#启动配置原理\" class=\"headerlink\" title=\"启动配置原理\"></a>启动配置原理</h2><p>到这里，我们讲解的只是其实已经够我们开发一些比较小的SpringBoot应用了，但是我们随着在使用的过程中，可以对SpringBoot进行底层原理分析，这样我们才能真正将SpringBoot使用的得心应手，然后我们还可以自己开发自己的starter场景启动器，是不是很刺激，后面我们会将，而我这里首先要几个重要的事件回调机制</p>\n<p>我们之前也提到过，一般我们的配置核心是在各个配置的META-INF/spring.factories中，而我们SpringBoot的启动相关是</p>\n<ul>\n<li>ApplicationContextInitializer</li>\n<li>SpringApplicationRunListener</li>\n</ul>\n<p>只需要放在ioc容器中</p>\n<ul>\n<li>ApplicationRunner</li>\n<li>CommandLineRunner</li>\n</ul>\n<h3 id=\"启动流程\"><a href=\"#启动流程\" class=\"headerlink\" title=\"启动流程\"></a>启动流程</h3><h4 id=\"创建SpringApplication对象\"><a href=\"#创建SpringApplication对象\" class=\"headerlink\" title=\"创建SpringApplication对象\"></a>创建SpringApplication对象</h4><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">initialize(sources);</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">initialize</span><span class=\"params\">(Object[] sources)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//保存主配置类</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (sources != <span class=\"keyword\">null</span> &amp;&amp; sources.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.sources.addAll(Arrays.asList(sources));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//判断当前是否一个web应用</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.webEnvironment = deduceWebEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">//从类路径下找到META-INF/spring.factories配置的所有ApplicationContextInitializer；然后保存起来</span></span><br><span class=\"line\">    setInitializers((Collection) getSpringFactoriesInstances(</span><br><span class=\"line\">        ApplicationContextInitializer.class));</span><br><span class=\"line\">    <span class=\"comment\">//从类路径下找到ETA-INF/spring.factories配置的所有ApplicationListener</span></span><br><span class=\"line\">    setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));</span><br><span class=\"line\">    <span class=\"comment\">//从多个配置类中找到有main方法的主配置类</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.mainApplicationClass = deduceMainApplicationClass();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20200227193716553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200227193729701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"运行run方法\"><a href=\"#运行run方法\" class=\"headerlink\" title=\"运行run方法\"></a>运行run方法</h4><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ConfigurableApplicationContext <span class=\"title\">run</span><span class=\"params\">(String... args)</span> </span>&#123;</span><br><span class=\"line\">   StopWatch stopWatch = <span class=\"keyword\">new</span> StopWatch();</span><br><span class=\"line\">   stopWatch.start();</span><br><span class=\"line\">   ConfigurableApplicationContext context = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">   FailureAnalyzers analyzers = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">   configureHeadlessProperty();</span><br><span class=\"line\">    </span><br><span class=\"line\">   <span class=\"comment\">//获取SpringApplicationRunListeners；从类路径下META-INF/spring.factories</span></span><br><span class=\"line\">   SpringApplicationRunListeners listeners = getRunListeners(args);</span><br><span class=\"line\">    <span class=\"comment\">//回调所有的获取SpringApplicationRunListener.starting()方法</span></span><br><span class=\"line\">   listeners.starting();</span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">       <span class=\"comment\">//封装命令行参数</span></span><br><span class=\"line\">      ApplicationArguments applicationArguments = <span class=\"keyword\">new</span> DefaultApplicationArguments(</span><br><span class=\"line\">            args);</span><br><span class=\"line\">      <span class=\"comment\">//准备环境</span></span><br><span class=\"line\">      ConfigurableEnvironment environment = prepareEnvironment(listeners,</span><br><span class=\"line\">            applicationArguments);</span><br><span class=\"line\">       \t\t<span class=\"comment\">//创建环境完成后回调SpringApplicationRunListener.environmentPrepared()；表示环境准备完成</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      Banner printedBanner = printBanner(environment);</span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"comment\">//创建ApplicationContext；决定创建web的ioc还是普通的ioc</span></span><br><span class=\"line\">      context = createApplicationContext();</span><br><span class=\"line\">       </span><br><span class=\"line\">      analyzers = <span class=\"keyword\">new</span> FailureAnalyzers(context);</span><br><span class=\"line\">       <span class=\"comment\">//准备上下文环境;将environment保存到ioc中；而且applyInitializers()；</span></span><br><span class=\"line\">       <span class=\"comment\">//applyInitializers()：回调之前保存的所有的ApplicationContextInitializer的initialize方法</span></span><br><span class=\"line\">       <span class=\"comment\">//回调所有的SpringApplicationRunListener的contextPrepared()；</span></span><br><span class=\"line\">       <span class=\"comment\">//</span></span><br><span class=\"line\">      prepareContext(context, environment, listeners, applicationArguments,</span><br><span class=\"line\">            printedBanner);</span><br><span class=\"line\">       <span class=\"comment\">//prepareContext运行完成以后回调所有的SpringApplicationRunListener的contextLoaded（）；</span></span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"comment\">//s刷新容器；ioc容器初始化（如果是web应用还会创建嵌入式的Tomcat）；Spring注解版</span></span><br><span class=\"line\">       <span class=\"comment\">//扫描，创建，加载所有组件的地方；（配置类，组件，自动配置）</span></span><br><span class=\"line\">      refreshContext(context);</span><br><span class=\"line\">       <span class=\"comment\">//从ioc容器中获取所有的ApplicationRunner和CommandLineRunner进行回调</span></span><br><span class=\"line\">       <span class=\"comment\">//ApplicationRunner先回调，CommandLineRunner再回调</span></span><br><span class=\"line\">      afterRefresh(context, applicationArguments);</span><br><span class=\"line\">       <span class=\"comment\">//所有的SpringApplicationRunListener回调finished方法</span></span><br><span class=\"line\">      listeners.finished(context, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">      stopWatch.stop();</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.logStartupInfo) &#123;</span><br><span class=\"line\">         <span class=\"keyword\">new</span> StartupInfoLogger(<span class=\"keyword\">this</span>.mainApplicationClass)</span><br><span class=\"line\">               .logStarted(getApplicationLog(), stopWatch);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">       <span class=\"comment\">//整个SpringBoot应用启动完成以后返回启动的ioc容器；</span></span><br><span class=\"line\">      <span class=\"keyword\">return</span> context;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   <span class=\"keyword\">catch</span> (Throwable ex) &#123;</span><br><span class=\"line\">      handleRunFailure(context, listeners, analyzers, ex);</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException(ex);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"事件监听机制\"><a href=\"#事件监听机制\" class=\"headerlink\" title=\"事件监听机制\"></a>事件监听机制</h2><p>配置在META-INF/spring.factories</p>\n<p><strong>ApplicationContextInitializer</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloApplicationContextInitializer</span> <span class=\"keyword\">implements</span> <span class=\"title\">ApplicationContextInitializer</span>&lt;<span class=\"title\">ConfigurableApplicationContext</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">initialize</span><span class=\"params\">(ConfigurableApplicationContext applicationContext)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;ApplicationContextInitializer...initialize...&quot;</span>+applicationContext);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>SpringApplicationRunListener</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloSpringApplicationRunListener</span> <span class=\"keyword\">implements</span> <span class=\"title\">SpringApplicationRunListener</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//必须有的构造器</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">HelloSpringApplicationRunListener</span><span class=\"params\">(SpringApplication application, String[] args)</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">starting</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;SpringApplicationRunListener...starting...&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">environmentPrepared</span><span class=\"params\">(ConfigurableEnvironment environment)</span> </span>&#123;</span><br><span class=\"line\">        Object o = environment.getSystemProperties().get(<span class=\"string\">&quot;os.name&quot;</span>);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;SpringApplicationRunListener...environmentPrepared..&quot;</span>+o);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">contextPrepared</span><span class=\"params\">(ConfigurableApplicationContext context)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;SpringApplicationRunListener...contextPrepared...&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">contextLoaded</span><span class=\"params\">(ConfigurableApplicationContext context)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;SpringApplicationRunListener...contextLoaded...&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">finished</span><span class=\"params\">(ConfigurableApplicationContext context, Throwable exception)</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;SpringApplicationRunListener...finished...&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>配置（META-INF/spring.factories）</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">org.springframework.context.ApplicationContextInitializer</span>=<span class=\"string\">\\</span></span><br><span class=\"line\"><span class=\"attr\">com.atguigu.springboot.listener.HelloApplicationContextInitializer</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">org.springframework.boot.SpringApplicationRunListener</span>=<span class=\"string\">\\</span></span><br><span class=\"line\"><span class=\"attr\">com.atguigu.springboot.listener.HelloSpringApplicationRunListener</span></span><br></pre></td></tr></table></figure>\n\n<p>只需要放在ioc容器中</p>\n<p><strong>ApplicationRunner</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloApplicationRunner</span> <span class=\"keyword\">implements</span> <span class=\"title\">ApplicationRunner</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(ApplicationArguments args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;ApplicationRunner...run....&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>CommandLineRunner</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloCommandLineRunner</span> <span class=\"keyword\">implements</span> <span class=\"title\">CommandLineRunner</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(String... args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;CommandLineRunner...run...&quot;</span>+ Arrays.asList(args));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"自定义starter\"><a href=\"#自定义starter\" class=\"headerlink\" title=\"自定义starter\"></a>自定义starter</h2><p>我们在之前的SpringBoot中，引入依赖的starter是不是很舒服，那么我们现在来学着怎么开发自己的starter，其实也非常简单，具体操作如下，首先我们使用Idea创建一个空项目<br><img src=\"https://img-blog.csdnimg.cn/20200227213918186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后我们会到这里的界面，在这里，我们可以给空项目创建模块，我们如果要自己的启动器的话，那么最基本需要两个模块，不过是哪两个模块呢？这里我们要来讲讲。<br><img src=\"https://img-blog.csdnimg.cn/20200227214124299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>我们需要搞清楚两个问题，如下</p>\n<ul>\n<li><p>这个场景需要使用到的依赖是什么？</p>\n</li>\n<li><p>如何编写自动配置</p>\n</li>\n</ul>\n<p>我们找个starter来进行分析，如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span>  <span class=\"comment\">//指定这个类是一个配置类</span></span><br><span class=\"line\"><span class=\"meta\">@ConditionalOnXXX</span>  <span class=\"comment\">//在指定条件成立的情况下自动配置类生效</span></span><br><span class=\"line\"><span class=\"meta\">@AutoConfigureAfter</span>  <span class=\"comment\">//指定自动配置类的顺序</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span>  <span class=\"comment\">//给容器中添加组件</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@ConfigurationPropertie</span>  <span class=\"comment\">//结合相关xxxProperties类来绑定相关的配置</span></span><br><span class=\"line\"><span class=\"meta\">@EnableConfigurationProperties</span> <span class=\"comment\">//让xxxProperties生效加入到容器中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//自动配置类要能加载</span></span><br><span class=\"line\"><span class=\"comment\">//将需要启动就加载的自动配置类，配置在META-INF/spring.factories</span></span><br><span class=\"line\">org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\</span><br></pre></td></tr></table></figure>\n<p>现在我们</p>\n<p>所以我们应该符合以下模式：</p>\n<ul>\n<li><p>启动器只用来做依赖导入；</p>\n</li>\n<li><p>专门来写一个自动配置模块；</p>\n</li>\n<li><p>启动器依赖自动配置；别人只需要引入启动器（starter）</p>\n</li>\n<li><p>像mybatis-spring-boot-starter，我们命名应该自定义启动器名-spring-boot-starter</p>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200227230211216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200227230242807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>又到了我们熟悉的界面，我们要创建两个，一个是啥都不勾选的SpringBoot项目，还有一个只勾选web模块，创建好了之后，在pom.xml中进行如下更改</p>\n<p><strong>启动器模块</strong></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">project</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">         <span class=\"attr\">xmlns:xsi</span>=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">         <span class=\"attr\">xsi:schemaLocation</span>=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">modelVersion</span>&gt;</span>4.0.0<span class=\"tag\">&lt;/<span class=\"name\">modelVersion</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>com.atguigu.starter<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>atguigu-spring-boot-starter<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.0-SNAPSHOT<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">&lt;!--启动器--&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dependencies</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">&lt;!--引入自动配置模块--&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>com.atguigu.starter<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>atguigu-spring-boot-starter-autoconfigurer<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.0.1-SNAPSHOT<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">dependencies</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">project</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p><strong>自动配置模块</strong></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">project</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class=\"attr\">xmlns:xsi</span>=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">   <span class=\"attr\">xsi:schemaLocation</span>=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">modelVersion</span>&gt;</span>4.0.0<span class=\"tag\">&lt;/<span class=\"name\">modelVersion</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>com.atguigu.starter<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>atguigu-spring-boot-starter-autoconfigurer<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.0.1-SNAPSHOT<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">packaging</span>&gt;</span>jar<span class=\"tag\">&lt;/<span class=\"name\">packaging</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>atguigu-spring-boot-starter-autoconfigurer<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Demo project for Spring Boot<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">parent</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.boot<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-boot-starter-parent<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.5.10.RELEASE<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">relativePath</span>/&gt;</span> <span class=\"comment\">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">parent</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">properties</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class=\"tag\">&lt;/<span class=\"name\">project.build.sourceEncoding</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">project.reporting.outputEncoding</span>&gt;</span>UTF-8<span class=\"tag\">&lt;/<span class=\"name\">project.reporting.outputEncoding</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">java.version</span>&gt;</span>1.8<span class=\"tag\">&lt;/<span class=\"name\">java.version</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">properties</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">dependencies</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">&lt;!--引入spring-boot-starter；所有starter的基本配置--&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.boot<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-boot-starter<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">dependencies</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">project</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.atguigu.starter;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.context.properties.ConfigurationProperties;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@ConfigurationProperties(prefix = &quot;atguigu.hello&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloProperties</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String prefix;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String suffix;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getPrefix</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> prefix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPrefix</span><span class=\"params\">(String prefix)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.prefix = prefix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getSuffix</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> suffix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setSuffix</span><span class=\"params\">(String suffix)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.suffix = suffix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.atguigu.starter;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloService</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    HelloProperties helloProperties;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> HelloProperties <span class=\"title\">getHelloProperties</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> helloProperties;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setHelloProperties</span><span class=\"params\">(HelloProperties helloProperties)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.helloProperties = helloProperties;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">sayHellAtguigu</span><span class=\"params\">(String name)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> helloProperties.getPrefix()+<span class=\"string\">&quot;-&quot;</span> +name + helloProperties.getSuffix();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.atguigu.starter;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.context.properties.EnableConfigurationProperties;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@ConditionalOnWebApplication</span> <span class=\"comment\">//web应用才生效</span></span><br><span class=\"line\"><span class=\"meta\">@EnableConfigurationProperties(HelloProperties.class)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HelloServiceAutoConfiguration</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    HelloProperties helloProperties;</span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> HelloService <span class=\"title\">helloService</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        HelloService service = <span class=\"keyword\">new</span> HelloService();</span><br><span class=\"line\">        service.setHelloProperties(helloProperties);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> service;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>SpringBoot官网其实给了很多案例给我们参考，集成了很多模块的示例代码，SpringBoot放在<a href=\"https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples\">GitHub上</a>可以下载运行然后研究，下一遍对缓存进行讲解。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","启动配置","原理"]},{"title":"详细SpringBoot教程之数据访问","url":"/Spring-Boot/8fcfcb4d5ce9/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"SpringBoot与数据访问\"><a href=\"#SpringBoot与数据访问\" class=\"headerlink\" title=\"SpringBoot与数据访问\"></a>SpringBoot与数据访问</h2><p>对于数据访问层，无论是SQL还是NOSQL，SpringBoot默认采用整合 Spring Data的方式进行统一处理，添加大量自动配置，屏蔽了很多设置。在这其中，SpringBoot引入各种xxxTemplate，xxxRepository来简化我们对数据访问层的操作（使用SpringBoot的JPA方式，非常方便，后面会进行讲解）。对我们来说只需要进行简单的设置即可。我们将在数据访问章节测试使用SQL相关。接下来我们将讲解SpringBoot使用如下三个数据访问方式</p>\n<ul>\n<li>JDBC</li>\n<li>MyBatis</li>\n<li>JPA<h2 id=\"SpringBoot中使用JDBC\"><a href=\"#SpringBoot中使用JDBC\" class=\"headerlink\" title=\"SpringBoot中使用JDBC\"></a>SpringBoot中使用JDBC</h2>其实JDBC算是比较原始的数据库访问方式了，但不得不说，很多东西都是基于它进行封装的，而且还有非常多的项目喜欢使用JDBC，所以我们学习的时候，还是需要对JDBC相关进行了解，但是这里我们部队JDBC的原理进行讲解，因为已经超过了本博文的范畴了，所以我们这里只讲怎么使用，在SpringBoot中使用JDBC非常简单。这里我们使用Idea向导创建一个新项目，在勾选依赖的时候，除了照常勾选web模块之外，我们把mysql Dirver和JDBC勾选上<br><img src=\"https://img-blog.csdnimg.cn/20200227184213921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>创建项目之后的pom.xml包下的内容像下面这样，当然你也可以创建Maven项目，然后按照这个引入相关依赖<br><img src=\"https://img-blog.csdnimg.cn/20200227163447260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\"><br>项目创建成功之后，我们怎么使用呢？非常简单，我们只需要在主配置文件中进行一些相关的配置就可以了，比如说数据库账号密码之后的，这里要说的是我们使用的是mysql关系型数据库，如下<br><img src=\"https://img-blog.csdnimg.cn/20200227164122830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以在测试类中编写下面的代码进行测试，看一下是否连接成功，没有连接成功的话就会报错，正常运行就是连接成功。<br><img src=\"https://img-blog.csdnimg.cn/2020022716444331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200227164641150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\">这里我们可以看到控制台输出日志，我们可以知道，SpringBoot默认使用的是class com.zaxxer.hikari.HikariDataSource作为数据源，而数据源的相关配置都在DataSourceProperties里面。</li>\n</ul>\n<p>创建数据库的表，我们可以通过SpringBoot帮我们自动创建，如果我们直接在resources下创建sql且不进行任何配置文件中配置的话，我们就需要将sql进行特定的命名，才会生效，当然我们一般都会在主配置文件中进行配置的，如下</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">schema-*.sql、data-*.sql</span></span><br><span class=\"line\"><span class=\"attr\">默认规则：schema.sql，schema-all.sql；</span></span><br><span class=\"line\"><span class=\"meta\">可以使用</span>   <span class=\"string\"></span></span><br><span class=\"line\">\t<span class=\"attr\">schema</span>:<span class=\"string\"></span></span><br><span class=\"line\">      <span class=\"meta\">-</span> <span class=\"string\">classpath:department.sql</span></span><br><span class=\"line\">      <span class=\"attr\">指定位置</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20200227170257980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"自动配置原理\"><a href=\"#自动配置原理\" class=\"headerlink\" title=\"自动配置原理\"></a>自动配置原理</h3><p>首先按照惯例，还是找到jdbc的自动配置类，然后通过查阅自动配置类进行分析，而jdbc的自动配置类就是org.springframework.boot.autoconfigure.jdbc</p>\n<p>我们可以参考DataSourceConfiguration，根据配置创建数据源，默认使用hikari连接池，其中可以使用spring.datasource.type指定自定义的数据源类型。</p>\n<p>这里要提一下，SpringBoot默认可以支持的数据源如下，以前的SpringBoot1.x中，默认使用的是tomcat数据源，而在2.x版本中使用的是Hikari作为数据源</p>\n<ul>\n<li>org.apache.tomcat.jdbc.pool.DataSource</li>\n<li>HikariDataSource</li>\n<li>BasicDataSource</li>\n</ul>\n<p>当然，既然有默认的数据源，我们当然可以进行自定义数据源类型</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Generic DataSource configuration.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@ConditionalOnMissingBean(DataSource.class)</span></span><br><span class=\"line\"><span class=\"meta\">@ConditionalOnProperty(name = &quot;spring.datasource.type&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generic</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"meta\">@Bean</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">dataSource</span><span class=\"params\">(DataSourceProperties properties)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">//使用DataSourceBuilder创建数据源，利用反射创建响应type的数据源，并且绑定相关属性</span></span><br><span class=\"line\">      <span class=\"keyword\">return</span> properties.initializeDataSourceBuilder().build();</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"整合Druid数据源\"><a href=\"#整合Druid数据源\" class=\"headerlink\" title=\"整合Druid数据源\"></a>整合Druid数据源</h3><p>SpringBoot的数据源的话，其实我这里推荐使用2.x默认的Hikari数据源，因为速度快等等优点，具体啥优点可以自己去网上查找，但是这里为了演示，我就演示怎么切换阿里的Druid数据源（Druid数据源用的也挺多的，生态不错，不过感觉停更很久了）<br><img src=\"https://img-blog.csdnimg.cn/20200227170518106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>首先在项目中导入druid数据源，然后自己在config目录下创建一个专门用来控制druid的配置类，配置类的测试内容如下，在里面我配置了过滤器和监听器作为示例</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DruidConfig</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">druid</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">       <span class=\"keyword\">return</span>  <span class=\"keyword\">new</span> DruidDataSource();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//配置Druid的监控</span></span><br><span class=\"line\">    <span class=\"comment\">//1、配置一个管理后台的Servlet</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> ServletRegistrationBean <span class=\"title\">statViewServlet</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        ServletRegistrationBean bean = <span class=\"keyword\">new</span> ServletRegistrationBean(<span class=\"keyword\">new</span> StatViewServlet(), <span class=\"string\">&quot;/druid/*&quot;</span>);</span><br><span class=\"line\">        Map&lt;String,String&gt; initParams = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">        initParams.put(<span class=\"string\">&quot;loginUsername&quot;</span>,<span class=\"string\">&quot;admin&quot;</span>);</span><br><span class=\"line\">        initParams.put(<span class=\"string\">&quot;loginPassword&quot;</span>,<span class=\"string\">&quot;123456&quot;</span>);</span><br><span class=\"line\">        initParams.put(<span class=\"string\">&quot;allow&quot;</span>,<span class=\"string\">&quot;&quot;</span>);<span class=\"comment\">//默认就是允许所有访问</span></span><br><span class=\"line\">        initParams.put(<span class=\"string\">&quot;deny&quot;</span>,<span class=\"string\">&quot;192.168.15.21&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        bean.setInitParameters(initParams);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> bean;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2、配置一个web监控的filter</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> FilterRegistrationBean <span class=\"title\">webStatFilter</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        FilterRegistrationBean bean = <span class=\"keyword\">new</span> FilterRegistrationBean();</span><br><span class=\"line\">        bean.setFilter(<span class=\"keyword\">new</span> WebStatFilter());</span><br><span class=\"line\"></span><br><span class=\"line\">        Map&lt;String,String&gt; initParams = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">        initParams.put(<span class=\"string\">&quot;exclusions&quot;</span>,<span class=\"string\">&quot;*.js,*.css,/druid/*&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        bean.setInitParameters(initParams);</span><br><span class=\"line\"></span><br><span class=\"line\">        bean.setUrlPatterns(Arrays.asList(<span class=\"string\">&quot;/*&quot;</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span>  bean;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"SpringBoot整合MyBatis\"><a href=\"#SpringBoot整合MyBatis\" class=\"headerlink\" title=\"SpringBoot整合MyBatis\"></a>SpringBoot整合MyBatis</h2><p>第一步当然是先导入MyBatis的依赖，依赖如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.1.1&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>使用步骤：</p>\n<ul>\n<li>配置数据源相关属性<br><img src=\"https://img-blog.csdnimg.cn/20200227190346160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>给数据库建表</li>\n<li>创建JavaBean</li>\n</ul>\n<h3 id=\"如何使用注解使用Mybatis\"><a href=\"#如何使用注解使用Mybatis\" class=\"headerlink\" title=\"如何使用注解使用Mybatis\"></a>如何使用注解使用Mybatis</h3><p>首先在src下创建一个mapper包，然后包下创建对应实体的mapper接口，像下面这样</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//指定这是一个操作数据库的mapper</span></span><br><span class=\"line\"><span class=\"meta\">@Mapper</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">DepartmentMapper</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Select(&quot;select * from department where id=#&#123;id&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Department <span class=\"title\">getDeptById</span><span class=\"params\">(Integer id)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Delete(&quot;delete from department where id=#&#123;id&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">deleteDeptById</span><span class=\"params\">(Integer id)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Options(useGeneratedKeys = true,keyProperty = &quot;id&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Insert(&quot;insert into department(departmentName) values(#&#123;departmentName&#125;)&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">insertDept</span><span class=\"params\">(Department department)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Update(&quot;update department set departmentName=#&#123;departmentName&#125; where id=#&#123;id&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">updateDept</span><span class=\"params\">(Department department)</span></span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后我们只需要通过对应的配置文件，就可以使用，因为SpringBoot有着自定义MyBatis的配置规则，给容器中添加一个ConfigurationCustomizer，使用MapperScan批量扫描所有的Mapper接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@org</span>.springframework.context.annotation.Configuration</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyBatisConfig</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> ConfigurationCustomizer <span class=\"title\">configurationCustomizer</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ConfigurationCustomizer()&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">customize</span><span class=\"params\">(Configuration configuration)</span> </span>&#123;</span><br><span class=\"line\">                configuration.setMapUnderscoreToCamelCase(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@MapperScan(value = &quot;com.atguigu.springboot.mapper&quot;)</span></span><br><span class=\"line\"><span class=\"meta\">@SpringBootApplication</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SpringBoot06DataMybatisApplication</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">\t\tSpringApplication.run(SpringBoot06DataMybatisApplication.class, args);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用配置文件\"><a href=\"#使用配置文件\" class=\"headerlink\" title=\"使用配置文件\"></a>使用配置文件</h3><figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mybatis:</span></span><br><span class=\"line\">  <span class=\"attr\">config-location:</span> <span class=\"string\">classpath:mybatis/mybatis-config.xml</span> <span class=\"string\">指定全局配置文件的位置</span></span><br><span class=\"line\">  <span class=\"attr\">mapper-locations:</span> <span class=\"string\">classpath:mybatis/mapper/*.xml</span>  <span class=\"string\">指定sql映射文件的位置</span></span><br></pre></td></tr></table></figure>\n<p>更多使用参照Mybatis的对SpringBoot的<a href=\"http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/\">官方文档</a></p>\n<h2 id=\"SpringBoot整合SpringData-JPA\"><a href=\"#SpringBoot整合SpringData-JPA\" class=\"headerlink\" title=\"SpringBoot整合SpringData JPA\"></a>SpringBoot整合SpringData JPA</h2><p>SpringData为我们提供使用统一的API来对数据访问层进行操作；这主要是Spring Data Commons项目来实现的。Spring Data Commons让我们在使用关系型或者非关系型数据访问 技术时都基于Spring提供的统一标准，标准包含了CRUD（创建、获取、更新、删除）、查询、 排序和分页的相关操作。<br><img src=\"https://img-blog.csdnimg.cn/20200227193533503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>而且SpringDataJPA有着统一的Repository接口如Repository&lt;T, ID extends Serializable&gt;统一接口、 RevisionRepository&lt;T, ID extends Serializable, N extends Number &amp; Comparable<N>&gt;基于乐观 锁机制、CrudRepository&lt;T, ID extends Serializable&gt;基本CRUD操作、PagingAndSortingRepository&lt;T, ID extends Serializable&gt;：基本CRUD及分页</p>\n<h3 id=\"整合SpringData-JPA\"><a href=\"#整合SpringData-JPA\" class=\"headerlink\" title=\"整合SpringData JPA\"></a>整合SpringData JPA</h3><p>还是老样子，先引入我们的依赖，引入依赖之后，和原先的数据访问方式不同的是，我们可以编写一个实体类（bean）和数据表进行映射，并且配置好映射关系；</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//使用JPA注解配置映射关系</span></span><br><span class=\"line\"><span class=\"meta\">@Entity</span> <span class=\"comment\">//告诉JPA这是一个实体类（和数据表映射的类）</span></span><br><span class=\"line\"><span class=\"meta\">@Table(name = &quot;tbl_user&quot;)</span> <span class=\"comment\">//@Table来指定和哪个数据表对应;如果省略默认表名就是user；</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Id</span> <span class=\"comment\">//这是一个主键</span></span><br><span class=\"line\">    <span class=\"meta\">@GeneratedValue(strategy = GenerationType.IDENTITY)</span><span class=\"comment\">//自增主键</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Integer id;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Column(name = &quot;last_name&quot;,length = 50)</span> <span class=\"comment\">//这是和数据表对应的一个列</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String lastName;</span><br><span class=\"line\">    <span class=\"meta\">@Column</span> <span class=\"comment\">//省略默认列名就是属性名</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String email;</span><br></pre></td></tr></table></figure>\n<p>创建好实体类并配置好映射关系之后，我们接着编写一个Dao接口来操作实体类对应的数据表（Repository）</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//继承JpaRepository来完成对数据库的操作</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">UserRepository</span> <span class=\"keyword\">extends</span> <span class=\"title\">JpaRepository</span>&lt;<span class=\"title\">User</span>,<span class=\"title\">Integer</span>&gt; </span>&#123;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>注意了，我们jpa默认使用的hibernate，所以我们可以在配置文件中进行如下基本的配置</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">spring:</span>  </span><br><span class=\"line\"> <span class=\"attr\">jpa:</span></span><br><span class=\"line\">    <span class=\"attr\">hibernate:</span></span><br><span class=\"line\"><span class=\"comment\">#     更新或者创建数据表结构</span></span><br><span class=\"line\">      <span class=\"attr\">ddl-auto:</span> <span class=\"string\">update</span></span><br><span class=\"line\"><span class=\"comment\">#    控制台显示SQL</span></span><br><span class=\"line\">    <span class=\"attr\">show-sql:</span> <span class=\"literal\">true</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>这里我们大致介绍了SpringBoot的数据访问方式，带大家了解了一些基本的操作，更具体的还需要对相关数据访问方式进行学习，下一篇博文我们将讲解SpringBoot的启动原理。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","数据访问"]},{"title":"详细SpringBoot教程之日志框架","url":"/Spring-Boot/9fd545d5be72/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"先通俗来理解日志\"><a href=\"#先通俗来理解日志\" class=\"headerlink\" title=\"先通俗来理解日志\"></a>先通俗来理解日志</h2><p>假设现在我们没有日志框架，那么这个时候我们有一个需求，开发一个大型的系统，那么在我们开发的过程中，我们需要检查一些代码的正确与否、是否正常运行、监控代码运行的状况，这时候我们怎么做？当然，我们在需要检查的地方使用System.out.print(“”)进行输出相关信息，当然，我们也可以控制流输出到一个文件中，不过这个方式有个问题就是，如果我们需要改动一些输出的格式或者信息，那么我们又要到项目代码的各个地方去修改，非常的麻烦，那么这个时候我们会想到进行改进。</p>\n<p>如果改进呢，就是专门写一些输出信息的方法，然后把这些方法打包成一个jar进行引用，这样做是的我们可以对使用的方法进行统一的修改，而且还使得我们可以在其他的项目中接着使用，我们可以说已经有一个日志框架的雏形了。不过这个时候我们又有一些想法，既然已经可以输出一些日志信息了，那么我们想要增加几个高大上的功能，比如异步模式、自动扫描等等功能。这个时候我们对jar进行修改升级，加入了这些功能，这样我们的日志框架有有点样子了，可以说是一个正儿八经的日志框架了。</p>\n<p>不过不要高兴的太早，正儿八经的日志框架并不代表成熟，我们还有一个问题需要解决，就是如果我们想要对正在使用的jar包API进行修改升级，这个时候难道我们需要对项目中使用到的地方全部 进行修改么？这样当然不行，所以这个时候日志框架就需要分层，即抽象成和实现层，我们可以像JDBC那样拥有统一的接口层，底层无论使用什么数据库都没关系，我们都可以使用。完成了抽象层和接口层的分离，这个时候的日志框架算是一个成熟的日志框架了。</p>\n<h2 id=\"SpringBoot日志框架选择\"><a href=\"#SpringBoot日志框架选择\" class=\"headerlink\" title=\"SpringBoot日志框架选择\"></a>SpringBoot日志框架选择</h2><p>开发中存在非常多的日志框架，JUL（java.util.logging），JCL（Apche Commons Logging），Log4j， Log4j2，Logback，SLF4j，jboss-logging等。SpringBoot在框架内容部分使用JCL，spring-boot-starter-logging采用了slf4j+logback的形式，SpringBoot也能自动适配（jul，log4j2，logback）并简化配置，可以进行如下分类<br>| 日志的抽象层 | 日志实现 |<br>|–|–|<br>| JCL（Jakarta Commons Logging） SLF4j（Simple Logging Facade for Java） jboss-logging | Log4j JUL（java.util.logging） Log4j2 Logback |</p>\n<p>我们要使用SpringBoot的日志框架，也就是说我们需要一个抽象层和一个实现层，我们这里选择SLF4j作为抽象层，logback作为实现层。这里要说一下的是，SpringBoot底层是Spring框架，Spring框架默认的是JCL作为抽象层。</p>\n<h2 id=\"SLF4j使用\"><a href=\"#SLF4j使用\" class=\"headerlink\" title=\"SLF4j使用\"></a>SLF4j使用</h2><p>（我们创建新项目中已经帮我们导入了slf4j和logback）如何在系统中使用SLF4j？在开发的时候不应该直接使用日志实现类，应该使用日志的抽象层。具体参考 SLF4J 官方。下图是 SLF4J 结合各种日志框架的官方示例，从图中可以清晰的看出 SLF4J API 永远作为日志的门面，直接应用与应用程序中。<br><a href=\"http://www.slf4j.org/manual.html\">slf4j官网用户手册</a><br><img src=\"https://img-blog.csdnimg.cn/20200223114438100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>同时 SLF4 官方给出了简单示例。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">import org.slf4j.Logger;</span><br><span class=\"line\">import org.slf4j.LoggerFactory;</span><br><span class=\"line\"></span><br><span class=\"line\">public class HelloWorld &#123;</span><br><span class=\"line\">  public static void main(String[] args) &#123;</span><br><span class=\"line\">    Logger logger = LoggerFactory.getLogger(HelloWorld.class);</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Hello World&quot;</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>需要注意的是，要为系统导入 SLF4J 的 jar 和 日志框架的实现 jar。由于每一个日志的实现框架都有自己的配置文件，所以在使用 SLF4 之后，配置文件还是要使用实现日志框架的配置文件。</p>\n<h2 id=\"思考一个问题\"><a href=\"#思考一个问题\" class=\"headerlink\" title=\"思考一个问题\"></a>思考一个问题</h2><p>是什么问题呢？这么描述，我们想要写一个SpringBoot项目，日志框架组合我们打算使用Slf4j+logback，然后项目中集成了Spring，Hibernate，MyBatis等等组件依赖，这个时候会有一个问题，就是我们集成的组件框架中存在自己的日志框架，比如Spring自带commons-logging、Hibernate（jboss-logging）等等，那么这个时候，我们的项目里面就像一个日志框架的大杂烩一样，非常的乱，那么我们就需要给我们项目中日志框架做统一，也就是说，不管我依赖的组件自带了什么日志框架，我只要配置我的Slf4j+logback就可以了。<br>问题当然也有解决办法，这里我们来看看<a href=\"http://www.slf4j.org/legacy.html\">官方文档</a><br><img src=\"https://img-blog.csdnimg.cn/20200223115214323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>也就是说，我们需要分别引入我们组件日志的覆盖层，因为我们想要去除组件中的日志，不可能跑去删除组件的日志框架包，这样会导致组件崩溃，底层源码都被该了还不崩溃，所以我们在删除其他组件的日志框架jar包的时候，需要导入一层覆盖层jar包，这个jar中依旧有原来组件的日志框架的API，组件运行就不会报错，就相当于多做了一层适配，我们调用Slf4j+logback，然后通过这层适配层调用各组件的日志框架。</p>\n<h3 id=\"步骤总结：\"><a href=\"#步骤总结：\" class=\"headerlink\" title=\"步骤总结：\"></a>步骤总结：</h3><p>排除系统中的其他日志框架。<br>使用中间包替换要替换的日志框架。<br>导入我们选择的 SLF4J 实现。</p>\n<h2 id=\"SpringBoot日志关系\"><a href=\"#SpringBoot日志关系\" class=\"headerlink\" title=\"SpringBoot日志关系\"></a>SpringBoot日志关系</h2><p>我们从我们新创建的项目出发要阐述和研究，这样更方便我们理解和说明，首先我们来看我们新项目中的pom.xml文件中的内容，我们是一个新项目，什么都没引入，这些都是自动引入的内容。<br><img src=\"https://img-blog.csdnimg.cn/20200223121218143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>发现pom.xml中引入了starter启动器，其实每个启动器都会引入大量的依赖，那么我们怎么查看呢，有两种方法，第一种如下<br><img src=\"https://img-blog.csdnimg.cn/20200223121320149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>从上面的分析，Spring Boot 对日志框架的使用已经是清晰明了了，我们也可以使用 IDEA 工具查看 Maven 依赖关系，可以清晰的看到日志框架的引用。<br><img src=\"https://img-blog.csdnimg.cn/2020022312141669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200223121910280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可是话的依赖关系是不是眼前一亮，现在，我们可以来看spring-boot-starter这里，找到spring-boot-starter-logging（每个节点都可以双击点击，点击将进入对应的自动配置类）<br><img src=\"https://img-blog.csdnimg.cn/20200223122124764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>进入到spring-boot-starter-logging中，可以看到如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;2.2.4.RELEASE&lt;/version&gt;</span><br><span class=\"line\">  &lt;scope&gt;compile&lt;/scope&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>这就是SpringBoot的日志使用，我们前面博文说过，每一个starter都是一个场景启动器，而这个就是我们SpringBoot用于日志场景的启动器，里面有非常多的依赖<br><img src=\"https://img-blog.csdnimg.cn/20200223122800324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"关系总结\"><a href=\"#关系总结\" class=\"headerlink\" title=\"关系总结\"></a>关系总结</h3><p>SpringBoot底层也是使用slf4j+logback的方式进行日志记录，SpringBoot也把其他日志框架替换成了slf4j，默认的中间替换包如下<br><img src=\"https://img-blog.csdnimg.cn/20200223123400280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><strong>如果我们要引入其他的框架，一定要把这个框架的默认日志依赖都移除掉。</strong></p>\n<p>SpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要把这个框架的依赖的日志框架移除掉。</p>\n<h2 id=\"日志使用\"><a href=\"#日志使用\" class=\"headerlink\" title=\"日志使用\"></a>日志使用</h2><h3 id=\"logging-level\"><a href=\"#logging-level\" class=\"headerlink\" title=\"logging.level\"></a>logging.level</h3><p>用我们新创建的项目直接运行，SpringBoot是帮我们配置了日志的。我们现在测试类中写入如下内容，说明一下的是，这里的顺序是日志的级别，由低到高。我们可以调整输出日志的级别，日志就只会在这个级别及以后的高级别生效。我们先不调整，运行一下这个测试类<br><img src=\"https://img-blog.csdnimg.cn/20200223152731498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br> 默认是输出info级别及以上的日志，也就是说info是我们的root级别，当然我们可以在主配置文件中修改，如下<br> <img src=\"https://img-blog.csdnimg.cn/20200223153701391.png#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200223154003582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"logging-file-name\"><a href=\"#logging-file-name\" class=\"headerlink\" title=\"logging.file.name\"></a>logging.file.name</h3><p><img src=\"https://img-blog.csdnimg.cn/20200223154415463.png#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200223154500763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然我们还可以使用logging.path指定我们日志输出的目录，日志文件默认为spring.log。</p>\n<h3 id=\"logging-pattern-console\"><a href=\"#logging-pattern-console\" class=\"headerlink\" title=\"logging.pattern.console\"></a>logging.pattern.console</h3><p><img src=\"https://img-blog.csdnimg.cn/20200223155331285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>日志的输出格式如下</p>\n<ul>\n<li>%d表示日期时间</li>\n<li>%thread表示线程名</li>\n<li>%-5level级别从左显示5个字符串</li>\n<li>%logger{50}表示从logger名字最长50个字符，否则按照句点分隔</li>\n<li>%n换行符</li>\n</ul>\n<p>不过这其实是日志的一小部分功能，如果我们还需要使用异步日志等功能的话，那么我们可能需要专门进行编写日志的配置文件，可能一听到编写日志配置文件就头疼，其实不要担心，不难的，<a href=\"https://docs.spring.io/spring-boot/docs/2.2.4.RELEASE/reference/html/spring-boot-features.html#boot-features-custom-log-levels\">官方日志配置文件说明</a><br><img src=\"https://img-blog.csdnimg.cn/20200223160345530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>也就是我们使用的是logback，所以我们可以创建一个logback.xml放在resources目录下，里面写着我们日志配置的相关内容，没错，直接创建就可以了，不需要配置什么，SpringBoot就会自动识别（主要要和官网的命名一样哦）</p>\n<p>这里值得说明一下的是，我们可以创建另一种命名logback-spring.xml，这个文件依然可以被SpringBoot自动识别，但是和logback.xml的区别就是，在这个里面我们可以使用SpringBoot的高级profile功能（profile就是我们之前说的环境分发）<br><img src=\"https://img-blog.csdnimg.cn/202002231612559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"切换日志框架\"><a href=\"#切换日志框架\" class=\"headerlink\" title=\"切换日志框架\"></a>切换日志框架</h2><p>如果我们想要切换日志框架（当然不建议哈，logback挺好用的，这里只是教你如果有需要的话怎么去做，依旧是打开Diagrams，然后进行如下步骤）<br><img src=\"https://img-blog.csdnimg.cn/20200223161947519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后在依赖中就会被排除掉，这个时候我们只要在pom.xml中引入我们想要换的那个日志实现层就可以了，切换的依据记得按照上面说的各种日志框架的配置使用，不要切换错了哦。</p>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>SpringBoot的日志框架的相关基础知识我们就讲到这里，写一篇博文我们将要正式开始写一个web应用，体验一下SpringBoot的代码乐趣。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","日志框架"]},{"title":"详细SpringBoot教程之配置文件（一）","url":"/Spring-Boot/8f1135d292e8/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h2><p>SpringBoot可以使用全局配置或者其他配置文件，并且我们在SpringBoot开发中会经常需要去配置某些模块的配置，这样有利于我们业务分离，理清逻辑关系。这个时候可能会说了，不是说SpringBoot的优点之一就是省去了繁琐的配置，为什么这里又要学配置呢？哈哈哈，省去了繁琐的配置是因为SpringBoot帮我们把所有配置弄好了默认，如果我们需要更改一些默认配置，当然需要我们自己手动更改呀。对于一些分布式大型项目，我们需要分各种端口，分各种环境，这些都需要我们修改。而且我们可能需要对可以重复利用的模块进行封装，然后在新项目中作为默认配置进新项目的时候，也需要知道如何进行SpringBoot的配置，所以还是很重要的。</p>\n<p>说了这么多，我们开始来将SpringBoot的配置文件吧，在SpringBoot中配置文件有两种，一种是application.properties，另一种是application.yml，其中yml配置文件是在properties之后才有的。配置文件放在src/main/resources 目录或者类路径/config下，配置文件的作用，就是修改SpringBoot自动配置的默认值，SpringBoot底层都给我们自动配置好了很多默认值，我们就需要通过灵活的使用properties和yml来进行修改默认配置。</p>\n<p>我们通过Idea向导帮我们创建的新项目，在Resources目录下面会帮我们自动生成一个properties配置文件，如下图<br><img src=\"https://img-blog.csdnimg.cn/2020022217204759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在resources目录下我们可以暂时手动再创建一个application.yml文件，为了比较学习他们的区别（说明一下的就是，二者可以同时存在，不过最终只会运行一个，一般默认运行application.properties，不过我们可以通过配置，在application.properties中同时应用yml，达到多个配置文件的目的），依旧在resources目录下，直接右键new-&gt;File，创建一个名为application.yml的文件，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200222172330728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>YAML 是 “YAML Ain’t a Markup Language”（YAML 不是一种标记语言）。在开发的这种语言时，YAML 的意思其实是：”Yet Another Markup Language”（仍是一种标记语言）。它使用空白符号缩进和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件，即以数据为中心，比json、xml等更加适合做配置文件，以前的项目大都是使用xml作为配置文件的。在全局配置文件中，可以对一些默认配置值进行修改，比如我们修改默认的端口号，我们如果使用xml作为配置文件，配置端口可能会是如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;server&gt;</span><br><span class=\"line\">\t&lt;port&gt;8090&lt;/port&gt;</span><br><span class=\"line\">&lt;/server&gt;</span><br></pre></td></tr></table></figure>\n<p>发现了没有，xml会浪费大量的内容给标签的开闭上，那如果我们在SpringBoot项目中使用properties和yml设置端口，又该如何呢，如下，左边是yml，右边是properties，两者看起来是不是都很简略<br><img src=\"https://img-blog.csdnimg.cn/202002221737225.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"YAML语法\"><a href=\"#YAML语法\" class=\"headerlink\" title=\"YAML语法\"></a>YAML语法</h2><p>properties我们常见，我这里说一下YAML的一些基本语法，想要深入的可以直接到网上查，应该有很多详细的文档，我这里只是说一下基本语法，方便我们后面的讲解。</p>\n<h3 id=\"基本语法\"><a href=\"#基本语法\" class=\"headerlink\" title=\"基本语法\"></a>基本语法</h3><ul>\n<li>大小写敏感</li>\n<li>使用缩进表示层级关系</li>\n<li>缩进不允许使用tab，只允许空格</li>\n<li>缩进的空格数不重要，只要相同层级的元素左对齐即可</li>\n<li>‘#’表示注释</li>\n</ul>\n<h3 id=\"数据类型\"><a href=\"#数据类型\" class=\"headerlink\" title=\"数据类型\"></a>数据类型</h3><p>YAML 支持以下几种数据类型：</p>\n<ul>\n<li>对象：键值对的集合，又称为映射/ 哈希 / 字典</li>\n<li>数组：一组按次序排列的值，又称为序列 / 列表</li>\n<li>字面量：单个的、不可再分的值</li>\n</ul>\n<h3 id=\"字面量写法\"><a href=\"#字面量写法\" class=\"headerlink\" title=\"字面量写法\"></a>字面量写法</h3><p>字面量是最基本的，不可再分的值，包括：字符串、布尔值、整数、浮点数、时间、Null、日期。使用一个例子来快速了解字面量的基本使用：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">boolean: </span><br><span class=\"line\">    - TRUE  <span class=\"comment\">#true,True都可以</span></span><br><span class=\"line\">    - FALSE  <span class=\"comment\">#false，False都可以</span></span><br><span class=\"line\"><span class=\"comment\">#例子：isEmpty: false</span></span><br><span class=\"line\"><span class=\"built_in\">float</span>:</span><br><span class=\"line\">    - 3.14</span><br><span class=\"line\">    - 6.8523015e+5  <span class=\"comment\">#可以使用科学计数法</span></span><br><span class=\"line\"><span class=\"comment\">#例子：value: 3.14</span></span><br><span class=\"line\">int:</span><br><span class=\"line\">    - 123</span><br><span class=\"line\">    - 0b1010_0111_0100_1010_1110    <span class=\"comment\">#二进制表示</span></span><br><span class=\"line\"><span class=\"comment\">#例子：value: 123</span></span><br><span class=\"line\">null:</span><br><span class=\"line\">    nodeName: <span class=\"string\">&#x27;node&#x27;</span></span><br><span class=\"line\">    parent: ~  <span class=\"comment\">#使用~表示null</span></span><br><span class=\"line\"><span class=\"comment\">#例子：value: ~</span></span><br><span class=\"line\">string:</span><br><span class=\"line\">    - 哈哈</span><br><span class=\"line\">    - <span class=\"string\">&#x27;Hello world&#x27;</span>  <span class=\"comment\">#可以使用双引号或者单引号包裹特殊字符</span></span><br><span class=\"line\">    - newline</span><br><span class=\"line\">      newline2    <span class=\"comment\">#字符串可以拆成多行，每一行会被转化成一个空格</span></span><br><span class=\"line\"><span class=\"comment\">#例子：value: 哈哈</span></span><br><span class=\"line\">date:</span><br><span class=\"line\">    - 2018-02-17    <span class=\"comment\">#日期必须使用ISO 8601格式，即yyyy-MM-dd</span></span><br><span class=\"line\"><span class=\"comment\">#例子：value: 2018-02-17</span></span><br><span class=\"line\">datetime: </span><br><span class=\"line\">    -  2018-02-17T15:02:31+08:00    <span class=\"comment\">#时间使用ISO 8601格式，时间和</span></span><br><span class=\"line\">                              <span class=\"comment\">#日期之间使用T连接，最后使用+代表时区</span></span><br></pre></td></tr></table></figure>\n<p>这里多说一些字符串，字符串默认不用加上单引号或者双引号。其中，双引号不会转移字符串里面的特殊字符，特殊字符会作为本身祥表示的意思，比如<br>name: “I am \\n dbc”  输出 I am 换行 dbc<br>而单引号则会转义字符串里面的特殊字符，特殊字符最终只是一个普通的字符串数据，比如<br>name: “I am \\n dbc”  输出 I am \\n dbc</p>\n<h3 id=\"YAML-数组\"><a href=\"#YAML-数组\" class=\"headerlink\" title=\"YAML 数组\"></a>YAML 数组</h3><p>以 - 开头的行表示构成一个数组：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">num:</span><br><span class=\"line\"> - A</span><br><span class=\"line\"> - B</span><br><span class=\"line\"> - C</span><br></pre></td></tr></table></figure>\n<p>YAML 支持多维数组，可以使用行内表示：key: [value1, value2, …]</p>\n<h3 id=\"YAML-对象\"><a href=\"#YAML-对象\" class=\"headerlink\" title=\"YAML 对象\"></a>YAML 对象</h3><p>对象键值对使用冒号结构表示 key: value，冒号后面要加一个空格。<br>也可以使用 key:{key1: value1, key2: value2, …}。还可以使用缩进表示层级关系，如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">key: </span><br><span class=\"line\">    child-key: value</span><br><span class=\"line\">    child-key2: value2</span><br></pre></td></tr></table></figure>\n<p>较为复杂的对象格式，可以使用问号加一个空格代表一个复杂的 key，配合一个冒号加一个空格代表一个 value：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">?  </span><br><span class=\"line\">    - complexkey1</span><br><span class=\"line\">    - complexkey2</span><br><span class=\"line\">:</span><br><span class=\"line\">    - complexvalue1</span><br><span class=\"line\">    - complexvalue2</span><br></pre></td></tr></table></figure>\n<p>意思即对象的属性是一个数组 [complexkey1,complexkey2]，对应的值也是一个数组 [complexvalue1,complexvalue2]<br>YMAL基本语法先讲到这里，需要深入了解的自行查阅其他教程。</p>\n<h2 id=\"配置文件注入简单案例\"><a href=\"#配置文件注入简单案例\" class=\"headerlink\" title=\"配置文件注入简单案例\"></a>配置文件注入简单案例</h2><p>这里我们在application.yml中写入一些数据，就是写一个类的属性值以及对应的数值，如下<br><img src=\"https://img-blog.csdnimg.cn/20200222180708544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为了方便，我也把内容粘贴出来</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">person:</span><br><span class=\"line\">  lastName: dbc</span><br><span class=\"line\">  age: 22</span><br><span class=\"line\">  boss: <span class=\"literal\">false</span></span><br><span class=\"line\">  birth: 2017/12/12</span><br><span class=\"line\">  maps: &#123;k1: v2,k2: ;2&#125;</span><br><span class=\"line\">  lists:</span><br><span class=\"line\">   - lisi</span><br><span class=\"line\">   - dbc</span><br><span class=\"line\">  dog:</span><br><span class=\"line\">   name: 小狗</span><br><span class=\"line\">   age: 12</span><br></pre></td></tr></table></figure>\n<p>写好了之后，我们创建一个Person类和一个Dog类，类中的属性值和配置文件中的属性值要一一对应，并在Person的类上加上@ConfigurationProperties注解，注解中的前缀填写我们在配置文件中的person，如下<br><img src=\"https://img-blog.csdnimg.cn/20200222181024192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222181146591.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>因为我们需要将配置文件中配置的每一个属性值，映射到组件中，所以我们在Person类上添加一个@ConfigurationProperties，意思是告诉SpringBoot将本类中所有属性和配置文件中相关的配置进行绑定，prefix意思就是配置映射关系。这个时候发现编辑器提示如下<br><img src=\"https://img-blog.csdnimg.cn/20200222181337829.png#pic_center\" alt=\"在这里插入图片描述\"><br>我这里直接把依赖粘贴出来，然后把这段依赖粘贴进pom.xml文件中，Idea会自动加载依赖，加载完依赖之后，原本红色的提示，变成 了绿色即可，如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;optional&gt;<span class=\"literal\">true</span>&lt;/optional&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20200222181645652.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在，完成了依赖的导入，也完成了配置文件的映射（即将配置文件和类进行了关联映射），映射之后，我们成这个映射为组件，不过现在我们只是完成了组件映射，我们还需要将这个组件注入到Spring容器中，因为只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能获取，所以我们还需要添加@Component，把类组件加到容器中，如图<br><img src=\"https://img-blog.csdnimg.cn/20200222182033549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"测试案例\"><a href=\"#测试案例\" class=\"headerlink\" title=\"测试案例\"></a>测试案例</h2><p>这样就完成了一个组件的注入，我们可以在容器中使用这个组件了，接下来我们进行单元测试了，细心的人会发现，我们使用Idea创建的SpringBoot项目中，在test目录下有一个java文件夹，被Idea标为绿色（如下），这个文件夹就是单元测试用的，当然我们手动创建的Maven项目，也可以手动配置，很简单，创建项目属性就可以了。接下来我们讲一下单元测试中的内容，如何使用<br><img src=\"https://img-blog.csdnimg.cn/20200222182257394.png#pic_center\" alt=\"在这里插入图片描述\"><br>SpringBoot 的单元测试 ,可以在测试期间很方便的通过类似编码一样的方式，进行自动注入容器的功能，我们进入到那个DemoApplicationTests里面，其中有一个注解@SpringBootTest ，它标注是SpringBoot的单元测试。接着我们只需要在测试类中注入我们刚刚编写好的Person类，然后在方法中输出一下就可以了，毕竟我们只是检查一下我们编写的配置文件有没有成功配置好。执行之后在下方的日志中我们可以检查输出，测试类的内容和输出的日志结果如下图<br><img src=\"https://img-blog.csdnimg.cn/20200222182635867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2020022218270061.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"使用properties完成案例\"><a href=\"#使用properties完成案例\" class=\"headerlink\" title=\"使用properties完成案例\"></a>使用properties完成案例</h2><p>我们前面完成了使用yml类型的配置文件，进行映射注入，现在我们来试试使用properties类型的配置文件进行映射注入，其实不难，我们可以先将yml配置文件中的内容注释掉（当然不注释也可以，因为默认是读取properties）。我们开始编写properties里面的内容，会发现我们在properties中编写的时候，已经有person的提示了，是因为我们已经建类组件注入容器了，现在我们在里面编写内容如下<br><img src=\"https://img-blog.csdnimg.cn/20200222183223947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>流程和yml中写的一样，获取配置文件值注入，导入处理器，注入写一个类，然后使用@ConfigurationProperties注解，注意了还得把类注入容器组件@Component<br>（在这里我们只要把内容写进properties就可以了，类、映射和注入在测试yml的时候已经写好了）。编写好之后运行测试，发现依旧能够读取到，结果如下<br><img src=\"https://img-blog.csdnimg.cn/20200222183531224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>细心的人会发现，我们输出的结果出现了乱码，而出现乱码的原因是因为我们Idea默认的是utf-8编码，而properties用的是ASCII码，我们只要在Idea中将文件编码设置成Utf-8就可以了，如下<br><img src=\"https://img-blog.csdnimg.cn/20200222183650417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222183710548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>写到这里篇幅有点长了，关于配置的剩下内容，我将在下一篇博文中进行讲解，可以先把上面学习的进行消化，加油，奥利给。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","配置文件"]},{"title":"轻松搞定SpringBoot的邮件服务","url":"/Spring-Boot/f76f168ee368/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>我们都知道，如果系统出现预警，或者有一些监控需求，我们可以通过发送短信或者邮件来进行通知，本篇文章呢，我就打算来讲解一下SpringBoot的邮件服务。我们都知道发送邮件应该是网站的必备功能之一，什么注册验证，忘记密码或者是给用户发送营销信息。以前我们会使用 JavaMail 相关 api 来写发送邮件的相关代码，后来 Spring 推出了 JavaMailSender 更加简化了邮件发送的过程，在之后 Spring Boot 对此进行了封装就有了现在的 spring-boot-starter-mail ，如果你看了我前面的文章的话，就会知道，SpringBoot把大部分的需求封装成了一个个场景启动器，而mail也就是相应的场景启动器。</p>\n<h2 id=\"了解邮件服务\"><a href=\"#了解邮件服务\" class=\"headerlink\" title=\"了解邮件服务\"></a>了解邮件服务</h2><p>经常出现和邮件相关的协议是SMTP、IMAP和POP3，所以在这里我们首先来认识了解这三个协议。</p>\n<p>SMTP全称为Simple Mail Transfer Protocol（简单邮件传输协议），它是一组用于从源地址到目的地址传输邮件的规范，通过它来控制邮件的中转方式。SMTP认证要求必须提供账号和密码才能登陆服务器，其设计目的在于避免用户受到垃圾邮件的侵扰。</p>\n<p>IMAP全称为Internet Message Access Protocol（互联网邮件访问协议），IMAP允许从邮件服务器上获取邮件的信息、下载邮件等。IMAP与POP类似，都是一种邮件获取协议。</p>\n<p>POP3全称为Post Office Protocol 3（邮局协议），POP3支持客户端远程管理服务器端的邮件。POP3常用于“离线”邮件处理，即允许客户端下载服务器邮件，然后服务器上的邮件将会被删除。目前很多POP3的邮件服务器只提供下载邮件功能，服务器本身并不删除邮件，这种属于改进版的POP3协议。</p>\n<p>那么问题来了，IMAP和POP3协议有什么不同呢？两者最大的区别在于，IMAP允许双向通信，即在客户端的操作会反馈到服务器上，例如在客户端收取邮件、标记已读等操作，服务器会跟着同步这些操作。而对于POP协议虽然也允许客户端下载服务器邮件，但是在客户端的操作并不会同步到服务器上面的，例如在客户端收取或标记已读邮件，服务器不会同步这些操作。</p>\n<h2 id=\"SpringBoot相关类\"><a href=\"#SpringBoot相关类\" class=\"headerlink\" title=\"SpringBoot相关类\"></a>SpringBoot相关类</h2><p>SpringBoot中针对邮件服务的两个工具类是，JavaMailSender和JavaMailSenderImpl，它们是Spring官方提供的集成邮件服务的接口和实现类，以简单高效的设计著称，目前是Java后端发送邮件和集成邮件服务的主流工具。那如何通过JavaMailSenderImpl发送邮件？非常简单，直接在业务类注入JavaMailSenderImpl并调用send方法发送邮件。其中简单邮件可以通过SimpleMailMessage来发送邮件，而复杂的邮件（例如添加附件）可以借助MimeMessageHelper来构建MimeMessage发送邮件。</p>\n<p>我们不难理解，SpringBoot对于邮件服务能做到开箱即用，其实就是基于官方内置的自动配置，翻看源码可知晓邮件自动配置类(MailSenderPropertiesConfiguration) 为上下文提供了邮件服务实例(JavaMailSenderImpl)。</p>\n<h2 id=\"具体教程\"><a href=\"#具体教程\" class=\"headerlink\" title=\"具体教程\"></a>具体教程</h2><h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>首先我们创建一个新的项目，只要包含最基本的web场景就可以了，然后我们在pom.xml中引入依赖就可以了，依赖如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependencies&gt;</span><br><span class=\"line\">    &lt;dependency&gt; </span><br><span class=\"line\">        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;/dependency&gt; </span><br><span class=\"line\">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>\n<p>接着，我们在application.properties主配置文件中对mail进行相关的配置，配置内容如下，我做了相关注释</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">spring.mail.host=smtp.163.com</span><br><span class=\"line\">spring.mail.username=amazing</span><br><span class=\"line\">spring.mail.password=xxxxxx  <span class=\"comment\">#这里填的不是账号密码，是的第三方登录校验码</span></span><br><span class=\"line\">spring.mail.default-encoding=UTF-8</span><br><span class=\"line\">mail.fromMail.address=amazing@163.com</span><br></pre></td></tr></table></figure>\n<p>上面的邮箱服务器的地址，我这里放出一下常用有限发邮箱服务器地址</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">QQ邮箱（mail.qq.com）</span><br><span class=\"line\">POP3服务器地址：pop.qq.com（端口：110）</span><br><span class=\"line\">SMTP服务器地址：smtp.qq.com（端口：25）</span><br><span class=\"line\">SMTP服务器需要身份验证。</span><br><span class=\"line\"></span><br><span class=\"line\">网易邮箱（163.com）:</span><br><span class=\"line\">POP3服务器地址:pop.163.com（端口：110）</span><br><span class=\"line\">SMTP服务器地址:smtp.163.com（端口：25）</span><br><span class=\"line\"></span><br><span class=\"line\">谷歌邮箱(google.com)：</span><br><span class=\"line\">POP3服务器地址:pop.gmail.com（SSL启用端口：995）</span><br><span class=\"line\">SMTP服务器地址:smtp.gmail.com（SSL启用端口：587）</span><br><span class=\"line\"></span><br><span class=\"line\">阿里云邮箱（mail.aliyun.com）:</span><br><span class=\"line\">POP3服务器地址:pop3.aliyun.com（SSL加密端口：995；非加密端口：110）</span><br><span class=\"line\">SMTP服务器地址:smtp.aliyun.com（SSL加密端口：465；非加密端口：25）</span><br><span class=\"line\">IMAP服务器地址：imap.aliyun.com（SSL加密端口：993；非加密端口：143）</span><br><span class=\"line\"></span><br><span class=\"line\">新浪邮箱（sina.com）:</span><br><span class=\"line\">POP3服务器地址:pop3.sina.com.cn（端口：110）</span><br><span class=\"line\">SMTP服务器地址:smtp.sina.com.cn（端口：25）</span><br></pre></td></tr></table></figure>\n<h3 id=\"简单使用\"><a href=\"#简单使用\" class=\"headerlink\" title=\"简单使用\"></a>简单使用</h3><p>这样就完成了我们SpringBoot使用邮件服务的基本配置，那么接下来我们简单使用一下，首先编写Service，目录结构如下<br><img src=\"https://img-blog.csdnimg.cn/20200306120738183.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">MailService</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendSimpleMail</span><span class=\"params\">(String to, String subject, String content)</span></span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MailServiceImpl</span> <span class=\"keyword\">implements</span> <span class=\"title\">MailService</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Logger logger = LoggerFactory.getLogger(<span class=\"keyword\">this</span>.getClass());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> JavaMailSender mailSender;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Value(&quot;$&#123;mail.fromMail.addr&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String from;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendSimpleMail</span><span class=\"params\">(String to, String subject, String content)</span> </span>&#123;</span><br><span class=\"line\">        SimpleMailMessage message = <span class=\"keyword\">new</span> SimpleMailMessage();</span><br><span class=\"line\">        message.setFrom(from);</span><br><span class=\"line\">        message.setTo(to);</span><br><span class=\"line\">        message.setSubject(subject);</span><br><span class=\"line\">        message.setText(content);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            mailSender.send(message);</span><br><span class=\"line\">            logger.info(<span class=\"string\">&quot;简单邮件已经发送。&quot;</span>);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">            logger.error(<span class=\"string\">&quot;发送简单邮件时发生异常！&quot;</span>, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>编写 test 类进行测试，至此一个简单的文本发送就完成了。<br><img src=\"https://img-blog.csdnimg.cn/20200306120933338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"丰富邮件内容\"><a href=\"#丰富邮件内容\" class=\"headerlink\" title=\"丰富邮件内容\"></a>丰富邮件内容</h2><p>但是在正常使用的过程中，我们通常在邮件中加入图片或者附件来丰富邮件的内容，下面讲介绍如何使用 Spring Boot 来发送丰富的邮件。</p>\n<h3 id=\"发送-html-格式邮件\"><a href=\"#发送-html-格式邮件\" class=\"headerlink\" title=\"发送 html 格式邮件\"></a>发送 html 格式邮件</h3><p>其它都不变在 MailService 添加 sendHtmlMail 方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendHtmlMail</span><span class=\"params\">(String to, String subject, String content)</span> </span>&#123;</span><br><span class=\"line\">    MimeMessage message = mailSender.createMimeMessage();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//true表示需要创建一个multipart message</span></span><br><span class=\"line\">        MimeMessageHelper helper = <span class=\"keyword\">new</span> MimeMessageHelper(message, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        helper.setFrom(from);</span><br><span class=\"line\">        helper.setTo(to);</span><br><span class=\"line\">        helper.setSubject(subject);</span><br><span class=\"line\">        helper.setText(content, <span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        mailSender.send(message);</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;html邮件发送成功&quot;</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (MessagingException e) &#123;</span><br><span class=\"line\">        logger.error(<span class=\"string\">&quot;发送html邮件时发生异常！&quot;</span>, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在测试类中构建 html 内容，测试发送</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testHtmlMail</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">    String content=<span class=\"string\">&quot;&lt;html&gt;\\n&quot;</span> +</span><br><span class=\"line\">            <span class=\"string\">&quot;&lt;body&gt;\\n&quot;</span> +</span><br><span class=\"line\">            <span class=\"string\">&quot;    &lt;h3&gt;hello world ! 这是一封Html邮件!&lt;/h3&gt;\\n&quot;</span> +</span><br><span class=\"line\">            <span class=\"string\">&quot;&lt;/body&gt;\\n&quot;</span> +</span><br><span class=\"line\">            <span class=\"string\">&quot;&lt;/html&gt;&quot;</span>;</span><br><span class=\"line\">    MailService.sendHtmlMail(<span class=\"string\">&quot;xxxxxx@163.com&quot;</span>,<span class=\"string\">&quot;test simple mail&quot;</span>,content);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"发送带附件的邮件\"><a href=\"#发送带附件的邮件\" class=\"headerlink\" title=\"发送带附件的邮件\"></a>发送带附件的邮件</h3><p>还是老样子，在 MailService 添加 sendAttachmentsMail 方法。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendAttachmentsMail</span><span class=\"params\">(String to, String subject, String content, String filePath)</span></span>&#123;</span><br><span class=\"line\">    MimeMessage message = mailSender.createMimeMessage();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        MimeMessageHelper helper = <span class=\"keyword\">new</span> MimeMessageHelper(message, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        helper.setFrom(from);</span><br><span class=\"line\">        helper.setTo(to);</span><br><span class=\"line\">        helper.setSubject(subject);</span><br><span class=\"line\">        helper.setText(content, <span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        FileSystemResource file = <span class=\"keyword\">new</span> FileSystemResource(<span class=\"keyword\">new</span> File(filePath));</span><br><span class=\"line\">        String fileName = filePath.substring(filePath.lastIndexOf(File.separator));</span><br><span class=\"line\">        helper.addAttachment(fileName, file);</span><br><span class=\"line\"></span><br><span class=\"line\">        mailSender.send(message);</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;带附件的邮件已经发送。&quot;</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (MessagingException e) &#123;</span><br><span class=\"line\">        logger.error(<span class=\"string\">&quot;发送带附件的邮件时发生异常！&quot;</span>, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加多个附件可以使用多条 <code>helper.addAttachment(fileName, file)</code>，然后在测试类中添加测试方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendAttachmentsMail</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    String filePath=<span class=\"string\">&quot;e:\\\\tmp\\\\application.log&quot;</span>;</span><br><span class=\"line\">    mailService.sendAttachmentsMail(<span class=\"string\">&quot;xxxxx@163.com&quot;</span>, <span class=\"string\">&quot;主题：带附件的邮件&quot;</span>, <span class=\"string\">&quot;有附件，请查收！&quot;</span>, filePath);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"发送带静态资源的邮件\"><a href=\"#发送带静态资源的邮件\" class=\"headerlink\" title=\"发送带静态资源的邮件\"></a>发送带静态资源的邮件</h3><p>邮件中的静态资源一般就是指图片，在 MailService 添加 sendAttachmentsMail 方法。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendInlineResourceMail</span><span class=\"params\">(String to, String subject, String content, String rscPath, String rscId)</span></span>&#123;</span><br><span class=\"line\">    MimeMessage message = mailSender.createMimeMessage();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        MimeMessageHelper helper = <span class=\"keyword\">new</span> MimeMessageHelper(message, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        helper.setFrom(from);</span><br><span class=\"line\">        helper.setTo(to);</span><br><span class=\"line\">        helper.setSubject(subject);</span><br><span class=\"line\">        helper.setText(content, <span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        FileSystemResource res = <span class=\"keyword\">new</span> FileSystemResource(<span class=\"keyword\">new</span> File(rscPath));</span><br><span class=\"line\">        helper.addInline(rscId, res);</span><br><span class=\"line\"></span><br><span class=\"line\">        mailSender.send(message);</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;嵌入静态资源的邮件已经发送。&quot;</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (MessagingException e) &#123;</span><br><span class=\"line\">        logger.error(<span class=\"string\">&quot;发送嵌入静态资源的邮件时发生异常！&quot;</span>, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在测试类中添加测试方法</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendInlineResourceMail</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    String rscId = <span class=\"string\">&quot;neo006&quot;</span>;</span><br><span class=\"line\">    String content=<span class=\"string\">&quot;&lt;html&gt;&lt;body&gt;这是有图片的邮件：&lt;img src=\\&#x27;cid:&quot;</span> + rscId + <span class=\"string\">&quot;\\&#x27; &gt;&lt;/body&gt;&lt;/html&gt;&quot;</span>;</span><br><span class=\"line\">    String imgPath = <span class=\"string\">&quot;C:\\\\Users\\\\summer\\\\Pictures\\\\favicon.png&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    mailService.sendInlineResourceMail(<span class=\"string\">&quot;XXXXX@163.com&quot;</span>, <span class=\"string\">&quot;主题：这是有图片的邮件&quot;</span>, content, imgPath, rscId);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加多个图片可以使用多条 <code>&lt;img src=&#39;cid:&quot; + rscId + &quot;&#39; &gt;</code> 和 <code>helper.addInline(rscId, res)</code> 来实现</p>\n<h2 id=\"邮件系统\"><a href=\"#邮件系统\" class=\"headerlink\" title=\"邮件系统\"></a>邮件系统</h2><p>上面发送邮件的基础服务就这些了，但是如果我们要做成一个邮件系统的话还需要考虑以下几个问题，首先是邮件模板的问题，我们会经常收到类似这样的邮件<br><img src=\"https://img-blog.csdnimg.cn/20200306121725805.png#pic_center\" alt=\"在这里插入图片描述\"><br>其中只有 neo 这个用户名在变化，其它邮件内容均不变，如果每次发送邮件都需要手动拼接的话会不够优雅，并且每次模板的修改都需要改动代码的话也很不方便，因此对于这类邮件需求，都建议做成邮件模板来处理。模板的本质很简单，就是在模板中替换变化的参数，转换为 html 字符串即可，这里以thymeleaf为例来演示，第一步当然是导入thymeleaf的包啦。</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>然后在在 <code>resorces/templates</code> 下创建 <code>emailTemplate.html</code></p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=<span class=\"string\">&quot;zh&quot;</span> xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">    &lt;head&gt;</span><br><span class=\"line\">        &lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>/&gt;</span><br><span class=\"line\">        &lt;title&gt;Title&lt;/title&gt;</span><br><span class=\"line\">    &lt;/head&gt;</span><br><span class=\"line\">    &lt;body&gt;</span><br><span class=\"line\">        您好,这是验证邮件,请点击下面的链接完成验证,&lt;br/&gt;</span><br><span class=\"line\">        &lt;a href=&quot;#&quot; th:href=&quot;@&#123; http://www.xxx.com/neo/&#123;id&#125;(id=$&#123;id&#125;) &#125;&quot;&gt;激活账号&lt;/a&gt;</span><br><span class=\"line\">    &lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Test</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">sendTemplateMail</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//创建邮件正文</span></span><br><span class=\"line\">    Context context = <span class=\"keyword\">new</span> Context();</span><br><span class=\"line\">    context.setVariable(<span class=\"string\">&quot;id&quot;</span>, <span class=\"string\">&quot;006&quot;</span>);</span><br><span class=\"line\">    String emailContent = templateEngine.process(<span class=\"string\">&quot;emailTemplate&quot;</span>, context);</span><br><span class=\"line\"></span><br><span class=\"line\">    mailService.sendHtmlMail(<span class=\"string\">&quot;ityouknow@126.com&quot;</span>,<span class=\"string\">&quot;主题：这是模板邮件&quot;</span>,emailContent);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>当然啦，这里要强调一点的是，我们在实现邮箱服务的时候，因为各种原因，总会有邮件发送失败的情况，比如：邮件发送过于频繁、网络异常等。在出现这种情况的时候，我们一般会考虑重新重试发送邮件，会分为以下几个步骤来实现：</p>\n<ul>\n<li>接收到发送邮件请求，首先记录请求并且入库。</li>\n<li>调用邮件发送接口发送邮件，并且将发送结果记录入库。</li>\n<li>启动定时系统扫描时间段内，未发送成功并且重试次数小于3次的邮件，进行再次发送</li>\n</ul>\n<h2 id=\"最后\"><a href=\"#最后\" class=\"headerlink\" title=\"最后\"></a>最后</h2><p>很多时候邮件发送并不是我们主业务必须关注的结果，比如通知类、提醒类的业务可以允许延时或者失败。这个时候可以采用异步的方式来发送邮件，加快主交易执行速度，在实际项目中可以采用MQ发送邮件相关参数，监听到消息队列之后启动发送邮件。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","邮件服务"]},{"title":"LDA、PCA、ZCA、ICA回顾","url":"/Deep-Learning/f5b6040bcaf2/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>白化是一种线性变换，用于对源信号进行去相关，是一种重要的预处理过程，其目的就是降低输入数据的冗余性，使得经过白化处理的输入数据具有如下性质：</p>\n<ul>\n<li>特征之间相关性较低</li>\n<li>所有特征具有相同的方差。</li>\n</ul>\n<blockquote>\n<p>其中，PCA白化保证了所有特征分布均值为0，方差为1，而ZCA白化则保证了所有特征分布均值为0，方差相同。PCA白化可以用于降维也可以去相关性，而ZCA白化主要用于去相关性，且尽量使白化后的数据接近原始输入数据。</p>\n</blockquote>\n<p>白化是机器学习里面常用的一种规范化数据分布的方法，特别是用来在机器学习模型中用来解决Covariate Shift问题（Covariate Shift是啥可以看这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记：Covariate Shift: 基于机器学习分类器的回顾和分析</a>）</p>\n<p>在深度学习中，Covariate Shift问题也存在，不过叫做另一个名字Internal Covariate Shift，同样在深度学习中解决Internal Covariate Shift问题的办法有很多，比如我们熟悉的Batch Normalization、Layer Normalization等等归一化方法，感兴趣的小伙伴可以参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/340747455\">深度学习中眼花缭乱的Normalization学习总结</a></p>\n<p>这篇文章就来对PCA (主成分分析)、ZCA (零相成分分析)和ICA（独立成分分析）进行回顾和总结。本文涉及到的代码已放置在<a href=\"https://github.com/DengBoCong/Experiment\">GitHub：Experiment</a></p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>在我们接下来进行下面讲解之前，我们先来了解几个概念理解结论：</p>\n<ul>\n<li>如果假设 B 的模为 1，则<strong>A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小</strong>，这就是内积的一种几何解释，即让$|B|=1$，那么$A\\cdot B=|A||B|cos(\\alpha)=|A|cos(\\alpha)$。注意投影是一个标量，所以可以为负。</li>\n<li>要准确描述向量，首先<strong>要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。</li>\n<li>矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列向量 [公式] 变换到左边矩阵中以每一行行向量为基所表示的空间中去</strong>。也就是说一个矩阵可以表示一种线性变换，如下矩阵乘法。<br>$$\\begin{bmatrix}p_1\\p_2\\…\\p_R\\end{bmatrix}(a_1\\ a_2\\ …\\ a_M)=\\begin{bmatrix}p_1a_1 &amp; p_1a_2 &amp; … &amp; p_1a_M\\ p_2a_1 &amp; p_2a_2 &amp; … &amp; p_2a_M\\ … &amp; … &amp; … &amp; …\\ p_Ra_1 &amp; p_Ra_2 &amp; … &amp; p_Ra_M\\end{bmatrix}$$<br>其中 $p_i$ 是一个行向量，表示第 $i$ 个基， $a_j$ 是一个列向量，表示第 $j$ 个原始数据记录，实际上也就是做了一个向量矩阵化的操作。</li>\n<li>我们应该如何选择 K 个基才能最大程度保留原有的信息？问题可以形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></li>\n<li>降维问题的优化目标：<strong>将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）</strong>。</li>\n</ul>\n<h1 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h1><p>首先理解降维，降维就意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。</p>\n<p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。也就是我们接下来要讨论的PCA和ZCA，不过在此之前，我还想举一个更加生动的例子。下图的平面直角坐标系中有五条数据：<br><img src=\"https://img-blog.csdnimg.cn/20210109201656942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多，用数学来衡量就是使用方差和协方差。</p>\n<h1 id=\"LDA\"><a href=\"#LDA\" class=\"headerlink\" title=\"LDA\"></a>LDA</h1><p>LDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。有些资料上也称为是Fisher’s Linear Discriminant，因为它被Ronald Fisher发明自1936年，Discriminant这次词我个人的理解是，一个模型，不需要去通过概率的方法来训练、预测数据，比如说各种贝叶斯方法，就需要获取数据的先验、后验概率等等。</p>\n<p> LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。即给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，比如在二分类问题中，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20210109203942814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><p>PCA（Principal Component Analysis） 是一种常见的数据分析方式，通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。</p>\n<p>关于PCA的讲解，直接看这一篇文章就好了（<a href=\"http://blog.codinglabs.org/articles/pca-tutorial.html\">PCA的数学原理</a>），里面从基开始讲解，把每个点讲的通俗易懂，很值得阅读。我这里就单纯的陈列总结一下PCA的算法步骤：</p>\n<p>设有m条n维数据。</p>\n<ul>\n<li>将原始数据按列组成n行m列矩阵$X$</li>\n<li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li>\n<li>求出协方差矩阵 $C=\\frac{1}{m}XX^T$</li>\n<li>求出协方差矩阵的特征值及对应的特征向量</li>\n<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵$P$</li>\n<li>$Y=PX$ 即为降维到k维后的数据</li>\n</ul>\n<p>PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>\n<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>\n<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p>\n<h1 id=\"ZCA\"><a href=\"#ZCA\" class=\"headerlink\" title=\"ZCA\"></a>ZCA</h1><p>白化的目的是去相关和方差归一化，那么在上述PCA-Whtening中，只要达到这两个目的即可，计算方法并不唯一。换句话说，如果我们换一种方差归一化方法也是可以实现白化的，这就是下面要介绍的ZCA-Whitening。</p>\n<p>如果 $R$ 是任意正交矩阵（any orthogonal matrix），即满足 $RR^T = R^TR = I$ (说它正交不太严格， $R$ 可以是旋转或反射矩阵)，那么 $R$ 乘以 $x_{PCAwhite}$ 仍然具有单位协方差。在ZCA-Whitening中，令$R=U$（其中 U 是PCA白化中使用的特征向量矩阵） 。则我们定义ZCA白化的结果为：<br>$$x_{ZCAwhite}=Ux_{PCAwhite}$$</p>\n<p>此时，数据的协方差矩阵依然是单位矩阵。</p>\n<h1 id=\"ICA\"><a href=\"#ICA\" class=\"headerlink\" title=\"ICA\"></a>ICA</h1><p>ICA又称盲源分离(Blind source separation, BSS)，它假设观察到的随机信号 $x$ 服从模型 $x=As$，其中 $s$ 为未知源信号，其分量相互独立，$A$ 为一未知混合矩阵。ICA的目的是通过且仅通过观察 $x$ 来估计混合矩阵 $A$ 以及源信号 $s$。</p>\n<p>大多数ICA的算法需要进行“数据预处理”（data preprocessing）：先用PCA得到 $y$，再把 $y$ 的各个分量标准化（即让各分量除以自身的标准差）得到 $z$。预处理后得到的 $z$ 满足下面性质：</p>\n<ul>\n<li>$z$ 的各个分量不相关</li>\n<li>$z$ 的各个分量的方差都为1。</li>\n</ul>\n<p>有许多不同的ICA算法可以通过 $z$ 把 $A$ 和 $s$ 估计出来。以著名的FastICA算法为例，该算法寻找方向使得随机变量 $w^Tz$ 的某种“非高斯性”(non-Gaussianity)的度量最大化。一种常用的非高斯性的度量是四阶矩 $E[(w^Tx)^4]$。类似PCA的流程，我们首先找 $w_1$ 使得 $E[(w^Tx)^4]$ 最大；然后在 $w_1$ 与正交的空间里找 $w_2$，使得 $E[(w^Tx)^4]$ 最大，以此类推直到找到所有的 $w_1,…,w_n$。可以证明，用这种方法得到的 $w_1^Tz,…,w_n^T$ 是相互独立的。</p>\n<p>ICA认为一个信号可以被分解成若干个统计独立的分量的线性组合，而后者携带更多的信息。我们可以证明，只要源信号非高斯，那么这种分解是唯一的。若源信号为高斯的话，那么显然可能有无穷多这样的分解。</p>\n<p>总的来说，ICA认为观测信号是若干个统计独立的分量的线性组合，ICA要做的是一个解混过程。而PCA是一个信息提取的过程，将原始数据降维，现已成为ICA将数据标准化的预处理步骤。</p>\n<h1 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h1><p>实践中需要实现PCA白化或ZCA白化时，有时一些特征值在数值上接近于0，这样在缩放步骤时我们除以 $\\sqrt{\\lambda_i}$ 将导致除以一个接近0的值；这可能使数据上溢 (赋为大数值)或造成数值不稳定。因而在实践中，我们使用少量的正则化实现这个缩放过程，即在取平方根和倒数之前给特征值加上一个很小的常数：</p>\n<h1 id=\"PCA白化和ZCA白化可视化\"><a href=\"#PCA白化和ZCA白化可视化\" class=\"headerlink\" title=\"PCA白化和ZCA白化可视化\"></a>PCA白化和ZCA白化可视化</h1><p>为了说明PCA和ZCA白化之间的区别，我们创建一些玩具数据。 具体来说，我们生成了两个相关时间序列 $x_1$ 和 $x_2$ 的1000个样本。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">mu = [<span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">sigma = [[<span class=\"number\">5</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>]]  <span class=\"comment\"># must be positive semi-definite</span></span><br><span class=\"line\">n = <span class=\"number\">1000</span></span><br><span class=\"line\">x = np.random.multivariate_normal(mu, sigma, size=n).T</span><br></pre></td></tr></table></figure>\n<p>这两个时间序列存储在shape为(2,1000)的NumPy数组 $x$ 中，同时，为了便于稍后可视化，这里固定20个最极端的值，并将它们的索引表示为set1（其余数据点的索引存储在set2中）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">set1 = np.argsort(np.linalg.norm(x, axis=<span class=\"number\">0</span>))[-<span class=\"number\">20</span>:]</span><br><span class=\"line\">set2 = <span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(<span class=\"built_in\">range</span>(n)) - <span class=\"built_in\">set</span>(set1))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109160718843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>显然，这两个时间序列看起来高度相关，散点图的椭圆形表明，随着 $x_1$ 的值增加，$x_2$ 的值也趋于增加，实际上， $x_1$ 和 $x_2$ 之间的皮尔逊相关系数为0.80，可以通过以下公式计算：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">print(np.corrcoef(x)[<span class=\"number\">0</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\"># 0.8020186259500502</span></span><br></pre></td></tr></table></figure>\n<p>PCA和ZCA均基于（经验）协方差矩阵的特征向量和特征值，特别地，协方差矩阵可以分解为其特征向量 $U$ 和特征值 $\\Lambda$，例如（$\\Sigma$是协方差矩阵）：<br>$$\\Sigma = U\\Lambda U^T$$<br>我们来计算玩具数据的这些值：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">sigma = np.cov(x)</span><br><span class=\"line\">print(sigma)</span><br><span class=\"line\">evals, evecs = np.linalg.eigh(sigma)</span><br></pre></td></tr></table></figure>\n<p>请注意，在这里使用从数据得出的经验协方差矩阵，而不是普遍未知的真实协方差矩阵，此外，请注意，由于协方差矩阵始终是对称的，因此在这里使用优化的<code>np.linalg.eigh</code>函数，而不是更通用的<code>np.linalg.eig</code>版本（这也确保了我们总是得到真实的特征值而不是复杂的特征值）。当然，我们也可以直接在数据 $x$ 上使用<code>np.linalg.svd</code>（而不是协方差矩阵）来计算特征向量和特征值，在某些情况下，它们在数值上可能更稳定。</p>\n<h2 id=\"PCA白化\"><a href=\"#PCA白化\" class=\"headerlink\" title=\"PCA白化\"></a>PCA白化</h2><p>现在，我们的PCA白化矩阵 $W^{PCA}$ 可以写为：$W^{PCA}=\\Lambda^{-\\frac{1}{2}}U^T$，这也就意味着数据可以转换为：$z=W^{PCA}x=\\Lambda^{-\\frac{1}{2}}U^Tx$，因此，我们可以相应的白化我们的数据，代码实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">z = np.diag(evals**(-<span class=\"number\">1</span>/<span class=\"number\">2</span>)) @ evecs.T @ x</span><br></pre></td></tr></table></figure>\n<p>现在我们可以来看看白化之后的数据是啥样：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109163615638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以很明显的看到该变换消除了两个时间序列之间的相关性，因为散点图现在看起来像一个球体（二维圆），因此PCA的另一个名称叫做sphering。<code>np.corrcoef(z)[0,1]</code>产生的值实际上等于零，从数据分布图我们可以看到，PCA旋转了所有数据点，如红点的新位置所示，它们不再位于大约45度的对角线上，而是现在与垂直轴对齐。</p>\n<h2 id=\"ZCA白化\"><a href=\"#ZCA白化\" class=\"headerlink\" title=\"ZCA白化\"></a>ZCA白化</h2><p>同样的，ZCA的白化矩阵 $W^{ZCA}$ 可以表示为：$W^{ZCA}=U\\Lambda^{-\\frac{1}{2}}U^T$，ZCA白化类似于PCA白化，但不同点在于，$U$ 会进行额外的旋转。原始数据可以按如下方式转换：$z=W^{ZCA}x=U\\Lambda^{-\\frac{1}{2}}U^Tx$。接着，我们使用代码，将使用ZCA算法将白化数据并生成数据散点图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">z = evecs @ np.diag(evals**(-<span class=\"number\">1</span>/<span class=\"number\">2</span>)) @ evecs.T @ x</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20210109170835355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>同样，由于散点图看起来是球形的，因此ZCA已将数据解相关（且<code>np.corrcoef(z)[0,1]</code>产生的值实际上等于零），与PCA相比，ZCA保留了原始数据点的方向，此属性为其命名为 “zero-phase”，因为它最小化了数据的原始相位（即方向）。</p>\n<p>通过上面的实验，<strong>PCA和ZCA执行不同的旋转，所以如果目标是压缩原始数据，则使用PCA更好，而如果目标是使转换后尽可能与原始数据相似，则使用ZCA更好，因此ZCA无法用于压缩数据</strong>。值得一提的是，有时在白化之前对数据进行<a href=\"https://en.wikipedia.org/wiki/Standard_score\">标准化</a>可能会很有用，特别是如果各个源信号的比例不同。</p>\n<p><em>参考资料</em>：</p>\n<ul>\n<li><a href=\"https://cbrnr.github.io/2018/12/17/whitening-pca-zca/\">Whitening with PCA and ZCA</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/pca-tutorial.html\">PCA的数学原理</a></li>\n<li><a href=\"https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html\">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li>\n<li><a href=\"https://my.oschina.net/findbill/blog/535044\">从SVD到PCA——奇妙的数学游戏</a></li>\n<li><a href=\"https://www.cnblogs.com/robert-dlut/p/4211174.html\">预处理：主成分分析与白化</a></li>\n<li><a href=\"https://www.zhihu.com/question/28845451\">独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["白化","机器学习","PCA","LDA"]},{"title":"NLP中遇到的各类Attention结构汇总以及代码复现","url":"/Deep-Learning/2a74b1f93c8b/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>我们所熟知的encoder和decoder结构中，通常采用RNN结构如GRU或LSTM等，在encoder RNN中将输入语句信息总结到最后一个hidden vector中，并将其作为decoder的初始hidden vector，从而利用decoder的解码成对应的其他语言中的文字。但是这样的结构会出现一些问题，比如老生常谈的长程梯度消失的问题，对于较长的句子很难寄希望于将输入的序列转化为定长的向量而保存所有的有效的信息，所以随着输入序列的长度增加，这种结构的效果就会显著下降。因此这个时候就是Attention出场了，用一个浅显描述总结Attention就是，分配权重系数，保留序列的有效信息，而不是局限于原来模型中的定长隐藏向量，并且不会丧失长程的信息。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201218114440343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<p>本篇文章主要是汇总我目前在对话和语音方面遇到的各类Attention，针对这些Attention进行理解阐述、总结、论文、代码复现。本文只对各Attention的关键处进行阐述，具体细节可查阅资料或阅读原论文了解。**本文所述的结构不是很多，主要是目前我再学习中遇到的比较重要的Attention（一些用的不多的在最后提了一下），后续还会持续更新。</p>\n<h1 id=\"Bahdanau-Attention\"><a href=\"#Bahdanau-Attention\" class=\"headerlink\" title=\"Bahdanau Attention\"></a>Bahdanau Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\">Paper Link</a></li>\n</ul>\n<p>Bahdanau Attention实现可以说是Attention的开创者之一，该实现的论文名叫“Neural Machine Translation by Learning to Jointly Align and Translate”，其中使用到了“Align”一次，意思是在训练模型的同时调整直接影响得分的权重，下面是论文中的结构图：<br><img src=\"https://img-blog.csdnimg.cn/20201218201405508.png#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$c_t = \\sum_{j=1}^{T_x}a_{tj}h_j$$    $$a_{tj}=\\frac{exp(e_{tj})}{\\sum_{k=1}^{T_x}exp(e_{tk})}$$    $$e_{tj}=V_a^Ttanh(W_a[s_{t-1};h_j])$$</p>\n<p>其中，$c_t$ 是 $t$ 时刻的语义向量，$e_ij$ 是encoder中 $j$ 时刻Encoder隐藏层状态 $h_j$ 对decoder中 $t$ 时刻隐藏层状态 $s_t$ 的影响程度，然后通过softmax函数（第二个式子）将 $e_{tj}$ 概率归一化为 $a_{tj}$</p>\n<p>论文是使用Seq2seq结构对Attention进行阐述的，所以需要注意几点的是：</p>\n<ul>\n<li>在模型结构的encoder中，是使用双向RNN处理序列的，并将方向RNN的最后一个隐藏层作为decoder的初始化隐藏层。</li>\n<li>attention层中的分数计算方式是使用 <strong>additive/concat</strong></li>\n<li>解码器的下一个时间步的输入是前一个解码器时间步生成的单词（或ground-truth）与当前时间步的上下文向量之间的concat。</li>\n</ul>\n<p><strong>下面附一张更清晰的结构图</strong>：<br><img src=\"https://img-blog.csdnimg.cn/20201218212755333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bahdanau_attention</span>(<span class=\"params\">hidden_dim: <span class=\"built_in\">int</span>, units: <span class=\"built_in\">int</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    :param units: 全连接层单元数</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    query = tf.keras.Input(shape=(hidden_dim))</span><br><span class=\"line\">    values = tf.keras.Input(shape=(<span class=\"literal\">None</span>, hidden_dim))</span><br><span class=\"line\">    V = tf.keras.layers.Dense(<span class=\"number\">1</span>)</span><br><span class=\"line\">    W1 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">    W2 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">    <span class=\"comment\"># query其实就是decoder的前一个状态，decoder的第一个状态就是上</span></span><br><span class=\"line\">    <span class=\"comment\"># 面提到的encoder反向RNN的最后一层，它作为decoderRNN中的初始隐藏层状态</span></span><br><span class=\"line\">    <span class=\"comment\"># values其实就是encoder每个时间步的隐藏层状态，所以下面需要将query扩展一个时间步维度进行之后的操作</span></span><br><span class=\"line\">    hidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">    score = V(tf.nn.tanh(W1(values) + W2(hidden_with_time_axis)))</span><br><span class=\"line\">    attention_weights = tf.nn.softmax(score, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    context_vector = attention_weights * values</span><br><span class=\"line\">    context_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.keras.Model(inputs=[query, values], outputs=[context_vector, attention_weights])</span><br></pre></td></tr></table></figure>\n<h1 id=\"Luong-Attention\"><a href=\"#Luong-Attention\" class=\"headerlink\" title=\"Luong Attention\"></a>Luong Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1508.04025.pdf\">Paper Link</a></li>\n</ul>\n<p>论文名为“Effective Approaches to Attention-based Neural Machine Translation”，文章其实是基于Bahdanau Attention进行研究的，但在架构上更加简单。论文研究了两种简单有效的注意力机制：一种始终关注所有词的global方法和一种仅一次查看词子集的local方法。结构如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201218220809256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$a_t(s)=align(h_t,\\bar{h}<em>s)=\\frac{exp(score(h_t, \\bar{h}_s))}{\\sum</em>{s’}exp(score(h_t, \\bar{h}_{s’}))}$$    $$score(h_t, \\bar{h}_s)\\left{\\begin{matrix} h_t^T\\bar{h}_s &amp; dot \\ h_t^TW_a\\bar{h}_s &amp;general \\ v_a^Ttanh(W_a[h_t;\\bar{h}_s]) &amp;concat \\end{matrix}\\right.$$ </p>\n<p>同样的，论文中也是使用Seq2Seq结构进行阐述，需要注意如下几点：</p>\n<ul>\n<li>在encoder部分是使用两层堆叠的LSTM，decoder也是同样的结构，不过它使用encoder最后一个隐藏层作为初始化隐藏层。</li>\n<li>用作Attention计算的隐藏层向量是使用堆叠的最后一个LSTM的隐层</li>\n<li>论文中实验的注意力分数计算方式有：（1）additive/concat，（2）dot product，（3）location-based，（4）‘general’</li>\n<li>当前时间步的解码器输出与当前时间步的上下文向量之间的concat喂给前馈神经网络，从而给出当前时间步的解码器的最终输出。</li>\n</ul>\n<p><strong>下面附一张更清晰的结构图</strong>：你会发现和Bahdanau Attention很像区别在于score计算方法和最后decoder中和context vector合并部分。<br><img src=\"https://img-blog.csdnimg.cn/20201218234135371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">luong_attention_concat</span>(<span class=\"params\">hidden_dim: <span class=\"built_in\">int</span>, units: <span class=\"built_in\">int</span></span>) -&gt; tf.keras.Model:</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t:param units: 全连接层单元数</span></span><br><span class=\"line\"><span class=\"string\">\t&quot;&quot;&quot;</span></span><br><span class=\"line\">\tquery = tf.keras.Input(shape=(hidden_dim))</span><br><span class=\"line\">\tvalues = tf.keras.Input(shape=(<span class=\"literal\">None</span>, hidden_dim))</span><br><span class=\"line\">\tW1 = tf.keras.layers.Dense(units)</span><br><span class=\"line\">\tV = tf.keras.layers.Dense(<span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"comment\"># query其实就是decoder的前一个状态，decoder的第一个状态就是上</span></span><br><span class=\"line\">\t<span class=\"comment\"># 面提到的encoder反向RNN的最后一层，它作为decoderRNN中的初始隐藏层状态</span></span><br><span class=\"line\">\t<span class=\"comment\"># values其实就是encoder每个时间步的隐藏层状态，所以下面需要将query扩展一个时间步维度进行之后的操作</span></span><br><span class=\"line\">\thidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">\tscores = V(tf.nn.tanh(W1(hidden_with_time_axis + values)))</span><br><span class=\"line\">\tattention_weights = tf.nn.softmax(scores, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\tcontext_vector = tf.matmul(attention_weights, values)</span><br><span class=\"line\">\tcontext_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> tf.keras.Model(inputs=[query, values], outputs=[attention_weights, context_vector])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">luong_attention_dot</span>(<span class=\"params\">query: tf.Tensor, value: tf.Tensor</span>) -&gt; tf.Tensor:</span></span><br><span class=\"line\">\t <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t :param query: decoder的前一个状态</span></span><br><span class=\"line\"><span class=\"string\">\t :param value: encoder的output</span></span><br><span class=\"line\"><span class=\"string\">\t &quot;&quot;&quot;</span></span><br><span class=\"line\">\t hidden_with_time_axis = tf.expand_dims(query, <span class=\"number\">1</span>)</span><br><span class=\"line\">\t scores = tf.matmul(hidden_with_time_axis, value, transpose_b=<span class=\"literal\">True</span>)</span><br><span class=\"line\">\t attention_weights = tf.nn.softmax(scores, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">\t context_vector = tf.matmul(attention_weights, value)</span><br><span class=\"line\">\t context_vector = tf.reduce_mean(context_vector, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"Self-Attention、Multi-Head-Attention\"><a href=\"#Self-Attention、Multi-Head-Attention\" class=\"headerlink\" title=\"Self-Attention、Multi-Head Attention\"></a>Self-Attention、Multi-Head Attention</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Link</a></li>\n</ul>\n<p>Transformer用的就是Self-Attention、Multi-Head Attention。对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入，首先我们要计算Q与K之间的点乘，然后为了防止其结果过大，会除以一个尺度标度 $\\sqrt{d_k}$ ，其中 $d_k$ 为一个query和key向量的维度。再利用Softmax操作将其结果归一化为概率分布，然后再乘以矩阵V就得到权重求和的表示。多头Attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention，两个的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201219161334371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>计算公式如下：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$   $$head_i=Attention(q_i,K,V)$$    $$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O$$</p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">scaled_dot_product_attention</span>(<span class=\"params\">query: tf.Tensor, key: tf.Tensor, value: tf.Tensor, mask: tf.Tensor=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t计算注意力权重。</span></span><br><span class=\"line\"><span class=\"string\">    q, k, v 必须具有匹配的前置维度。</span></span><br><span class=\"line\"><span class=\"string\">    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。</span></span><br><span class=\"line\"><span class=\"string\">    虽然 mask 根据其类型（填充或前瞻）有不同的形状，</span></span><br><span class=\"line\"><span class=\"string\">    但是 mask 必须能进行广播转换以便求和。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    参数:</span></span><br><span class=\"line\"><span class=\"string\">      q: 请求的形状 == (..., seq_len_q, depth)</span></span><br><span class=\"line\"><span class=\"string\">      k: 主键的形状 == (..., seq_len_k, depth)</span></span><br><span class=\"line\"><span class=\"string\">      v: 数值的形状 == (..., seq_len_v, depth_v)</span></span><br><span class=\"line\"><span class=\"string\">      mask: Float 张量，其形状能转换成</span></span><br><span class=\"line\"><span class=\"string\">            (..., seq_len_q, seq_len_k)。默认为None。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    返回值:</span></span><br><span class=\"line\"><span class=\"string\">      输出，注意力权重</span></span><br><span class=\"line\"><span class=\"string\">\t&quot;&quot;&quot;</span></span><br><span class=\"line\">\tmatmul_qk = tf.matmul(q, k, transpose_b=<span class=\"literal\">True</span>)  <span class=\"comment\"># (..., seq_len_q, seq_len_k)</span></span><br><span class=\"line\">    <span class=\"comment\"># 缩放 matmul_qk</span></span><br><span class=\"line\">    dk = tf.cast(tf.shape(k)[-<span class=\"number\">1</span>], tf.float32)</span><br><span class=\"line\">    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)</span><br><span class=\"line\">    <span class=\"comment\"># 将 mask 加入到缩放的张量上。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scaled_attention_logits += (mask * -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    <span class=\"comment\"># softmax 在最后一个轴（seq_len_k）上归一化，因此分数相加等于1。</span></span><br><span class=\"line\">    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-<span class=\"number\">1</span>)  <span class=\"comment\"># (..., seq_len_q, seq_len_k)</span></span><br><span class=\"line\">    output = tf.matmul(attention_weights, v)  <span class=\"comment\"># (..., seq_len_q, depth_v)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output, attention_weights</span><br></pre></td></tr></table></figure>\n<h1 id=\"Location-Sensitive-Attention\"><a href=\"#Location-Sensitive-Attention\" class=\"headerlink\" title=\"Location Sensitive Attention\"></a>Location Sensitive Attention</h1><ul>\n<li><a href=\"https://proceedings.neurips.cc/paper/2015/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf\">Link</a><br>语音合成中的Tacotron2用的就是Location Sensitive Attention，即对位置敏感的Attention，也就是说加入了位置特征，是一种混合注意力机制（见最后一节说明）。原论文中提出，基于内容的Attention对于所输入内容的输入序列中的绝对位置能够跟踪捕获信息，但是在较长的语音片段中性能迅速下降，所以作者为了解决这个问题，通过将辅助的卷积特征作为输入添加到注意机制中来实现的，而这些卷积特征是通过将前一步的注意力权重进行卷积而提取的。结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201219221547858.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>计算公式如下：<br>$$e_{ij}=score(s_{i-1},ca_{i-1}, h_j)=v_a^Ttanh(Ws_i+Vh_j+Uf_{i,j}+b)$$</p>\n<p>其中，$s_i$ 为当前解码器隐状态而非上一步解码器隐状态，偏置值 $b$ 被初始化为 $0$。位置特征 $f_i$ 使用累加注意力权重 $ca_i$ 卷积而来：<br>$$f_i=F*ca_{i-1}$$    $$ca_i=\\sum_{j=1}^{i-1}a_j$$</p>\n<p>复现代码（以TensorFlow2为例），注意，将如下实现应用到实际模型中，需要根据具体模型微调：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Attention</span>(<span class=\"params\">tf.keras.layers.Layer</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, attention_dim, attention_filters, attention_kernel</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Attention, self).__init__()</span><br><span class=\"line\">        self.attention_dim = attention_dim</span><br><span class=\"line\">        self.attention_location_n_filters = attention_filters</span><br><span class=\"line\">        self.attention_location_kernel_size = attention_kernel</span><br><span class=\"line\">        self.query_layer = tf.keras.layers.Dense(</span><br><span class=\"line\">            self.attention_dim, use_bias=<span class=\"literal\">False</span>, activation=<span class=\"string\">&quot;tanh&quot;</span>)</span><br><span class=\"line\">        self.memory_layer = tf.keras.layers.Dense(</span><br><span class=\"line\">            self.attention_dim, use_bias=<span class=\"literal\">False</span>, activation=<span class=\"string\">&quot;tanh&quot;</span>)</span><br><span class=\"line\">        self.V = tf.keras.layers.Dense(<span class=\"number\">1</span>, use_bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.location_layer = LocationLayer(self.attention_location_n_filters, self.attention_location_kernel_size,</span><br><span class=\"line\">                                            self.attention_dim)</span><br><span class=\"line\">        self.score_mask_value = -<span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_alignment_energies</span>(<span class=\"params\">self, query, memory, attention_weights_cat</span>):</span></span><br><span class=\"line\">        processed_query = self.query_layer(tf.expand_dims(query, axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">        processed_memory = self.memory_layer(memory)</span><br><span class=\"line\"></span><br><span class=\"line\">        attention_weights_cat = tf.transpose(attention_weights_cat, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        processed_attention_weights = self.location_layer(</span><br><span class=\"line\">            attention_weights_cat)</span><br><span class=\"line\">        energies = tf.squeeze(self.V(tf.nn.tanh(</span><br><span class=\"line\">            processed_query + processed_attention_weights + processed_memory)), -<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> energies</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span>(<span class=\"params\">self, attention_hidden_state, memory, attention_weights_cat</span>):</span></span><br><span class=\"line\">        alignment = self.get_alignment_energies(</span><br><span class=\"line\">            attention_hidden_state, memory, attention_weights_cat)</span><br><span class=\"line\">        attention_weights = tf.nn.softmax(alignment, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention_context = tf.expand_dims(attention_weights, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        attention_context = tf.matmul(attention_context, memory)</span><br><span class=\"line\">        attention_context = tf.squeeze(attention_context, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> attention_context, attention_weights</span><br></pre></td></tr></table></figure>\n<h1 id=\"Attention形式\"><a href=\"#Attention形式\" class=\"headerlink\" title=\"Attention形式\"></a>Attention形式</h1><p>关于Attention形式和获取信息方式的总结，可参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/35739040\">Attention用于NLP的一些小结</a>。我接下来陈列出具体形式下的相关论文（这里的陈列的论文我并没有全部研读，单纯在这里汇总，往后有空或者需要用到对应Attention时，再仔细研读）。</p>\n<h2 id=\"Soft-attention、global-attention、动态attention\"><a href=\"#Soft-attention、global-attention、动态attention\" class=\"headerlink\" title=\"Soft attention、global attention、动态attention\"></a>Soft attention、global attention、动态attention</h2><p>这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量可能会比较大一些。</p>\n<h2 id=\"Hard-attention\"><a href=\"#Hard-attention\" class=\"headerlink\" title=\"Hard attention\"></a>Hard attention</h2><p>这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）</p>\n<h2 id=\"Local-Attention（半软半硬attention）\"><a href=\"#Local-Attention（半软半硬attention）\" class=\"headerlink\" title=\"Local Attention（半软半硬attention）\"></a>Local Attention（半软半硬attention）</h2><p>这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">A Context-aware Attention Network for Interactive Question Answering</a></li>\n<li><a href=\"https://discovery.ucl.ac.uk/id/eprint/10066102/1/Wang_Dynamic%20attention%20deep%20model%20for%20article%20recommendation%20by%20learning%20human%20editors%27%20demonstration_AAM.pdf\">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors’ Demonstration</a></li>\n</ul>\n<h2 id=\"Concatenation-based-Attention\"><a href=\"#Concatenation-based-Attention\" class=\"headerlink\" title=\"Concatenation-based Attention\"></a>Concatenation-based Attention</h2><ul>\n<li><a href=\"https://ai.tencent.com/ailab/media/publications/Wei_Liu-Attentive_Collaborative_Filtering_Multimedia_Recommendation-SIGIR17.pdf\">Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention</a></li>\n<li><a href=\"https://arxiv.org/pdf/1706.05764.pdf\">Dipole: Diagnosis Prediction in Healthcare via Aention-based Bidirectional Recurrent Neural Networks</a></li>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3077136.3080699\">Enhancing Recurrent Neural Networks with Positional Attention for Question Answering</a></li>\n<li><a href=\"http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2031.pdf\">Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.06664.pdf\">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a></li>\n</ul>\n<h2 id=\"静态attention\"><a href=\"#静态attention\" class=\"headerlink\" title=\"静态attention\"></a>静态attention</h2><p>对输出句子共用一个 $s_t$ 的attention就够了，一般用在Bilstm的首位hidden state输出拼接起来作为 $s_t$ </p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">Teaching Machines to Read and Comprehend</a></li>\n<li><a href=\"https://pdfs.semanticscholar.org/a97b/5db17acc731ef67321832dbbaf5766153135.pdf\">Supervised Sequence Labelling with Recurrent Neural Networks</a></li>\n</ul>\n<h2 id=\"多层Attention\"><a href=\"#多层Attention\" class=\"headerlink\" title=\"多层Attention\"></a>多层Attention</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1612.07411.pdf\">A Context-aware Attention Network for Interactive Question Answering</a></li>\n<li><a href=\"http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2031.pdf\">Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention</a></li>\n<li><a href=\"https://ai.tencent.com/ailab/media/publications/Wei_Liu-Attentive_Collaborative_Filtering_Multimedia_Recommendation-SIGIR17.pdf\">Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention</a></li>\n<li><a href=\"https://www.researchgate.net/profile/Pengjie_Ren/publication/318764168_Leveraging_Contextual_Sentence_Relations_for_Extractive_Summarization_Using_a_Neural_Attention_Model/links/5d890c34299bf1996f98c6d6/Leveraging-Contextual-Sentence-Relations-for-Extractive-Summarization-Using-a-Neural-Attention-Model.pdf\">Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model</a><h1 id=\"说在最后\"><a href=\"#说在最后\" class=\"headerlink\" title=\"说在最后\"></a>说在最后</h1>Attention的提出到现在拥有很多的变种，但是经典的还是Bahdanau Attention和Luong Attention，很多Attention都是对这两个进行改进的。其实学习了Attention的伙伴会发现，对于Attention而言，重要的是Score计算方法，对于不同的计算方法在下面做个总结：</li>\n<li>基于内容的注意力机制(content-based attention)：<br>$$e_{ij}=score(s_{i-1}, h_j)=v_a^Ttanh(W_as_{i-1}+U_ah_j)$$<br>其中，$s_{i−1}$ 为上一个时间步中解码器的输出(解码器隐状态，decoder hidden states)，$h_j$ 是编码器此刻输入(编码器隐状态，encoder hidden state j)，$v_a$、$W_a$ 和 $U_a$ 是待训练参数张量。由于 $U_ah_j$ 是独立于解码步i的，因此可以独立提前计算。基于内容的注意力机制能够将不同的输出与相应的输入元素连接，而与其位置无关。</li>\n<li>基于位置的注意力机制(location-based attention)：<br>$$e_{ij}=score(a_{i-1}, h_j)=v_a^Ttanh(Wh_j+Uf_{i,j})$$<br>其中，$f_{i,j}$ 是之前的注意力权重，$a_{i-1}$ 是经卷积而得的位置特征，$f_i=F∗α_{i−1}$，$v_a$、$W_a$、$U_a$ 和 $F$ 是待训练参数。基于位置的注意力机制仅关心序列元素的位置和它们之间的距离。基于位置的注意力机制会忽略静音或减少它们，因为该注意力机制没有发现输入的内容。</li>\n<li>混合注意力机制(hybrid attention)：<br>$$e_{ij}=score(s_{i-1},a_{i-1}, h_j)=v_a^Ttanh(Ws_{i-1}+Uh_j+Uf_{i,j})$$<br>顾名思义，混合注意力机制是上述两者注意力机制的结合。其中，$s_{i-1}$ 为之前的解码器隐状态，$a_{i-1}$ 是之前的注意力权重，$h_j$ 是第j个编码器隐状态。为其添加偏置值b，最终的score函数计算如下：<br>$$e_{ij}=v_a^Ttanh(Ws_{i-1}+Vh_j+Uf_{i,j}+b)$$<br>其中，$v_a$、$W$、$V$、$U$ 和 $b$ 为待训练参数，$s_{i−1}$ 为上一个时间步中解码器隐状态，$h_j$ 是当前编码器隐状态，$f_{i,j}$ 是之前的注意力权重 $a_{i-1}$ 经卷积而得的位置特征(location feature)，$f_i=F∗α_{i−1}$。混合注意力机制能够同时考虑内容和输入元素的位置。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\">Attn: Illustrated Attention</a></li>\n<li><a href=\"https://blog.floydhub.com/attention-mechanism/\">Attention Mechanism</a></li>\n<li><a href=\"https://www.cnblogs.com/mengnan/p/9527797.html\">声谱预测网络</a></li>\n<li><a href=\"https://krntneja.github.io/posts/2018/attention-based-models-1\">Tutorial on Attention-based Models</a> </li>\n<li><a href=\"https://blog.csdn.net/BVL10101111/article/details/78470716\">Attention Model（mechanism） 的 套路</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/319866371\">Performer: 基于正交随机特征的快速注意力计算</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","NLP","Attention","注意力机制","Paper"]},{"title":"ReentrantLock你了解多少（结合Lock、AQS进行讲解）","url":"/Java/f18e9be6919f/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p><strong>如果觉得有所收获，记得的点个关注和点个赞，感谢支持。</strong><br>本篇文章要讲的是<code>Lock</code> 接口，重点强调 <code>ReentrantLock</code> 类，相关的接口在JUC 包里面，自 JDK 5 起，Java 类库中新提供了 <code>java.util.concurrent</code> 包（通常简称为 JUC 包）。Java 中有两种对并发资源加锁的方式，除了 <code>synchronized</code> 之外（不清楚的可以查看我之前写过的一篇关于<a href=\"https://dengbocong.blog.csdn.net/article/details/105303316\">synchronize文章</a>），还有本篇文章要讲的 <code>Lock</code>。<code>synchronized</code> 是 JVM 通过底层实现的，而 <code>Lock</code> 是通过 JDK 纯粹在软件层面上实现的。</p>\n<h2 id=\"先来讲讲-Lock-接口\"><a href=\"#先来讲讲-Lock-接口\" class=\"headerlink\" title=\"先来讲讲 Lock 接口\"></a>先来讲讲 Lock 接口</h2><p>Lock 类本身是一个接口，对锁进行了规范，Lock 接口的定义如下（我这里删除了源码的注释，这样不占用版面）：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">Lock</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">lockInterruptibly</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException</span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">boolean</span> <span class=\"title\">tryLock</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">boolean</span> <span class=\"title\">tryLock</span><span class=\"params\">(<span class=\"keyword\">long</span> time, TimeUnit unit)</span> <span class=\"keyword\">throws</span> InterruptedException</span>;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">unlock</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Condition <span class=\"title\">newCondition</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面可以看到，Lock 接口一共规范给定了 6 个方法。其中最为常用的，是 <code>lock()</code> 方法和 <code>unlock()</code> 方法，这两个方法必须成对出现，否则就有可能出现异常，使用逻辑如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 假如已经创建了一个lock对象</span></span><br><span class=\"line\">lock.lock();</span><br><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    lock.unlock();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里使用 <code>lock</code> 上锁，与使用 <code>synchronized</code> 上锁的效果是相同的，但在使用上从大括号代码块变为 try 代码块，并且一定要使用 finally 语句为 <code>lock</code> 对象解锁。可以查阅阿里巴巴的 Java 代码规约，在里面已经说的非常明白了，内容如下：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200412170128889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Lock 接口规定了四种上锁，除了上文说到的最传统的 lock() 方法之外，还有以下三种：</p>\n<ul>\n<li>lockInterruptibly() 会处理线程中断的上锁</li>\n<li>tryLock() 尝试上锁并立即返回，上锁成功则 true，上锁失败则 false</li>\n<li>tryLock(long time, TimeUnit unit) 尝试一段时间上锁后返回，上锁成功则 true，上锁失败则 false</li>\n</ul>\n<p>除以上上锁方法之外，最后还有一个方法 newCondition()，该方法用于协调线程，这个后面再提。</p>\n<h2 id=\"讲讲线程相关的知识\"><a href=\"#讲讲线程相关的知识\" class=\"headerlink\" title=\"讲讲线程相关的知识\"></a>讲讲线程相关的知识</h2><p>在讲解线程中断之前呢，需要来了解一下线程相关的一些知识，我之前写过一篇博文，是有关在Java中如何使用线程，不清楚的可以过去看看，这里讲解线程的使用逻辑，即线程的状态，以及线程中断的逻辑。<br>通常意义上线程有六种状态，但依我来看线程实际上只有两种状态：可运行状态、不可运行状态。</p>\n<ul>\n<li>可运行状态：线程可以运行，但是并不一定正在运行，细分的话可以分为正在运行和等待运行两种状态。</li>\n<li>不可运行状态：线程不能运行，可能是主动的（主动等待），也可能是被动的（要用的资源被锁住了）。细分的话能分为三种状态：无限期等待状态、限期等待状态、阻塞状态，前两种是线程自己发起的，第三种是线程被迫的。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200412173448627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>对各个状态分别进行解释：</p>\n<ul>\n<li><p><code>New</code> 新增：线程刚刚创建（例如 <code>Thread t = new Thread()</code>），还没有执行代码</p>\n</li>\n<li><p><code>Runnable</code> 可运行：线程可以运行（例如 <code>thread.start()</code>），但并不代表一定在运行，是否正在运行要看虚拟机和 CPU 的线程调度情况。CPU 将时间划分为 <code>10-20 ms</code> 的一个个时间片，在每一个时间片中执行一条线程，到时间就切换（切换地太快导致似乎在并行执行多条线程），这被称为 CPU 在调度线程。在 <code>Runnable</code> 状态下，每一条线程都有可能会被执行，但是执行和切换的速度都很快，非要分出来是在执行还是在等待并没有太大的意义。</p>\n<ul>\n<li><code>Ready</code> 等待运行：等待 CPU 调度</li>\n<li><code>Running</code> 正在运行：CPU 正在执行</li>\n</ul>\n</li>\n<li><p><code>Waiting</code> 无限期等待：线程主动等待，并且不设置等待结束的时间，直到被其他线程“唤醒”（例如 <code>thread.join()</code>）。</p>\n</li>\n<li><p><code>Timed Waiting</code> 限期等待：线程主动等待，但是设置一个等待的时长，到时间就自动唤醒（例如 <code>thread.sleep(sleepTime)</code>），在等待的这段时间也可以被其他线程“唤醒”。</p>\n</li>\n<li><p><code>Blocked</code> 阻塞等待：线程被动等待，因为抢锁失败了，被迫等着（例如使用 <code>synchronized</code> 同时让多条线程获取资源，总有线程会被迫等待）。</p>\n</li>\n</ul>\n<p>有关线程状态还可以剖析地更深一些：</p>\n<ul>\n<li>Java 的 <code>Thread</code> 类看似是一个寻常的 Java 对象，实际上可以视为对底层系统操作线程的封装，因此使用 <code>Thread</code> 类时不能完全按照面向对象的常规思维来思考，而是要以底层硬件的实现逻辑来思考。</li>\n<li>上文我将线程分为了可运行状态和不可运行状态，细分析的话，这实际上是指 CPU 有没有为线程分配时间片。在另外的地方（线程和进程的区别）学习到，线程是操作系统能够调度的最小单位，“能调度的最小单位“这种说法，就是指 CPU 划分出一个个时间片，每一个时间片”调度“一个线程。可运行状态指的是 CPU 能够调度线程，而不可运行状态指的是 CPU 不能调度线程，比如某一个线程中执行 <code>Thread.sleep(sleepTime)</code> 方法，那么这个线程进入 <code>Timed Waiting</code> 状态，在这种状态下 CPU 不再调度该线程，直到该线程休眠时间结束，回到 <code>Runnable</code> 状态，CPU 才可以调度该线程，这个行为被称作线程的“挂起”。</li>\n<li>线程通过 <code>sleep(time)</code> 和 <code>wait(time)</code> 方法都可以进入 <code>Timed Waiting</code> 状态，CPU 都不再会调度该线程，但是 <code>sleep</code> 的一方不会释放锁，<code>wait</code> 的一方会释放锁。其他线程如果需要正在 <code>sleep</code> 的线程的资源，将一直阻塞到那个线程醒来再释放资源。</li>\n<li>只有使用 <code>synchronized</code> 才能导致线程进入 <code>Blocked</code> 状态，线程从 <code>Waiting</code> 状态无法直接进入 <code>Runnable</code> 状态，只能先进入 <code>Blocked</code> 状态去获取锁。（顺便一提，进入 <code>Waiting</code> 状态的 <code>wait()、notify()、notifyAll()</code> 方法，只能在 <code>synchronized</code> 代码块中使用）</li>\n</ul>\n<p>线程中断，这里的“中断”是一个颇有迷惑性的词语，它并不是指线程就此停止，而是指线程收到了一个“中断信号”，线程应该根据这个信号来自行了断一些事情（但是收到中断信号也可以不处理）。比如，线程 1 向线程 2 发送了一条中断信息，线程 2 的中断状态发生了改变，线程 2 根据中断状态来进行逻辑处理。所以我认为，中断是线程间通信的一种方式，通信的内容是“建议另一条线程停止行为”，但是线程并不一定采取意见，即使采取意见也绝不是终止线程，而是停止某个一直重复运行的行为，继续执行后续的代码。我目前所见，中断有两种使用场景：</p>\n<ul>\n<li>线程根据中断状态，停止某个循环（例如下面这段伪代码）</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(还没中断)&#123;</span><br><span class=\"line\">    循环执行</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">中断了，进行后续操作</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>如果线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 <code>InterruptedException</code>，从而提前结束该线程，但是不能中断 <code>I/O</code> 阻塞和 <code>synchronized</code> 锁阻塞。这里的用法是，当线程处于不可运行状态时（暂停 CPU 调度），以异常的形式，强制让线程处理中断，以恢复回到可运行状态（CPU 可调度）。虽然这是在处理异常，但实际上并不是指程序有什么错误，而是代表一种强制手段：必须要对中断进行处理。再换句话说，这是一种恢复线程状态，停止发呆的一种机制。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 当前线程休眠1秒</span></span><br><span class=\"line\">    Thread.sleep(<span class=\"number\">1000</span>);</span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 线程中断，不让继续休眠了，处理后续的业务逻辑</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>线程中断有三个相关方法：<br>| API |介绍  |<br>|–|–|<br>|public void interrupt()    |中断线程|<br>|public boolean isInterrupted()    |查看线程是否中断|<br>|public static boolean interrupted()    |静态方法，查看当前线程是否中断的同时，清除中断状态，即如果线程中断，执行之后将不再处于中断状态|</p>\n<p>中断的源码，以及阻塞状态下的线程抛出中断异常的原理，这里暂不考究了。在此只掌握到两点即可：</p>\n<ul>\n<li>线程中断不代表线程活动终止</li>\n<li>线程中断的基本原理，是给线程的中断标志位赋 true</li>\n</ul>\n<h2 id=\"聊一聊AQS\"><a href=\"#聊一聊AQS\" class=\"headerlink\" title=\"聊一聊AQS\"></a>聊一聊AQS</h2><p>AQS 可以算是 JUC 包的核心，一大片并发类，包括要学习的 <code>ReentrantLock</code> 锁，都是以 AQS 为内核，不了解 AQS 则无法继续学习。</p>\n<p>AQS 的全称是 <code>AbstractQueuedSynchronizer</code>（抽象队列同步器，中文一般简称“队列同步器”），它的作用正如其名，是一个队列，需要同步的线程们在队列里排队，每次让一个线程占用资源，剩下的线程在队列同步器里待命。这样的设计实现了这种效果：当多个线程争抢资源时，保证只会有一条线程在运行，其他线程都在等待队列里等候安排。打开 AQS 接口看源码，会看到多如牛毛的方法，初识 AQS 如果从这些方法着手，就可以准备去世了，因此我们从 AQS 的成员变量着手，对 AQS 进行猜测性学习。以下代码部分，基本全部参考自<a href=\"https://www.javadoop.com/post/AbstractQueuedSynchronizer\">《一行一行源码分析清楚 AbstractQueuedSynchronizer》</a>，这篇博文写的真的非常好</p>\n<p>AQS 重要的成员变量有四个，分别是：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 头结点，你直接把它当做【当前持有锁的线程】可能是最好理解的</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> <span class=\"keyword\">volatile</span> Node head;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 阻塞的尾节点，每个新的节点进来，都插入到最后，也就形成了一个链表</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> <span class=\"keyword\">volatile</span> Node tail;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 这个是最重要的，代表当前锁的状态，0代表没有被占用，大于 0 代表有线程持有当前锁</span></span><br><span class=\"line\"><span class=\"comment\">// 这个值可以大于 1，是因为锁可以重入，每次重入都加上 1</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">volatile</span> <span class=\"keyword\">int</span> state;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 代表当前持有独占锁的线程（该变量继承自父类），举个最重要的使用例子</span></span><br><span class=\"line\"><span class=\"comment\">// 因为锁可以重入，reentrantLock.lock()可以嵌套调用多次，所以每次用这个来判断当前线程是否已经拥有了锁</span></span><br><span class=\"line\"><span class=\"comment\">// if (currentThread == getExclusiveOwnerThread()) &#123;state++&#125;</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> Thread exclusiveOwnerThread;</span><br></pre></td></tr></table></figure>\n<p>AQS 接口中定义了一个内部类：Node，这个类是 AQS 队列的基本构成元素，即并发线程们在 AQS 队列里等候时，都是装在这个 Node 对象里排序的。Node 类源码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Node</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 标识节点当前在共享模式下</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Node SHARED = <span class=\"keyword\">new</span> Node();</span><br><span class=\"line\">    <span class=\"comment\">// 标识节点当前在独占模式下</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Node EXCLUSIVE = <span class=\"keyword\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// ======== 下面的几个int常量是给waitStatus用的 ===========</span></span><br><span class=\"line\">    <span class=\"comment\">// 代表此线程取消了争抢这个锁</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> CANCELLED = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 官方的描述是，其表示当前node的后继节点对应的线程需要被唤醒</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> SIGNAL = -<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 本文不分析condition</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> CONDITION = -<span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 同样的不分析，略过吧</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> PROPAGATE = -<span class=\"number\">3</span>;</span><br><span class=\"line\">    <span class=\"comment\">// =====================================================</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 取值为上面的1、-1、-2、-3，或者0(以后会讲到)</span></span><br><span class=\"line\">    <span class=\"comment\">// 这么理解，暂时只需要知道如果这个值 大于0 代表此线程取消了等待，</span></span><br><span class=\"line\">    <span class=\"comment\">//    ps: 半天抢不到锁，不抢了，ReentrantLock是可以指定timeouot的。。。</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> <span class=\"keyword\">int</span> waitStatus;</span><br><span class=\"line\">    <span class=\"comment\">// 前驱节点的引用</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Node prev;</span><br><span class=\"line\">    <span class=\"comment\">// 后继节点的引用</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Node next;</span><br><span class=\"line\">    <span class=\"comment\">// 这个就是线程本尊</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Thread thread;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Node 类的代码容易看得人一头雾水，初学时应当将其视为一个普通的链表节点，它必须需要</p>\n<ul>\n<li>Node prev：指向前个节点</li>\n<li>Node next：指向后个节点</li>\n<li>Thread Thread：本节点需要存储的内容</li>\n</ul>\n<p>除此之外该节点还有一个状态位：</p>\n<ul>\n<li>int waitStatus：节点状态，在之后的代码中很重要</li>\n</ul>\n<p>Node 类定义的其他内容不用太过纠结，看之后的代码会懂。根据学习这个类，以及参考学习其他 AQS 相关的博文，可以大概知道 AQS 队列的基本结构和设计逻辑是这样的：<br><img src=\"https://img-blog.csdnimg.cn/20200412190225398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>看图应该就能明白 AQS 的数据结构，需要注意的是，head 并不在 AQS 的阻塞队列当中。以下部分是 AQS 的源码分析，这部分的内容很难，可以不看，不会影响到 Lock 接口的学习。之前的代码中说过，使用 Lock 接口上锁的基本步骤是：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">lock.lock();\t\t--&gt; AQS#acquire()</span><br><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    lock.unlock();\t--&gt; AQS#release()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>实际上，<code>lock()</code> 和 <code>unlock()</code> 方法的原理，是使用 AQS 的 <code>acquire()</code> 和 <code>release()</code> 方法实现的，因此我们来粗略地学习这两个方法，并大致了解 AQS 的原理。（以下代码说明均为简略版，查看详细代码说明请参见上述博文）</p>\n<h4 id=\"上锁（新线程加入队列）\"><a href=\"#上锁（新线程加入队列）\" class=\"headerlink\" title=\"上锁（新线程加入队列）\"></a>上锁（新线程加入队列）</h4><p><img src=\"https://img-blog.csdnimg.cn/20200412190559687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"解锁（老线程执行完毕，传唤下一个线程）\"><a href=\"#解锁（老线程执行完毕，传唤下一个线程）\" class=\"headerlink\" title=\"解锁（老线程执行完毕，传唤下一个线程）\"></a>解锁（老线程执行完毕，传唤下一个线程）</h4><p><img src=\"https://img-blog.csdnimg.cn/20200412190802782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>AQS 的具体实现代码，我自认为是又长又难的，因此不把全部代码整理出来了，只在此记录一些点吧：</p>\n<ul>\n<li>AQS 中有大量的方法，是为了处理并发的，例如队列还是空的，同时有两个线程进来申请锁，如何来让一个线程拿到锁，另一个线程去队列里排队等候。AQS 解决并发问题的原理是 CAS（CAS 的原理去看上篇介绍 synchronized 的博文），AQS 去调用 JDK5 刚刚出现的 <code>sun.misc.Unsafe</code> 类里面的方法，这个类对 CPU 的 CAS 指令进行了封装。</li>\n<li>进入阻塞队列排队的线程会被挂起，而唤醒的操作是由前驱节点完成的。当占用锁的线程结束，调用 <code>unlock()</code> 方法，此时 AQS 会去队列里唤醒排在最前面的节点线程。</li>\n<li>AQS 接口确定了队列同步的主要逻辑，也就是上锁时线程先尝试获取锁，失败则加入队列；解锁时队列先尝试解除锁，如果解锁成功则唤醒后继节点。但是尝试获取锁和尝试解除锁这两个操作，都是交由子类去实现的。这就使得 AQS 框架确立了基础的并发队列机制，但锁的形式可以有各种不同。实际上每个锁（每个 AQS 接口的实现类）就是在重写 AQS 的 <code>tryAcquire()</code> 和 <code>tryRelease()</code> 方法，其他的都依赖于 AQS 接口代码。</li>\n<li>AQS 有两个很重要的变量，分别是队列的状态 <code>state</code>，以及队列节点的状态 <code>waitStatus</code>。<ul>\n<li><code>state</code>：0 代表锁没有被占用，1 代表有线程正在占用锁，1 往上代表有线程正在重入占用锁</li>\n<li><code>waitStatus</code>：0 代表初始化，大于 0 代表该节点取消了等待，-1 代表后继节点需要被唤醒</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"ReentrantLock\"><a href=\"#ReentrantLock\" class=\"headerlink\" title=\"ReentrantLock\"></a>ReentrantLock</h2><p>不容易呀，终于到了ReentrantLock，ReentrantLock 的字面意义是可重入锁，代表线程可以多次执行 <code>lock()</code> 方法占有锁，不会导致死锁问题。ReentrantLock 允许公平锁，只要在构造方法中传入 true（<code>new ReentrantLock(true)</code>）即可。公平锁的意思是，当多个线程获取锁时，按照先来后到的顺序，先申请锁的线程一定先得到锁，后申请锁的线程一定后得到锁。如果是非公平锁，那么各个线程获取到锁的顺序是“随机”的。对于 ReentrantLock 的非公平锁而言，后到的线程可以先试着获取一次锁，获取到了就直接返回，获取不到就跟公平锁一样在后面排队。ReentrantLock 实现公平锁和非公平锁的方式，是在内部维护两种 AQS 队列。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 非公平锁（Sync是一个AQS队列）</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NonfairSync</span> <span class=\"keyword\">extends</span> <span class=\"title\">Sync</span> </span>&#123;...&#125;</span><br><span class=\"line\"><span class=\"comment\">// 公平锁</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FairSync</span> <span class=\"keyword\">extends</span> <span class=\"title\">Sync</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure>\n<p>经过刚才对 AQS 的学习，我们知道学习锁实际上只需要看 tryAcquire() 和 tryRelease() 方法，其他都交由 AQS 接口就可以了。</p>\n<h3 id=\"上锁-tryAcquire\"><a href=\"#上锁-tryAcquire\" class=\"headerlink\" title=\"上锁 tryAcquire()\"></a>上锁 tryAcquire()</h3><h4 id=\"公平锁\"><a href=\"#公平锁\" class=\"headerlink\" title=\"公平锁\"></a>公平锁</h4><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 尝试直接获取锁，返回值是boolean，代表是否获取到锁</span></span><br><span class=\"line\"><span class=\"comment\">// 返回true：1.没有线程在等待锁；2.重入锁，线程本来就持有锁，也就可以理所当然可以直接获取</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Thread current = Thread.currentThread();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = getState();</span><br><span class=\"line\">    <span class=\"comment\">// state == 0 此时此刻没有线程持有锁</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 虽然此时此刻锁是可以用的，但是这是公平锁，既然是公平，就得讲究先来后到，</span></span><br><span class=\"line\">        <span class=\"comment\">// 看看有没有别人在队列中等了半天了</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasQueuedPredecessors() &amp;&amp;</span><br><span class=\"line\">            <span class=\"comment\">// 如果没有线程在等待，那就用CAS尝试一下，成功了就获取到锁了，</span></span><br><span class=\"line\">            <span class=\"comment\">// 不成功的话，只能说明一个问题，就在刚刚几乎同一时刻有个线程抢先了 =_=</span></span><br><span class=\"line\">            <span class=\"comment\">// 因为刚刚还没人的，我判断过了</span></span><br><span class=\"line\">            compareAndSetState(<span class=\"number\">0</span>, acquires)) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 到这里就是获取到锁了，标记一下，告诉大家，现在是我占用了锁</span></span><br><span class=\"line\">            setExclusiveOwnerThread(current);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 会进入这个else if分支，说明是重入了，需要操作：state=state+1</span></span><br><span class=\"line\">    <span class=\"comment\">// 这里不存在并发问题</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> nextc = c + acquires;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (nextc &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(<span class=\"string\">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class=\"line\">        setState(nextc);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 如果到这里，说明前面的if和else if都没有返回true，说明没有获取到锁</span></span><br><span class=\"line\">    <span class=\"comment\">// 回到上面一个外层调用方法（AQS的acquire()方法）继续看:</span></span><br><span class=\"line\">    <span class=\"comment\">// if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) </span></span><br><span class=\"line\">    <span class=\"comment\">//     selfInterrupt();</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"非公平锁\"><a href=\"#非公平锁\" class=\"headerlink\" title=\"非公平锁\"></a>非公平锁</h4><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 调用了nonfairTryAcquire()方法，往下看</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nonfairTryAcquire(acquires);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">nonfairTryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Thread current = Thread.currentThread();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = getState();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 与公平锁相比，只有这里有区别</span></span><br><span class=\"line\">        <span class=\"comment\">// 非公平锁不会先判断AQS队列中是否有等候的节点，而是直接试着获取一次锁</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果这次尝试获取不到，则和公平锁一样尾插队列</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (compareAndSetState(<span class=\"number\">0</span>, acquires)) &#123;</span><br><span class=\"line\">            setExclusiveOwnerThread(current);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> nextc = c + acquires;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (nextc &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(<span class=\"string\">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class=\"line\">        setState(nextc);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"公平锁和非公平锁只有两点区别\"><a href=\"#公平锁和非公平锁只有两点区别\" class=\"headerlink\" title=\"公平锁和非公平锁只有两点区别\"></a>公平锁和非公平锁只有两点区别</h4><ul>\n<li>非公平锁实际上会先 CAS 获取一次锁，如果失败则调用 AQS 的 <code>acquire()</code> 方法（这段上面没提）</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 非公平锁的lock()方法（会先CAS获取一次锁，获取不到再走AQS接口）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (compareAndSetState(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        setExclusiveOwnerThread(Thread.currentThread());</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        acquire(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 公平锁的lock()方法</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    acquire(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>在首次试着获取锁失败的情况下，非公平锁会在 <code>tryAcquire()</code> 方法中再试着获取一次锁，但是公平锁会严格地按照先来后到的顺序获取</li>\n</ul>\n<p>可以总结出来，非公平锁比公平锁多尝试获取了两次锁，如果成功就不用进入队列了。这样可以提高并发的线程吞吐量，但是有可能导致先等待的线程一直获取不到锁。</p>\n<h3 id=\"解锁-tryRelease\"><a href=\"#解锁-tryRelease\" class=\"headerlink\" title=\"解锁 tryRelease()\"></a>解锁 tryRelease()</h3><p>公平锁和非公平锁，共用一套解锁方法，也就是 <code>Lock#unlock() -&gt; AQS#release() -&gt; Lock#tryRelease() -&gt; AQS#unparkSuccessor()</code>，其中 <code>tryRelease()</code> 方法是交由实现类 ReentrantLock 去重写的（不明白的话回到上面看一看 AQS 的解锁逻辑）。ReentrantLock 重写的 <code>tryRelease()</code> 方法的代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryRelease</span><span class=\"params\">(<span class=\"keyword\">int</span> releases)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = getState() - releases;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.currentThread() != getExclusiveOwnerThread())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalMonitorStateException();</span><br><span class=\"line\">    <span class=\"comment\">// 是否完全释放锁</span></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> free = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 处理重入的问题，如果c==0，也就是说没有嵌套锁了，可以释放了，否则还不能释放掉</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        free = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        setExclusiveOwnerThread(<span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setState(c);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> free;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>ReentrantLock 作为可重入锁，每次上锁就使 AQS 队列的状态（初始化是 0）增加 1，解锁使状态减少 1，如果 AQS 队列的状态变为 0 了，就代表没有线程持有锁。</p>\n<h2 id=\"ReentrantLock使用\"><a href=\"#ReentrantLock使用\" class=\"headerlink\" title=\"ReentrantLock使用\"></a>ReentrantLock使用</h2><p>这里模拟售票，通过ReentrantLock的方式实现线程的安全</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LockMain</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        Window window = <span class=\"keyword\">new</span> Window();</span><br><span class=\"line\">        Thread thread1 = <span class=\"keyword\">new</span> Thread(window);</span><br><span class=\"line\">        Thread thread2 = <span class=\"keyword\">new</span> Thread(window);</span><br><span class=\"line\">        Thread thread3 = <span class=\"keyword\">new</span> Thread(window);</span><br><span class=\"line\">        thread1.start();</span><br><span class=\"line\">        thread2.start();</span><br><span class=\"line\">        thread3.start();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 售票窗口</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Window</span> <span class=\"keyword\">implements</span> <span class=\"title\">Runnable</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">volatile</span> <span class=\"keyword\">int</span> num = <span class=\"number\">100</span>;</span><br><span class=\"line\">    ReentrantLock lock = <span class=\"keyword\">new</span> ReentrantLock();</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>)&#123;</span><br><span class=\"line\">            lock.lock();</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (num &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                    System.out.println(Thread.currentThread().getName()+<span class=\"string\">&quot;窗口在售票,票号为&quot;</span>+ num);</span><br><span class=\"line\">                    num --;</span><br><span class=\"line\">                &#125;<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                lock.unlock();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","categories":["Java"],"tags":["Java","线程","ReentrantLock"]},{"title":"SpringBoot整合JPA，配置多数据库","url":"/Spring-Boot/3ba32bdaaf99/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>SpringBoot创建项目非常方便，而且进行数据访问抛弃了很多繁琐的配置，我前面写的系列博文中，有教大家如何使用SpringBoot进行数据访问，里面谈到了整合JDBC、MyBatis以及JPA。我自己实际开发中，如果没有什么要求限制的话，比较习惯使用JPA进行数据访问，所以在这里，我专门编写一篇博文，来教如何使用SpringBoot整合JPA，进行多数据库的配置，<strong>如果有帮助，记得点个关注和点个赞哦</strong>。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>开始之前呢，我们需要先创建项目啦，创建项目使用的是Idea的Spring Initializr进行创建，选择SpringBoot场景的时候，勾选Web、Spring Data JPA、MySQL Driver三个就可以了，如下，然后项目创建成功。<br><img src=\"https://img-blog.csdnimg.cn/20200305220146190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200305220205741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"主配置文件\"><a href=\"#主配置文件\" class=\"headerlink\" title=\"主配置文件\"></a>主配置文件</h2><p>如果我们只是进行一个数据库的访问，我们只需要对数据库进行简单的配置，提供相应账号和密码就可以了，不过多个数据库也不是很麻烦，也就是相当于多一份配置内容出来而已，所有配置内容如下</p>\n<p><strong>application.properties</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">#数据库统一配置</span><br><span class=\"line\">spring.jpa.hibernate.ddl-auto=update</span><br><span class=\"line\">spring.jpa.show-sql=<span class=\"keyword\">true</span></span><br><span class=\"line\">spring.jpa.properties.hibernate.hbm2ddl.auto=update</span><br><span class=\"line\">spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5InnoDBDialect</span><br><span class=\"line\">spring.jpa.properties.hibernate.format_sql=<span class=\"keyword\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\">#主数据库</span><br><span class=\"line\">spring.datasource.primary.jdbc-url=jdbc:mysql:<span class=\"comment\">//localhost/ubiquity?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class=\"line\">spring.datasource.primary.username=root</span><br><span class=\"line\">spring.datasource.primary.password=<span class=\"number\">123456</span></span><br><span class=\"line\">spring.datasource.primary.driver-<span class=\"class\"><span class=\"keyword\">class</span>-<span class=\"title\">name</span></span>=com.mysql.jdbc.Driver</span><br><span class=\"line\"></span><br><span class=\"line\">#副数据库</span><br><span class=\"line\">spring.datasource.secondary.jdbc-url=jdbc:mysql:<span class=\"comment\">//localhost/ubiquity_vote?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class=\"line\">spring.datasource.secondary.username=root</span><br><span class=\"line\">spring.datasource.secondary.password=<span class=\"number\">123456</span></span><br><span class=\"line\">spring.datasource.secondary.driver-<span class=\"class\"><span class=\"keyword\">class</span>-<span class=\"title\">name</span></span>=com.mysql.jdbc.Driver</span><br></pre></td></tr></table></figure>\n<h2 id=\"配置类\"><a href=\"#配置类\" class=\"headerlink\" title=\"配置类\"></a>配置类</h2><p>我们都知道，我们在配置文件中写的配置，需要我们通过配置类的注入，覆盖掉默认的配置，这样才会生效，所以，我们能够想到，既然是需要使用多个数据库，对应的自然会有多个数据库的相关配置类（这篇博文的示例中，我使用两个数据库，所以有两个数据配置类）<br><img src=\"https://img-blog.csdnimg.cn/20200305220757374.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>我们知道，我们首先在只配置类中编写两个配置数据库的组件，然后分别其组件名，具体内容如下。</p>\n<p><strong>DataSourceConfig.java</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Config;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.autoconfigure.orm.jpa.HibernateProperties;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.autoconfigure.orm.jpa.HibernateSettings;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.autoconfigure.orm.jpa.JpaProperties;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.context.properties.ConfigurationProperties;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.jdbc.DataSourceBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Primary;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.sql.DataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Map;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataSourceConfig</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> JpaProperties jpaProperties;</span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> HibernateProperties hibernateProperties;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;primaryDataSource&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"meta\">@ConfigurationProperties(&quot;spring.datasource.primary&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">firstDataSource</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> DataSourceBuilder.create().build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;secondaryDataSource&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@ConfigurationProperties(&quot;spring.datasource.secondary&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">secondDataSource</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> DataSourceBuilder.create().build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;vendorProperties&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Map&lt;String, Object&gt; <span class=\"title\">getVendorProperties</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hibernateProperties.determineHibernateProperties(</span><br><span class=\"line\">                jpaProperties.getProperties(), <span class=\"keyword\">new</span> HibernateSettings());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>然后我们编写了针对两个数据库的配置类，里面的代码非常的相似，相信你敲了一遍之后，能够感悟到点什么。</p>\n<p><strong>PrimaryConfig.java</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Config;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Qualifier;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Primary;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.data.jpa.repository.config.EnableJpaRepositories;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.orm.jpa.JpaTransactionManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.transaction.PlatformTransactionManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.transaction.annotation.EnableTransactionManagement;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.EntityManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.sql.DataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Map;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableTransactionManagement</span></span><br><span class=\"line\"><span class=\"meta\">@EnableJpaRepositories(</span></span><br><span class=\"line\"><span class=\"meta\">        entityManagerFactoryRef = &quot;entityManagerFactoryPrimary&quot;,</span></span><br><span class=\"line\"><span class=\"meta\">        transactionManagerRef = &quot;transactionManagerPrimary&quot;,</span></span><br><span class=\"line\"><span class=\"meta\">        basePackages = &#123;&quot;com.dbc.ubiquity.Repository.Primary&quot;&#125;//Dao层的位置</span></span><br><span class=\"line\"><span class=\"meta\">)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrimaryConfig</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"meta\">@Qualifier(&quot;primaryDataSource&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> DataSource primaryDataSource;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"meta\">@Qualifier(&quot;vendorProperties&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Map&lt;String, Object&gt; vendorProperties;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;entityManagerFactoryPrimary&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> LocalContainerEntityManagerFactoryBean <span class=\"title\">entityManagerFactoryPrimary</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> builder</span><br><span class=\"line\">                .dataSource(primaryDataSource)</span><br><span class=\"line\">                .properties(vendorProperties)</span><br><span class=\"line\">                .packages(<span class=\"string\">&quot;com.dbc.ubiquity.Model.Primary&quot;</span>)<span class=\"comment\">//实体类的位置</span></span><br><span class=\"line\">                .persistenceUnit(<span class=\"string\">&quot;primaryPersistenceUnit&quot;</span>)</span><br><span class=\"line\">                .build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;entityManagerPrimary&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> EntityManager <span class=\"title\">entityManager</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> entityManagerFactoryPrimary(builder).getObject().createEntityManager();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;transactionManagerPrimary&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"function\">PlatformTransactionManager <span class=\"title\">transactionManagerPrimary</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> JpaTransactionManager(entityManagerFactoryPrimary(builder).getObject());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>SecondaryConfig.java</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Config;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Qualifier;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.data.jpa.repository.config.EnableJpaRepositories;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.orm.jpa.JpaTransactionManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.transaction.PlatformTransactionManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.transaction.annotation.EnableTransactionManagement;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.EntityManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.sql.DataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Map;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableTransactionManagement</span></span><br><span class=\"line\"><span class=\"meta\">@EnableJpaRepositories(</span></span><br><span class=\"line\"><span class=\"meta\">        entityManagerFactoryRef = &quot;entityManagerFactorySecondary&quot;,</span></span><br><span class=\"line\"><span class=\"meta\">        transactionManagerRef = &quot;transactionManagerSecondary&quot;,</span></span><br><span class=\"line\"><span class=\"meta\">        basePackages = &#123;&quot;com.dbc.ubiquity.Repository.Secondary&quot;&#125;</span></span><br><span class=\"line\"><span class=\"meta\">)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecondaryConfig</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"meta\">@Qualifier(&quot;secondaryDataSource&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> DataSource secondaryDataSource;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    <span class=\"meta\">@Qualifier(&quot;vendorProperties&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Map&lt;String, Object&gt; vendorProperties;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;entityManagerFactorySecondary&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> LocalContainerEntityManagerFactoryBean <span class=\"title\">entityManagerFactorySecondary</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> builder</span><br><span class=\"line\">                .dataSource(secondaryDataSource)</span><br><span class=\"line\">                .properties(vendorProperties)</span><br><span class=\"line\">                .packages(<span class=\"string\">&quot;com.dbc.ubiquity.Model.Secondary&quot;</span>)</span><br><span class=\"line\">                .persistenceUnit(<span class=\"string\">&quot;secondaryPersistenceUnit&quot;</span>)</span><br><span class=\"line\">                .build();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;entityManagerSecondary&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> EntityManager <span class=\"title\">entityManager</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> entityManagerFactorySecondary(builder).getObject().createEntityManager();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;transactionManagerSecondary&quot;)</span></span><br><span class=\"line\">    <span class=\"function\">PlatformTransactionManager <span class=\"title\">transactionManagerSecondary</span><span class=\"params\">(EntityManagerFactoryBuilder builder)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> JpaTransactionManager(entityManagerFactorySecondary(builder).getObject());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"实体类\"><a href=\"#实体类\" class=\"headerlink\" title=\"实体类\"></a>实体类</h2><p>首先我先把实体类的目录结构放在这，方便后面编写，然后说道实体类的创建，这里可以使用Idea帮我们逆向生成实体类，也就是依照在数据库中已经创建好的数据库表，自动生成实体类，不过这种方式生成出来的实体类，不符合现在编写JPA的序列化格式，生成出来的还是要增改（当然啦，你可以直接去更改实体类生成的模板，没错是可以改的，具体怎么改我这里就不赘述了，自己百度就可以知道），我就来说手动创建，手动创建的好处就是能够对知识点进行更加深入的掌握，当然啦，敲起来比较费时间。<br><img src=\"https://img-blog.csdnimg.cn/20200305221401892.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><strong>User.java</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Entity.Primary;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Entity</span></span><br><span class=\"line\"><span class=\"meta\">@Table(name = &quot;USER&quot;, schema = &quot;ubiquity&quot;, catalog = &quot;&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> <span class=\"keyword\">implements</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">1L</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Id</span></span><br><span class=\"line\">    <span class=\"meta\">@GeneratedValue(strategy = GenerationType.IDENTITY)</span></span><br><span class=\"line\">    <span class=\"meta\">@Column(name = &quot;id&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> id;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String userName;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String passWord;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String email;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = true, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String nickName;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String regTime;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">User</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">User</span><span class=\"params\">(String userName, String passWord, String email, String nickName, String regTime)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.userName = userName;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.passWord = passWord;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.email = email;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.nickName = nickName;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.regTime = regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">getId</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setId</span><span class=\"params\">(<span class=\"keyword\">long</span> id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.id = id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getUserName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> userName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setUserName</span><span class=\"params\">(String userName)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.userName = userName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getPassWord</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> passWord;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPassWord</span><span class=\"params\">(String passWord)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.passWord = passWord;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getEmail</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> email;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setEmail</span><span class=\"params\">(String email)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.email = email;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getNickName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nickName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setNickName</span><span class=\"params\">(String nickName)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.nickName = nickName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getRegTime</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRegTime</span><span class=\"params\">(String regTime)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.regTime = regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>Userq.java</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Entity.Secondary;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Entity</span></span><br><span class=\"line\"><span class=\"meta\">@Table(name = &quot;USERQ&quot;, schema = &quot;ubiquity_vote&quot;, catalog = &quot;&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Userq</span> <span class=\"keyword\">implements</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">1L</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Id</span></span><br><span class=\"line\">    <span class=\"meta\">@GeneratedValue(strategy = GenerationType.IDENTITY)</span></span><br><span class=\"line\">    <span class=\"meta\">@Column(name = &quot;id&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> id;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String userName;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String passWord;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String email;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = true, unique = true)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String nickName;</span><br><span class=\"line\">    <span class=\"meta\">@Column(nullable = false)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String regTime;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Userq</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Userq</span><span class=\"params\">(String userName, String passWord, String email, String nickName, String regTime)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.userName = userName;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.passWord = passWord;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.email = email;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.nickName = nickName;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.regTime = regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">getId</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setId</span><span class=\"params\">(<span class=\"keyword\">long</span> id)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.id = id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getUserName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> userName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setUserName</span><span class=\"params\">(String userName)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.userName = userName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getPassWord</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> passWord;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPassWord</span><span class=\"params\">(String passWord)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.passWord = passWord;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getEmail</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> email;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setEmail</span><span class=\"params\">(String email)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.email = email;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getNickName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nickName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setNickName</span><span class=\"params\">(String nickName)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.nickName = nickName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getRegTime</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRegTime</span><span class=\"params\">(String regTime)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.regTime = regTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>写到这里，就要说一下前面主配置文件里面的一些配置是什么了，如下<br><img src=\"https://img-blog.csdnimg.cn/20200305222133532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>它还有其他属性，如下：</p>\n<ul>\n<li>ddl-auto:create—-每次运行该程序，没有表格会新建表格，表内有数据会清空</li>\n<li>ddl-auto:create-drop—-每次程序结束的时候会清空表</li>\n<li>ddl-auto:update—-每次运行程序，没有表格会新建表格，表内有数据不会清空，只会更新</li>\n<li>ddl-auto:validate—-运行程序会校验数据与数据库的字段类型是否相同，不同会报错</li>\n</ul>\n<h2 id=\"Dao层\"><a href=\"#Dao层\" class=\"headerlink\" title=\"Dao层\"></a>Dao层</h2><p><img src=\"https://img-blog.csdnimg.cn/20200305222354158.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Repository.Primary;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Entity.Primary.User;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.data.jpa.repository.JpaRepository;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">UserPrimaryPository</span> <span class=\"keyword\">extends</span> <span class=\"title\">JpaRepository</span>&lt;<span class=\"title\">User</span>, <span class=\"title\">Long</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"function\">User <span class=\"title\">findById</span><span class=\"params\">(<span class=\"keyword\">long</span> id)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">User <span class=\"title\">findByUserName</span><span class=\"params\">(String userName)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">User <span class=\"title\">findByUserNameOrEmail</span><span class=\"params\">(String username, String email)</span></span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity.Repository.Secondary;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Entity.Secondary.Userq;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.data.jpa.repository.JpaRepository;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">UserSecondaryPository</span> <span class=\"keyword\">extends</span> <span class=\"title\">JpaRepository</span>&lt;<span class=\"title\">Userq</span>, <span class=\"title\">Long</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"function\">Userq <span class=\"title\">findById</span><span class=\"params\">(<span class=\"keyword\">long</span> id)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Userq <span class=\"title\">findByUserName</span><span class=\"params\">(String userName)</span></span>;</span><br><span class=\"line\">    <span class=\"function\">Userq <span class=\"title\">findByUserNameOrEmail</span><span class=\"params\">(String username, String email)</span></span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>到此，我们的多数据库配置流程到此结束，最后就是测试阶段了，我们来验证一下我们的配置是否有用。测试类的内容如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.dbc.ubiquity;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Entity.Primary.User;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Entity.Secondary.Userq;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Repository.Primary.UserPrimaryPository;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.dbc.ubiquity.Repository.Secondary.UserSecondaryPository;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.jupiter.api.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.text.DateFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UbiquityApplicationTests</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Resource</span></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> UserPrimaryPository userPrimaryPository;</span><br><span class=\"line\">\t<span class=\"meta\">@Resource</span></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> UserSecondaryPository userSecondaryPository;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testSave</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception</span>&#123;</span><br><span class=\"line\">\t\tDate date = <span class=\"keyword\">new</span> Date();</span><br><span class=\"line\">\t\tDateFormat dateFormat = DateFormat.getDateTimeInstance(DateFormat.LONG, DateFormat.LONG);</span><br><span class=\"line\">\t\tString formattedDate = dateFormat.format(date);</span><br><span class=\"line\">\t\tuserPrimaryPository.save(<span class=\"keyword\">new</span> User(<span class=\"string\">&quot;aa&quot;</span>, <span class=\"string\">&quot;aa123456&quot;</span>,<span class=\"string\">&quot;aa@126.com&quot;</span>, <span class=\"string\">&quot;aa&quot;</span>,  formattedDate));</span><br><span class=\"line\">\t\tuserPrimaryPository.save(<span class=\"keyword\">new</span> User(<span class=\"string\">&quot;bb&quot;</span>, <span class=\"string\">&quot;bb123456&quot;</span>,<span class=\"string\">&quot;bb@126.com&quot;</span>, <span class=\"string\">&quot;bb&quot;</span>,  formattedDate));</span><br><span class=\"line\">\t\tuserSecondaryPository.save(<span class=\"keyword\">new</span> Userq(<span class=\"string\">&quot;cc&quot;</span>, <span class=\"string\">&quot;cc123456&quot;</span>,<span class=\"string\">&quot;cc@126.com&quot;</span>, <span class=\"string\">&quot;cc&quot;</span>,  formattedDate));</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testDelete</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\t\tuserPrimaryPository.deleteAll();</span><br><span class=\"line\">\t\tuserSecondaryPository.deleteAll();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testBaseQuery</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\tDate date = <span class=\"keyword\">new</span> Date();</span><br><span class=\"line\">\t\tDateFormat dateFormat = DateFormat.getDateTimeInstance(DateFormat.LONG, DateFormat.LONG);</span><br><span class=\"line\">\t\tString formattedDate = dateFormat.format(date);</span><br><span class=\"line\">\t\tUser user=<span class=\"keyword\">new</span> User(<span class=\"string\">&quot;ff&quot;</span>, <span class=\"string\">&quot;ff123456&quot;</span>,<span class=\"string\">&quot;ff@126.com&quot;</span>, <span class=\"string\">&quot;ff&quot;</span>,  formattedDate);</span><br><span class=\"line\">\t\tUserq userq=<span class=\"keyword\">new</span> Userq(<span class=\"string\">&quot;ff&quot;</span>, <span class=\"string\">&quot;ff123456&quot;</span>,<span class=\"string\">&quot;ff@126.com&quot;</span>, <span class=\"string\">&quot;ff&quot;</span>,  formattedDate);</span><br><span class=\"line\">\t\tuserPrimaryPository.findAll();</span><br><span class=\"line\">\t\tuserSecondaryPository.findById(<span class=\"number\">3l</span>);</span><br><span class=\"line\">\t\tuserSecondaryPository.save(userq);</span><br><span class=\"line\">\t\tuser.setId(<span class=\"number\">2l</span>);</span><br><span class=\"line\">\t\tuserPrimaryPository.delete(user);</span><br><span class=\"line\">\t\tuserPrimaryPository.count();</span><br><span class=\"line\">\t\tuserSecondaryPository.findById(<span class=\"number\">3l</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">contextLoads</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20200305222802445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200305222821677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","JPA","数据库"]},{"title":"Transformer的9种变体概览","url":"/Deep-Learning/c649454aeb0a/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：The Transformer Family<br>原文链接：<a href=\"https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html\">Link</a><br>nlp-paper：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>nlp-dialogue：<a href=\"https://github.com/DengBoCong/nlp-dialogue\">一个开源的全流程对话系统，更新中！</a><br>说明：阅读原文时进行相关思想、结构、优缺点，内容进行提炼和记录，原文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>正式进入文章之前，我们先来看看后续将会涉及到的数学符号的含义，如下：<br><img src=\"https://img-blog.csdnimg.cn/20210220153552563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Attention-and-Self-Attention\"><a href=\"#Attention-and-Self-Attention\" class=\"headerlink\" title=\"Attention and Self-Attention\"></a>Attention and Self-Attention</h1><p>Attention是神经网络中的一种机制，即<strong>模型可以通过选择性地关注给定的数据集来学习做出预测</strong>。Attention的个数是通过学习权重来量化的，输出则通常是一个加权平均值。而Self-Attention是一种Attention机制，模型利用对同一样本观测到的其他部分来对数据样本的剩下部分进行预测。从概念上讲，它感觉非常类似于non-local的方式。还要注意的是，Self-attention是置换不变的，换句话说，它是对集合的一种操作。</p>\n<p>关于attention和self-attention存在非常多的形式，我们之前常见的Transformer是依赖于scaled-dot-product的形式，即给定query矩阵Q, key矩阵K以及value矩阵V，那么我们的输出就是值向量的加权和，其中，分配给每个值槽的权重由Query与相应Key的点积确定：<br>$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$<br>对于一个query以及一个key向量，$q_i,k_j\\in R^d$，我们计算下面的值：<br>$$a_{ij}=softmax(\\frac{q_ik_j^T}{\\sqrt{d_k}})=\\frac{exp(q_ik_j^T)}{\\sqrt{d_k}\\sum_{r\\in S_i}exp(q_ik_r^T)}$$<br>其中，$S_i$ 是与第 $i$ 个query计算的keys的集合。</p>\n<h1 id=\"Multi-Head-Self-Attention\"><a href=\"#Multi-Head-Self-Attention\" class=\"headerlink\" title=\"Multi-Head Self-Attention\"></a>Multi-Head Self-Attention</h1><p>multi-head self-attention是Transformer的核心组成部分，和简单的attention不同之处在于，Multihead机制将输入拆分为许多小的chunks，然后并行计算每个子空间的scaled dot product，最后我们将所有的attention输出进行简单的串联拼接到期望的维度。<br>$$MultiheadAttention(X_q,X_k,X_v)=[head_1;…;head_h]W^ohead_i=Attention(X_qW_i^qX_kW_i^kX_vW_i^v)$$<br>其中，$[.;.]$ 是concate操作，$W_i^q,W_i^k\\in \\mathbb{R}^{d\\times d_k/h},W_i^v\\in \\mathbb{R}^{d\\times d_v/h}$ 是权重矩阵，它将我们的输出embeddings(大小为$L\\times d$)的映射到query,key,value矩阵，而且 $W^o\\in R^{d_v\\times d}$ 是输出的线性转化，这些权重都是在训练的时候进行训练的，结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20210220162256486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h1><p>Transformer，很多时候我们也称之为”vanilla Transformer”， 它有一个encoder-decoder的结构，就像许多 NMT 模型中常用的那样。decoder的Transformer可以在语言建模的时候获得非常好的效果，比如 GPT 和 BERT。</p>\n<h2 id=\"Encoder-Decoder结构\"><a href=\"#Encoder-Decoder结构\" class=\"headerlink\" title=\"Encoder-Decoder结构\"></a>Encoder-Decoder结构</h2><p>Encoder生成一个基于attention的表示，能够从一个大的上下文中定位一个特定的信息片段。它由6个身份识别模块组成，每个模块包含两个子模块、一个multihead self-attention和一个point-wise全连接前馈网络。按point-wise来说，这意味着它对序列中的每个元素应用相同的线性变换（具有相同的权重），<strong>这也可以看作是滤波器大小为 $1$ 的卷积层</strong>。每个子模块都有一个剩余连接和layer normalization。所有子模块输出相同维度 $d$ 的数据。</p>\n<p>Transformer的decoder功能是从encoder的表示中抽取信息。该结构与encoder非常相似，只是decoder包含两个多头注意子模块，而不是在每个相同的重复模块中包含一个。第一个多头注意子模块被屏蔽，以防止位置穿越。<br><img src=\"https://img-blog.csdnimg.cn/20210220165217493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Positional-Encoding\"><a href=\"#Positional-Encoding\" class=\"headerlink\" title=\"Positional Encoding\"></a>Positional Encoding</h2><p>因为self-attention操作是permutation不变的，所以使用正确的位置编码是非常重要的，此处我们使用如下的位置编码来提供order信息，位置编码 $P\\in R^{L\\times d}$，我们可以直接将它们加入到我们到vanilla Transformer中：</p>\n<ul>\n<li>Sinusoidal positional encoding（正弦位置编码）的定义如下，给定token的位置 $i=1,…,L$，维度 $\\delta=1,2,…,d$ ：<br>$$PR(i,\\delta)=\\left{\\begin{matrix} sin(\\frac{i}{10000^{2\\delta/d}}) &amp; if\\ \\delta=2\\delta^{‘} \\ cos(\\frac{i}{10000^{2\\delta/d}}) &amp; if\\ \\delta=2\\delta^{‘}+1 \\end{matrix}\\right.$$<br>这样，位置编码的每个维度对应一个不同波长、不同维度的正弦曲线，从 $2\\pi$ 到 $10000\\cdot 2\\pi$<br><img src=\"https://img-blog.csdnimg.cn/20210220170511607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>Learned positional encoding（学习位置编码），顾名思义，给每个元素赋予一个学习列向量，对其绝对位置进行编码，详细可参见论文：<a href=\"https://arxiv.org/pdf/1705.03122.pdf\">Convolutional Sequence to Sequence Learning</a>。</li>\n</ul>\n<h2 id=\"Quick-Follow-ups\"><a href=\"#Quick-Follow-ups\" class=\"headerlink\" title=\"Quick Follow-ups\"></a>Quick Follow-ups</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1808.04444.pdf\">Paper Link</a></li>\n</ul>\n<p>在vanilla Transformer之后，<a href=\"https://arxiv.org/pdf/1808.04444.pdf\">Al-Rfou 等人</a>增加了一组辅助损失，以便能够在字符级语言建模方面训练一个深层的Transformer模型，该模型的性能优于 LSTMs，使用了几种类型的辅助任务：</p>\n<ul>\n<li>除了在序列末尾只生成一个预测之外，还要求每个immediatge位置能做出正确的预测，迫使模型预测给定的较小上下文（例如，上下文窗口开头的前几个tokens）。</li>\n<li>每个中间Transformer也用于进行预测，随着训练的进行，较低层的权重对总损失的贡献越来越小。</li>\n<li>序列中的每个位置可以预测多个目标，即对未来token的两个或多个预测。<br><img src=\"https://img-blog.csdnimg.cn/20210220171848794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h1 id=\"Adaptive-Computation-Time-ACT\"><a href=\"#Adaptive-Computation-Time-ACT\" class=\"headerlink\" title=\"Adaptive Computation Time (ACT)\"></a>Adaptive Computation Time (ACT)</h1></li>\n<li><a href=\"https://arxiv.org/pdf/1603.08983.pdf\">Paper Link</a></li>\n</ul>\n<p>Adaptive Computation Time是一种动态决定递归神经网络需要多少计算步骤的机制。比如说，我们有一个RNN模型$R$，它由输入的权重$W_x$，一个参数化的状态迁移函数 $S(\\cdot)$，一个输出权重 $W_y$ 和一个输出的bias $b_y$ 组成。给定一个输入序列 $(x_1,…,x_L)$ ，输出的序列 $(y_1,…,y_L)$ 的计算公式如下：<br>$$s_t=S(s_{t-1},W_xx_t,y_t=W_ys_t+b_y,\\ for\\ t=1,…,L)$$<br>ACT使上述RNN设置能够在每个输入元素上执行数量可变的步骤。大量的计算步骤会导致中间状态序列 $(s_t^1,…,s_t^{N(t)})$ 并且输出$(y_t^1,…,y_t^{N(t)})$，它们都共享相同的迁移状态函数 $S(\\cdot)$，以及相同的输出权重 $W_y,b_y$：<br>$$s_t^0=s_{t-1}\\s_t^n=S(s_t^{n-1},x_t^n),y_t=S(s_t^{n-1},x_t+\\delta_{n,1}),\\ for\\ n=1,…,N(t)\\y_t^n=W_ys_t^n+b_y$$<br>其中 $\\delta_{n,1}$ 是一个二元的flag，来表示是否输入步是递增的。step的个数 $N(t)$ 是由额外的sigmoidal halting单元 $h$ 决定的，带有相关的权重矩阵 $W_h$ 以及bias $b_h$, 对于第 $t$ 输入元素在中间步骤处 $n$ 输出一个中止概率 $p_t^n$：<br>$$h_t^n=\\delta(W_hs_t^n+b_n)$$<br>为了使计算在一个步骤后停止，ACT引入了一个小常数 $\\epsilon$（例如0.01），因此每当累积概率超过$1-\\epsilon$ 时，计算就会停止。<br>$$N(t)=min(min{n^{‘}:\\sum_{n=1}^{n^{‘}}h_t^n\\geq1-\\epsilon},M)$$    $$p_t^n=\\left{\\begin{matrix} h_t^n &amp; if\\ n&lt;N(t) \\ R(t)=1-\\sum_{n=1}^{N(t)-1}h_t^n &amp; if\\ n=N(t)   \\end{matrix}\\right.  $$<br>其中 $M$ 为中间步骤个数的上限，最终状态和输出的mean-field的update：<br><img src=\"https://img-blog.csdnimg.cn/20210220174033572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为了避免对每个输入进行不必要的思考，ACT增加了ponder cost：$\\mathcal{P}(x) = \\sum_{t=1}^L N(t) + R(t)$，用此来鼓励中间计算步骤的小的次数。</p>\n<h1 id=\"Improved-Attention-Span\"><a href=\"#Improved-Attention-Span\" class=\"headerlink\" title=\"Improved Attention Span\"></a>Improved Attention Span</h1><p>提高Attention Span的目的是使可用于self-attention的上下文更长、更有效、更灵活。</p>\n<h2 id=\"Longer-Attention-Span-Transformer-XL\"><a href=\"#Longer-Attention-Span-Transformer-XL\" class=\"headerlink\" title=\"Longer Attention Span(Transformer-XL)\"></a>Longer Attention Span(Transformer-XL)</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1901.02860.pdf\">Paper Link</a></li>\n</ul>\n<p>vanilla Transformer有一个固定的和有限的attention span。在每个更新步骤中，该模型只能处理同一段中的其他元素，并且没有任何信息可以在分离的固定长度段之间流动。也就是说层数固定不够灵活，同时对于算力需求非常大，导致其并不适合处理超长序列。这种context segmentation会导致几个问题：</p>\n<ul>\n<li>模型不能捕获非常长的依赖关系;</li>\n<li>在没有上下文或上下文很弱的情况下，很难预测每个片段中的前几个tokens。</li>\n<li>评估成本昂贵，每当segment右移一位时，新的segment就会从头开始重新处理，尽管有很多重叠的tokens。</li>\n</ul>\n<p>Transformer-XL通过两个主要的改进来解决上下文的segmentation问题：</p>\n<ul>\n<li>对于segments之间的隐藏状态进行重复使用；</li>\n<li>使用位置编码使其适用于重新使用的states;</li>\n</ul>\n<h3 id=\"Hidden-state-Reuse\"><a href=\"#Hidden-state-Reuse\" class=\"headerlink\" title=\"Hidden state Reuse\"></a>Hidden state Reuse</h3><p><img src=\"https://img-blog.csdnimg.cn/20210220175513996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们对第 $n$ 层 $r+1$ 的segment的隐藏状态打标签为 $\\mathbf{h}<em>{\\tau+1}^{(n)} \\in \\mathbb{R}^{L \\times d}$，除了对相同segment $\\mathbf{h}</em>{\\tau+1}^{(n-1)}$ 的最后一层的隐藏状态，我们还依赖于之前的segment $\\mathbf{h}<em>{\\tau}^{(n)}$ 的相同层的隐藏状态。通过从前面隐藏状态加入信息，模型可以将attention的span（广度）进行扩大，可以在多个segments之间发挥作用：<br>$$\\begin{aligned}<br>\\color{red}{\\widetilde{\\mathbf{h}}</em>{\\tau+1}^{(n-1)}} &amp;= [\\text{stop-gradient}(\\mathbf{h}<em>{\\tau}^{(n-1)}) \\circ \\mathbf{h}</em>{\\tau+1}^{(n-1)}] \\<br>\\mathbf{Q}<em>{\\tau+1}^{(n)} &amp;= \\mathbf{h}</em>{\\tau+1}^{(n-1)}\\mathbf{W}^q \\<br>\\mathbf{K}<em>{\\tau+1}^{(n)} &amp;= \\color{red}{\\widetilde{\\mathbf{h}}</em>{\\tau+1}^{(n-1)}} \\mathbf{W}^k \\<br>\\mathbf{V}<em>{\\tau+1}^{(n)} &amp;= \\color{red}{\\widetilde{\\mathbf{h}}</em>{\\tau+1}^{(n-1)}} \\mathbf{W}^v \\<br>\\mathbf{h}<em>{\\tau+1}^{(n)} &amp;= \\text{transformer-layer}(\\mathbf{Q}</em>{\\tau+1}^{(n)}, \\mathbf{K}<em>{\\tau+1}^{(n)}, \\mathbf{V}</em>{\\tau+1}^{(n)})<br>\\end{aligned}$$<br>key和value依赖于扩展的隐藏状态，同时query仅仅只依赖于当前步的隐藏状态，$[. \\odot .]$ 是序列长度的维度的concatenation操作。</p>\n<h2 id=\"Relative-Positional-Encoding\"><a href=\"#Relative-Positional-Encoding\" class=\"headerlink\" title=\"Relative Positional Encoding\"></a>Relative Positional Encoding</h2><p>为了处理这种新的attention span的形式，Transformer-XL提出了一种新的位置编码。如果使用相同的方法对绝对位置进行编码，则前一段和当前段将分配相同的编码，这是不需要的。为了保持位置信息流在各段之间的一致性，Transformer XL对相对位置进行编码，因为它足以知道位置的offset，从而做出更好的预测，即 $i-j$，在一个key向量 $\\mathbf{k}<em>{\\tau, j}$ 以及它的query  $\\mathbf{q}</em>{\\tau, i}$ 之间。</p>\n<p>如果我们省略 $1/\\sqrt{d_k}$ 并且对它们以softmax的形式进行normalize，我们可以重写在位置 $i$ 的query和位置 $j$ 的key之间的attention分数：<br>$$\\begin{aligned}<br>a_{ij}<br>&amp;= \\mathbf{q}<em>i {\\mathbf{k}<em>j}^\\top = (\\mathbf{x}_i + \\mathbf{p}_i)\\mathbf{W}^q ((\\mathbf{x}_j + \\mathbf{p}_j)\\mathbf{W}^k)^\\top \\<br>&amp;= \\mathbf{x}_i\\mathbf{W}^q {\\mathbf{W}^k}^\\top\\mathbf{x}_j^\\top + \\mathbf{x}_i\\mathbf{W}^q {\\mathbf{W}^k}^\\top\\mathbf{p}_j^\\top + \\mathbf{p}_i\\mathbf{W}^q {\\mathbf{W}^k}^\\top\\mathbf{x}_j^\\top + \\mathbf{p}_i\\mathbf{W}^q {\\mathbf{W}^k}^\\top\\mathbf{p}_j^\\top<br>\\end{aligned}$$<br>上面的几项又可以被表示为：<br>$$a</em>{ij}^\\text{rel} =<br>\\underbrace{ \\mathbf{x}_i\\mathbf{W}^q \\color{blue}{ {\\mathbf{W}_E^k}^\\top } \\mathbf{x}_j^\\top }_\\text{content-based addressing} +<br>\\underbrace{ \\mathbf{x}_i\\mathbf{W}^q \\color{blue}{ {\\mathbf{W}_R^k}^\\top } \\color{green}{\\mathbf{r}</em>{i-j}^\\top} }<em>\\text{content-dependent positional bias} +<br>\\underbrace{ \\color{red}{\\mathbf{u}} \\color{blue}{ {\\mathbf{W}_E^k}^\\top } \\mathbf{x}_j^\\top }_\\text{global content bias} +<br>\\underbrace{ \\color{red}{\\mathbf{v}} \\color{blue}{ {\\mathbf{W}_R^k}^\\top } \\color{green}{\\mathbf{r}</em>{i-j}^\\top} }_\\text{global positional bias}$$</p>\n<ul>\n<li>用相对位置编码 $\\mathbf{r}_{i-j} \\in \\mathbf{R}^{d}$ 替换$p_j$</li>\n<li>用两个可训练的参数 $\\mu$(针对内容)和 $v$(针对位置)替换$\\mathbf{p}_i\\mathbf{W}^q$;</li>\n<li>将 $\\mathbf{W}^k$ 划分为两个矩阵，$\\mathbf{W}^k_E$ 用于内容信息，$\\mathbf{W}^k_R$ 用于位置信息</li>\n</ul>\n<h2 id=\"Adaptive-Attention-Span\"><a href=\"#Adaptive-Attention-Span\" class=\"headerlink\" title=\"Adaptive Attention Span\"></a>Adaptive Attention Span</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1905.07799.pdf\">Paper Link</a></li>\n</ul>\n<p>Transformer的一个关键优势是能够捕获长期依赖关系。根据上下文的不同，模型可能倾向于注意到更远的序列，或者一个attention head可能有不同于另一个attention head的注意模式。如果attention span能够灵活地调整其长度，并且只在需要时再往回看，这将有助于减少计算和内存开销，从而在模型中支持更长的最大上下文大小(这就是Adaptive Attention Span的动机)。</p>\n<p>后来Sukhbaatar等人提出了一种self-attention机制以寻找最优的attention span，他们假设不同的attention heads可以在相同的上下文窗口中赋予不同的分数，因此最优的span可以被每个头分开训练：<br><img src=\"https://img-blog.csdnimg.cn/20210220182622357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>给定第 $i$ 个token,我们需要计算该token和其它在位置 $j \\in S_i$ 的keys的attention权重，其中 $S_i$ 定义了第 $i$ 个token第上下文窗口：<br>$$\\begin{aligned}<br>e_{ij} &amp;= \\mathbf{q}<em>i {\\mathbf{k}<em>j}^\\top \\<br>a</em>{ij} &amp;= \\text{softmax}(e</em>{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{r=i-s}^{i-1} \\exp(e_{ir})} \\<br>\\mathbf{y}<em>i &amp;= \\sum</em>{r=i-s}^{i-1}a_{ir}\\mathbf{v}<em>r = \\sum</em>{r=i-s}^{i-1}a_{ir}\\mathbf{x}<em>r\\mathbf{W}^v<br>\\end{aligned}$$<br>增加了一个soft mask函数 $m_z$ 来控制有效的可调attention span，它将query和key之间的距离映射成一个[0, 1]值，$m_z \\in [0, s]$参数化，$z$ 要学习：<br>$$m_z(x) = \\text{clamp}(\\frac{1}{R}(R+z-x), 0, 1)$$<br>其中 $R$ 是一个超参数，它可以定义 $m_z$ 的softness：<br><img src=\"https://img-blog.csdnimg.cn/20210220182948660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>soft mask函数应用于注意权重中的softmax元素：<br>$$a</em>{ij} = \\frac{m_z(i-j)\\exp(s_{ij})}{\\sum_{r=i-s}^{i-1}m_z(i-r) \\exp(s_{ir})}$$<br>在上面的等式，$z$ 是可微的，所以可以和模型的其它部分一起联合训练，参数 $z^{(i)}, i=1, \\dots, h$ 每个head可以分开学习，此外，损失函数有额外的 $L_1$ 惩罚 $\\sum_{i=1}^h z^{(i)}$。利用Adaptive Computation Time，该方法可以进一步增强attention span的长度，动态地适应当前输入。attention head在时间 $t$ 的跨度参数 $z_t$ 是一个sigmoid函数，$z_t = S \\sigma(\\mathbf{v} \\cdot \\mathbf{x}_t +b)$，其中向量和偏置标量与其他参数一起学习。</p>\n<p>在具有自适应attention span的Transformer实验中，Sukhbatar等人发现了一个普遍趋势，即较低层不需要很长的attention span，而较高层的一些attention heads可能会使用非常长的attention span。适应性attention span有助于大大减少失败的次数，特别是在一个有许多注意层和大上下文长度的大模型中。</p>\n<h2 id=\"Localized-Attention-Span-Image-Transformer\"><a href=\"#Localized-Attention-Span-Image-Transformer\" class=\"headerlink\" title=\"Localized Attention Span (Image Transformer)\"></a>Localized Attention Span (Image Transformer)</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1802.05751.pdf\">Paper Link</a></li>\n</ul>\n<p>Transformer最初用于语言建模。文本序列是一维的，具有明确的时间顺序，因此attention span随着上下文大小的增加而线性增长。</p>\n<p>然而，如果我们想在图像上使用Transformer，我们还不清楚如何定义上下文的范围或顺序。Image Transformer采用了一种图像生成公式，类似于Transformer框架内的序列建模。此外，图像Transformer将self-attention span限制在局部邻域内，因此模型可以放大以并行处理更多的图像，并保持可能性损失可控。encoder-decoder架构保留用于image-conditioned生成：</p>\n<ul>\n<li>encoder生成源图像的上下文化的每像素信道表示；</li>\n<li>decoder自回归地生成输出图像，每个时间步每像素一个通道。</li>\n</ul>\n<p>让我们将要生成的当前像素的表示标记为查询 $q$。其表示将用于计算 $q$ 的其他位置是关键向量 $\\mathbf{k}_1, \\mathbf{k}_2, \\dots$，它们一起形成一个内存矩阵 $M$。$M$ 的范围定义了像素查询 $q$ 的上下文窗口。Image Transformer引入了两种类型的localized $M$ ，如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20210220183903806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li><p>1D Local Attention：输入图像按光栅扫描顺序（即从左到右、从上到下）展平。然后将线性化后的图像分割成不重叠的查询块。上下文窗口由与 $q$ 相同的查询块中的像素和在此查询块之前生成的固定数量的附加像素组成。</p>\n</li>\n<li><p>2D Local Attention：图像被分割成多个不重叠的矩形查询块。查询像素可以处理相同内存块中的所有其他像素。为了确保左上角的像素也可以有一个有效的上下文窗口，内存块将分别向上、左和右扩展一个固定的量。</p>\n</li>\n</ul>\n<h1 id=\"Less-Time-and-Memory-Cost\"><a href=\"#Less-Time-and-Memory-Cost\" class=\"headerlink\" title=\"Less Time and Memory Cost\"></a>Less Time and Memory Cost</h1><p>本节介绍如何减少计算和内存的消耗。</p>\n<h2 id=\"Sparse-Attention-Matrix-Factorization-Sparse-Transformers\"><a href=\"#Sparse-Attention-Matrix-Factorization-Sparse-Transformers\" class=\"headerlink\" title=\"Sparse Attention Matrix Factorization (Sparse Transformers)\"></a>Sparse Attention Matrix Factorization (Sparse Transformers)</h2><p>一般Transformer的计算和存储开销随序列长度呈二次增长，因此很难应用于很长的序列。</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1904.10509.pdf\">Paper Link</a><h3 id=\"Sparse-Transformer\"><a href=\"#Sparse-Transformer\" class=\"headerlink\" title=\"Sparse Transformer\"></a>Sparse Transformer</h3>引入分解的self-attention，通过稀疏矩阵分解，我们可以训练上百层的dense的attention网络，这样序列长度就可以到达16384。给定attention连接的模式集合 $\\mathcal{S} = {S_1, \\dots, S_n}$，其中 $S_i$ 记录key位置的集合，第 $i$ 个query向量可以扩展为：<br>$$\\begin{aligned}<br>\\text{Attend}(\\mathbf{X}, \\mathcal{S}) &amp;= \\Big( a(\\mathbf{x}<em>i, S_i) \\Big)_{i \\in {1, \\dots, L}} \\<br>\\text{ where } a(\\mathbf{x}_i, S_i) &amp;= \\text{softmax}\\Big(\\frac{(\\mathbf{x}_i \\mathbf{W}^q)(\\mathbf{x}_j \\mathbf{W}^k)</em>{j \\in S_i}^\\top}{\\sqrt{d_k}}\\Big) (\\mathbf{x}<em>j \\mathbf{W}^v)</em>{j \\in S_i}<br>\\end{aligned}$$<br>尽管 $S_i$ 的size是不固定的，$a(\\mathbf{x}_i, S_i)$ 是size为 $d_v$ 的，因此，$\\text{Attend}(\\mathbf{X}, \\mathcal{S}) \\in \\mathbb{R}^{L \\times d_v}$。在自回归的模型中，一个attention span被定义为 $S_i = {j: j \\leq i}$，它允许每个token可以处理过去的所有其它位置。</li>\n</ul>\n<p>在分解的self-attention中，$S_i$ 被分解为树的依赖，例如对于没对 $(i,j)$，其中 $j \\leq i$, 存在一条路径链接 $i$ 和 $j$。更加精确地说，集合 $S_i$ 被划分为 $p$ 个non-overlapping的子集，第 $m$ 个子集被表示为$A^{(m)}_i\\subset S_i,m=1,\\dots,p$，所以输出位置 $i$ 和任意的 $j$ 的路径有最大长度 $p+1$，例如，如果 $(j, a, b, c, \\dots, i)$ 是 $i$ 和 $j$ 的索引路径，我们有$j \\in A_a^{(1)}, a \\in A_b^{(2)}, b \\in A_c^{(3)}, \\dots$。</p>\n<h3 id=\"Sparse-Factorized-Attention\"><a href=\"#Sparse-Factorized-Attention\" class=\"headerlink\" title=\"Sparse Factorized Attention\"></a>Sparse Factorized Attention</h3><p>Sparse Transformer提出了两类分解的attention，如下：<br><img src=\"https://img-blog.csdnimg.cn/20210220185438999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>Strided attention(stride $\\ell \\sim \\sqrt{n}$ ，在图像中,每个像素可以链接到所有到之前 $\\ell$ 个像素raster scanning顺序，然后那些像素在相同列中相互链接。<br>$$\\begin{aligned}<br>A_i^{(1)} &amp;= { t, t+1, \\dots, i} \\text{, where } t = \\max(0, i - \\ell) \\<br>A_i^{(2)} &amp;= {j: (i-j) \\mod \\ell = 0}<br>\\end{aligned}$$</li>\n<li>Fixed attention，一个小的tokens集合总结之前的位置并且向未来的位置传递信息：<br>$$\\begin{aligned}<br>A_i^{(1)} &amp;= {j: \\lfloor \\frac{j}{\\ell} \\rfloor = \\lfloor \\frac{i}{\\ell} \\rfloor } \\<br>A_i^{(2)} &amp;= {j: j \\mod \\ell \\in {\\ell-c, \\dots, \\ell-1} }<br>\\end{aligned}$$<br>其中 $c$ 是一个超参数。</li>\n</ul>\n<h3 id=\"Use-Factorized-Self-Attention-in-Transformer\"><a href=\"#Use-Factorized-Self-Attention-in-Transformer\" class=\"headerlink\" title=\"Use Factorized Self-Attention in Transformer\"></a>Use Factorized Self-Attention in Transformer</h3><p>存在三种方式使用sparse factorized attention模式的方法：</p>\n<ul>\n<li>每个residual block的attention type，把它们交错起来，$\\text{attention}(\\mathbf{X}) = \\text{Attend}(\\mathbf{X}, A^{(n \\mod p)}) \\mathbf{W}^o$，其中 $n$ 是当前residual模块的index</li>\n<li>设置一个单独的head，它负责所有分解head负责的位置，$\\text{attention}(\\mathbf{X}) = \\text{Attend}(\\mathbf{X}, \\cup_{m=1}^p A^{(m)}) \\mathbf{W}^o$</li>\n<li>使用一个和原始Transformer不同的multi-head attention机制，每个head可以接受上面的一种模式，1或者2，这往往效果表现更好</li>\n</ul>\n<p>Sparse Transformer还提出了一套改进方案，将Transformer训练到上百层，包括梯度检查点、在backward pass的时候重新计算attention和FF层、混合精度训练、高效的块稀疏实现等。</p>\n<h2 id=\"Locality-Sensitive-Hashing-Reformer\"><a href=\"#Locality-Sensitive-Hashing-Reformer\" class=\"headerlink\" title=\"Locality-Sensitive Hashing (Reformer)\"></a>Locality-Sensitive Hashing (Reformer)</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/2001.04451.pdf\">Paper Link</a></li>\n</ul>\n<p>Reformer模型旨在解决Transformer中的下面几个痛点：</p>\n<ul>\n<li>具有N层的模型中的内存比单层模型中的内存大N倍，因为我们需要存储反向传播的activations。</li>\n<li>中间FF层通常相当大。</li>\n<li>长度为 $L$ 的序列上的注意矩阵通常在内存和时间上都需要 $O(L^2)$ 的内存和时间；</li>\n</ul>\n<p>Reformer进行了两种改进：</p>\n<ul>\n<li>将dot-product的attention替换为locality-sensitive hashing(LSH) attention,这将时间复杂度从 $O(L^2)$ 降到 $O(LlogL)$</li>\n<li>标准residual block替换为reversible residual layer，这样在训练期间只允许存储一次激活，而不是 $N$ 次（即与层数成比例）。</li>\n</ul>\n<h3 id=\"Locality-Sensitive-Hashing-Attention\"><a href=\"#Locality-Sensitive-Hashing-Attention\" class=\"headerlink\" title=\"Locality-Sensitive Hashing Attention\"></a>Locality-Sensitive Hashing Attention</h3><p>在attention $QK^T$ 中，我们更加关注大的只，对于每个$\\mathbf{q}_i \\in \\mathbf{Q}$，我们在寻找 $K$ 中于 $q_i$ 最近的一个行向量，为了寻找它，我们在attention机制中加入：Locality-Sensitive Hashing (LSH)</p>\n<p>如果它保留了数据点之间的距离信息，我们称hashing机制 $x \\mapsto h(x)$ 是locality-sensitive的，这么做相近的向量可以获得相似的hash，Reformer中，给定一个固定的随机矩阵 $\\mathbf{R} \\in \\mathbb{R}^{d \\times b/2}$，其中 $b$ 是超参数，hash函数为 $h(x) = \\arg\\max([xR; −xR])$<br><img src=\"https://img-blog.csdnimg.cn/20210220190636923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在LSH attention中，一个query只可以和在相同的hashing bucket中的位置进行交互，$S_i = {j: h(\\mathbf{q}_i) = h(\\mathbf{k}_j)}$</p>\n<ul>\n<li>attention矩阵通常是稀疏的;</li>\n<li>使用LSH, 我们基于hash buckets可以对keys和queries进行排序</li>\n<li>设置 $Q=K$，这样,一个bucket中的keys和queries相等，更便于批处理。有趣的是，这种“共享QK”配置并不影响Transformer的性能。</li>\n<li>使用 $m$ 个连续的group在一起的query的batching<br><img src=\"https://img-blog.csdnimg.cn/20210220190801936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><h3 id=\"Reversible-Residual-Network\"><a href=\"#Reversible-Residual-Network\" class=\"headerlink\" title=\"Reversible Residual Network\"></a>Reversible Residual Network</h3>Reversible Residual Network的动机是设计一种结构，使任何给定层的激活都可以从下一层的激活中恢复，只需使用模型参数。因此，我们可以通过在backprop期间重新计算激活来节省内存，而不是存储所有激活。给定一层 $x \\mapsto y$，传统的residual layer都是做的$y = x + F(x)$，但是reversible layer将输入和输出split为$(x_1, x_2) \\mapsto (y_1, y_2)$，然后执行下面的操作：<br>$$y_1 = x_1 + F(x_2),; y_2 = x_2 + G(y_1)$$<br>reversing就是：<br>$$x_2 = y_2 - G(y_1), ; x_1 = y_1 − F(x_2)$$<br>我们将相同的思想应用到Transformer中得到：<br>$$Y_1 = X_1 + \\text{Attention}(X_2), ; Y_2 = X_2 + \\text{FeedForward}(Y_1)$$<br>内存可以通过chunking 前向计算进行操作：<br>$$Y_2 = [Y_2^{(1)}; \\dots; Y_2^{(c)}] = [X_2^{(1)} + \\text{FeedForward}(Y_1^{(1)}); \\dots; X_2^{(c)} + \\text{FeedForward}(Y_1^{(c)})]$$<h1 id=\"Make-it-Recurrent-Universal-Transformer\"><a href=\"#Make-it-Recurrent-Universal-Transformer\" class=\"headerlink\" title=\"Make it Recurrent (Universal Transformer)\"></a>Make it Recurrent (Universal Transformer)</h1>Universal Transformer将Transformer中的自我注意与RNN中的循环机制结合起来，旨在受益于Transformer的长期全局receptive field和RNN的学习inductive偏差。Universal Transformer使用自适应计算时间动态调整步长。如果我们固定步数，一个Universal Transformer就相当于一个多层Transformer，具有跨层共享的参数。在较高的层次上，Universal Transformer可以看作是学习每个token的隐藏状态表示的递归函数。递归函数在标记位置之间并行演化，位置之间的信息通过self-attention进行共享，如下结构：<br><img src=\"https://img-blog.csdnimg.cn/20210220191209962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>给定长度为 $L$ 的序列，Universal Transformer在第 $t$ 步迭代更新表示$\\mathbf{H}^t \\in \\mathbb{R}^{L \\times d}$，在第0步，$H^0$ 被表示为输入embedding矩阵，所以的位置编码在multi-head self-attenion中被并行处理，然后在经过一个recurrent transition function：<br>$$\\begin{aligned}<br>\\mathbf{A}^t &amp;= \\text{LayerNorm}(\\mathbf{H}^{t-1} + \\text{MultiHeadAttention}(\\mathbf{H}^{t-1} + \\mathbf{P}^t) \\<br>\\mathbf{H}^t &amp;= \\text{LayerNorm}(\\mathbf{A}^{t-1} + \\text{Transition}(\\mathbf{A}^t))<br>\\end{aligned}$$<br>$\\text{Transition}(.)$ 可以是一个 separable convolution或者fully-connected neural network：<br>$$\\text{PE}(i, t, \\delta) =<br>\\begin{cases}<br>\\sin(\\frac{i}{10000^{2\\delta’/d}}) \\oplus \\sin(\\frac{t}{10000^{2\\delta’/d}}) &amp; \\text{if } \\delta = 2\\delta’\\<br>\\cos(\\frac{i}{10000^{2\\delta’/d}}) \\oplus \\cos(\\frac{t}{10000^{2\\delta’/d}}) &amp; \\text{if } \\delta = 2\\delta’ + 1\\<br>\\end{cases}$$<br><img src=\"https://img-blog.csdnimg.cn/20210220191423424.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在Universal Transformer的自适应版本中，循环步数 $T$ 由ACT动态确定。每个位置都配有一个动态停止机制。一旦token循环块停止，它将停止进行更多的循环更新，只是将当前值复制到下一步，直到所有块停止或直到模型达到最大步长限制。</li>\n</ul>\n<h1 id=\"Stabilization-for-RL-GTrXL\"><a href=\"#Stabilization-for-RL-GTrXL\" class=\"headerlink\" title=\"Stabilization for RL (GTrXL)\"></a>Stabilization for RL (GTrXL)</h1><p>Self-attention避免了将整个过去压缩成一个固定大小的隐藏状态，并且不像RNN那样受到梯度消失或爆炸的影响。强化学习任务肯定能从这些特质中受益。然而，即使在有监督学习中，也很难训练Transformer，更不用说在RL环境中了。毕竟，稳定和训练一个LSTM代理本身可能是相当具有挑战性的。</p>\n<p>Gated Transformer-XL (GTrXL)是使用Transformer到RL中的一次尝试，GTrXL可以在Transformer-XL上成功稳定的训练。</p>\n<ul>\n<li>layer normalization应用于residual模块中的输入流，而不应用于shortcut流。这种重新排序的一个关键好处是允许原始输入从第一层流到最后一层。</li>\n<li>Residual连接被GRU样式选通机制取代。<br>$$\\begin{aligned}<br>r &amp;= \\sigma(W_r^{(l)} y + U_r^{(l)} x) \\<br>z &amp;= \\sigma(W_z^{(l)} y + U_z^{(l)} x - b_g^{(l)}) \\<br>\\hat{h} &amp;= \\tanh(W_g^{(l)} y + U_g^{(l)} (r \\odot x)) \\<br>g^{(l)}(x, y) &amp;= (1-z)\\odot x + z\\odot \\hat{h}<br>\\end{aligned}$$<br><img src=\"https://img-blog.csdnimg.cn/20210220191650331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","Transformer","Attention"]},{"title":"不要错过，SpringBoot好玩的动态Banner","url":"/Spring-Boot/ec9c19283a76/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>还记得SpringBoot项目启动时会在控制台打印一个默认的启动图案么，就是长下面这样子的图案，这个图案就是我们要讲的banner。我们这篇文章就是要讲对这个图案玩出花样，也算是给自己代码增加点乐趣，<strong>如果觉得有用记得点个关注和点个赞哦，嘿嘿嘿</strong>。<br><img src=\"https://img-blog.csdnimg.cn/20200307171109755.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"如何更改使用\"><a href=\"#如何更改使用\" class=\"headerlink\" title=\"如何更改使用\"></a>如何更改使用</h2><h3 id=\"使用banner-txt\"><a href=\"#使用banner-txt\" class=\"headerlink\" title=\"使用banner.txt\"></a>使用banner.txt</h3><p>SpringBoot2.0可以更改TXT格式的和gif格式的，SpringBoot1.0的时候，是只支持txt格式的，也就是1.0的时候是不能使用动态的Banner，我们先来讲讲txt格式的。其实SpringBoot更改banner特别简单，我们只需要在resources目录下创建一个<code>banner.txt</code> 然后在在里面放进你想要的ASCII内容就可以了，举个例子，首先你在resources目录下创建一个banner.txt ，然后放进如下内容<br><img src=\"https://img-blog.csdnimg.cn/20200307172234913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">////////////////////////////////////////////////////////////////////</span></span><br><span class=\"line\"><span class=\"comment\">//                          _ooOoo_                               //</span></span><br><span class=\"line\"><span class=\"comment\">//                         o8888888o                              //</span></span><br><span class=\"line\"><span class=\"comment\">//                         88&quot; . &quot;88                              //</span></span><br><span class=\"line\"><span class=\"comment\">//                         (| ^_^ |)                              //</span></span><br><span class=\"line\"><span class=\"comment\">//                         O\\  =  /O                              //</span></span><br><span class=\"line\"><span class=\"comment\">//                      ____/`---&#x27;\\____                           //</span></span><br><span class=\"line\"><span class=\"comment\">//                    .&#x27;  \\\\|     |//  `.                         //</span></span><br><span class=\"line\"><span class=\"comment\">//                   /  \\\\|||  :  |||//  \\                        //</span></span><br><span class=\"line\"><span class=\"comment\">//                  /  _||||| -:- |||||-  \\                       //</span></span><br><span class=\"line\"><span class=\"comment\">//                  |   | \\\\\\  -  /// |   |                       //</span></span><br><span class=\"line\"><span class=\"comment\">//                  | \\_|  &#x27;&#x27;\\---/&#x27;&#x27;  |   |                       //</span></span><br><span class=\"line\"><span class=\"comment\">//                  \\  .-\\__  `-`  ___/-. /                       //</span></span><br><span class=\"line\"><span class=\"comment\">//                ___`. .&#x27;  /--.--\\  `. . ___                     //</span></span><br><span class=\"line\"><span class=\"comment\">//              .&quot;&quot; &#x27;&lt;  `.___\\_&lt;|&gt;_/___.&#x27;  &gt;&#x27;&quot;&quot;.                  //</span></span><br><span class=\"line\"><span class=\"comment\">//            | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |                 //</span></span><br><span class=\"line\"><span class=\"comment\">//            \\  \\ `-.   \\_ __\\ /__ _/   .-` /  /                 //</span></span><br><span class=\"line\"><span class=\"comment\">//      ========`-.____`-.___\\_____/___.-`____.-&#x27;========         //</span></span><br><span class=\"line\"><span class=\"comment\">//                           `=---=&#x27;                              //</span></span><br><span class=\"line\"><span class=\"comment\">//      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^        //</span></span><br><span class=\"line\"><span class=\"comment\">//            佛祖保佑       永不宕机     永无BUG                  //</span></span><br><span class=\"line\"><span class=\"comment\">////////////////////////////////////////////////////////////////////</span></span><br></pre></td></tr></table></figure>\n<p>然后这个时候重启一下项目，就会发现，我们的启动画面变了，是不是很有意思，在文章后面我会把一些有趣的ASCII放出来，以及如何去生成好玩的ASCII。</p>\n<h2 id=\"使用banner-gif\"><a href=\"#使用banner-gif\" class=\"headerlink\" title=\"使用banner.gif\"></a>使用banner.gif</h2><p>接下来在项目中测试打印动态 Banner ，同样我们将 <code>banner.gif</code> 文件放到项目的<code>resources</code>目录下，启动项目进行测试，gif如下，以及输出栏打印信息如下：<br><img src=\"https://img-blog.csdnimg.cn/20200307172935623.gif#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">                                  </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                      ..                                    </span><br><span class=\"line\">                                     .::*                                   </span><br><span class=\"line\">                                      ...                                   </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                       </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">.....                                                             ....      </span><br><span class=\"line\">........                                                            .    ...</span><br><span class=\"line\">........                                                                . ..</span><br><span class=\"line\">......                                                                  ....</span><br><span class=\"line\">....                                                                     ...</span><br><span class=\"line\">.                                                                          .</span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                  ..**::**..                                </span><br><span class=\"line\">                                .*::::::::::*.                              </span><br><span class=\"line\">                               .*::::::::::::*.                             </span><br><span class=\"line\">                               .*::::::::::::*.                             </span><br><span class=\"line\">                               .*::::::::::::*.                             </span><br><span class=\"line\">                                .*::::::::::*.                              </span><br><span class=\"line\">                                  ..**::***.                                </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">..                                                                          </span><br><span class=\"line\">.....                                                                     ..</span><br><span class=\"line\">.....                                                                    ...</span><br><span class=\"line\">......                                                                ......</span><br><span class=\"line\">. ...    .                                                             .....</span><br><span class=\"line\">.       ....                                                             . .</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">              ............................   .............                  </span><br><span class=\"line\">            .................. .........       ................    .        </span><br><span class=\"line\">        ...................                   ... . ... ............        </span><br><span class=\"line\">     .............   . ...                             ...............      </span><br><span class=\"line\">    .   .........                                         ...........       </span><br><span class=\"line\">       ..........                                            ....... ....   </span><br><span class=\"line\">      ............                                                ........  </span><br><span class=\"line\">      ........                                                     ........ </span><br><span class=\"line\">    .... .                                                        ......... </span><br><span class=\"line\">   ........                                                        ........ </span><br><span class=\"line\"> .........                       ..********..                      ......*..</span><br><span class=\"line\">........                      .**::::::::::::**.                    ........</span><br><span class=\"line\">...........                 .**::::::::::::::::**.                   .......</span><br><span class=\"line\">......                     .*::::::::::::::::::::*.                  .......</span><br><span class=\"line\">... ..                    .*::::::::::::::::::::::*.                 .......</span><br><span class=\"line\">......                    .::::::::::::::::::::::::.                 .......</span><br><span class=\"line\">..........                .::::::::::::::::::::::::.                ... ....</span><br><span class=\"line\">.........                 .*:::::::::::::::::::::::.                    ....</span><br><span class=\"line\">.........                 .*::::::::::::::::::::::*.                     ...</span><br><span class=\"line\">.........                  .*::::::::::::::::::::*.                     ....</span><br><span class=\"line\"> ........                   .**::::::::::::::::**.                 .........</span><br><span class=\"line\">  ... ....                    .**::::::::::::**.                   .........</span><br><span class=\"line\">. ........                        .********..                      .........</span><br><span class=\"line\">   ....... .                                                      ......*.. </span><br><span class=\"line\">   .........                                                     .   .....  </span><br><span class=\"line\">    .......                                                    .........    </span><br><span class=\"line\">    ........     .                                          ............    </span><br><span class=\"line\">    ............  ..                                        ...........     </span><br><span class=\"line\">      . .............                                        .........      </span><br><span class=\"line\">       ................                              ....   ..........      </span><br><span class=\"line\">         ............. ....    .                   ......... . ..... .      </span><br><span class=\"line\">          .... ...... .........       . .. .... .............. ....         </span><br><span class=\"line\">            ..       .............  ...........  ..............             </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                                                                            </span><br><span class=\"line\">                              ::.        .*:*                               </span><br><span class=\"line\">                           :.        *:*      *. .*:*                       </span><br><span class=\"line\">                        .:*.    *:   .*:*.       :.   .:                    </span><br><span class=\"line\">                    :* :     :.                .::::*    :                  </span><br><span class=\"line\">                  :  :    *         *****.....       *.:   :                </span><br><span class=\"line\">                :   : .:*         .::::::::::**..     ..  : *               </span><br><span class=\"line\">               :   *:           .*:::::::::::::::*.     *   **              </span><br><span class=\"line\">              o  ***        ...**::::::::::::::::::**.   *   :.:            </span><br><span class=\"line\">             : .* :    .....***::::::::::::::::::::::*.   :   : o           </span><br><span class=\"line\">             :*.  *   ..*****:::::::::::::::::::::::::*.   *  :  o          </span><br><span class=\"line\">             :   .   .*::::::::::::::::::::::::::::::::*.    ::   *         </span><br><span class=\"line\">            :*   :   *::::::::::::::::::::::::::::::::::*     :   o         </span><br><span class=\"line\">           * o  *   .*::::::::::::::::::::::::::::::::::*.   .:o  :         </span><br><span class=\"line\">           :  : :   .*::::::::::::::::::::::::::::::::::*.   :  . :         </span><br><span class=\"line\">           :  ..    .*::::::::::::::::::::::::::::::::::*.   *  : *         </span><br><span class=\"line\">           :   :    .*::::::::::::::::::::::::::::::::::*   :   *:          </span><br><span class=\"line\">           .   :*    .*o:::::::::::::::::::::::::::::::*.   .   :           </span><br><span class=\"line\">            :  :  *   .*::::::::::::::::::::::::::::::*.   *   :*           </span><br><span class=\"line\">             o *   :   .*::::::::::::::::::::::::::::*.    : ** :           </span><br><span class=\"line\">               :*   *    .*::::::::::::::::::::::::**.    *..  *            </span><br><span class=\"line\">                **   *    ..*::::::::::::::::::::*..     :*   *             </span><br><span class=\"line\">                 * o  ..     ..**::::::::::::**..    .:. :   :              </span><br><span class=\"line\">                  :   o.:        ...******...      *.   :  :.               </span><br><span class=\"line\">                    :     ::o:.                *:     * *:                  </span><br><span class=\"line\">                      :.    :       .*:*.   :*    .*::                      </span><br><span class=\"line\">                         .:*  .*      *o:        .:                         </span><br><span class=\"line\">                                 .:*.        .*:                                          </span><br><span class=\"line\">               </span><br><span class=\"line\">      </span><br><span class=\"line\"></span><br><span class=\"line\">      ...                                                                      </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>通过上述输出我们发现 Spring Boot 在启动的时候，会将 gif 图片的每一个画面，按照顺序打印在日志中，所有的画面打印完毕后，才会启动 Spring Boot 项目。如果目录resources下同时存在banner.txt和banner.gif，项目会先将banner.gif每一个画面打印完毕之后，再打印banner.txt中的内容。</p>\n<h2 id=\"获取banner\"><a href=\"#获取banner\" class=\"headerlink\" title=\"获取banner\"></a>获取banner</h2><p>这个<a href=\"http://www.network-science.de/ascii/\">ASCII Generator</a>可以把文字生成ASCII，如下<br><img src=\"https://img-blog.csdnimg.cn/20200307173256296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这个<a href=\"http://www.degraeve.com/img2txt.php\">IMG2TXT</a>可以将图片生成ASCII，非常方便，如下<br><img src=\"https://img-blog.csdnimg.cn/20200307173424722.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"好玩的banner\"><a href=\"#好玩的banner\" class=\"headerlink\" title=\"好玩的banner\"></a>好玩的banner</h3><p>我这里放一些好玩的banner</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">.__           .__  .__                               .__       .___</span><br><span class=\"line\">|  |__   ____ |  | |  |   ____   __  _  _____________|  |    __| _/</span><br><span class=\"line\">|  |  \\_/ __ \\|  | |  |  /  _ \\  \\ \\/ \\/ /  _ \\_  __ \\  |   / __ | </span><br><span class=\"line\">|   Y  \\  ___/|  |_|  |_(  &lt;_&gt; )  \\     (  &lt;_&gt; )  | \\/  |__/ /_/ | </span><br><span class=\"line\">|___|  /\\___  &gt;____/____/\\____/    \\/\\_/ \\____/|__|  |____/\\____ | </span><br><span class=\"line\">     \\/     \\/                                                  \\/ </span><br></pre></td></tr></table></figure>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">                                    ::WWWWWWWWWWWWKKWWWWKKDDDDKKDDKKKKWWKKKKKKWWWWii                </span><br><span class=\"line\">                                    LL######WWWW##############KKGGKKKKDDDDKKKKWW##WW;;              </span><br><span class=\"line\">                                ttEEWW####KK####KKEEEEWW##WWKK##GGKKDDLLWW##KKDDWWWWKK;;            </span><br><span class=\"line\">                              ..jjff####WW##KKLLWW######KKKKDDWWKKKKKKLL####LLDDWW####WW            </span><br><span class=\"line\">                              ;;ttLL##WW##KKGGWW######WW##DDDDGG##WWDDWWWWWWDDWW#######<span class=\"meta\">#tt          </span></span><br><span class=\"line\">                                ffWWWW##EEKKWW############WWGGGGDD##WW##WWWWKKKKWW######KK          </span><br><span class=\"line\">                              LL##########################WWDDDDGG##WWWWKK####KKDD########          </span><br><span class=\"line\">                              GGEE######################KKLLGGKKWWDDWWWWffKK##KKDDKK####WW..        </span><br><span class=\"line\">                              ffWW######################DDEEEEffffffffDDGGGG##EEGGKK######          </span><br><span class=\"line\">                            ..########################KKfftt;;,,..,,;;jjffttDDWWDDWWWW##GG          </span><br><span class=\"line\">                            ;;WW####################WWffjj;;::    ..,,;;;;,,;;LLWWEEKK#<span class=\"meta\">#jj          </span></span><br><span class=\"line\">                            LLEEEEKK####WW##DDffffffii;;,,::........::,,,,,,,,;;EEDDWW##;;          </span><br><span class=\"line\">                          LL##WW##WWWW##WWjj,,,,;;,,,,,,::::......::::::,,::,,,,GGDDWW##..          </span><br><span class=\"line\">                      ..GGLL##DDffGGDDDD,,,,,,,,;;tttt,,............::::::,,,,;;DDKKWWWWii          </span><br><span class=\"line\">                    ffjjjj####LL,,,,;;ii;;,,,,,,;;;;,,iittii......::::::::,,,,iiWW##WW##..          </span><br><span class=\"line\">                  ..ffKK##WW###<span class=\"meta\">#jjii;;tt;;,,::..,,jjLLffttjjtt::::::::,,,,,,;;ff######DD            </span></span><br><span class=\"line\">                      jjKK########KKttjj;;;;,,....;;ttDDDDDDjjtt,,,,,,,,,,,,;;GG#####<span class=\"meta\">#ff            </span></span><br><span class=\"line\">                    ..;;KK########LLjjttii;;,,....::iiii,,ttjjii,,,,,,ttjjLLDD##WW##KKtt            </span><br><span class=\"line\">                    ..LL#########<span class=\"meta\">#ffjjiiii;;,,::......;;tttt,,;;..iiDDWWDDDDff##WW##LL;;            </span></span><br><span class=\"line\">                      LLKKWWWW###<span class=\"meta\">#jjjj;;ii;;;;::....::::::..,,,,::jjiiii;;EEWWWWKKDDjj..            </span></span><br><span class=\"line\">                    ..DDWWWWEE#<span class=\"meta\">#jjtttt;;ii;;;;,,....::......::::,,ii,,ttffttLLDDDDGG,,              </span></span><br><span class=\"line\">                    DDDDLLEEWWGGtttttt;;;;;;;;,,,,::::::..::::..;;;;,,,,;;;;iiGGLLii                </span><br><span class=\"line\">                  ttGGDDEEWWWW;;iitttt;;;;;;;;,,,,,,,,,,,,,,::  ;;;;,,;;;;ttiiGGff                  </span><br><span class=\"line\">                  ttGGWWWW#<span class=\"meta\">#ff;;;;ttjj;;;;;;,,,,,,,,,,..;;::::..ii,,,,;;ttffGGff,,                  </span></span><br><span class=\"line\">                    LLKK#<span class=\"meta\">#ff;;;;;;;;jj;;;;,,,,,,,,::::..,,tt::::ii,,;;iiffttttii::                  </span></span><br><span class=\"line\">                      LLGG;;;;,,;;;;jjii;;;;,,,,;;tt::....::ttfftt;;ii,,,,..;;..,,                  </span><br><span class=\"line\">                    ..ii,,;;,,,,,,;;;;ff;;,,,,,,::,,ffff;;,,;;iittttff  ,,  ,,  ii                  </span><br><span class=\"line\">                ::iitt,,,,;;,,,,,,;;;;jjjj;;,,,,,,::tt;;LLLLLLffttWWtt....  ::..,,                  </span><br><span class=\"line\">          ,,;;;;;;,,,,,,,,,,::,,,,,,,,;;jjtt;;,,::::,,ttttjjffjjtt##,,..        ,,                  </span><br><span class=\"line\">..,,,,;;;;;;,,,,,,,,,,,,,,,,,,,,,,,,,,,,;;jjii,,,,::::,,;;jjjj;;DDDD..        ::,,                  </span><br><span class=\"line\">;;;;;;;;;;,,,,,,,,::::::,,::::::::,,,,,,;;iiffjj,,,,,,,,;;LLEEGGjjGG          ,,;;..                </span><br><span class=\"line\">,,,,,,,,;;;;;;,,,,::..::,,::..::::::::,,;;;;ttfffftt;;;;GGtt;;jjttii          ,,::..                </span><br><span class=\"line\">,,::....::,,;;;;;;,,,,,,,,::..::::::::::,,,,;;ffGGKKEEDDLLLLDDffii      ..    ii..                  </span><br><span class=\"line\">,,,,::....,,,,,,,,;;;;,,,,,,......::::..::::;;ttLLLLLLffffjjjjjj..    ,,      ,,  ..                </span><br><span class=\"line\">,,::::::..::,,......::,,,,,,,,::::......::::;;;;jjjjjjffffjjttjjjj,,..      ..,,                    </span><br><span class=\"line\">::::......::::............::::,,,,::::::::,,,,;;iijjffffjjjjjjffjjjjjj,,    ::;;                    </span><br><span class=\"line\">::........::..................::::::,,::::,,,,,,,,,,,,,,,,,,,,jj;;;;;;iitt,,jj                      </span><br><span class=\"line\">....................................,,::..::,,::,,::::::,,,,,,ii,,,,,,,,,,;;jj,,                    </span><br></pre></td></tr></table></figure>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">.___  .____                        _____.___.             </span><br><span class=\"line\">|   | |    |    _______  __ ____   \\__  |   | ____  __ __ </span><br><span class=\"line\">|   | |    |   /  _ \\  \\/ <span class=\"comment\">// __ \\   /   |   |/  _ \\|  |  \\</span></span><br><span class=\"line\">|   | |    |__(  &lt;_&gt; )   /\\  ___/   \\____   (  &lt;_&gt; )  |  /</span><br><span class=\"line\">|___| |_______ \\____/ \\_/  \\___  &gt;  / ______|\\____/|____/ </span><br><span class=\"line\">              \\/               \\/   \\/                    </span><br></pre></td></tr></table></figure>","categories":["Spring-Boot"],"tags":["Sprint Boot","动态Banner"]},{"title":"好好琢磨一下TF-IDF，结合Sklearn","url":"/Deep-Learning/c6980c31d7a3/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>首先我们需要了解TF-IDF的相关知识和原理，最后我们通过代码来学习使用。</p>\n<h1 id=\"词集、词袋、词汇表模型\"><a href=\"#词集、词袋、词汇表模型\" class=\"headerlink\" title=\"词集、词袋、词汇表模型\"></a>词集、词袋、词汇表模型</h1><p>文本类的分类任务，特征提取几种方式：</p>\n<ul>\n<li>词集模型（SOW）：单词构成的集合，集合中每个元素只有一个，即词集中的每个单词都只有一个。</li>\n<li>词袋模型 （BOW）：在词集的基础上加入了频率这个维度，即统计单词在文档中出现的次数（token化和出现频数统计），通常我们在应用中都选用词袋模型。</li>\n<li>词汇表模型：前面两个以及TF-IDF中模型没有表达单词间的关系，于是又了词汇表模型。该模型在词袋模型思想的基础上，按照句子中单词顺序进行排序输出特征</li>\n</ul>\n<p>词集模型和词袋模型两者本质上的区别，词袋是在词集的基础上增加了频率的维度，词集只关注有和没有，词袋还要关注有几个。词袋模型可以很好的表现文本由哪些单词组成，但是却无法表达出单词之间的前后关系，于是人们借鉴了词袋模型的思想，使用生成的词汇表对原有句子按照单词逐个进行编码。</p>\n<p>其实我们经常在模型中使用到的词嵌入模型，也和上述的词统计方法密切相关，有兴趣的可以看我另一篇写<a href=\"https://dengbocong.blog.csdn.net/article/details/109319937\">词嵌入的文章</a>。</p>\n<h1 id=\"TF-IDF模型\"><a href=\"#TF-IDF模型\" class=\"headerlink\" title=\"TF-IDF模型\"></a>TF-IDF模型</h1><p>TF-IDF（Term Frequency-Inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。一个词的重要程度跟它在文章中出现的次数成正比，跟它在语料库出现的次数成反比。这种计算方式能有效避免常用词对关键词的影响，提高了关键词与文章之间的相关性。原理说简单点，不难理解。</p>\n<p>向量空间模型就是希望把查询关键字和文档都表达成向量，然后利用向量之间的运算来进一步表达向量间的关系。比如，一个比较常用的运算就是计算查询关键字所对应的向量和文档所对应的向量之间的 “相关度”，如下。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201108160531502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li><strong>词频（TF）</strong><br>在一份给定的文件里，词频（Term Frequency，即TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（Term count）的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 $t_i$ 来说，它的重要性可表示为：<br>$$TF_{i,j}=\\frac{n_{i,j}}{\\sum_kn_{k,j}}$$<br>其中，$n_{i,j}$ 是该词在文件 $d_j$ 中的出现次数，而分母则是在文件 $d_j$ 中所有字词的出现次数之和。</li>\n<li><strong>逆向文件频率（IDF）</strong><br>逆向文件频率（Inverse Document Frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的 IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取以10为底的对数得到，如下：<br>$$IDF_i=lg\\frac{|D|}{|{j:t_i\\in d_j}|}$$<br>其中，$|D|$ 是语料库中的文件总数，$|{j:t_i\\in d_j}|$ 表示包含词语 $t_i$ 的文件数目（即 $n_{i,j}\\neq 0$ 的文件数目），如果词语不在资料中，就导致分母为零，因此一般情况下使用 $1+|j:t_i\\in d_j|$</li>\n</ul>\n<p>有了上述TF和IDF的计算值之后，我们就可以计算TF-IDF值了，如下：<br>$$TFIDF_{i,j}=tf_{i,j}\\times idf_i$$<br>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。</p>\n<h1 id=\"TF-IDF的不足之处\"><a href=\"#TF-IDF的不足之处\" class=\"headerlink\" title=\"TF-IDF的不足之处\"></a>TF-IDF的不足之处</h1><p>TF-IDF算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取TF词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，TF-IDF法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。</p>\n<p>因此引入了逆文本频度IDF的概念，以TF和IDF的乘积作为特征空间坐标系的取值测度，并用它完成对权值TF的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上IDF是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。IDF的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以TF-IDF法的精度并不是很高。</p>\n<p>此外，在TF-IDF算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。总结而言就是：</p>\n<ul>\n<li>没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。</li>\n<li>按照传统TF-IDF，往往一些生僻词的IDF(逆文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。</li>\n<li>传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。</li>\n<li>对于文档中出现次数较少的重要人名、地名信息提取效果不佳。</li>\n</ul>\n<p>当然TF-IDF算法被广泛使用的原因是因为它简单快速，结果比较符合实际情况，所以结合很多其他的方法进行应用，比如结合余弦相似性，应用于搜索相似文章等。在Sklearn的TF-IDF算法实现中，我们可以通过正则表达式表规定过滤的词，这个操作有助于我们更好的利用和提升TF-IDF的准确度，后续会讲到。</p>\n<h1 id=\"TF-IDF-的4个变种\"><a href=\"#TF-IDF-的4个变种\" class=\"headerlink\" title=\"TF-IDF 的4个变种\"></a>TF-IDF 的4个变种</h1><ul>\n<li><strong>通过对数函数避免 TF 线性增长</strong></li>\n</ul>\n<p>很多人注意到 TF 的值在原始的定义中没有任何上限。虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。例如，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，很多人意识到，超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>\n<p>所以这里用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 5.6，而出现 200 次，新的值是 6.3。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>\n<ul>\n<li><strong>标准化解决长文档、短文档问题</strong></li>\n</ul>\n<p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧。</p>\n<ul>\n<li><strong>对数函数处理 IDF线性增长问题</strong></li>\n</ul>\n<p>第三个常用的技巧，也是利用了对数函数进行变换的，是对 IDF 进行处理。相对于直接使用 IDF 来作为 “惩罚因素”，我们可以使用 N+1 然后除以 DF 作为一个新的 DF 的倒数，并且再在这个基础上通过一个对数变化。这里的 N 是所有文档的总数。这样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>\n<ul>\n<li><strong>查询词及文档向量标准化解决长短文档问题</strong></li>\n</ul>\n<p>还有一个重要的 TF-IDF 变种，则是对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>\n<h1 id=\"结合Sklearn实现TF-IDF算法\"><a href=\"#结合Sklearn实现TF-IDF算法\" class=\"headerlink\" title=\"结合Sklearn实现TF-IDF算法\"></a>结合Sklearn实现TF-IDF算法</h1><p>首先引入Sklearn中的CountVectorizer、TfidfTransformer和TfidfVectorizer</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction.text <span class=\"keyword\">import</span> CountVectorizer, TfidfTransformer</span><br><span class=\"line\"><span class=\"keyword\">from</span>  sklearn.feature_extraction.text <span class=\"keyword\">import</span> TfidfVectorizer</span><br><span class=\"line\"></span><br><span class=\"line\">corpus=[<span class=\"string\">&quot;I come to China to travel&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;This is a car polupar in China&quot;</span>,          </span><br><span class=\"line\">    <span class=\"string\">&quot;I love tea and Apple &quot;</span>,   </span><br><span class=\"line\">    <span class=\"string\">&quot;The work is to write some papers in science&quot;</span>]</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\">CountVectorizer</a>搭配<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\">TfidfTransformer</a></li>\n</ul>\n<p>CountVectorizer会将文本中的词语转换为词频矩阵，它通过<code>fit_transform</code>函数计算各个词语出现的次数，通过<code>get_feature_names()</code>可获得所有文本的关键词，通过<code>toarray()</code>可看到词频矩阵的结果。TfidfTransformer用于统计vectorizer中每个词语的TFIDF值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">vectorizer=CountVectorizer()</span><br><span class=\"line\">transformer = TfidfTransformer()</span><br><span class=\"line\">tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus)) </span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">TfidfVectorizer</a></li>\n</ul>\n<p>将原始文档的集合转化为TF-IDF特性的矩阵，相当于CountVectorizer配合TfidfTransformer使用的效果。即TfidfVectorizer类将CountVectorizer和TfidfTransformer类封装在一起。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tfidf2 = TfidfVectorizer()</span><br><span class=\"line\">re = tfidf2.fit_transform(corpus)</span><br></pre></td></tr></table></figure>\n<p>上面两种方式的结果都是：<br><img src=\"https://img-blog.csdnimg.cn/20201108164025603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>下面是涉及到的一些比较关键的参数解释，更详细的参数情况可前往官网查看<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">input：string &#123;&#39;filename&#39;，&#39;file&#39;，&#39;content&#39;&#125;</span><br><span class=\"line\">    如果&#39;filename&#39;，作为参数传递的顺序适合，预计将是需要读取以获取原始内容进行分析的文件名列表。</span><br><span class=\"line\">    如果&#39;file&#39;，序列项必须有一个&#39;read&#39;方法（类文件对象），被调用来获取内存中的字节。</span><br><span class=\"line\">    否则，输入将被预期是顺序字符串或字节项预期直接分析。</span><br><span class=\"line\">encoding：string，&#39;utf-8&#39;。</span><br><span class=\"line\">    如果要分配字节或文件，则使用该编码进行解码。</span><br><span class=\"line\">decode_error：&#123;&#39;strict&#39;，&#39;ignore&#39;，&#39;replace&#39;&#125;</span><br><span class=\"line\">    如果给出分析字节序列包含不是给定编码的字符，该怎么做。默认情况下，它是&#39;strict&#39;，这意味着将会引发一个UnicodeDecodeError。其他值是“忽略”和“替换”。</span><br><span class=\"line\">strip_accents：&#123;&#39;ascii&#39;，&#39;unicode&#39;，无&#125;</span><br><span class=\"line\">    在预处理步骤中删除口音。&#39;ascii&#39;是一种快速的方法，只适用于具有直接ASCII映射的字符。&#39;unicode&#39;是一种稍慢的方法，适用于任何字符。无（默认）不起作用。</span><br><span class=\"line\">analyzer：string，&#123;&#39;word&#39;，&#39;char&#39;&#125;或可调用</span><br><span class=\"line\">    该功能是否应由字符或字符n-gram组成。</span><br><span class=\"line\">    如果传递了一个可调用函数，它将用于从原始未处理的输入中提取特征序列。</span><br><span class=\"line\">预处理器：可调用或无（默认）</span><br><span class=\"line\">    覆盖预处理（字符串转换）阶段，同时保留令牌化和n-gram生成步骤。</span><br><span class=\"line\">tokenizer：可调用或无（默认）</span><br><span class=\"line\">    覆盖字符串标记化步骤，同时保留预处理和n-gram生成步骤。仅适用如果。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">ngram_range：tuple（min_n，max_n）</span><br><span class=\"line\">    不同n值的n值范围的下边界和上边界被提取。将使用所有n值，使得min_n &lt;&#x3D; n &lt;&#x3D; max_n。</span><br><span class=\"line\">stop_words：string &#123;&#39;english&#39;&#125;，list或None（默认）</span><br><span class=\"line\">    如果是字符串，则将其传递给_check_stop_list，并返回相应的停止列表。&#39;english&#39;是目前唯一支持的字符串值。</span><br><span class=\"line\">    如果一个列表，该列表被假定为包含停止词，所有这些都将从生成的令牌中删除。仅适用如果。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">    如果没有，将不会使用停止的单词。max_df可以设置为[0.7,1.0]范围内的值，以根据术语的语料库文档频率自动检测和过滤停止词。</span><br><span class=\"line\">小写：布尔值，默认值为True</span><br><span class=\"line\">    在标记化之前将所有字符转换为小写。</span><br><span class=\"line\">token_pattern：string</span><br><span class=\"line\">    表示什么构成“令牌”的正则表达式，仅用于。默认正则表达式选择2个或更多字母数字字符的标记（标点符号被完全忽略，并始终作为令牌分隔符处理）。analyzer &#x3D;&#x3D; &#39;word&#39;</span><br><span class=\"line\">max_df：float in range [ 0.0，1.0 ]或int，default &#x3D; 1.0</span><br><span class=\"line\">    当构建词汇时，忽略文档频率严格高于给定阈值（语料库特定停止词）的术语。如果为float，则该参数代表一定比例的文档，整数绝对计数。如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">min_df：float in range [ 0.0，1.0 ]或int，default &#x3D; 1</span><br><span class=\"line\">    当构建词汇时，忽略文档频率严格低于给定阈值的术语。这个值在文献中也被称为截止值。如果为float，则该参数代表一定比例的文档，整数绝对计数。如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">max_features：int或None，default &#x3D; None</span><br><span class=\"line\">    如果不是无，建立一个词汇，只考虑由词汇频率排序的顶级max_feature。</span><br><span class=\"line\">    如果词汇不是无，则忽略此参数。</span><br><span class=\"line\">词汇表：映射或迭代，可选</span><br><span class=\"line\">    键是术语和值的映射（例如，dict）是特征矩阵中的索引，或者可迭代的术语。如果没有给出，则从输入文档确定词汇表。</span><br><span class=\"line\">binary：boolean，default &#x3D; False</span><br><span class=\"line\">    如果为True，则所有非零项计数都设置为1.这并不意味着输出将只有0&#x2F;1值，只有tf-idf中的tf项是二进制的。（将idf归一化为False，得到0&#x2F;1输出。）</span><br><span class=\"line\">dtype：type，可选</span><br><span class=\"line\">    由fit_transform（）或transform（）返回的矩阵的类型。</span><br><span class=\"line\">规范：&#39;l1&#39;，&#39;l2&#39;或无，可选</span><br><span class=\"line\">    用于规范化术语向量的规范。没有没有规范化。</span><br><span class=\"line\">use_idf：boolean，default &#x3D; True</span><br><span class=\"line\">    启用逆文档频率重新加权。</span><br><span class=\"line\">smooth_idf：boolean，default &#x3D; True</span><br><span class=\"line\">    通过将文档频率添加一个平滑的idf权重，就好像一个额外的文档被看到包含一个集合中的每个术语一次。防止零分。</span><br><span class=\"line\">sublinear_tf：boolean，default &#x3D; False</span><br><span class=\"line\">    应用子线性tf缩放，即用1 + log（tf）替换tf。</span><br></pre></td></tr></table></figure>\n<h1 id=\"补充问题\"><a href=\"#补充问题\" class=\"headerlink\" title=\"补充问题\"></a>补充问题</h1>在使用TfidfVectorizer和CountVectorizer的时候，可能会出现了错误<code>ValueError: empty vocabulary; perhaps the documents only contain stop words</code>。原因是，创建CountVectorizer实例时，有一个默认参数<code>analyzer=&#39;word&#39;</code>，在该参数作用下，词频矩阵构建过程会默认过滤所有的单字token，例如<code>&#39;a b c d&#39;</code>以空格分隔以后全是单字，也就全被过滤了，所以就empty vocabulary了。解决的办法我们就要来看看其中的一个参数：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">analyzer : &#123;‘word’, ‘char’, ‘char_wb’&#125; or callable, default&#x3D;’word’</span><br><span class=\"line\">Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</span><br><span class=\"line\">If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</span><br><span class=\"line\">Since v0.21, if input is filename or file, the data is first read from the file and then passed to the given callable analyzer.</span><br></pre></td></tr></table></figure>\n<p>当然，如果上述三种不能满足需求，可以使用正则表达式达到，即使用<code>token_pattern</code>参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">token_pattern</span><br><span class=\"line\">Regular expression denoting what constitutes a “token”, only used if analyzer &#x3D;&#x3D; &#39;word&#39;. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).</span><br></pre></td></tr></table></figure>\n<p>通过正则的方式来解决，具体解决代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">CountVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>,token_pattern=<span class=\"string\">u&quot;(?u)\\\\b\\\\w+\\\\b&quot;</span>)</span><br><span class=\"line\">TfidfVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>,token_pattern=<span class=\"string\">u&quot;(?u)\\\\b\\\\w+\\\\b&quot;</span>)</span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["词袋","TensorFlow","TF-IDF","Sklearn"]},{"title":"带你搞定SpringBoot构建分模块项目","url":"/Spring-Boot/8acc7bd801a8/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>Spingboot项目对开发来说，最简单的当然是单模块开发，只有一个pom文件。但随着项目的不断发展，需求的不断细化与添加，工程项目中的代码越来越多，包结构也越来越复杂，比起传统复杂的单体工程，使用Maven的多模块配置，可以帮助项目划分模块，鼓励重用，防止POM变得过于庞大，方便某个模块的构建，而不用每次都构建整个项目，并且使得针对某个模块的特殊控制更为方便。那么这里呢，我就来讲解一下如何使用SpringBoot来构建分模块项目，<strong>如果觉得有用，记得点个关注和点个赞哦</strong>。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>我们这个项目呢使用的是Idea进行演示，JDK版本是1.8，然后准备构建的项目模块如下（我自己的习惯分的模块，这里只是用于演示，你可以按照你的习惯分模块）</p>\n<ul>\n<li>entity：包含实体类目</li>\n<li>dao：包括持久化类目</li>\n<li>service：包含业务逻辑类目</li>\n<li>web：api层面或者是视图层</li>\n</ul>\n<p><strong>这里说明一下我们这里的web可以理解成应用层，其他子模块理解为公共层，然后这样的话，我们可以有多个web层（web1、web2…），这样我就可以就可以实现多个应用服务集成在一起（更确切的说，叫做同一个业务分离详细，比如一个企业管理系统，拆分成财务系统、人事系统等等），不过要注意了，这样做的话，创建子模块的时候，除了web模块以外，要删除其他模块的Applicatin启动项，和resources目录下的application.properties配置文件。</strong></p>\n<p>这三个模块依赖的关系如下</p>\n<ul>\n<li>web依赖entity、dao、service</li>\n<li>service依赖dao、entity</li>\n<li>dao依赖entity</li>\n<li>entity独立，谁都不依赖</li>\n</ul>\n<h2 id=\"创建父工程\"><a href=\"#创建父工程\" class=\"headerlink\" title=\"创建父工程\"></a>创建父工程</h2><p>首先呢，我们先通过Idea的Spring Initilazr创建一个啥场景启动器都不包含的空SpringBoot项目，然后把下图所示的多余文件全部删掉，只剩下一个pom.xml，当然，如果你习惯用mvn指令运行，就不要删了（Idea的工具文件删不删都无所谓，因为你运行后，idea还是会创建的）。<br><img src=\"https://img-blog.csdnimg.cn/20200308115150679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>到这一步，我们的父模块就创建完成，此时的pom文件内容如下，此pom即是下文所说的父pom文件</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">\txsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">\t&lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">\t&lt;parent&gt;</span><br><span class=\"line\">\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t&lt;version&gt;<span class=\"number\">2.2</span><span class=\"number\">.5</span>.RELEASE&lt;/version&gt;</span><br><span class=\"line\">\t\t&lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">\t&lt;/parent&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">\t&lt;name&gt;demo&lt;/name&gt;</span><br><span class=\"line\">\t&lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;properties&gt;</span><br><span class=\"line\">\t\t&lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">\t&lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;dependencies&gt;</span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">\t\t\t&lt;exclusions&gt;</span><br><span class=\"line\">\t\t\t\t&lt;exclusion&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;groupId&gt;org.junit.vintage&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/exclusion&gt;</span><br><span class=\"line\">\t\t\t&lt;/exclusions&gt;</span><br><span class=\"line\">\t\t&lt;/dependency&gt;</span><br><span class=\"line\">\t&lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;build&gt;</span><br><span class=\"line\">\t\t&lt;plugins&gt;</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t\t&lt;/plugins&gt;</span><br><span class=\"line\">\t&lt;/build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"创建子模块\"><a href=\"#创建子模块\" class=\"headerlink\" title=\"创建子模块\"></a>创建子模块</h2><p>我们上面创建好了父工程，现在我们可以在父工程的基础上来创建上面提到的三个子模块，子模块的创建方式很简单，具体步骤如下<br><img src=\"https://img-blog.csdnimg.cn/2020030812100175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后我们创建一个SpringBoot空项目，啥都不选（当然，如果我们创建的模块，不需要交给SpringBoot管理，我们可以直接创建一个空的Maven项目，但是由于我们创建的）<br><img src=\"https://img-blog.csdnimg.cn/20200308153908868.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>此处填写的name就是我们的模块名，创建该模块时，如果第一步选中了父模块，那么此处GroupId和Version都会自动填充，如果没有自动填充，说明创建该模块的时候没有选中。<br><img src=\"https://img-blog.csdnimg.cn/20200308174006411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200308154251705.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>src下的启动项以及配置文件也记得删除，然后现在按照上面的步骤，把其他三个子模块都创建出来，最后创建工程如下<br><img src=\"https://img-blog.csdnimg.cn/20200308155744973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"添加依赖关系\"><a href=\"#添加依赖关系\" class=\"headerlink\" title=\"添加依赖关系\"></a>添加依赖关系</h2><p><img src=\"https://img-blog.csdnimg.cn/2020030816022072.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"><br>先说明一个注意的点，这个是父工程的pom.xml的内容，这里指出来是说我们项目的打包方式是pom，不是jar，但是你去子模块的pom.xml里就会发现，没有packing这个标签，说明是jar的形式（当然你如果不放心，也可以加packing标签过去，声明jar形式）。</p>\n<p>然后我们现在开始配置父工程以及子模块的pom.xml依赖关系，首先是父工程的pom.xml的内容，配置内容如下，父工程中不配置web模块哦，只配置公共模块，然后以后一些公共依赖，都可以直接放在父工程的pom.xml里面进行统一版本号管理（注意了，这里知识管理版本号，并不是引入依赖，哪个模块需要哪个依赖，还是需要自己引入）。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">         xsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 继承说明：这里继承SpringBoot提供的父工程 --&gt;</span><br><span class=\"line\">    &lt;parent&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">2.2</span><span class=\"number\">.5</span>.RELEASE&lt;/version&gt;</span><br><span class=\"line\">        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">    &lt;/parent&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!-- 项目说明：这里作为聚合工程的父工程 --&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!-- 基本信息 --&gt;</span><br><span class=\"line\">    &lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">    &lt;packaging&gt;pom&lt;/packaging&gt;</span><br><span class=\"line\">    &lt;name&gt;demo&lt;/name&gt;</span><br><span class=\"line\">    &lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;properties&gt;</span><br><span class=\"line\">        &lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">        &lt;demo.version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/demo.version&gt;</span><br><span class=\"line\">    &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!-- 模块说明：这里声明多个子模块 --&gt;</span><br><span class=\"line\">    &lt;modules&gt;</span><br><span class=\"line\">        &lt;<span class=\"keyword\">module</span>&gt;dao&lt;/<span class=\"keyword\">module</span>&gt;</span><br><span class=\"line\">        &lt;<span class=\"keyword\">module</span>&gt;entity&lt;/<span class=\"keyword\">module</span>&gt;</span><br><span class=\"line\">        &lt;<span class=\"keyword\">module</span>&gt;service&lt;/<span class=\"keyword\">module</span>&gt;</span><br><span class=\"line\">        &lt;<span class=\"keyword\">module</span>&gt;web&lt;/<span class=\"keyword\">module</span>&gt;</span><br><span class=\"line\">    &lt;/modules&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;!-- 版本说明：这里统一管理依赖的版本号 --&gt;</span><br><span class=\"line\">    &lt;dependencyManagement&gt;</span><br><span class=\"line\">        &lt;dependencies&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;dao&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;$&#123;demo.version&#125;&lt;/version&gt;</span><br><span class=\"line\">            &lt;/dependency&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;entity&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;$&#123;demo.version&#125;&lt;/version&gt;</span><br><span class=\"line\">            &lt;/dependency&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;service&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;$&#123;demo.version&#125;&lt;/version&gt;</span><br><span class=\"line\">            &lt;/dependency&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;web&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;$&#123;demo.version&#125;&lt;/version&gt;</span><br><span class=\"line\">            &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;/dependencies&gt;</span><br><span class=\"line\">    &lt;/dependencyManagement&gt;</span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>然后我这里就直接把service、dao、entity三个子模块的pom.xml内容直接贴出来</p>\n<h4 id=\"dao\"><a href=\"#dao\" class=\"headerlink\" title=\"dao\"></a>dao</h4><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">         xsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">    &lt;parent&gt;</span><br><span class=\"line\">        &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">        &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">    &lt;/parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;dao&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;name&gt;dao&lt;/name&gt;</span><br><span class=\"line\">    &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\">    &lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;properties&gt;</span><br><span class=\"line\">        &lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">    &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;entity&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;/plugin&gt;</span><br><span class=\"line\">        &lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"entity\"><a href=\"#entity\" class=\"headerlink\" title=\"entity\"></a>entity</h4><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">         xsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">    &lt;parent&gt;</span><br><span class=\"line\">        &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">        &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">    &lt;/parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;entity&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;name&gt;entity&lt;/name&gt;</span><br><span class=\"line\">    &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\">    &lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;properties&gt;</span><br><span class=\"line\">        &lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">    &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;/plugin&gt;</span><br><span class=\"line\">        &lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"service\"><a href=\"#service\" class=\"headerlink\" title=\"service\"></a>service</h4><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">         xsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">    &lt;parent&gt;</span><br><span class=\"line\">        &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">        &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">    &lt;/parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;service&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;name&gt;service&lt;/name&gt;</span><br><span class=\"line\">    &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\">    &lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;properties&gt;</span><br><span class=\"line\">        &lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">    &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;entity&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;dao&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;/plugin&gt;</span><br><span class=\"line\">        &lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>现在我们来配置web应用模块，这个模块相对于其他模块比较特殊，因为它有着启动入口</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span> encoding=<span class=\"string\">&quot;UTF-8&quot;</span>?&gt;</span><br><span class=\"line\">&lt;project xmlns=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> xmlns:xsi=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class=\"line\">         xsi:schemaLocation=<span class=\"string\">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;modelVersion&gt;<span class=\"number\">4.0</span><span class=\"number\">.0</span>&lt;/modelVersion&gt;</span><br><span class=\"line\">    &lt;parent&gt;</span><br><span class=\"line\">        &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;demo&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">        &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class=\"line\">    &lt;/parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;web&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;<span class=\"number\">0.0</span><span class=\"number\">.1</span>-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">    &lt;name&gt;web&lt;/name&gt;</span><br><span class=\"line\">    &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\">    &lt;description&gt;Demo project <span class=\"keyword\">for</span> Spring Boot&lt;/description&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;properties&gt;</span><br><span class=\"line\">        &lt;java.version&gt;<span class=\"number\">1.8</span>&lt;/java.version&gt;</span><br><span class=\"line\">    &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependencies&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;entity&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;com.example&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;service&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">            &lt;exclusions&gt;</span><br><span class=\"line\">                &lt;exclusion&gt;</span><br><span class=\"line\">                    &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt;</span><br><span class=\"line\">                    &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt;</span><br><span class=\"line\">                &lt;/exclusion&gt;</span><br><span class=\"line\">            &lt;/exclusions&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;/dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;build&gt;</span><br><span class=\"line\">        &lt;plugins&gt;</span><br><span class=\"line\">            &lt;plugin&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;/plugin&gt;</span><br><span class=\"line\">        &lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;/project&gt;</span><br></pre></td></tr></table></figure>\n<p>每个web模块对应一个main方法，启动时找到各自的main方法，点击启动即可，idea一般会自动检测SpringBoot程序入口，想跑哪个点哪个，最后的目录结构如下，我们可以空跑一下项目，是没有问题的，但是为了更好的理解，接下来我们编写一些测试代码进行测试。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308162931749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"编写子模块代码\"><a href=\"#编写子模块代码\" class=\"headerlink\" title=\"编写子模块代码\"></a>编写子模块代码</h2><h3 id=\"Web模块\"><a href=\"#Web模块\" class=\"headerlink\" title=\"Web模块\"></a>Web模块</h3><p>我们在web子模块中创建两个Controller类，UserController和HelloController两个类，如下<br><img src=\"https://img-blog.csdnimg.cn/20200308184747933.png#pic_center\" alt=\"在这里插入图片描述\"><br>UserController内容如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"meta\">@RequestMapping(&quot;/user/*&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserController</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Autowired</span></span><br><span class=\"line\">    UserService userService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@GetMapping(&quot;list&quot;)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> R <span class=\"title\">list</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            List&lt;User&gt; list = userService.list();</span><br><span class=\"line\">            <span class=\"keyword\">return</span> R.isOk().data(userService.list());</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> R.isFail(e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>application.properties内容如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">spring.jpa.hibernate.ddl-<span class=\"keyword\">auto</span>=update</span><br><span class=\"line\">spring.jpa.show-sql=<span class=\"literal\">true</span></span><br><span class=\"line\">spring.jpa.properties.hibernate.hbm2ddl.<span class=\"keyword\">auto</span>=update</span><br><span class=\"line\">spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5InnoDBDialect</span><br><span class=\"line\">spring.jpa.properties.hibernate.format_sql=<span class=\"literal\">true</span></span><br><span class=\"line\">spring.datasource.url=jdbc:mysql:<span class=\"comment\">//localhost:3306/demo?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true</span></span><br><span class=\"line\">spring.datasource.username=root</span><br><span class=\"line\">spring.datasource.password=<span class=\"number\">123456</span></span><br><span class=\"line\">spring.datasource.driver-class-name=com.mysql.jdbc.Driver</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"service模块\"><a href=\"#service模块\" class=\"headerlink\" title=\"service模块\"></a>service模块</h3><p><img src=\"https://img-blog.csdnimg.cn/20200308185007634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.stereotype.Service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.ArrayList;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\">@Service</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserServiceImpl</span>  <span class=\"title\">implements</span> <span class=\"title\">UserService</span> &#123;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    UserRepository userRepository;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> List&lt;User&gt; <span class=\"title\">list</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> userRepository.findAll();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Entity模块\"><a href=\"#Entity模块\" class=\"headerlink\" title=\"Entity模块\"></a>Entity模块</h3><p><img src=\"https://img-blog.csdnimg.cn/20200308185058612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.Column;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.Entity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.Id;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.persistence.Table;</span><br><span class=\"line\"></span><br><span class=\"line\">@Entity</span><br><span class=\"line\">@Table(name = <span class=\"string\">&quot;T_USER&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> &#123;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    @Id</span><br><span class=\"line\">    @Column(name = <span class=\"string\">&quot;USERID&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">public</span> String userId;</span><br><span class=\"line\">    @Column(name = <span class=\"string\">&quot;USERNAME&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">public</span> String username;</span><br><span class=\"line\">    @Column(name = <span class=\"string\">&quot;PASSWORD&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">public</span> String password;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//Getter&amp;Setters</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">R</span>&lt;</span>T&gt; implements Serializable &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">-4577255781088498763L</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> OK = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> FAIL = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> UNAUTHORIZED = <span class=\"number\">2</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> T data; <span class=\"comment\">//服务端数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> status = OK; <span class=\"comment\">//状态码</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String msg = <span class=\"string\">&quot;&quot;</span>; <span class=\"comment\">//描述信息</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//APIS</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> R <span class=\"title\">isOk</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> R();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> R <span class=\"title\">isFail</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> R().status(FAIL);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> R <span class=\"title\">isFail</span><span class=\"params\">(Throwable e)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> isFail().msg(e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> R <span class=\"title\">msg</span><span class=\"params\">(Throwable e)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.setMsg(e.toString());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> R <span class=\"title\">data</span><span class=\"params\">(T data)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.setData(data);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> R <span class=\"title\">status</span><span class=\"params\">(<span class=\"keyword\">int</span> status)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.setStatus(status);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//Constructors</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">R</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> T <span class=\"title\">getData</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> data;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setData</span><span class=\"params\">(T data)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.data = data;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getStatus</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> status;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setStatus</span><span class=\"params\">(<span class=\"keyword\">int</span> status)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.status = status;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getMsg</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> msg;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setMsg</span><span class=\"params\">(String msg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.msg = msg;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"Dao模块\"><a href=\"#Dao模块\" class=\"headerlink\" title=\"Dao模块\"></a>Dao模块</h3><p><img src=\"https://img-blog.csdnimg.cn/2020030818514943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">UserRepository</span> <span class=\"keyword\">extends</span> <span class=\"title\">JpaRepository</span>&lt;<span class=\"title\">User</span>,<span class=\"title\">String</span>&gt; </span>&#123; &#125;</span><br></pre></td></tr></table></figure>\n<p>启动项目之后，成功创建数据表，并返回结果，如下<br><img src=\"https://img-blog.csdnimg.cn/20200308185306298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"多模块打包\"><a href=\"#多模块打包\" class=\"headerlink\" title=\"多模块打包\"></a>多模块打包</h2><p>多模块项目仅仅需要在启动类所在的模块添加打包插件即可！！不要在父类添加打包插件，因为那样会导致全部子模块都使用spring-boot-maven-plugin的方式来打包（例如BOOT-INF/com/example/xx），而web模块引入xx 的jar 需要的是裸露的类文件，即目录格式为（/com/example/xx）。</p>\n<p>首先在IDE打开Maven插件，然后在聚合父工程中点击 clean ，然后点击 package 进行打包。如图：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308202028505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308201739339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308201824543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","分模块","build"]},{"title":"彻底理解AbstractQueuedSynchronizer（一）","url":"/Java/23e87073ef40/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>在分析 Java 并发包 java.util.concurrent 源码的时候，少不了需要了解 AbstractQueuedSynchronizer（以下简写AQS）这个抽象类，因为它是 Java 并发包的基础工具类，是实现 ReentrantLock、CountDownLatch、Semaphore、FutureTask 等类的基础。</p>\n<p>我本人在研究AQS的时候，寻找了许多资料，过程中遇到了大神的这篇<a href=\"https://www.javadoop.com/post/AbstractQueuedSynchronizer\">文章</a>，写的非常清晰，内容很充实，所以我就打算引用这篇文章的内容，并在需要的地方加入我个人的观点和理解，也就是站在大神的肩膀上学习。有兴趣的可以直接前往查看原文，非常值得一看。</p>\n<h2 id=\"AQS-结构\"><a href=\"#AQS-结构\" class=\"headerlink\" title=\"AQS 结构\"></a>AQS 结构</h2><p>先来看看 AQS 有哪些属性，搞清楚这些基本就知道 AQS 是什么套路了，毕竟可以猜嘛！</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 头结点，你直接把它当做 当前持有锁的线程 可能是最好理解的</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> <span class=\"keyword\">volatile</span> Node head;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 阻塞的尾节点，每个新的节点进来，都插入到最后，也就形成了一个链表</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> <span class=\"keyword\">volatile</span> Node tail;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 这个是最重要的，代表当前锁的状态，0代表没有被占用，大于 0 代表有线程持有当前锁</span></span><br><span class=\"line\"><span class=\"comment\">// 这个值可以大于 1，是因为锁可以重入，每次重入都加上 1</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">volatile</span> <span class=\"keyword\">int</span> state;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 代表当前持有独占锁的线程，举个最重要的使用例子，因为锁可以重入</span></span><br><span class=\"line\"><span class=\"comment\">// reentrantLock.lock()可以嵌套调用多次，所以每次用这个来判断当前线程是否已经拥有了锁</span></span><br><span class=\"line\"><span class=\"comment\">// if (currentThread == getExclusiveOwnerThread()) &#123;state++&#125;</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> Thread exclusiveOwnerThread; <span class=\"comment\">//继承自AbstractOwnableSynchronizer</span></span><br></pre></td></tr></table></figure>\n<p>怎么样，看样子应该是很简单的吧，毕竟也就四个属性啊。AbstractQueuedSynchronizer 的等待队列示意如下所示，注意了，之后分析过程中所说的 queue，也就是阻塞队列<strong>不包含 head，不包含 head，不包含 head</strong>。<br><img src=\"https://img-blog.csdnimg.cn/20200413184108317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>等待队列中每个线程被包装成一个 Node 实例，数据结构是链表，一起看看源码吧：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Node</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 标识节点当前在共享模式下</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Node SHARED = <span class=\"keyword\">new</span> Node();</span><br><span class=\"line\">    <span class=\"comment\">// 标识节点当前在独占模式下</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Node EXCLUSIVE = <span class=\"keyword\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// ======== 下面的几个int常量是给waitStatus用的 ===========</span></span><br><span class=\"line\">    <span class=\"comment\">/** waitStatus value to indicate thread has cancelled */</span></span><br><span class=\"line\">    <span class=\"comment\">// 代码此线程取消了争抢这个锁</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> CANCELLED =  <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"comment\">/** waitStatus value to indicate successor&#x27;s thread needs unparking */</span></span><br><span class=\"line\">    <span class=\"comment\">// 官方的描述是，其表示当前node的后继节点对应的线程需要被唤醒</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> SIGNAL    = -<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"comment\">/** waitStatus value to indicate thread is waiting on condition */</span></span><br><span class=\"line\">    <span class=\"comment\">// 本文不分析condition，所以略过吧，下一篇文章会介绍这个</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> CONDITION = -<span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * waitStatus value to indicate the next acquireShared should</span></span><br><span class=\"line\"><span class=\"comment\">     * unconditionally propagate</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 同样的不分析，略过吧</span></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> PROPAGATE = -<span class=\"number\">3</span>;</span><br><span class=\"line\">    <span class=\"comment\">// =====================================================</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 取值为上面的1、-1、-2、-3，或者0(以后会讲到)</span></span><br><span class=\"line\">    <span class=\"comment\">// 这么理解，暂时只需要知道如果这个值 大于0 代表此线程取消了等待，</span></span><br><span class=\"line\">    <span class=\"comment\">//    ps: 半天抢不到锁，不抢了，ReentrantLock是可以指定timeouot的。。。</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> <span class=\"keyword\">int</span> waitStatus;</span><br><span class=\"line\">    <span class=\"comment\">// 前驱节点的引用</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Node prev;</span><br><span class=\"line\">    <span class=\"comment\">// 后继节点的引用</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Node next;</span><br><span class=\"line\">    <span class=\"comment\">// 这个就是线程本尊</span></span><br><span class=\"line\">    <span class=\"keyword\">volatile</span> Thread thread;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Node 的数据结构其实也挺简单的，就是 <code>thread + waitStatus + pre + next</code> 四个属性而已，大家先要有这个概念在心里。上面的是基础知识，后面会多次用到，心里要时刻记着它们，心里想着这个结构图就可以了。下面，我们开始说 ReentrantLock 的公平锁。再次强调，我说的阻塞队列不包含 head 节点。首先，我们先看下 ReentrantLock 的使用方式。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 我用个web开发中的service概念吧</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">OrderService</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 使用static，这样每个线程拿到的是同一把锁，当然，spring mvc中service默认就是单例，别纠结这个</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> ReentrantLock reentrantLock = <span class=\"keyword\">new</span> ReentrantLock(<span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">createOrder</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 比如我们同一时间，只允许一个线程创建订单</span></span><br><span class=\"line\">        reentrantLock.lock();</span><br><span class=\"line\">        <span class=\"comment\">// 通常，lock 之后紧跟着 try 语句</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这块代码同一时间只能有一个线程进来(获取到锁的线程)，</span></span><br><span class=\"line\">            <span class=\"comment\">// 其他的线程在lock()方法上阻塞，等待获取到锁，再进来</span></span><br><span class=\"line\">            <span class=\"comment\">// 执行代码...</span></span><br><span class=\"line\">            <span class=\"comment\">// 执行代码...</span></span><br><span class=\"line\">            <span class=\"comment\">// 执行代码...</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 释放锁</span></span><br><span class=\"line\">            reentrantLock.unlock();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>ReentrantLock 在内部用了内部类 Sync 来管理锁，所以真正的获取锁和释放锁是由 Sync 的实现类来控制的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Sync</span> <span class=\"keyword\">extends</span> <span class=\"title\">AbstractQueuedSynchronizer</span> </span>&#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Sync 有两个实现，分别为 NonfairSync（非公平锁）和 FairSync（公平锁），我们看 FairSync 部分。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ReentrantLock</span><span class=\"params\">(<span class=\"keyword\">boolean</span> fair)</span> </span>&#123;</span><br><span class=\"line\">    sync = fair ? <span class=\"keyword\">new</span> FairSync() : <span class=\"keyword\">new</span> NonfairSync();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"线程抢锁\"><a href=\"#线程抢锁\" class=\"headerlink\" title=\"线程抢锁\"></a>线程抢锁</h2><p>很多人肯定开始嫌弃上面废话太多了，下面跟着代码走，我就不废话了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FairSync</span> <span class=\"keyword\">extends</span> <span class=\"title\">Sync</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = -<span class=\"number\">3000897897090466540L</span>;</span><br><span class=\"line\">      <span class=\"comment\">// 争锁</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        acquire(<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">      <span class=\"comment\">// 来自父类AQS，我直接贴过来这边，下面分析的时候同样会这样做，不会给读者带来阅读压力</span></span><br><span class=\"line\">    <span class=\"comment\">// 我们看到，这个方法，如果tryAcquire(arg) 返回true, 也就结束了。</span></span><br><span class=\"line\">    <span class=\"comment\">// 否则，acquireQueued方法会将线程压到队列中</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquire</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123; <span class=\"comment\">// 此时 arg == 1</span></span><br><span class=\"line\">        <span class=\"comment\">// 首先调用tryAcquire(1)一下，名字上就知道，这个只是试一试</span></span><br><span class=\"line\">        <span class=\"comment\">// 因为有可能直接就成功了呢，也就不需要进队列排队了，</span></span><br><span class=\"line\">        <span class=\"comment\">// 对于公平锁的语义就是：本来就没人持有锁，根本没必要进队列等待(又是挂起，又是等待被唤醒的)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class=\"line\">            <span class=\"comment\">// tryAcquire(arg)没有成功，这个时候需要把当前线程挂起，放到阻塞队列中。</span></span><br><span class=\"line\">            acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) &#123;</span><br><span class=\"line\">              selfInterrupt();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Fair version of tryAcquire.  Don&#x27;t grant access unless</span></span><br><span class=\"line\"><span class=\"comment\">     * recursive call or no waiters or is first.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 尝试直接获取锁，返回值是boolean，代表是否获取到锁</span></span><br><span class=\"line\">    <span class=\"comment\">// 返回true：1.没有线程在等待锁；2.重入锁，线程本来就持有锁，也就可以理所当然可以直接获取</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Thread current = Thread.currentThread();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = getState();</span><br><span class=\"line\">        <span class=\"comment\">// state == 0 此时此刻没有线程持有锁</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 虽然此时此刻锁是可以用的，但是这是公平锁，既然是公平，就得讲究先来后到，</span></span><br><span class=\"line\">            <span class=\"comment\">// 看看有没有别人在队列中等了半天了</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!hasQueuedPredecessors() &amp;&amp;</span><br><span class=\"line\">                <span class=\"comment\">// 如果没有线程在等待，那就用CAS尝试一下，成功了就获取到锁了，</span></span><br><span class=\"line\">                <span class=\"comment\">// 不成功的话，只能说明一个问题，就在刚刚几乎同一时刻有个线程抢先了 =_=</span></span><br><span class=\"line\">                <span class=\"comment\">// 因为刚刚还没人的，我判断过了</span></span><br><span class=\"line\">                compareAndSetState(<span class=\"number\">0</span>, acquires)) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\">// 到这里就是获取到锁了，标记一下，告诉大家，现在是我占用了锁</span></span><br><span class=\"line\">                setExclusiveOwnerThread(current);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">          <span class=\"comment\">// 会进入这个else if分支，说明是重入了，需要操作：state=state+1</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里不存在并发问题</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> nextc = c + acquires;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (nextc &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(<span class=\"string\">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class=\"line\">            setState(nextc);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 如果到这里，说明前面的if和else if都没有返回true，说明没有获取到锁</span></span><br><span class=\"line\">        <span class=\"comment\">// 回到上面一个外层调用方法继续看:</span></span><br><span class=\"line\">        <span class=\"comment\">// if (!tryAcquire(arg) </span></span><br><span class=\"line\">        <span class=\"comment\">//        &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) </span></span><br><span class=\"line\">        <span class=\"comment\">//     selfInterrupt();</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 假设tryAcquire(arg) 返回false，那么代码将执行：</span></span><br><span class=\"line\">      <span class=\"comment\">//        acquireQueued(addWaiter(Node.EXCLUSIVE), arg)，</span></span><br><span class=\"line\">    <span class=\"comment\">// 这个方法，首先需要执行：addWaiter(Node.EXCLUSIVE)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Creates and enqueues node for current thread and given mode.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> the new node</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 此方法的作用是把线程包装成node，同时进入到队列中</span></span><br><span class=\"line\">    <span class=\"comment\">// 参数mode此时是Node.EXCLUSIVE，代表独占模式</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> Node <span class=\"title\">addWaiter</span><span class=\"params\">(Node mode)</span> </span>&#123;</span><br><span class=\"line\">        Node node = <span class=\"keyword\">new</span> Node(Thread.currentThread(), mode);</span><br><span class=\"line\">        <span class=\"comment\">// Try the fast path of enq; backup to full enq on failure</span></span><br><span class=\"line\">        <span class=\"comment\">// 以下几行代码想把当前node加到链表的最后面去，也就是进到阻塞队列的最后</span></span><br><span class=\"line\">        Node pred = tail;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// tail!=null =&gt; 队列不为空(tail==head的时候，其实队列是空的，不过不管这个吧)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pred != <span class=\"keyword\">null</span>) &#123; </span><br><span class=\"line\">            <span class=\"comment\">// 将当前的队尾节点，设置为自己的前驱 </span></span><br><span class=\"line\">            node.prev = pred; </span><br><span class=\"line\">            <span class=\"comment\">// 用CAS把自己设置为队尾, 如果成功后，tail == node 了，这个节点成为阻塞队列新的尾巴</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndSetTail(pred, node)) &#123; </span><br><span class=\"line\">                <span class=\"comment\">// 进到这里说明设置成功，当前node==tail, 将自己与之前的队尾相连，</span></span><br><span class=\"line\">                <span class=\"comment\">// 上面已经有 node.prev = pred，加上下面这句，也就实现了和之前的尾节点双向连接了</span></span><br><span class=\"line\">                pred.next = node;</span><br><span class=\"line\">                <span class=\"comment\">// 线程入队了，可以返回了</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span> node;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 仔细看看上面的代码，如果会到这里，</span></span><br><span class=\"line\">        <span class=\"comment\">// 说明 pred==null(队列是空的) 或者 CAS失败(有线程在竞争入队)</span></span><br><span class=\"line\">        <span class=\"comment\">// 读者一定要跟上思路，如果没有跟上，建议先不要往下读了，往回仔细看，否则会浪费时间的</span></span><br><span class=\"line\">        enq(node);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> node;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Inserts node into queue, initializing if necessary. See picture above.</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> node the node to insert</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> node&#x27;s predecessor</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 采用自旋的方式入队</span></span><br><span class=\"line\">    <span class=\"comment\">// 之前说过，到这个方法只有两种可能：等待队列为空，或者有线程竞争入队，</span></span><br><span class=\"line\">    <span class=\"comment\">// 自旋在这边的语义是：CAS设置tail过程中，竞争一次竞争不到，我就多次竞争，总会排到的</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> Node <span class=\"title\">enq</span><span class=\"params\">(<span class=\"keyword\">final</span> Node node)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">            Node t = tail;</span><br><span class=\"line\">            <span class=\"comment\">// 之前说过，队列为空也会进来这里</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (t == <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">// Must initialize</span></span><br><span class=\"line\">                <span class=\"comment\">// 初始化head节点</span></span><br><span class=\"line\">                <span class=\"comment\">// 细心的读者会知道原来 head 和 tail 初始化的时候都是 null 的</span></span><br><span class=\"line\">                <span class=\"comment\">// 还是一步CAS，你懂的，现在可能是很多线程同时进来呢</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (compareAndSetHead(<span class=\"keyword\">new</span> Node()))</span><br><span class=\"line\">                    <span class=\"comment\">// 给后面用：这个时候head节点的waitStatus==0, 看new Node()构造方法就知道了</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\">// 这个时候有了head，但是tail还是null，设置一下，</span></span><br><span class=\"line\">                    <span class=\"comment\">// 把tail指向head，放心，马上就有线程要来了，到时候tail就要被抢了</span></span><br><span class=\"line\">                    <span class=\"comment\">// 注意：这里只是设置了tail=head，这里可没return哦，没有return，没有return</span></span><br><span class=\"line\">                    <span class=\"comment\">// 所以，设置完了以后，继续for循环，下次就到下面的else分支了</span></span><br><span class=\"line\">                    tail = head;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 下面几行，和上一个方法 addWaiter 是一样的，</span></span><br><span class=\"line\">                <span class=\"comment\">// 只是这个套在无限循环里，反正就是将当前线程排到队尾，有线程竞争的话排不上重复排</span></span><br><span class=\"line\">                node.prev = t;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (compareAndSetTail(t, node)) &#123;</span><br><span class=\"line\">                    t.next = node;</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> t;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 现在，又回到这段代码了</span></span><br><span class=\"line\">    <span class=\"comment\">// if (!tryAcquire(arg) </span></span><br><span class=\"line\">    <span class=\"comment\">//        &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) </span></span><br><span class=\"line\">    <span class=\"comment\">//     selfInterrupt();</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 下面这个方法，参数node，经过addWaiter(Node.EXCLUSIVE)，此时已经进入阻塞队列</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意一下：如果acquireQueued(addWaiter(Node.EXCLUSIVE), arg))返回true的话，</span></span><br><span class=\"line\">    <span class=\"comment\">// 意味着上面这段代码将进入selfInterrupt()，所以正常情况下，下面应该返回false</span></span><br><span class=\"line\">    <span class=\"comment\">// 这个方法非常重要，应该说真正的线程挂起，然后被唤醒后去获取锁，都在这个方法里了</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">acquireQueued</span><span class=\"params\">(<span class=\"keyword\">final</span> Node node, <span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> failed = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">boolean</span> interrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">final</span> Node p = node.predecessor();</span><br><span class=\"line\">                <span class=\"comment\">// p == head 说明当前节点虽然进到了阻塞队列，但是是阻塞队列的第一个，因为它的前驱是head</span></span><br><span class=\"line\">                <span class=\"comment\">// 注意，阻塞队列不包含head节点，head一般指的是占有锁的线程，head后面的才称为阻塞队列</span></span><br><span class=\"line\">                <span class=\"comment\">// 所以当前节点可以去试抢一下锁</span></span><br><span class=\"line\">                <span class=\"comment\">// 这里我们说一下，为什么可以去试试：</span></span><br><span class=\"line\">                <span class=\"comment\">// 首先，它是队头，这个是第一个条件，其次，当前的head有可能是刚刚初始化的node，</span></span><br><span class=\"line\">                <span class=\"comment\">// enq(node) 方法里面有提到，head是延时初始化的，而且new Node()的时候没有设置任何线程</span></span><br><span class=\"line\">                <span class=\"comment\">// 也就是说，当前的head不属于任何一个线程，所以作为队头，可以去试一试，</span></span><br><span class=\"line\">                <span class=\"comment\">// tryAcquire已经分析过了, 忘记了请往前看一下，就是简单用CAS试操作一下state</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class=\"line\">                    setHead(node);</span><br><span class=\"line\">                    p.next = <span class=\"keyword\">null</span>; <span class=\"comment\">// help GC</span></span><br><span class=\"line\">                    failed = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> interrupted;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"comment\">// 到这里，说明上面的if分支没有成功，要么当前node本来就不是队头，</span></span><br><span class=\"line\">                <span class=\"comment\">// 要么就是tryAcquire(arg)没有抢赢别人，继续往下看</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class=\"line\">                    parkAndCheckInterrupt())</span><br><span class=\"line\">                    interrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 什么时候 failed 会为 true???</span></span><br><span class=\"line\">            <span class=\"comment\">// tryAcquire() 方法抛异常的情况</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (failed)</span><br><span class=\"line\">                cancelAcquire(node);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Checks and updates status for a node that failed to acquire.</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns true if thread should block. This is the main signal</span></span><br><span class=\"line\"><span class=\"comment\">     * control in all acquire loops.  Requires that pred == node.prev</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> pred node&#x27;s predecessor holding status</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> node the node</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> &#123;<span class=\"doctag\">@code</span> true&#125; if thread should block</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 刚刚说过，会到这里就是没有抢到锁呗，这个方法说的是：&quot;当前线程没有抢到锁，是否需要挂起当前线程？&quot;</span></span><br><span class=\"line\">    <span class=\"comment\">// 第一个参数是前驱节点，第二个参数才是代表当前线程的节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">boolean</span> <span class=\"title\">shouldParkAfterFailedAcquire</span><span class=\"params\">(Node pred, Node node)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ws = pred.waitStatus;</span><br><span class=\"line\">        <span class=\"comment\">// 前驱节点的 waitStatus == -1 ，说明前驱节点状态正常，当前线程需要挂起，直接可以返回true</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ws == Node.SIGNAL)</span><br><span class=\"line\">            <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">             * This node has already set status asking a release</span></span><br><span class=\"line\"><span class=\"comment\">             * to signal it, so it can safely park.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 前驱节点 waitStatus大于0 ，之前说过，大于0 说明前驱节点取消了排队。</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要知道这点：进入阻塞队列排队的线程会被挂起，而唤醒的操作是由前驱节点完成的。</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以下面这块代码说的是将当前节点的prev指向waitStatus&lt;=0的节点，</span></span><br><span class=\"line\">        <span class=\"comment\">// 简单说，就是为了找个好爹，因为你还得依赖它来唤醒呢，如果前驱节点取消了排队，</span></span><br><span class=\"line\">        <span class=\"comment\">// 找前驱节点的前驱节点做爹，往前遍历总能找到一个好爹的</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ws &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">             * Predecessor was cancelled. Skip over predecessors and</span></span><br><span class=\"line\"><span class=\"comment\">             * indicate retry.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">                node.prev = pred = pred.prev;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">while</span> (pred.waitStatus &gt; <span class=\"number\">0</span>);</span><br><span class=\"line\">            pred.next = node;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">             * waitStatus must be 0 or PROPAGATE.  Indicate that we</span></span><br><span class=\"line\"><span class=\"comment\">             * need a signal, but don&#x27;t park yet.  Caller will need to</span></span><br><span class=\"line\"><span class=\"comment\">             * retry to make sure it cannot acquire before parking.</span></span><br><span class=\"line\"><span class=\"comment\">             */</span></span><br><span class=\"line\">            <span class=\"comment\">// 仔细想想，如果进入到这个分支意味着什么</span></span><br><span class=\"line\">            <span class=\"comment\">// 前驱节点的waitStatus不等于-1和1，那也就是只可能是0，-2，-3</span></span><br><span class=\"line\">            <span class=\"comment\">// 在我们前面的源码中，都没有看到有设置waitStatus的，所以每个新的node入队时，waitStatu都是0</span></span><br><span class=\"line\">            <span class=\"comment\">// 正常情况下，前驱节点是之前的 tail，那么它的 waitStatus 应该是 0</span></span><br><span class=\"line\">            <span class=\"comment\">// 用CAS将前驱节点的waitStatus设置为Node.SIGNAL(也就是-1)</span></span><br><span class=\"line\">            compareAndSetWaitStatus(pred, ws, Node.SIGNAL);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 这个方法返回 false，那么会再走一次 for 循序，</span></span><br><span class=\"line\">        <span class=\"comment\">//     然后再次进来此方法，此时会从第一个分支返回 true</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// private static boolean shouldParkAfterFailedAcquire(Node pred, Node node)</span></span><br><span class=\"line\">    <span class=\"comment\">// 这个方法结束根据返回值我们简单分析下：</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果返回true, 说明前驱节点的waitStatus==-1，是正常情况，那么当前线程需要被挂起，等待以后被唤醒</span></span><br><span class=\"line\">    <span class=\"comment\">//        我们也说过，以后是被前驱节点唤醒，就等着前驱节点拿到锁，然后释放锁的时候叫你好了</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果返回false, 说明当前不需要被挂起，为什么呢？往后看</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 跳回到前面是这个方法</span></span><br><span class=\"line\">    <span class=\"comment\">// if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span></span><br><span class=\"line\">    <span class=\"comment\">//                parkAndCheckInterrupt())</span></span><br><span class=\"line\">    <span class=\"comment\">//                interrupted = true;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 1. 如果shouldParkAfterFailedAcquire(p, node)返回true，</span></span><br><span class=\"line\">    <span class=\"comment\">// 那么需要执行parkAndCheckInterrupt():</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这个方法很简单，因为前面返回true，所以需要挂起线程，这个方法就是负责挂起线程的</span></span><br><span class=\"line\">    <span class=\"comment\">// 这里用了LockSupport.park(this)来挂起线程，然后就停在这里了，等待被唤醒=======</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">parkAndCheckInterrupt</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Thread.interrupted();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 2. 接下来说说如果shouldParkAfterFailedAcquire(p, node)返回false的情况</span></span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 仔细看shouldParkAfterFailedAcquire(p, node)，我们可以发现，其实第一次进来的时候，一般都不会返回true的，原因很简单，前驱节点的waitStatus=-1是依赖于后继节点设置的。也就是说，我都还没给前驱设置-1呢，怎么可能是true呢，但是要看到，这个方法是套在循环里的，所以第二次进来的时候状态就是-1了。</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 解释下为什么shouldParkAfterFailedAcquire(p, node)返回false的时候不直接挂起线程：</span></span><br><span class=\"line\">    <span class=\"comment\">// =&gt; 是为了应对在经过这个方法后，node已经是head的直接后继节点了。剩下的读者自己想想吧。</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>说到这里，也就明白了，多看几遍 <code>final boolean acquireQueued(final Node node, int arg)</code> 这个方法吧。自己推演下各个分支怎么走，哪种情况下会发生什么，走到哪里。</p>\n<h2 id=\"解锁操作\"><a href=\"#解锁操作\" class=\"headerlink\" title=\"解锁操作\"></a>解锁操作</h2><p>最后，就是还需要介绍下唤醒的动作了。我们知道，正常情况下，如果线程没获取到锁，线程会被 <code>LockSupport.park(this);</code> 挂起停止，等待被唤醒。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 唤醒的代码还是比较简单的，你如果上面加锁的都看懂了，下面都不需要看就知道怎么回事了</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">unlock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    sync.release(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">release</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 往后看吧</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tryRelease(arg)) &#123;</span><br><span class=\"line\">        Node h = head;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (h != <span class=\"keyword\">null</span> &amp;&amp; h.waitStatus != <span class=\"number\">0</span>)</span><br><span class=\"line\">            unparkSuccessor(h);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 回到ReentrantLock看tryRelease方法</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryRelease</span><span class=\"params\">(<span class=\"keyword\">int</span> releases)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = getState() - releases;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.currentThread() != getExclusiveOwnerThread())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalMonitorStateException();</span><br><span class=\"line\">    <span class=\"comment\">// 是否完全释放锁</span></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> free = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 其实就是重入的问题，如果c==0，也就是说没有嵌套锁了，可以释放了，否则还不能释放掉</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        free = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        setExclusiveOwnerThread(<span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setState(c);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> free;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Wakes up node&#x27;s successor, if one exists.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> node the node</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"comment\">// 唤醒后继节点</span></span><br><span class=\"line\"><span class=\"comment\">// 从上面调用处知道，参数node是head头结点</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">unparkSuccessor</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">     * If status is negative (i.e., possibly needing signal) try</span></span><br><span class=\"line\"><span class=\"comment\">     * to clear in anticipation of signalling.  It is OK if this</span></span><br><span class=\"line\"><span class=\"comment\">     * fails or if status is changed by waiting thread.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> ws = node.waitStatus;</span><br><span class=\"line\">    <span class=\"comment\">// 如果head节点当前waitStatus&lt;0, 将其修改为0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ws &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">        compareAndSetWaitStatus(node, ws, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">     * Thread to unpark is held in successor, which is normally</span></span><br><span class=\"line\"><span class=\"comment\">     * just the next node.  But if cancelled or apparently null,</span></span><br><span class=\"line\"><span class=\"comment\">     * traverse backwards from tail to find the actual</span></span><br><span class=\"line\"><span class=\"comment\">     * non-cancelled successor.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"comment\">// 下面的代码就是唤醒后继节点，但是有可能后继节点取消了等待（waitStatus==1）</span></span><br><span class=\"line\">    <span class=\"comment\">// 从队尾往前找，找到waitStatus&lt;=0的所有节点中排在最前面的</span></span><br><span class=\"line\">    Node s = node.next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s == <span class=\"keyword\">null</span> || s.waitStatus &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        s = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 从后往前找，仔细看代码，不必担心中间有节点取消(waitStatus==1)的情况</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Node t = tail; t != <span class=\"keyword\">null</span> &amp;&amp; t != node; t = t.prev)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (t.waitStatus &lt;= <span class=\"number\">0</span>)</span><br><span class=\"line\">                s = t;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s != <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"comment\">// 唤醒线程</span></span><br><span class=\"line\">        LockSupport.unpark(s.thread);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>唤醒线程以后，被唤醒的线程将从以下代码中继续往前走：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">parkAndCheckInterrupt</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    LockSupport.park(<span class=\"keyword\">this</span>); <span class=\"comment\">// 刚刚线程被挂起在这里了</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Thread.interrupted();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 又回到这个方法了：acquireQueued(final Node node, int arg)，这个时候，node的前驱是head了</span></span><br></pre></td></tr></table></figure>\n<p>好了，后面就不分析源码了，剩下的还有问题自己去仔细看看代码吧。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>在并发环境下，加锁和解锁需要以下三个部件的协调：</p>\n<ul>\n<li>锁状态。我们要知道锁是不是被别的线程占有了，这个就是 state 的作用，它为 0 的时候代表没有线程占有锁，可以去争抢这个锁，用 CAS 将 state 设为 1，如果 CAS 成功，说明抢到了锁，这样其他线程就抢不到了，如果锁重入的话，state进行 +1 就可以，解锁就是减 1，直到 state 又变为 0，代表释放锁，所以 lock() 和 unlock() 必须要配对啊。然后唤醒等待队列中的第一个线程，让其来占有锁。</li>\n<li>线程的阻塞和解除阻塞。AQS 中采用了 <code>LockSupport.park(thread)</code> 来挂起线程，用 unpark 来唤醒线程。</li>\n<li>阻塞队列。因为争抢锁的线程可能很多，但是只能有一个线程拿到锁，其他的线程都必须等待，这个时候就需要一个 queue 来管理这些线程，AQS 用的是一个 FIFO 的队列，就是一个链表，每个 node 都持有后继节点的引用。AQS 采用了 CLH 锁的变体来实现，感兴趣的读者可以参考这篇文章关于CLH的介绍，写得简单明了。</li>\n</ul>\n<h2 id=\"示例图解析\"><a href=\"#示例图解析\" class=\"headerlink\" title=\"示例图解析\"></a>示例图解析</h2><p>下面属于回顾环节，用简单的示例来说一遍，如果上面的有些东西没看懂，这里还有一次帮助你理解的机会。首先，第一个线程调用 <code>reentrantLock.lock()</code>，翻到最前面可以发现，<code>tryAcquire(1)</code> 直接就返回 true 了，结束。只是设置了 state=1，连 head 都没有初始化，更谈不上什么阻塞队列了。要是线程 1 调用 unlock() 了，才有线程 2 来，那世界就太太太平了，完全没有交集嘛，那我还要 AQS 干嘛。</p>\n<p>如果线程 1 没有调用 unlock() 之前，线程 2 调用了 lock()，想想会发生什么？线程 2 会初始化 <code>head【new Node()】</code>，同时线程 2 也会插入到阻塞队列并挂起 (注意看这里是一个 for 循环，而且设置 head 和 tail 的部分是不 return 的，只有入队成功才会跳出循环)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Node <span class=\"title\">enq</span><span class=\"params\">(<span class=\"keyword\">final</span> Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        Node t = tail;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t == <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">// Must initialize</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndSetHead(<span class=\"keyword\">new</span> Node()))</span><br><span class=\"line\">                tail = head;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            node.prev = t;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (compareAndSetTail(t, node)) &#123;</span><br><span class=\"line\">                t.next = node;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> t;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>首先，是线程 2 初始化 head 节点，此时 <code>head==tail, waitStatus==0</code><br><img src=\"https://img-blog.csdnimg.cn/20200413184946972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后线程 2 入队：<br><img src=\"https://img-blog.csdnimg.cn/20200413184958274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>同时我们也要看此时节点的 waitStatus，我们知道 head 节点是线程 2 初始化的，此时的 waitStatus 没有设置， java 默认会设置为 0，但是到 shouldParkAfterFailedAcquire 这个方法的时候，线程 2 会把前驱节点，也就是 head 的waitStatus设置为 -1。</p>\n<p>那线程 2 节点此时的 waitStatus 是多少呢，由于没有设置，所以是 0；如果线程 3 此时再进来，直接插到线程 2 的后面就可以了，此时线程 3 的 waitStatus 是 0，到 shouldParkAfterFailedAcquire 方法的时候把前驱节点线程 2 的 waitStatus 设置为 -1。<br><img src=\"https://img-blog.csdnimg.cn/20200413185051837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里可以简单说下 waitStatus 中 SIGNAL(-1) 状态的意思，Doug Lea 注释的是：代表后继节点需要被唤醒。也就是说这个 waitStatus 其实代表的不是自己的状态，而是后继节点的状态，我们知道，每个 node 在入队的时候，都会把前驱节点的状态改为 SIGNAL，然后阻塞，等待被前驱唤醒。这里涉及的是两个问题：有线程取消了排队、唤醒操作。其实本质是一样的，读者也可以顺着 “waitStatus代表后继节点的状态” 这种思路去看一遍源码。</p>\n","categories":["Java"],"tags":["Java","AbstractQueuedSynchronizer"]},{"title":"损失函数理解汇总，结合PyTorch和TensorFlow2","url":"/Deep-Learning/f3029f0768bb/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>本文打算讨论在深度学习中常用的十余种损失函数（含变种），结合PyTorch和TensorFlow2对其概念、公式及用途进行阐述，希望能达到看过的伙伴对各种损失函数有个大致的了解以及使用。本文对原理只是浅尝辄止，不进行深挖，感兴趣的伙伴可以针对每个部分深入翻阅资料。</p>\n<p>使用版本：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0</li>\n</ul>\n<h1 id=\"交叉熵损失（CrossEntropyLoss）\"><a href=\"#交叉熵损失（CrossEntropyLoss）\" class=\"headerlink\" title=\"交叉熵损失（CrossEntropyLoss）\"></a>交叉熵损失（CrossEntropyLoss）</h1><p>对于单事件的信息量而言，当事件发生的概率越大时，信息量越小，需要明确的是，信息量是对于单个事件来说的，实际事件存在很多种可能，所以这个时候熵就派上用场了，熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。<strong>交叉熵用来描述两个分布之间的差距，交叉熵越小，假设分布离真实分布越近，模型越好</strong>。</p>\n<p>在分类问题模型中（不一定是二分类），如逻辑回归、神经网络等，在这些模型的最后通常会经过一个sigmoid函数（softmax函数），输出一个概率值（一组概率值），这个概率值反映了预测为正类的可能性（一组概率值反应了所有分类的可能性）。而对于预测的概率分布和真实的概率分布之间，使用交叉熵来计算他们之间的差距，换句不严谨的话来说，交叉熵损失函数的输入，是softmax或者sigmoid函数的输出。交叉熵损失可以从理论公式推导出几个结论（优点），具体公式推导不在这里详细讲解，如下：</p>\n<ul>\n<li>预测的值跟目标值越远时，参数调整就越快，收敛就越快；</li>\n<li>不会陷入局部最优解</li>\n</ul>\n<p>交叉熵损失函数的标准形式（也就是二分类交叉熵损失）如下:<br>$$L = \\frac{1}{N}\\sum_{i}L_i=\\frac{1}{N}\\sum_{i}-[y_i\\cdot log(p_i)+(1-y_i)\\cdot log(1-p_i)]$$<br>其中，$y_i$ 表示样本 $i$ 的标签，正类为1，负类为0，$p_i$ 表示样本 $i$ 预测为正的概率。<br>多分类交叉熵损失如下：<br>$$L=\\frac{1}{N}\\sum_{i}L_i=\\frac{1}{N}\\sum_{i}-\\sum_{c=1}^{M}y_{ic}log(p_{ic})$$<br>其中，$M$ 表示类别的数量，$y_{ic}$ 表示变量（0或1），如果该类别和样本i的类别相同就是1，否则是0，$p_{ic}$ 表示对于观测样本 $i$ 属于类别 $c$ 的预测概率。</p>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy\">BinaryCrossentropy</a>：二分类，经常搭配Sigmoid使用<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.BinaryCrossentropy(from_logits&#x3D;False, label_smoothing&#x3D;0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;binary_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy\">binary_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\">CategoricalCrossentropy</a>：多分类，经常搭配Softmax使用<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CategoricalCrossentropy(from_logits&#x3D;False, label_smoothing&#x3D;0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;categorical_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy\">categorical_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\tlabel_smoothing：[0,1]之间浮点值，加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\">SparseCategoricalCrossentropy</a>：多分类，经常搭配Softmax使用，和CategoricalCrossentropy不同之处在于，CategoricalCrossentropy是one-hot编码，而SparseCategoricalCrossentropy使用一个位置整数表示类别</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.SparseCategoricalCrossentropy(from_logits&#x3D;False, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;sparse_categorical_crossentropy&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy\">sparse_categorical_crossentropy</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits&#x3D;False, axis&#x3D;-1)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tfrom_logits：默认False。为True，表示接收到了原始的logits，为False表示输出层经过了概率处理（softmax）</span><br><span class=\"line\">\taxis：默认是-1，计算交叉熵的维度</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\">BCELoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.BCELoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\">BCEWithLogitsLoss</a>：其实和TensorFlow是的<code>from_logits</code>参数很像，在BCELoss的基础上合并了Sigmoid</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.BCEWithLogitsLoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, pos_weight: Optional[torch.Tensor] &#x3D; None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br><span class=\"line\">\tpos_weight：正样本的权重, 当p&gt;1，提高召回率，当p&lt;1，提高精确度。可达到权衡召回率(Recall)和精确度(Precision)的作用。 </span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">CrossEntropyLoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.CrossEntropyLoss(weight: Optional[torch.Tensor] &#x3D; None, size_average&#x3D;None, ignore_index: int &#x3D; -100, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tweight：每个分类的缩放权重，传入的大小必须和类别数量一至</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\tignore_index：忽略某一类别，不计算其loss，其loss会为0，并且，在采用size_average时，不会计算那一类的loss，除的时候的分母也不会统计那一类的样本</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction：string类型，&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;三种参数值</span><br></pre></td></tr></table></figure>\n<h1 id=\"KL散度\"><a href=\"#KL散度\" class=\"headerlink\" title=\"KL散度\"></a>KL散度</h1><p>我们在计算预测和真实标签之间损失时，需要拉近他们分布之间的差距，即模型得到的预测分布应该与数据的实际分布情况尽可能相近。KL散度(相对熵)是用来衡量两个概率分布之间的差异。模型需要得到最大似然估计，乘以负Log以后就相当于求最小值，此时等价于求最小化KL散度(相对熵)。所以得到KL散度就得到了最大似然。又因为KL散度中包含两个部分，第一部分是交叉熵，第二部分是信息熵，即KL=交叉熵−信息熵。信息熵是消除不确定性所需信息量的度量，简单来说就是真实的概率分布，而这部分是固定的，所以优化KL散度就是近似于优化交叉熵。下面是KL散度的公式：<br>$$D_{KL}(p||q)=\\sum_{i=1}^Np(x_i)\\cdot (logp(x_i)-logq(x_i))$$<br>联系上面的交叉熵，我们可以将公式简化为（KL散度 = 交叉熵 - 熵）：<br>$$D_{KL}(A||B)=H(A,B)-S(A)$$<br>监督学习中，因为训练集中每个样本的标签是已知的，此时标签和预测的标签之间的KL散度等价于交叉熵。<br><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLD\">KLD | kullback_leibler_divergence</a></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.KLD(y_true, y_pred)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLDivergence\">KLDivergence</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.KLDivergence(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;kl_divergence&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<p><strong>Pytorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\">KLDivLoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.KLDivLoss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, log_target: bool &#x3D; False)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br><span class=\"line\">\tlog_target：默认False，指定是否在日志空间中传递目标</span><br></pre></td></tr></table></figure>\n<h1 id=\"平均绝对误差（L1范数损失）\"><a href=\"#平均绝对误差（L1范数损失）\" class=\"headerlink\" title=\"平均绝对误差（L1范数损失）\"></a>平均绝对误差（L1范数损失）</h1><p>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值 $Y_i$ 与估计值 $f(x_i)$ 的绝对差值的总和 $S$ 最小化：<br>$$S=\\sum_{i=1}^n|Y_i-f(x_i)|$$<br>缺点：</p>\n<ul>\n<li>梯度恒定，不论预测值是否接近真实值，这很容易导致发散，或者错过极值点。</li>\n<li>导数不连续，导致求解困难。这也是L1损失函数不广泛使用的主要原因。</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>收敛速度比L2损失函数要快，这是通过对比函数图像得出来的，L1能提供更大且稳定的梯度。</li>\n<li>对异常的离群点有更好的鲁棒性，下面会以例子证实。</li>\n</ul>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAE\">MAE | mean_absolute_error</a><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MAE(y_true, y_pred)</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\">MeanAbsoluteError</a></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanAbsoluteError(reduction=losses_utils.ReductionV2.AUTO, name=<span class=\"string\">&#x27;mean_absolute_error&#x27;</span>)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError\">MeanAbsolutePercentageError</a>：平均绝对百分比误差</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanAbsolutePercentageError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_absolute_percentage_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; 100 * abs(y_true - y_pred) &#x2F; y_true</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE\">MAPE | mean_absolute_percentage_error</a>：平均绝对百分比误差</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MAPE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; 100 * mean(abs((y_true - y_pred) &#x2F; y_true), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\">Huber</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.Huber(delta&#x3D;1.0, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;huber_loss&#39;)</span><br><span class=\"line\">公式：error &#x3D; y_true - y_pred</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tdelta：float类型，Huber损失函数从二次变为线性的点。</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html\">L1Loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.L1Loss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.l1_loss\">l1_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.l1_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;) → Tensor</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html\">SmoothL1Loss</a>：平滑版L1损失，也被称为 Huber 损失函数。<br>$$loss(x,y)=\\frac{1}{n}\\sum_iz_i$$<br>其中，当 $|x_i-y_i|&lt;beta$ 时， $0.5(x_i-y_i)^2/beta$，否则 $|x_i-y_i|-0.5*beta$<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.SmoothL1Loss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;, beta: float &#x3D; 1.0)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br><span class=\"line\">\tbeta：默认为1，指定在L1和L2损耗之间切换的阈值</span><br></pre></td></tr></table></figure></li>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.smooth_l1_loss\">smooth_l1_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.smooth_l1_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;, beta&#x3D;1.0)</span><br></pre></td></tr></table></figure>\n<h1 id=\"均方误差损失（L2范数损失）\"><a href=\"#均方误差损失（L2范数损失）\" class=\"headerlink\" title=\"均方误差损失（L2范数损失）\"></a>均方误差损失（L2范数损失）</h1><p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值 $Y_i$ 与估计值 $f(x_i)$ 的差值的平方和 $S$ 最小化：<br>$$S=\\sum_{i=1}^n(Y_i-f(x_i))^2$$<br>缺点：</p>\n<ul>\n<li>收敛速度比L1慢，因为梯度会随着预测值接近真实值而不断减小。</li>\n<li>对异常数据比L1敏感，这是平方项引起的，异常数据会引起很大的损失。</li>\n</ul>\n<p>优点：</p>\n<ul>\n<li>它使训练更容易，因为它的梯度随着预测值接近真实值而不断减小，那么它不会轻易错过极值点，但也容易陷入局部最优。</li>\n<li>它的导数具有封闭解，优化和编程非常容易，所以很多回归任务都是用MSE作为损失函数。</li>\n</ul>\n<p><strong>TensorFlow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\">MeanSquaredError</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanSquaredError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_squared_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; square(y_true - y_pred)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE\">MSE | mean_squared_error</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MSE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(y_true - y_pred), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\">MeanSquaredLogarithmicError</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MeanSquaredLogarithmicError(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;mean_squared_logarithmic_error&#39;)</span><br><span class=\"line\">公式：loss &#x3D; square(log(y_true + 1.) - log(y_pred + 1.))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSLE\">MSLE | mean_squared_logarithmic_error</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.MSLE(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(log(y_true + 1) - log(y_pred + 1)), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\">MSELoss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.MSELoss(size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.functional.html?highlight=loss#torch.nn.functional.mse_loss\">mse_loss</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.functional.mse_loss(input, target, size_average&#x3D;None, reduce&#x3D;None, reduction&#x3D;&#39;mean&#39;)</span><br></pre></td></tr></table></figure>\n<h1 id=\"Hinge-loss\"><a href=\"#Hinge-loss\" class=\"headerlink\" title=\"Hinge loss\"></a>Hinge loss</h1><p>有人把hinge loss称为铰链损失函数，它可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的损失函数。hinge loss专用于二分类问题，标签值 $y=\\pm 1$，预测值 $\\hat{y}\\in R$。二分类问题的目标函数的要求如下：当 $\\hat{y}$ 大于等于 $\\pm 1$或者小于等于 $-1$时，都是分类器确定的分类结果，此时的损失函数loss为0。而当预测值 $\\hat{y}\\in(-1,1)$ 时，分类器对分类结果不确定，loss不为0。显然，当 $\\hat{y} = 0$ 时，loss达到最大值。对于输出 $y=\\pm 1$，当前 $\\hat{y}$ 的损失为：<br>$$L(y)=max(0,1-y\\cdot \\hat{y})$$<br>扩展到多分类问题上就需要多加一个边界值，然后叠加起来。公式如下：<br>$$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+\\Delta)$$</p>\n<p><strong>Tensorflow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalHinge\">CategoricalHinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CategoricalHinge(reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;categorical_hinge&#39;)</span><br><span class=\"line\">公式：loss &#x3D; maximum(neg - pos + 1, 0) where neg&#x3D;maximum((1-y_true)*y_pred) and pos&#x3D;sum(y_true*y_pred)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge\">categorical_hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.categorical_hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; maximum(neg - pos + 1, 0) where neg&#x3D;maximum((1-y_true)*y_pred) and pos&#x3D;sum(y_true*y_pred)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/Hinge\">Hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.Hinge(</span><br><span class=\"line\">    reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;hinge&#39;</span><br><span class=\"line\">)</span><br><span class=\"line\">公式：loss &#x3D; maximum(1 - y_true * y_pred, 0)，y_true值应为-1或1。如果提供了二进制（0或1）标签，会将其转换为-1或1</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/hinge\">hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(maximum(1 - y_true * y_pred, 0), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/SquaredHinge\">SquaredHinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.SquaredHinge(</span><br><span class=\"line\">    reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;squared_hinge&#39;</span><br><span class=\"line\">)</span><br><span class=\"line\">公式：loss &#x3D; square(maximum(1 - y_true * y_pred, 0))，y_true值应为-1或1。如果提供了二进制（0或1）标签，会将其转换为-1或1。</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/squared_hinge\">squared_hinge</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.squared_hinge(y_true, y_pred)</span><br><span class=\"line\">公式：loss &#x3D; mean(square(maximum(1 - y_true * y_pred, 0)), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html\">HingeEmbeddingLoss</a>：当 $y_n=1$时，$l_n=x_n$，当 $y_n=-1$ 时， $l_n=max{0,\\Delta-x_n}$<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.HingeEmbeddingLoss(margin: float &#x3D; 1.0, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tmargin：float类型，默认为1.</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>\n<h1 id=\"余弦相似度\"><a href=\"#余弦相似度\" class=\"headerlink\" title=\"余弦相似度\"></a>余弦相似度</h1>余弦相似度是机器学习中的一个重要概念，在Mahout等MLlib中有几种常用的相似度计算方法，如欧氏相似度，皮尔逊相似度，余弦相似度，Tanimoto相似度等。其中，余弦相似度是其中重要的一种。余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。</li>\n</ul>\n<p>余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感），公式如下：<br>$$sim(X,Y)=cos\\theta=\\frac{\\vec{x}\\cdot \\vec{y}}{||x||\\cdot ||y||}$$</p>\n<p><strong>Tensorflow：</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\">CosineSimilarity</a>：请注意，所得值是介于-1和0之间的负数，其中0表示正交性，而接近-1的值表示更大的相似性。 如果y_true或y_pred是零向量，则余弦相似度将为0，而与预测值和目标值之间的接近程度无关。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.CosineSimilarity(axis&#x3D;-1, reduction&#x3D;losses_utils.ReductionV2.AUTO, name&#x3D;&#39;cosine_similarity&#39;)</span><br><span class=\"line\">公式：loss &#x3D; -sum(l2_norm(y_true) * l2_norm(y_pred))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\taxis：默认-1，沿其计算余弦相似度的维</span><br><span class=\"line\">\treduction：传入tf.keras.losses.Reduction类型值，默认AUTO，定义对损失的计算方式。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity\">cosine_similarity</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.keras.losses.cosine_similarity(y_true, y_pred, axis&#x3D;-1)</span><br><span class=\"line\">公式：loss &#x3D; -sum(l2_norm(y_true) * l2_norm(y_pred))</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\taxis：默认-1，沿其计算余弦相似度的维</span><br></pre></td></tr></table></figure>\n<p><strong>PyTorch：</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\">CosineEmbeddingLoss</a>：当 $y=1$时，$loss(x,y)1-cos(x_1,x_2)$，当 $y=-1$ 时，$loss(x,y)=max(0,cos(x_1,x_2)-margin)$ </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.nn.CosineEmbeddingLoss(margin: float &#x3D; 0.0, size_average&#x3D;None, reduce&#x3D;None, reduction: str &#x3D; &#39;mean&#39;)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tmargin：float类型，应为-1到1之间的数字，建议为0到0.5，默认值为0</span><br><span class=\"line\">\tsize_average：bool类型，为True时，返回的loss为平均值，为False时，返回的各样本的loss之和</span><br><span class=\"line\">\treduce：bool类型，返回值是否为标量，默认为True</span><br><span class=\"line\">\treduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean</span><br></pre></td></tr></table></figure>","categories":["Deep-Learning"],"tags":["TensorFlow","PyTorch","损失函数","梯度下降"]},{"title":"搞定检索式对话系统的候选response检索--使用pysolr调用Solr","url":"/Deep-Learning/351df5ecefe5/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>构建对话机器人的现有方法中，可以分为 generation-based（生成式）和retrieval-based（检索式），相对于生成式而言，检索式拥有的信息更加丰富，且运行流畅的特点。本篇文章不具体讲解模型，而是来好好阐述关于检索候选回复的实现，比如SMN模型、DAM模型等中，关于检索候选回复的实现。关于SMN模型的论文笔记和实现代码可以参考我的<a href=\"https://dengbocong.blog.csdn.net/article/details/109392033\">另一篇文章</a>和<a href=\"https://github.com/DengBoCong\">GitHub</a>，后续我还会对DAM论文和模型写一篇文章。</p>\n<p>使用到的工具版本如下：</p>\n<ul>\n<li>Solr：8.6.3</li>\n<li>pysolr：3.9.0</li>\n<li>python：3.7</li>\n<li>CentOS：7.6</li>\n<li>Docker：19.03.9</li>\n</ul>\n<h1 id=\"整体流程\"><a href=\"#整体流程\" class=\"headerlink\" title=\"整体流程\"></a>整体流程</h1><p>我们讲解工具使用之前，首先简要的阐述一下我们的目的，如果已经了解过检索式对话系统或者阅读过相应论文，就不用看了。首先我们知道目的是检索候选回复，用什么检索呢？这个和具体模型结构和需求有关。拿SMN模型为例，利用启发式方法从索引中获取候选response，将前一轮的utterances ${u_1,…,u_{n-1}}$ （也就是对话的历史）和 $u_n$ 进行计算，根据他们的<strong>tf-idf</strong>得分，从 ${u_1,…,u_{n-1}}$ 中提取前 $5$ 个关键字，然后将扩展后的message用于索引，并使用索引的内联检索算法来检索候选response。</p>\n<p>模型结构和训练至关重要，但是检索候选回复也是使得整个对话流程实现闭环的关键。我们了解了检索的目的和整体流程，那我们从何实现？方式有很多，可以自行编写一个脚本从数据集中生成一个索引候选数据集（这个是我最开始用的方法，但毕竟没专门研究过检索，所以写的很粗糙，勉强验证功能可以，用作正式使用就不行了），还有一种就是使用现有的检索工具，比如Lucene、Solr、ElasticSearch等等。所以这篇文章就是来讲解部署solr和使用python实现检索（为什么选用Solr？不是说那种工具好坏，而是佛系使用，貌似ElasticSearch现在很火的样子，哈哈哈）。</p>\n<h1 id=\"Solr和Pysolr\"><a href=\"#Solr和Pysolr\" class=\"headerlink\" title=\"Solr和Pysolr\"></a>Solr和Pysolr</h1><p>Solr它是一种开放源码的、基于 Lucene Java 的搜索服务器，易于加入到 Web 应用程序中。Lucene很底层，从底层代码层面来实现需求，而Solr在其上进行了封装，你如果想要实现脱机检索，那还是使用Lucene吧。Solr 提供了层面搜索(就是统计)、命中醒目显示并且支持多种输出格式（包括XML/XSLT 和JSON等格式）。它易于安装和配置，而且附带了一个基于 HTTP 的管理界面。Solr已经在众多大型的网站中使用，较为成熟和稳定。Solr 包装并扩展了 Lucene，所以Solr的基本上沿用了Lucene的相关术语。更重要的是，Solr 创建的索引与 Lucene 搜索引擎库完全兼容。通过对Solr 进行适当的配置，某些情况下可能需要进行编码，Solr 可以阅读和使用构建到其他 Lucene 应用程序中的索引。此外，很多 Lucene 工具（如Nutch、 Luke）也可以使用Solr 创建的索引。可以使用 Solr 的表现优异的基本搜索功能，也可以对它进行扩展从而满足企业的需要，<a href=\"https://lucene.apache.org/\">Solr官网</a>（官方将其和Lucene并列放在一起，嘿嘿嘿，万变不离其宗，看官方文档）。</p>\n<p>而Pysolr是基于Python的Solr轻量级封装，它提供了服务器查询并返回基于查询的结果接口。简单来说就是Pysolr封装了Solr的各种http请求，使用起来非常方便，你可以直接从pypi中直接导入（这个就要吐槽一下Pylucene了，不能从pipy直接导入），<a href=\"https://pypi.org/project/pysolr/\">PySolr官方地址</a>，上面有使用的示例和API，可以自行去看，这里配一张Solr的示意图，方面后面理解：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201118232526492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"部署Solr\"><a href=\"#部署Solr\" class=\"headerlink\" title=\"部署Solr\"></a>部署Solr</h1><h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><p>都0202年了，部署服务应用都是用容器了吧，我这里讲解用Docker部署solr，不了解的可以参考我的关于<a href=\"https://blog.csdn.net/dbc_121/category_9650661.html\">Docker的几篇文章</a>，我这里就不介绍Docker了，默认会就接着往下讲了。</p>\n<p>有了docker环境之后，首先先将solr拉下来，我这里拉的是8.6.3的版本（ps：不喜欢拉最新的，因为最新的可能其他的附属库跟不上更新，出问题）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker pull solr:8.6.3</span><br><span class=\"line\"><span class=\"comment\"># 然后启动solr</span></span><br><span class=\"line\">docker run -itd --name solr -p 8983:8983 solr:8.6.3</span><br><span class=\"line\"><span class=\"comment\"># 然后创建core核心选择器，我这里因为以SMN模型讲解，所以取名SMN</span></span><br><span class=\"line\"><span class=\"comment\"># exec -it ：交互式执行容器</span></span><br><span class=\"line\"><span class=\"comment\"># -c  内核的名称（必须）</span></span><br><span class=\"line\">docker <span class=\"built_in\">exec</span> -it --user=solr solr bin/solr create_core -c smn</span><br></pre></td></tr></table></figure>\n<p>指令具体含义以及core是啥，请自行查阅资料，或者研究一下solr，毕竟先学习基础再来实战。上面构建solr运行容器是简单粗暴且实用的方法，也可以和我一样使用Dockerfile进行构建镜像和容器，Dockerfile内容如下（内容来自<a href=\"https://github.com/docker-solr/docker-solr\">docker-solr</a>项目，官方的docker镜像项目）：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">FROM openjdk:11-jre</span><br><span class=\"line\"></span><br><span class=\"line\">LABEL maintainer=<span class=\"string\">&quot;The Apache Lucene/Solr Project&quot;</span></span><br><span class=\"line\">LABEL repository=<span class=\"string\">&quot;https://github.com/docker-solr/docker-solr&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">ARG SOLR_VERSION=<span class=\"string\">&quot;8.6.3&quot;</span></span><br><span class=\"line\">ARG SOLR_SHA512=<span class=\"string\">&quot;f040d4489118b655bd27451a717c1f22f180c398638d944a53889a1a449e7032b016cecbff1979c2e8bfd51fc037dd613f3b968254001d34fe0e8fc4f6761dcf&quot;</span></span><br><span class=\"line\">ARG SOLR_KEYS=<span class=\"string\">&quot;902CC51935C140BF820230961FD5295281436075&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># If specified, this will override SOLR_DOWNLOAD_SERVER and all ASF mirrors. Typically used downstream for custom builds</span></span><br><span class=\"line\">ARG SOLR_DOWNLOAD_URL</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Override the solr download location with e.g.:</span></span><br><span class=\"line\"><span class=\"comment\">#   docker build -t mine --build-arg SOLR_DOWNLOAD_SERVER=http://www-eu.apache.org/dist/lucene/solr .</span></span><br><span class=\"line\">ARG SOLR_DOWNLOAD_SERVER</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  apt-get update; \\</span><br><span class=\"line\">  apt-get -y install acl dirmngr gpg lsof procps wget netcat gosu tini; \\</span><br><span class=\"line\">  rm -rf /var/lib/apt/lists/*; \\</span><br><span class=\"line\">  <span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/bin; wget -nv https://github.com/apangin/jattach/releases/download/v1.5/jattach; chmod 755 jattach; \\</span><br><span class=\"line\">  <span class=\"built_in\">echo</span> &gt;jattach.sha512 <span class=\"string\">&quot;d8eedbb3e192a8596c08efedff99b9acf1075331e1747107c07cdb1718db2abe259ef168109e46bd4cf80d47d43028ff469f95e6ddcbdda4d7ffa73a20e852f9  jattach&quot;</span>; \\</span><br><span class=\"line\">  sha512sum -c jattach.sha512; rm jattach.sha512</span><br><span class=\"line\"></span><br><span class=\"line\">ENV SOLR_USER=<span class=\"string\">&quot;solr&quot;</span> \\</span><br><span class=\"line\">    SOLR_UID=<span class=\"string\">&quot;8983&quot;</span> \\</span><br><span class=\"line\">    SOLR_GROUP=<span class=\"string\">&quot;solr&quot;</span> \\</span><br><span class=\"line\">    SOLR_GID=<span class=\"string\">&quot;8983&quot;</span> \\</span><br><span class=\"line\">    SOLR_CLOSER_URL=<span class=\"string\">&quot;http://www.apache.org/dyn/closer.lua?filename=lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&amp;action=download&quot;</span> \\</span><br><span class=\"line\">    SOLR_DIST_URL=<span class=\"string\">&quot;https://www.apache.org/dist/lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> \\</span><br><span class=\"line\">    SOLR_ARCHIVE_URL=<span class=\"string\">&quot;https://archive.apache.org/dist/lucene/solr/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> \\</span><br><span class=\"line\">    PATH=<span class=\"string\">&quot;/opt/solr/bin:/opt/docker-solr/scripts:<span class=\"variable\">$PATH</span>&quot;</span> \\</span><br><span class=\"line\">    SOLR_INCLUDE=/etc/default/solr.in.sh \\</span><br><span class=\"line\">    SOLR_HOME=/var/solr/data \\</span><br><span class=\"line\">    SOLR_PID_DIR=/var/solr \\</span><br><span class=\"line\">    SOLR_LOGS_DIR=/var/solr/logs \\</span><br><span class=\"line\">    LOG4J_PROPS=/var/solr/log4j2.xml</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  groupadd -r --gid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GID</span>&quot;</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GROUP</span>&quot;</span>; \\</span><br><span class=\"line\">  useradd -r --uid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_UID</span>&quot;</span> --gid <span class=\"string\">&quot;<span class=\"variable\">$SOLR_GID</span>&quot;</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_USER</span>&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  <span class=\"built_in\">export</span> GNUPGHOME=<span class=\"string\">&quot;/tmp/gnupg_home&quot;</span>; \\</span><br><span class=\"line\">  mkdir -p <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span>; \\</span><br><span class=\"line\">  chmod 700 <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"string\">&quot;disable-ipv6&quot;</span> &gt;&gt; <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>/dirmngr.conf&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"variable\">$SOLR_KEYS</span>; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">    found=<span class=\"string\">&#x27;&#x27;</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">for</span> server <span class=\"keyword\">in</span> \\</span><br><span class=\"line\">      ha.pool.sks-keyservers.net \\</span><br><span class=\"line\">      hkp://keyserver.ubuntu.com:80 \\</span><br><span class=\"line\">      hkp://p80.pool.sks-keyservers.net:80 \\</span><br><span class=\"line\">      pgp.mit.edu \\</span><br><span class=\"line\">    ; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">      <span class=\"built_in\">echo</span> <span class=\"string\">&quot;  trying <span class=\"variable\">$server</span> for <span class=\"variable\">$key</span>&quot;</span>; \\</span><br><span class=\"line\">      gpg --batch --keyserver <span class=\"string\">&quot;<span class=\"variable\">$server</span>&quot;</span> --keyserver-options timeout=10 --recv-keys <span class=\"string\">&quot;<span class=\"variable\">$key</span>&quot;</span> &amp;&amp; found=yes &amp;&amp; <span class=\"built_in\">break</span>; \\</span><br><span class=\"line\">      gpg --batch --keyserver <span class=\"string\">&quot;<span class=\"variable\">$server</span>&quot;</span> --keyserver-options timeout=10 --recv-keys <span class=\"string\">&quot;<span class=\"variable\">$key</span>&quot;</span> &amp;&amp; found=yes &amp;&amp; <span class=\"built_in\">break</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">test</span> -z <span class=\"string\">&quot;<span class=\"variable\">$found</span>&quot;</span> &amp;&amp; <span class=\"built_in\">echo</span> &gt;&amp;2 <span class=\"string\">&quot;error: failed to fetch <span class=\"variable\">$key</span> from several disparate servers -- network issues?&quot;</span> &amp;&amp; <span class=\"built_in\">exit</span> 1; \\</span><br><span class=\"line\">  <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">  <span class=\"built_in\">exit</span> 0</span><br><span class=\"line\"></span><br><span class=\"line\">RUN <span class=\"built_in\">set</span> -ex; \\</span><br><span class=\"line\">  <span class=\"built_in\">export</span> GNUPGHOME=<span class=\"string\">&quot;/tmp/gnupg_home&quot;</span>; \\</span><br><span class=\"line\">  MAX_REDIRECTS=1; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ -n <span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_URL</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    <span class=\"comment\"># If a custom URL is defined, we download from non-ASF mirror URL and allow more redirects and skip GPG step</span></span><br><span class=\"line\">    <span class=\"comment\"># This takes effect only if the SOLR_DOWNLOAD_URL build-arg is specified, typically in downstream Dockerfiles</span></span><br><span class=\"line\">    MAX_REDIRECTS=4; \\</span><br><span class=\"line\">    SKIP_GPG_CHECK=<span class=\"literal\">true</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">elif</span> [ -n <span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_SERVER</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    SOLR_DOWNLOAD_URL=<span class=\"string\">&quot;<span class=\"variable\">$SOLR_DOWNLOAD_SERVER</span>/<span class=\"variable\">$SOLR_VERSION</span>/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">for</span> url <span class=\"keyword\">in</span> <span class=\"variable\">$SOLR_DOWNLOAD_URL</span> <span class=\"variable\">$SOLR_CLOSER_URL</span> <span class=\"variable\">$SOLR_DIST_URL</span> <span class=\"variable\">$SOLR_ARCHIVE_URL</span>; <span class=\"keyword\">do</span> \\</span><br><span class=\"line\">    <span class=\"keyword\">if</span> [ -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> ]; <span class=\"keyword\">then</span> <span class=\"built_in\">break</span>; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;downloading <span class=\"variable\">$url</span>&quot;</span>; \\</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wget -t 10 --max-redirect <span class=\"variable\">$MAX_REDIRECTS</span> --retry-connrefused -nv <span class=\"string\">&quot;<span class=\"variable\">$url</span>&quot;</span> -O <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"keyword\">then</span> <span class=\"built_in\">break</span>; <span class=\"keyword\">else</span> rm -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">done</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ ! -f <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> ]; <span class=\"keyword\">then</span> <span class=\"built_in\">echo</span> <span class=\"string\">&quot;failed all download attempts for solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; <span class=\"built_in\">exit</span> 1; <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">if</span> [ -z <span class=\"string\">&quot;<span class=\"variable\">$SKIP_GPG_CHECK</span>&quot;</span> ]; <span class=\"keyword\">then</span> \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;downloading <span class=\"variable\">$SOLR_ARCHIVE_URL</span>.asc&quot;</span>; \\</span><br><span class=\"line\">    wget -nv <span class=\"string\">&quot;<span class=\"variable\">$SOLR_ARCHIVE_URL</span>.asc&quot;</span> -O <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span>; \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;<span class=\"variable\">$SOLR_SHA512</span> */opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> | sha512sum -c -; \\</span><br><span class=\"line\">    (&gt;&amp;2 ls -l <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span>); \\</span><br><span class=\"line\">    gpg --batch --verify <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz.asc&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">&quot;Skipping GPG validation due to non-Apache build&quot;</span>; \\</span><br><span class=\"line\">  <span class=\"keyword\">fi</span>; \\</span><br><span class=\"line\">  tar -C /opt --extract --file <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>; \\</span><br><span class=\"line\">  (<span class=\"built_in\">cd</span> /opt; ln -s <span class=\"string\">&quot;solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> solr); \\</span><br><span class=\"line\">  rm <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>.tgz&quot;</span>*; \\</span><br><span class=\"line\">  rm -Rf /opt/solr/docs/ /opt/solr/dist/&#123;solr-core-<span class=\"variable\">$SOLR_VERSION</span>.jar,solr-solrj-<span class=\"variable\">$SOLR_VERSION</span>.jar,solrj-lib,solr-test-framework-<span class=\"variable\">$SOLR_VERSION</span>.jar,test-framework&#125;; \\</span><br><span class=\"line\">  mkdir -p /opt/solr/server/solr/lib /docker-entrypoint-initdb.d /opt/docker-solr; \\</span><br><span class=\"line\">  chown -R 0:0 <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span>; \\</span><br><span class=\"line\">  find <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> -<span class=\"built_in\">type</span> d -print0 | xargs -0 chmod 0755; \\</span><br><span class=\"line\">  find <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>&quot;</span> -<span class=\"built_in\">type</span> f -print0 | xargs -0 chmod 0644; \\</span><br><span class=\"line\">  chmod -R 0755 <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/bin&quot;</span> <span class=\"string\">&quot;/opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/contrib/prometheus-exporter/bin/solr-exporter&quot;</span> /opt/solr-<span class=\"variable\">$SOLR_VERSION</span>/server/scripts/cloud-scripts; \\</span><br><span class=\"line\">  cp /opt/solr/bin/solr.in.sh /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  mv /opt/solr/bin/solr.in.sh /opt/solr/bin/solr.in.sh.orig; \\</span><br><span class=\"line\">  mv /opt/solr/bin/solr.in.cmd /opt/solr/bin/solr.in.cmd.orig; \\</span><br><span class=\"line\">  chown root:0 /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  chmod 0664 /etc/default/solr.in.sh; \\</span><br><span class=\"line\">  mkdir -p /var/solr/data /var/solr/logs; \\</span><br><span class=\"line\">  (<span class=\"built_in\">cd</span> /opt/solr/server/solr; cp solr.xml zoo.cfg /var/solr/data/); \\</span><br><span class=\"line\">  cp /opt/solr/server/resources/log4j2.xml /var/solr/log4j2.xml; \\</span><br><span class=\"line\">  find /var/solr -<span class=\"built_in\">type</span> d -print0 | xargs -0 chmod 0770; \\</span><br><span class=\"line\">  find /var/solr -<span class=\"built_in\">type</span> f -print0 | xargs -0 chmod 0660; \\</span><br><span class=\"line\">  sed -i -e <span class=\"string\">&quot;s/\\&quot;\\$(whoami)\\&quot; == \\&quot;root\\&quot;/\\$(id -u) == 0/&quot;</span> /opt/solr/bin/solr; \\</span><br><span class=\"line\">  sed -i -e <span class=\"string\">&#x27;s/lsof -PniTCP:/lsof -t -PniTCP:/&#x27;</span> /opt/solr/bin/solr; \\</span><br><span class=\"line\">  chown -R <span class=\"string\">&quot;0:0&quot;</span> /opt/solr-<span class=\"variable\">$SOLR_VERSION</span> /docker-entrypoint-initdb.d /opt/docker-solr; \\</span><br><span class=\"line\">  chown -R <span class=\"string\">&quot;<span class=\"variable\">$SOLR_USER</span>:0&quot;</span> /var/solr; \\</span><br><span class=\"line\">  &#123; <span class=\"built_in\">command</span> -v gpgconf; gpgconf --<span class=\"built_in\">kill</span> all || :; &#125;; \\</span><br><span class=\"line\">  rm -r <span class=\"string\">&quot;<span class=\"variable\">$GNUPGHOME</span>&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">COPY --chown=0:0 scripts /opt/docker-solr/scripts</span><br><span class=\"line\"></span><br><span class=\"line\">VOLUME /var/solr</span><br><span class=\"line\">EXPOSE 8983</span><br><span class=\"line\">WORKDIR /opt/solr</span><br><span class=\"line\">USER <span class=\"variable\">$SOLR_USER</span></span><br><span class=\"line\"></span><br><span class=\"line\">ENTRYPOINT [<span class=\"string\">&quot;docker-entrypoint.sh&quot;</span>]</span><br><span class=\"line\">CMD [<span class=\"string\">&quot;solr-foreground&quot;</span>]</span><br></pre></td></tr></table></figure>\n<p>容器运行情况如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118231257526.png#pic_center\" alt=\"在这里插入图片描述\"><br>接下来可以访问：<a href=\"http://xxxxxx:8983/solr/%EF%BC%8C%E8%BF%9B%E5%85%A5%E5%88%B0solr%E7%95%8C%E9%9D%A2%EF%BC%8C%E5%A6%82%E4%B8%8B%EF%BC%9A\">http://xxxxxx:8983/solr/，进入到solr界面，如下：</a><br><img src=\"https://img-blog.csdnimg.cn/20201118231420490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后点击Core Admin，查看一下自己刚刚创建的Core，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118231532138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后选择smn就可以使用了，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020111823163747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>结束了？当然没有，哪有那么简单的事儿，首先我们上面算是基本部署好了solr，但是我们需要进行一些必要的使得我们能更好的使用，比如我们需要对文档进行分词，添加相似度计算类（用于tf-idf计算），接下来就说明如何配置这两个东西。</p>\n<h2 id=\"配置IK\"><a href=\"#配置IK\" class=\"headerlink\" title=\"配置IK\"></a>配置IK</h2><p>首先是IK，IK Analyzer(中文分词器)是一个开源的，基于java语言开发的轻量级的中文分词工具包。最初，它是以开源项目 Lucene为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer3.0则发展为 面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。</p>\n<ul>\n<li>Solr 5以前的可以装上<a href=\"https://pan.baidu.com/s/1WAtY5kjI75Kg-e6OAH69cw\">老版本</a>，提取码：g5ib</li>\n<li>Solr 6使用这个<a href=\"https://github.com/cj96248/ik-analyzer-solr6\">版本</a></li>\n<li>Solr 7&amp;8使用这个<a href=\"https://github.com/magese/ik-analyzer-solr\">版本</a></li>\n</ul>\n<p>注意要将IK源码打成JAR包（作为一个老Java，打包还是不难的）。接着将jar包通过传输软件或其它方式传入宿主机的某一文件夹内，然后使用指令将jar包复制到Solr容器的分词包文件夹中：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker cp ik-analyzer.jar solr:/opt/solr-8.6.3/contrib/analysis-extras/lucene-libs</span><br></pre></td></tr></table></figure>\n<p>查看 Solr 容器在宿主机中数据卷的位置：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker inspect solr</span><br><span class=\"line\"><span class=\"comment\"># 找到 Mounts</span></span><br><span class=\"line\"><span class=\"comment\"># Destination : 容器里的路径</span></span><br><span class=\"line\"><span class=\"comment\"># Source : 对应宿主机里的路径</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20201118233446485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>将IK分词器配置到 Solr 的核心配置文件中，Source为上面的Mounts中的：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> &#123;Source&#125;/data/myIKCore/conf/</span><br><span class=\"line\">vim solrconfig.xml</span><br></pre></td></tr></table></figure>\n<p>然后添加如下内容：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dir\t容器存放自带分词JAR包的目录</span></span><br><span class=\"line\"><span class=\"comment\"># regex\tJAR包名</span></span><br><span class=\"line\">&lt;lib dir=<span class=\"string\">&quot;<span class=\"variable\">$&#123;solr.install.dir:../../../..&#125;</span>/contrib/analysis-extras/lucene-libs/&quot;</span> regex=<span class=\"string\">&quot;ik-analyzer.jar&quot;</span> /&gt;</span><br></pre></td></tr></table></figure>\n<p>声明中文分词器</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">vim managed-schema</span><br></pre></td></tr></table></figure>\n<p>找到指定位置添加配置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!-- IKAnalyzer --&gt;</span><br><span class=\"line\">&lt;fieldType name =<span class=\"string\">&quot;text_ik&quot;</span> class =<span class=\"string\">&quot;solr.TextField&quot;</span>&gt;</span><br><span class=\"line\">    &lt;!-- 索引时候的分词器 --&gt;</span><br><span class=\"line\">    &lt;analyzer <span class=\"built_in\">type</span> =<span class=\"string\">&quot;index&quot;</span> isMaxWordLength =<span class=\"string\">&quot;false&quot;</span> class=<span class=\"string\">&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;</span>/&gt;</span><br><span class=\"line\">    &lt;!-- 查询时候的分词器 --&gt;</span><br><span class=\"line\">    &lt;analyzer <span class=\"built_in\">type</span> =<span class=\"string\">&quot;query&quot;</span> isMaxWordLength =<span class=\"string\">&quot;true&quot;</span> class=<span class=\"string\">&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;</span>/&gt;</span><br><span class=\"line\">&lt;/fieldType&gt;</span><br></pre></td></tr></table></figure>\n<p>重启 Solr 容器</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">docker restart solr</span><br></pre></td></tr></table></figure>\n<p>选择刚刚创建的核心选择器<br><img src=\"https://img-blog.csdnimg.cn/202011182340266.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"配置相似度\"><a href=\"#配置相似度\" class=\"headerlink\" title=\"配置相似度\"></a>配置相似度</h2><p>到了这里，你其实可以直接用了，但是如果使用tf-idf的话，会报错，如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request</span><br><span class=\"line\">null:java.lang.UnsupportedOperationException: requires a TFIDFSimilarity (such as ClassicSimilarity)</span><br></pre></td></tr></table></figure>\n<p>所以还是需要配置，打开 <code>managed-schema</code> 将下面一行添加进就可以了：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;similarity class=<span class=\"string\">&quot;solr.ClassicSimilarityFactory&quot;</span>/&gt;</span><br></pre></td></tr></table></figure>\n<h1 id=\"pysolr使用\"><a href=\"#pysolr使用\" class=\"headerlink\" title=\"pysolr使用\"></a>pysolr使用</h1><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建solr</span></span><br><span class=\"line\">solr = pysolr.Solr(url=solr_server, always_commit=True, timeout=10)</span><br><span class=\"line\"><span class=\"comment\"># 使用前习惯性安全检查</span></span><br><span class=\"line\">solr.ping()</span><br><span class=\"line\"><span class=\"comment\"># 将回复数据添加索引，responses是一个json,形式如：[&#123;&#125;,&#123;&#125;,&#123;&#125;,...]，里面每个对象构建按照你回复的需求即可</span></span><br><span class=\"line\">solr.add(docs=responses)</span><br></pre></td></tr></table></figure>\n<p>接下来进行查询，首先我们提取关键词词，我这里将我的tf-idf方法的代码贴出来，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_tf_idf_top_k</span>(<span class=\"params\">history: <span class=\"built_in\">list</span>, k: <span class=\"built_in\">int</span> = <span class=\"number\">5</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    使用tf_idf算法计算权重最高的k个词，并返回</span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        history: 上下文语句</span></span><br><span class=\"line\"><span class=\"string\">        k: 返回词数量</span></span><br><span class=\"line\"><span class=\"string\">    Returns: top_5_key</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    tf_idf = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    vectorizer = TfidfVectorizer(analyzer=<span class=\"string\">&#x27;word&#x27;</span>)</span><br><span class=\"line\">    weights = vectorizer.fit_transform(history).toarray()[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    key_words = vectorizer.get_feature_names()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(weights)):</span><br><span class=\"line\">        tf_idf[key_words[i]] = weights[i]</span><br><span class=\"line\"></span><br><span class=\"line\">    top_k_key = []</span><br><span class=\"line\">    tf_idf_sorted = <span class=\"built_in\">sorted</span>(tf_idf.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)[:k]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> tf_idf_sorted:</span><br><span class=\"line\">        top_k_key.append(element[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> top_k_key</span><br></pre></td></tr></table></figure>\n<p>然后将得到的五个关键词通过query的语法进行组合，得到查询语句，我这里只返回前十个分数最高的候选回复：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">query = <span class=\"string\">&quot;&#123;!func&#125;sum(&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> keyin tf_idf:</span><br><span class=\"line\">    query += <span class=\"string\">&quot;product(idf(utterance,&quot;</span> + key + <span class=\"string\">&quot;),tf(utterance,&quot;</span> + key + <span class=\"string\">&quot;)),&quot;</span></span><br><span class=\"line\">query += <span class=\"string\">&quot;)&quot;</span></span><br><span class=\"line\">candidates = self.solr.search(q=query, start=<span class=\"number\">0</span>, rows=<span class=\"number\">10</span>).docs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#query合起来长这样：&#123;!func&#125;sum(product(idf(utterance,key1),tf(utterance,key1),product(idf(utterance,key2),tf(utterance,key2),...)</span></span><br></pre></td></tr></table></figure>\n<p>查询回复格式如下：<br><img src=\"https://img-blog.csdnimg.cn/20201118235834184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后检索得到了候选回复就可以喂给模型了。</p>\n","categories":["Deep-Learning"],"tags":["NLP","对话系统","检索式对话","solr"]},{"title":"教你如何估计各种神经网络的计算量和参数量","url":"/Deep-Learning/3bb4a04f32b7/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于深度学习模型来说，拥有一个非常好的设计思路和体系架构非常重要，对模型性能的影响非常之大，所以对于模型的研究倾向于在模型性能上的表现。但是对于商业应用来说，算法模型落地的另一个重要考量就是在满足业务场景内存占用、计算量等需要的同时，保证算法的性能，这个研究对于移动端的模型部署更加明显，这有利于压缩应用的体积。最近这段时间正好在研究关于移动端模型部署（TensorFlow Lite用的不是很顺心呀），所以要仔细研究一下模型的参数量等，这不仅可以让我们对模型的大小进行了解，还能更好的调整结构使得模型响应的更快。</p>\n<p>可能有时候觉得，模型的大小需要计算嘛，直接保存大小直接看不就完事儿了？运行速度就更直接了，多运行几次取平均速度不就行了嘛?so easy？这个想法也没啥错，但是大前提是你得有个整型的模型呀（训练成本多高心里没数嘛），因此很多时候我们想要在模型设计之初就估计一下模型的大小以及可能的运行速度（通过一些指标侧面反应速度），这个时候我们就需要更深入的理解模型的内部结构和原理，从而通过估算模型内部的参数量和计算量来对模型的大小和速度进行一个初步评估。</p>\n<p>一个朴素的评估模型速度的想法是评估它的计算量。一般我们用FLOPS，即每秒浮点操作次数FLoating point OPerations per Second这个指标来衡量GPU的运算能力。这里我们用MACC，即乘加数Multiply-ACCumulate operation，或者叫MADD，来衡量模型的计算量。不过这里要说明一下，用MACC来估算模型的计算量只能大致地估算一下模型的速度。模型最终的的速度，不仅仅是和计算量多少有关系，还和诸如内存带宽、优化程度、CPU流水线、Cache之类的因素也有很大关系。</p>\n<p><strong>下面我们对计算量来进行介绍和定义，方便我们后续展开层的讲解：</strong></p>\n<p>神经网络中的许多计算都是点积，例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">y = w[<span class=\"number\">0</span>]*x[<span class=\"number\">0</span>] + w[<span class=\"number\">1</span>]*x[<span class=\"number\">1</span>] + w[<span class=\"number\">2</span>]*x[<span class=\"number\">2</span>] + ... + w[n-<span class=\"number\">1</span>]*x[n-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>此处，$w$ 和 $x$ 是两个向量，结果 $y$ 是标量（单个数字）。对于卷积层或完全连接的层（现代神经网络中两种主要类型的层），$w$ 是该层的学习权重，$x$ 是该层的输入。我们将$w[0]*x[0]+…$计数为一个乘法累加或1个MACC，这里的“累加”运算是加法运算，因为我们将所有乘法的结果相加，所以上式具有 $n$ 个这样的MACC（从技术上讲，上式中只有 $n-1$ 个加法，比乘法数少一个，所以这里知识认为MACC的数量是一个近似值）。就FLOPS而言，因为有 $n$ 个乘法和 $n-1$ 个加法，所以点积执行 $2n-1$ FLOPS，因此，MACC大约是两个FLOPS。现在，我们来看几种不同的层类型，以了解如何计算这些层的MACC数量。</p>\n<p><strong>注意了：下面的阐述如果没有特别说明，默认都是batch为1。</strong></p>\n<h1 id=\"全连接层计算量和参数量估计\"><a href=\"#全连接层计算量和参数量估计\" class=\"headerlink\" title=\"全连接层计算量和参数量估计\"></a>全连接层计算量和参数量估计</h1><p>在完全连接的层中，所有输入都连接到所有输出。 对于具有 $I$ 输入值和 $J$ 输出值的图层，其权重 $W$ 可以存储在 $I×J$ 矩阵中。 全连接层执行的计算为：<br>$$y = matmul(x, W) + b$$<br>在这里，$x$ 是 $I$ 输入值的向量，$W$ 是包含图层权重的 $I×J$ 矩阵，$b$ 是 $J$ 偏差值的向量，这些值也被相加。 结果 $y$ 包含由图层计算的输出值，并且也是大小 $J$ 的向量。对于完全连接层来说，矩阵乘法为 $matmul(x,W)$ ，其中具有 $I\\times J$ 个MACC（和权重矩阵 $W$ 大小一样），对于偏置 $b$，正好补齐了前面我们所说的点积中正好少一个加法操作。因此，比如一个具有300个输入神经元和100个输出神经元的全连接层执行 $300\\times 100 = 30,000$ 个MACC。特别提示：上面我们讨论的批次大小 $Batch=1$ 需要具体计算需要乘上Batch。</p>\n<blockquote>\n<p>也就是说，通常，将长度为 $I$ 的向量与 $I×J$ 矩阵相乘以获得长度为 $J$ 的向量，则需要 $I×J$ MACC或$(2I-1)\\times J$ FLOPS。</p>\n</blockquote>\n<p>上面我们讨论了全连接层的计算量，那么它的参数量是多少呢？这个应该很容易就算出来，对于全连接层而言，它的参数分别是权重 $W$ 和 偏置$b$，所以对于上面的例子中具有300个输入神经元和100个输出神经元的全连接层的参数量是： $300 \\times 100 + 100=30100$ ，这个很容易进行验证，下图是使用TensorFlow进行验证参数量：<br><img src=\"https://img-blog.csdnimg.cn/20210107113022453.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"激活函数计算量估计\"><a href=\"#激活函数计算量估计\" class=\"headerlink\" title=\"激活函数计算量估计\"></a>激活函数计算量估计</h1><p>通常，一个层后面紧接着就是非线性激活函数，例如ReLU或sigmoid，理所当然的计算这些激活函数需要时间，但在这里我们不用MACC进行度量，而是使用FLOPS进行度量，原因是它们不做点积，一些激活函数比其他激活函数更难计算，例如一个ReLU只是：<br>$$y = max(x, 0)$$<br>这是在GPU上的一项操作，激活函数仅应用于层的输出，例如在具有 $J$ 个输出神经元的完全连接层上，ReLU计算 $J$ 次，因此我们将其判定为 $J$ FLOPS。而对于Sigmoid激活函数来说，有不一样了，它涉及到了一个指数，所以成本更高：<br>$$y = 1 / (1 + exp(-x))$$<br>在计算FLOPS时，我们通常将加，减，乘，除，求幂，平方根等作为单个FLOP进行计数，由于在Sigmoid激活函数中有四个不同的运算，因此将其判定为每个函数输出4 FLOPS或总层输出 $J\\times 4$ FLOPS。所以实际上，通常不计这些操作，因为它们只占总时间的一小部分，更多时候我们主要对（大）矩阵乘法和点积感兴趣，所以其实我们通常都是忽略激活函数的计算量。</p>\n<p>对于参数量？注意了它压根没有参数，请看它们的公式，用TensorFlow验证如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107114606186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"LSTM计算量和参数量估计\"><a href=\"#LSTM计算量和参数量估计\" class=\"headerlink\" title=\"LSTM计算量和参数量估计\"></a>LSTM计算量和参数量估计</h1><p>关于LSTM的原理可以参考这一篇文章：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>，LSTM结构如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107204715551.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>实际上LSTM里面有 4 个非线性变换（3 个 门 + 1 个 tanh），每一个非线性变换说白了就是一个全连接网络，形如：$W[h_{t-1},x_t] + b$。其中，第一层是 $x_i$ 和 $h_i$ 的结合，维度就是embedding_size + hidden_size，第二层就是输出层，维度为 hidden_size，则它的计算量按照上文我们对全连接层的阐述，易得MACC为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">(embedding_size + hidden_size) * hidden_size * <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<p>四个非线性变换中，还会对全连接层的输出进行激活函数计算（三个sigmoid和一个tanh），由上面讨论的sigmoid我们知道，对于sigmoid的计算量为：(embedding_size + hidden_size) * hidden_size * 3 *4个FLOPS，而tanh的计算公式为：$\\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$，其中共有八个加，减，乘，除，求幂，平方根等计算，所以计算量为：(embedding_size + hidden_size) * hidden_size * 8个FLOPS。除此之外，LSTM除了在四个非线性变换中的计算，还有三个矩阵乘法（不是点积）、一个加法、一个tanh计算，其中三个矩阵乘法都是shape为(batch, hidden_size)，则这四个运算的计算量为：batch * hidden_size + batch * hidden_size + batch * hidden_size + batch * hidden_size + batch * hidden_size * 8，综上所述，LSTM的计算量为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">(embedding_size + hidden_size) * hidden_size * <span class=\"number\">4</span> 个MACC</span><br><span class=\"line\">embedding_size * hidden_size * <span class=\"number\">8</span> + hidden_size * (hidden_size + <span class=\"number\">20</span>) 个FLOPS</span><br></pre></td></tr></table></figure>\n<p>而该网络的参数量就是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">((embedding_size + hidden_size) * hidden_size + hidden_size) * <span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n<p>对于特征维128的输入，LSTM单元数为64的网络来说，LSTM的参数量为：((128 + 64) * 64 + 64) * 4 = 49408，通过TensorFlow验证如下：<br><img src=\"https://img-blog.csdnimg.cn/20210107211244797.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"卷积层计算量和参数量估计\"><a href=\"#卷积层计算量和参数量估计\" class=\"headerlink\" title=\"卷积层计算量和参数量估计\"></a>卷积层计算量和参数量估计</h1><p>卷积层的输入和输出不是矢量，而是shape为 $H\\times W\\times C$的三维特征图，其中 $H$是特征图的高度，$W$ 是宽度，$C$ 是每个位置的通道数，正如我们所见今天使用的大多数卷积层都是二维正方内核，对于内核大小为 $K$ 的转换层，MACC的数量为：<br>$$K \\times K \\times Cin \\times Hout \\times Wout \\times Cout$$</p>\n<ul>\n<li>输出特征图中有Hout × Wout × Cout个像素；</li>\n<li>每个像素对应一个立体卷积核K x K x Cin在输入特征图上做立体卷积卷积出来的；</li>\n<li>而这个立体卷积操作，卷积核上每个点都对应一次MACC操作</li>\n</ul>\n<p>同样，我们在这里为了方便忽略了偏置和激活。我们不应该忽略的是层的stride，以及任何dilation因子，padding等。这就是为什么我们需要参看层的输出特征图的尺寸Hout × Wout，因它考虑到了stride等因素。比如，对于 $3\\times 3$，128个filter的卷积，在 $112\\times 112$ 带有64个通道的输入特征图上，我们执行MACC的次数是：</p>\n<p>$$3 \\times 3\\times 64\\times 112\\times 112\\times 128 = 924,844,032$$</p>\n<p>这几乎是10亿次累积运算！注意：在此示例中，我们使用“same”填充和$stride = 1$，以便输出特征图与输入特征图具有相同的大小。通常看到卷积层使用$stride = 2$，这会将输出特征图大小减少一半，在上面的计算中，我们将使用 $56 \\times 56$ 而不是 $112 \\times 112$。</p>\n<p>那我们现在来计算一下参数量，如果了解卷积的原理，应该也不难算出它的参数量（可能有人会说卷积原理怎么理解，这里推荐一篇写得通俗易懂的文章：<a href=\"https://zhuanlan.zhihu.com/p/77471866\">CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride)</a>），根据卷积的原理，对于上面的例子中， $3\\times 3$，128个filter的卷积，在 $112\\times 112$ 带有64个通道的输入特征图上的参数量为：$3\\times 3 \\times 64 \\times 128 + 128 = 73856$，用TensorFlow验证结果如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210107152351432.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"深度可分离卷积层\"><a href=\"#深度可分离卷积层\" class=\"headerlink\" title=\"深度可分离卷积层\"></a>深度可分离卷积层</h1><p>深度可分离卷积是将常规卷积因式分解为两个较小的运算，它们在一起占用的内存更少（权重更少），并且速度更快，这些层在移动设备上可以很好地工作，既是MobileNet的基础，也是Xception等大型模型的基础。深度可分离卷积中，第一个操作是深度卷积，它在很多方面与常规卷积相似，不同之处在于我们不合并输入通道，也就是说输出通道数始终与输入通道数相同，深度卷积的MACC总数为：<br>$$K × K × Cin × Hout × Wout$$<br>这减少了 $Cin$ 的工作量，比常规的卷积层效率更高。当然，仅深度卷积是不够的，我们还需要增加“可分离”，第二个操作是常规卷积，但始终使用内核大小 $1\\times 1$，即 $K=1$，也称为“逐点”卷积，MACC的数量为：<br>$$Cin × Hout × Wout × Cout$$<br>深度可分离卷积分为两个操作，深度卷积和可分离，所以现在我们对两种操作分别就上面的例子计算，和并和常规 $3\\times 3$ 卷积进行比较：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">3</span>×<span class=\"number\">3</span> depthwise          : <span class=\"number\">7</span>,<span class=\"number\">225</span>,<span class=\"number\">344</span></span><br><span class=\"line\"><span class=\"number\">1</span>×<span class=\"number\">1</span> pointwise          : <span class=\"number\">102</span>,<span class=\"number\">760</span>,<span class=\"number\">448</span></span><br><span class=\"line\">深度可分离卷积          : <span class=\"number\">109</span>,<span class=\"number\">985</span>,<span class=\"number\">792</span> MACCs</span><br><span class=\"line\"></span><br><span class=\"line\">常规 <span class=\"number\">3</span>×<span class=\"number\">3</span> 卷积           : <span class=\"number\">924</span>,<span class=\"number\">844</span>,032 MACCs</span><br></pre></td></tr></table></figure>\n<p>所以深度可分离卷积的计算量简化为：<br>$$Cin × Hout × Wout × (K × K + Cout)$$</p>\n<p>我们来看看MobileNet V2中的对深度可分离卷积的拓展，MobileNet V2相比与V1，主要是由DW+PW两层变成了下面的三层PW+DW+PW：</p>\n<ul>\n<li>一个 $1\\times 1$ 卷积，为特征图添加更多通道（称为expansion layer）</li>\n<li>$3\\times 3$ 深度卷积，用于过滤数据（depthwise convolution）</li>\n<li>$1\\times 1$ 卷积，再次减少通道数（projection layer，bottleneck convolution）</li>\n</ul>\n<p>这种扩展块中MACC数量的公式：Cexp = (Cin × expansion_factor)，（expansion_factor用于创建深度层要处理的额外通道，使得Cexp在此块内使用的通道数量）</p>\n<ul>\n<li>$MACC\\ expansion\\ layer = Cin \\times Hin \\times Win \\times Cexp$，(参照上面传统卷积，把卷积核设置为1x1即得)</li>\n<li>$MACC\\ depthwise\\ layer = K \\times K \\times Cexp \\times Hout \\times Wout$(参照MoblieNet V1分析)</li>\n<li>$MACC\\ projection\\ layer = Cexp \\times Hout \\times Wout \\times Cout$(参照MoblieNet V1分析，或者传统卷积把卷积核设置为1x1即得)</li>\n</ul>\n<p>把所有这些放在一起：</p>\n<p>$$MACC_v2 = Cin \\times Hin \\times Win \\times Cexp + (K \\times K + Cout) \\times Cexp \\times Hout \\times Wout$$</p>\n<p>这与MobileNet V1使用的深度可分层相比如何？如果我们使用输入特征图 $112\\times 112\\times 64$ 扩展因子6，以及 $stride = 1$ 的 $3\\times 3$ 深度卷积和128输出通道，那么MACC的总数是：</p>\n<p>$$(3 \\times 3 + 128 + 64) \\times (64 \\times 6) \\times 112 \\times 112 = 968,196,096$$</p>\n<p>这不是比以前更多吗？是的，它甚至超过了最初的 $3\\times 3$ 卷积。但是……请注意，由于扩展层，在这个块内，我们实际上使用了 $64 \\times 6 = 384$ 通道。因此，这组层比原始的$3\\times3$ 卷积做得更多（从64到128个通道），而计算成本大致相同。</p>\n<h1 id=\"Batch-normalization\"><a href=\"#Batch-normalization\" class=\"headerlink\" title=\"Batch normalization\"></a>Batch normalization</h1><p>在现代网络中，通常在每个卷积层之后都包含一个batch norm层。对于想要详细了解Batch normalization的原理，可参考这篇文章：<a href=\"https://zhuanlan.zhihu.com/p/340219662\">论文阅读笔记：看完也许能进一步了解Batch Normalization</a></p>\n<p>Batch normalization在层的输出上进行计算，针对每个输出值，计算公式如下：<br>$$z = gamma * (y - mean) / sqrt(variance + epsilon) + beta$$<br>此处，$y$ 是上一层的输出图中的元素。 我们首先通过减去该输出通道的平均值并除以标准偏差来对该值进行归一化（epsilon 用于确保不除以0，通常为0.001），然后，我们将系数gamma缩放，然后添加一个偏差或偏移beta。每个通道都有自己的gamma，beta，均值和方差值，因此，如果卷积层的输出中有 $C$ 个通道，则Batch normalization层将学习 $C\\times 4$ 参数，如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20210107164949918.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>通常将Batch normalization应用于卷积层的输出，而在ReLU之前，在这种情况下，我们可以做一些数学运算以使Batch normalization层消失！由于在全连接层中进行的卷积或矩阵乘法只是一堆点积，它们是线性变换，而上面给出的Batch normalization公式也是线性变换，因此我们可以将这两个公式组合为一个变换。我们只需要将Batch normalization参数合并到前面各层的权重中，其数学运算非常简单。 具体可以参见上面的那篇关于Batch Normalization的文章，也就是说我们可以完全忽略Batch Normalization层的影响，因为我们在进行推理时实际上将其从模型中删除了。</p>\n<blockquote>\n<p>注意：此trick仅在层的顺序为：卷积-&gt;BN-&gt;ReLU时才有效；不适用于：卷积-&gt;ReLU-&gt;BN。ReLU是一个非线性操作，它会把数据弄乱。（但如果批量标准化后面紧跟一个新的卷积层，你可以反过来折叠参数）</p>\n</blockquote>\n<h1 id=\"其它层\"><a href=\"#其它层\" class=\"headerlink\" title=\"其它层\"></a>其它层</h1><p><strong>池化层</strong><br>到此我们研究了卷积层和全连接层，这两个是现代神经网络中最重要的组成部分。但是也有其他类型的层，例如池化层。这些其他层类型肯定需要时间，但它们不使用点积，因此不能用MACC测量。如果你对计算FLOPS感兴趣，只需获取特征图大小并将其乘以表示处理单个输入元素的难度的常量。</p>\n<p>示例：在 $112\\times 112$ 具有128通道的特征图上具有过滤器大小2和步幅2的最大池化层需要 $112 \\times 112 \\times 128 = 1,605,632$ FLOPS或1.6兆FLOPS。当然，如果步幅与滤波器尺寸不同（例如 $3\\times 3$窗口，$2\\times 2$步幅），则这些数字会稍微改变。</p>\n<p>但是，在确定网络的复杂性时，通常会忽略这些附加层。毕竟，与具有100个MFLOPS的卷积/全连接层相比，1.6 MFLOPS非常小。因此，它成为网络总计算复杂度的舍入误差。</p>\n<p><strong>Concate层</strong><br>某些类型的操作，例如结果的连接，通常甚至可以免费完成。不是将两个层分别写入自己的输出张量中，然后有一个将这两个张量复制到一个大张量的连接层。相反，第一层可以直接写入大张量的前半部分，第二层可以直接写入后半部分。不需要单独的复制步骤。</p>\n<ul>\n<li>减少参数</li>\n<li>降低精度</li>\n<li>融合计算单元步骤</li>\n</ul>\n<p><em>参考资料</em>：</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/b8d48c99a47c\">卷积神经网络中参数量的计算与模型占用显存的计算</a></li>\n<li><a href=\"http://machinethink.net/blog/how-fast-is-my-model/\">How fast is my model?</a></li>\n<li><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a></li>\n<li><a href=\"https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\">Number of parameters in an LSTM model</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/77471991\">CNN卷积层、全连接层的参数量、计算量</a></li>\n<li><a href=\"https://arxiv.org/abs/1506.02626\">Learning both Weights and Connections for Efficient Neural Networks</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","LSTM","神经网络","CNN","参数量","计算量"]},{"title":"深度学习中眼花缭乱的Normalization学习总结","url":"/Deep-Learning/32dcc7d3f168/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>Github：本文代码放在该项目中：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>对于深度学习而言，正则化方法就是“通过把一部分不重要的复杂信息损失掉，以此来降低拟合难度以及过拟合的风险，从而加速了模型的收敛”，而本篇文章我们要讲的Normalization方法的目的就是让分布稳定下来（降低各维度数据的方差），不同的正则化方法的区别只是操作的信息维度不同，即选择损失信息的维度不同。</p>\n<p>本篇文章将结合TensorFlow和Pytorch计算框架，阐述几种归一化算法：Batch Normalization(BN)、Layer Normalization(LN)、Weight Normalization(WN)、Instance Normalization(IN)、Group Normalization(GN)、Cosine Normalization(CN)。</p>\n<h1 id=\"相关知识\"><a href=\"#相关知识\" class=\"headerlink\" title=\"相关知识\"></a>相关知识</h1><p>“Covariate Shift: A Review and Analysis on Classifiers”论文中对Covariate Shift的描述：</p>\n<blockquote>\n<p>In real world, the joint distribution of inputs to the model and outputs of the model differs between training and test data, which is called dataset shift</p>\n</blockquote>\n<p>关于Covariate Shift的一篇方法综述性论文，可参考我的另一篇<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记</a>。</p>\n<p>下面我用白话简单的阐述一下Covariate Shift和Internal Covariate Shift</p>\n<ul>\n<li>Covariate Shift：假设 $q_1(x)$ 是测试集中一个样本点的概率密度，$q_0(x)$ 是训练集中一个样本点的概率密度。最终我们估计一个条件概率密度 $p(y|x,\\theta)$，它由 $x$ 和一组参数 $\\theta={\\theta_1,\\theta_2,…,\\theta_m}$ 所决定。对于一组参数来说，对应 $loss(\\theta)$ 函数评估性能的好坏。综上，当我们找出在 $q_0(x)$ 分布上最优的一组 $\\theta^{‘}$ 时，能否保证 $q_1(x)$ 上测试时也最好呢？传统机器学习假设训练集和测试集是独立同分布的，即 $q_0(x)=q_1(x)$，所以可以推出最优 $\\theta^{‘}$ 依然可以保证 $q_1(x)$ 最优。但现实当中这个假设往往不成立，伴随新数据产生，老数据会过时，当 $q_0(x)$ 不再等于 $q_1(x)$ 时，就被称作Covariate Shift。</li>\n<li>Internal Covariate Shift：深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略，Google 将这一现象总结为Internal Covariate Shift，简称 ICS。</li>\n</ul>\n<p>ICS会导致什么问题？简而言之，每个神经元的输入数据不再是“独立同分布”。</p>\n<ul>\n<li>上层参数需要不断适应新的输入数据分布，降低学习速度。</li>\n<li>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</li>\n<li>每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li>\n</ul>\n<p>我们以神经网络中的一个普通神经元为例，神经元接收一组输入向量$x=(x_1,x_2,…,x_d)$，通过某种运算后，输出一个标量值：$y=f(x)$。由于 ICS 问题的存在，$x$ 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 $x$ 喂给神经元之前，先对其做平移和伸缩变换， 将 $x$ 的分布规范化成在固定区间范围的标准分布。通用变换框架就如下所示：<br>$$h=f(g\\cdot \\frac{x-\\mu}{\\sigma}+b)$$<br>其中，$\\mu$ 是平移参数，$\\sigma$ 是缩放参数，通过这两个参数进行 shift 和 scale 变换：$\\hat{x}=\\frac{x-\\mu}{\\sigma}$得到的数据符合均值为 0、方差为 1 的标准分布。$b$ 是再平移参数，$g$ 是再缩放参数。将上一步得到的 $\\hat{x}$ 进一步变换为：$y=g\\cdot \\hat{x}+b$，最终得到的数据符合均值为 $b$ 、方差为 $g^2$ 的分布。</p>\n<h1 id=\"Batch-Normalization-BN\"><a href=\"#Batch-Normalization-BN\" class=\"headerlink\" title=\"Batch Normalization(BN)\"></a>Batch Normalization(BN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>\n</ul>\n<p>针对Batch Normalization的详细介绍，可以参考这一篇文章：论文阅读笔记：<a href=\"https://zhuanlan.zhihu.com/p/340219662\">看完也许能进一步了解Batch Normalization</a></p>\n<p>Batch Normalization 于2015年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 $x_i$ 的均值和方差，因而称为 Batch Normalization。<br>$$\\mu_i=\\frac{1}{M}\\sum x_i \\\\ \\ \\sigma_i=\\sqrt{\\frac{1}{M}\\sum(x_i-\\mu_i)^2+\\epsilon}$$<br>其中$M$是mini-batch 的大小。按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。</p>\n<p>BN 独立地规范化每一个输入维度 $x_i$ ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。因此，<strong>BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多</strong>。</p>\n<p>另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。不过，也有研究者专门提出了适用于 RNN 的 BN 使用方法，这里先不展开了，附一张Batch Normalization的结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201230094634732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Layer-Normalization-LN\"><a href=\"#Layer-Normalization-LN\" class=\"headerlink\" title=\"Layer Normalization(LN)\"></a>Layer Normalization(LN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1607.06450.pdf\">Layer Normalization</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/258977332\">论文阅读笔记</a></li>\n</ul>\n<p>Layer Normalization就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。<br>$$\\mu=\\sum_ix_i,\\quad \\sigma=\\sqrt{\\sum_i(x_i-\\mu)^2+\\epsilon}$$</p>\n<p>其中 $i$ 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 $\\mu$, $\\sigma$, $g$, $b$ 均为标量（BN中是向量），所有输入共享一个normalization变换。</p>\n<p>LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。</p>\n<p>但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。</p>\n<p>在NLP任务中，对于不同的训练案例，通常有不同的句子长度。这在RNN中很容易处理，因为每个时间步使用相同的权重。但是，当我们以明显的方式将批归一化应用于RNN时，我们需要为序列中的每个时间步计算并存储单独的统计信息。如果测试序列比任何训练序列都要长，这是有问题的。层归一化不存在此类问题，因为其归一化项仅取决于当前时间步对层的求和输入。 在所有时间步中，它也只有一组增益和偏置参数共享。</p>\n<p>这里附一张Batch Normalization和Layer Normalization之间计算差异的图：<br><img src=\"https://img-blog.csdnimg.cn/20201230161155494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>LN用于RNN进行Normalization时，取得了比BN更好的效果。但用于CNN时，效果并不如BN明显。</p>\n<h1 id=\"Weight-Normalization-WN\"><a href=\"#Weight-Normalization-WN\" class=\"headerlink\" title=\"Weight Normalization(WN)\"></a>Weight Normalization(WN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1602.07868.pdf\">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a></li>\n</ul>\n<p>在前面列的变换框架中：<br>$$h=f(g\\cdot \\frac{x-\\mu}{\\sigma}+b)$$<br>中，经过规范化之后的 $y$ 作为输入送到下一个神经元，应用以 $w$ 为参数的 $f_w(\\cdot)$ 函数定义的变换。最普遍的变换是线性变换，即 $f_w(x)=w\\cdot x$。BN 和 LN 均将规范化应用于输入的特征数据 $x$ ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 $w$ ，这就是 WN 名称的来源。具体而言，WN 提出的方案是，将权重向量 $w$ 分解为向量方向 $\\hat{v}$ 和向量模 $g$ 两部分：<br>$$w=g\\cdot \\hat{v}=g\\cdot \\frac{v}{||v||}$$<br>其中 $v$ 是与 $w$ 同维度的向量， $||v||$ 是欧氏范数，因此 $\\hat{v}$ 是单位向量，决定了 $w$ 的方向。$g$ 是标量，决定了 $w$ 的长度。由于 $||w||\\equiv |g|$ ，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。通过推导（如下），我们会发现WN其实只是在前述框架上令 $\\sigma=||v||,\\quad \\mu=0, \\quad b=0$。<br>$$f_w(WN(x))=w\\cdot WN(x)=g\\cdot \\frac{v}{||v||}\\cdot x=v\\cdot g\\cdot \\frac{x}{||v||}=f_v(g\\cdot \\frac{x}{||v||})$$</p>\n<p>BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。<strong>虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同</strong>。另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 $\\mu=0$。 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 $\\mu$ 。WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。</p>\n<p>和目前主流归一化方法不同的是，WN的归一化操作作用在了权值矩阵之上。从其计算方法上来看，WN完全不像是一个归一化方法，更像是基于矩阵分解的一种优化策略，它带来了四点好处：</p>\n<ul>\n<li>更快的收敛速度；</li>\n<li>更强的学习率鲁棒性；</li>\n<li>可以应用在RNN等动态网络中；</li>\n<li>对噪声更不敏感，更适用在GAN，RL等场景中。</li>\n</ul>\n<p>说WN不像归一化的原因是它并没有对得到的特征范围进行约束的功能，所以WN依旧对参数的初始值非常敏感，这也是WN一个比较严重的问题。所以针对这个问题，论文中作者实验采用WN+Mean-Only BN的方法。在每一层的layer的激活函数之前，我们发现$t=w\\cdot x=\\frac{g}{||v||}v\\cdot x$，我们发现虽然我们将权重 $w$ 进行分离，但是每一层的激活函数之前的输出的均值仍然与 $v$ 有关。因此作者将WN与BN进行结合，采用移动平均去计算每个mini-batch上的均值 $\\mu [t]$，因此：<br>$$t=wx$$    $$\\hat{t}=t-\\mu [t] + b$$    $$y=\\phi (\\hat{t})$$<br>对激活函数之前的t的反向传播的损失函数为：<br>$$\\bigtriangledown_tL=\\bigtriangledown_{\\hat{t}}L-\\mu [\\bigtriangledown_{\\hat{t}}L]$$<br>对损失函数来说，mean-only batch normalization 可以有效的在反向传播中centering梯度。并且对于mean-only的BN来说，其计算量要小于full-BN的方法。另外，这个方法比Full-BN引入更轻微的噪声。</p>\n<h1 id=\"Cosine-Normalization-CN\"><a href=\"#Cosine-Normalization-CN\" class=\"headerlink\" title=\"Cosine Normalization(CN)\"></a>Cosine Normalization(CN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1702.05870v5.pdf\">Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks</a><br><img src=\"https://img-blog.csdnimg.cn/20201230201656422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量 $w$ 和 特征数据向量 $x$ 的点积，向量点积是无界（unbounded）的啊！那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围。于是，Cosine Normalization 就出世了。他们不处理权重向量 $w$ ，也不处理特征数据向量 $x$ ，就改了一下线性变换的函数：<br>$$f_w(x)=cos\\theta=\\frac{w\\cdot x}{||w||\\cdot ||x||}$$<br>其中 $\\theta$ 是  $w$ 和 $x$ 的夹角。不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是  $w$ 和 $x$ 的内积，而分母则可以看做用  $w$ 和 $x$ 二者的模之积进行规范化。对比一下 WN 的公式：<br>$$f_w(WN(x))=f_v(g\\cdot \\frac{x}{||v||})$$<br>一定程度上可以理解为，WN 用 权重的模 $||v||$ 对输入向量进行 scale，而 CN 在此基础上用输入向量的模 $||x||$ 对输入向量进行了进一步的 scale。CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。</p>\n<h1 id=\"Instance-Normalization\"><a href=\"#Instance-Normalization\" class=\"headerlink\" title=\"Instance Normalization\"></a>Instance Normalization</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1607.08022.pdf\">Instance Normalization:The Missing Ingredient for Fast Stylization</a></li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20201230212443641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>IN在计算归一化统计量时并没有像BN那样跨样本、单通道，也没有像LN那样单样本、跨通道。它是取的单通道，单样本上的数据进行计算，如图1最右侧所示。所以对比BN的公式，它只需要它只需要去掉批量维的求和即可：<br>$$u_{ti}=\\frac{1}{HW}\\sum_{l=1}^W\\sum_{m=1}^Hx_{tilm}\\quad \\sigma_{ti}^2=\\frac{1}{HW}\\sum_{l=1}^W\\sum_{m=1}^H(x_{tilm}-u_{ti})^2 \\quad y_{tijk}=\\frac{x_{tilm}-u_{ti}}{\\sqrt{\\sigma_{ti}^2+\\epsilon}}$$<br>在TensorFlow实现中，对于是否使用BN中的可学习参数 $\\beta$ 和 $\\gamma$ ，从LN的TensorFlow中源码中我们可以看出这两个参数是要使用的。但是我们也可以通过将其值置为False来停用它们，这一点和其它归一化方法在TensorFlow中的实现是相同的。</p>\n<h1 id=\"Group-Normalization-GN\"><a href=\"#Group-Normalization-GN\" class=\"headerlink\" title=\"Group Normalization(GN)\"></a>Group Normalization(GN)</h1><ul>\n<li>Paper Link：<a href=\"https://arxiv.org/pdf/1803.08494.pdf\">Group Normalization</a></li>\n</ul>\n<p>组归一化 (GN) 将输入的通道分成较小的子组，并根据其均值和方差归一化这些值。由于 GN 只对单一样本起作用，因此该技术与batch大小无关。在图像分类任务中，GN 的实验得分与BN十分接近。如果整体batch大小很小，则使用 GN 而不是BN可能更为有利，因为较小的batch大小会导致BN的性能不佳，下面附一张原论文中的图：<br><img src=\"https://img-blog.csdnimg.cn/20201230205516507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<blockquote>\n<p>从左到右一次是BN，LN，IN，GN。众所周知，深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式，N是batch size，H/W是feature的高/宽，C是feature的channel，压缩H/W至一个维度，其三维的表示如上图，假设单个方格的长度是1，那么其表示的是[6, 6，*, * ]</p>\n</blockquote>\n<ul>\n<li>BN在batch的维度上norm，归一化维度为[N，H，W]，对batch中对应的channel归一化；</li>\n<li>LN避开了batch维度，归一化的维度为[C，H，W]；</li>\n<li>IN 归一化的维度为[H，W]；</li>\n<li>而GN介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]</li>\n</ul>\n<p>传统角度来讲，在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。而更高维的特征比如VLAD和Fisher Vectors(FV)也可以看作是group-wise feature，此处的group可以被认为是每个聚类（cluster）下的子向量sub-vector。</p>\n<p>从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p>\n<p>导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。作者基于此，提出了组归一化（Group Normalization）的方式，且效果表明，显著优于BN、LN、IN等。</p>\n<h1 id=\"Normalization的不变性\"><a href=\"#Normalization的不变性\" class=\"headerlink\" title=\"Normalization的不变性\"></a>Normalization的不变性</h1><ul>\n<li><strong>权重伸缩不变性</strong>（weight scale invariance）指的是，当权重 $W$ 按照常量 $\\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：<br>$$Norm(W^{‘}x)=Norm(Wx)$$<br>其中，$W^{‘}=\\lambda W$。上述规范化方法均有这一性质，这是因为，当权重 $W$ 伸缩时，对应的均值和标准差均等比例伸缩，分子分母相抵：<br>$$Norm(W^{‘}x)=Norm(g\\cdot \\frac{W^{‘}x-\\mu^{‘}}{\\sigma^{‘}}+b)=Norm(g\\cdot \\frac{\\lambda Wx-\\lambda\\mu}{\\lambda \\sigma}+b)=Norm(g\\cdot \\frac{Wx-\\mu}{\\sigma}+b)=Norm(Wx)$$<br>权重伸缩不变性可以有效地提高反向传播的效率，由于：<br>$$\\frac{\\partial Norm(W^{‘}x)}{\\partial x}=\\cdot\\frac{\\partial Norm(Wx)}{\\partial x}$$<br>因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率。由于：<br>$$\\frac{\\partial Norm(W^{‘}x)}{\\partial W^{‘}}=\\frac{1}{\\lambda}\\cdot\\frac{\\partial Norm(Wx)}{\\partial W}$$<br>因此，下层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。</li>\n<li><strong>数据伸缩不变性</strong>：数据伸缩不变性（data scale invariance）指的是，当数据 $x$ 按照常量 $\\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：<br>$$Norm(Wx^{‘})=Norm(Wx)$$<br>其中，$x^{‘}=\\lambda x$。数据伸缩不变性仅对 BN、LN 和 CN 成立。因为这三者对输入数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母互相抵消。而 WN 不具有这一性质。数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。对于某一层神经元 $h_l=f_{W_l}(x_l)$ 而言，展开可得：<br>$$h_l=f_{W_l}(x_l)=f_{W_l}(f_{W_{l\\ 1}}(x_l\\ 1))=…=x_0 \\coprod_{k=0}^{l}W_k$$<br>每一层神经元的输出依赖于底下各层的计算结果。如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。加入 Normalization 之后，不论底层的数据如何变化，对于某一层神经元  $h_l=f_{W_l}(x_l)$ 而言，其输入 $x_l$ 永远保持标准的分布，这就使得高层的训练更加简单。从梯度的计算公式来看：<br>$$\\frac{\\partial Norm(Wx^{‘})}{\\partial W}=\\cdot\\frac{\\partial Norm(Wx)}{\\partial W}$$<br>数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1>选择什么样的归一化方式，取决于你关注数据的哪部分信息，如果某个维度信息的差异性很重要，需要被拟合，那就别在那个维度进行归一化。</li>\n</ul>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://theaisummer.com/normalization/\">In-layer normalization techniques for training very deep neural networks</a></li>\n<li><a href=\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\">归一化</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/166101119\">2020 年 BatchNorm 还能大水漫灌，吗</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/33173246\">详解深度学习中的Normalization，BN/LN/WN</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/56542480\">模型优化之Instance Normalization</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35005794\">全面解读Group Normalization-（吴育昕-何恺明 ）</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247491157&idx=1&sn=5ab87d7847666aac2cef8463a2f0f078&chksm=ec1ff3acdb687aba68bfeb32a79421cd350217cfd4804d98eab85fd7c8aa95923fbd6790788a&scene=21#wechat_redirect\">深度学习中的Normalization模型</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","归一化","机器学习","Normalization"]},{"title":"深度学习矩阵乘法的终极奥义einsum，结合多个计算框架上的使用","url":"/Deep-Learning/8d94140b2f7f/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>这里我们来好好探讨一下深度学习中，矩阵乘法的使用，其实主要是围绕einsum来进行探讨，即通过爱因斯坦求和约定来指导矩阵乘法，同时附带陈列其他矩阵乘法的API，方便进行直观感受。本文中的计算框架及版本如下：</p>\n<ul>\n<li>TensorFlow2.3</li>\n<li>PyTorch1.7.0</li>\n<li>Numpy1.19</li>\n</ul>\n<h1 id=\"爱因斯坦求和约定\"><a href=\"#爱因斯坦求和约定\" class=\"headerlink\" title=\"爱因斯坦求和约定\"></a>爱因斯坦求和约定</h1><p>我们讨论einsum绕不开爱因斯坦求和约定的，爱因斯坦求和约定（Einstein summation convention）是一种标记的约定，又称为爱因斯坦标记法（Einstein notation），在处理关于坐标的方程式时非常有用，用一句话来总结爱因斯坦求和约定，就是：</p>\n<blockquote>\n<p>当式子中任何一个角标出现了两次，并且一次是上标、一次是下标时，那么该式表示的实际上是对这个角标一切可能值的求和。换言之，如果角标 $i$ 作为上标和下标各出现了一次，那么式子相当于添加了一个关于 $i$ 的求和符号</p>\n</blockquote>\n<p>我们下面使用线性函数和矩阵运算来对爱因斯坦求和约定进行举例说明：</p>\n<ul>\n<li>线性函数：从张量中我们知道，一个1-线性函数可以表示为一个向量，这样的向量常被称为余向量、补向量或者1-形式。通常，我们用下标来表示一个余向量的各分量：$a=(a_1,a_2,a_3)$ ，而用上标来表示一个通常的几何向量：$v=(v^1,v^2,v^3)$。注意，上标不是乘方，则 $a$ 和 $v$ 的内积是：<br>$$\\sum_{i=1,2,3}^{}a_iv^i$$<br>用爱因斯坦求和约定， $a$ 和 $v$ 的内积就可以写为 $a_iv^i$</li>\n<li>矩阵运算：对于矩阵 $A$，我们把其第 $i$ 行第 $j$ 列的元素表示为 $A_j^i$。则矩阵乘法表示为：如果 $A=BC$，那么 $A_j^i=B_k^iC_j^k$。矩阵 $A$ 的迹为 $A_i^i$</li>\n</ul>\n<p>由于重复出现而实际上应该是求和的指标，被称为赝指标或者哑指标（dummy index），因为它们不是真正的指标，而是可以用任意字母代替的。没有求和的指标是固定的，是真正的指标．比如说 $B_k^iC_j^k$ 中 $k$ 可以是任何字母，但是 $i$ 和 $j$ 是不可以替换成别的字母的，因为它们由 $A_j^i$ 决定了。在这里，哑指标实际上是表示遍历全部可能的真指标。</p>\n<p>爱因斯坦求和约定的表示方法脱胎于矩阵乘法的要求，但是却不依赖于矩阵行和列的形式，转而关注指标间的配合，相比传统的矩阵表达，能更方便地推广到高阶张量的情形中。本文关于爱因斯坦求和约定的相关点到为止，如果感兴趣其公式，可以参考这一篇文章：<a href=\"https://zhuanlan.zhihu.com/p/46006162\">爱因斯坦求和约定</a>。</p>\n<h1 id=\"einsum介绍\"><a href=\"#einsum介绍\" class=\"headerlink\" title=\"einsum介绍\"></a>einsum介绍</h1><p>通过使用einsum函数，我们可以使用爱因斯坦求和约定（Einstein summation convention）在NumPy数组上指定操作。einsum函数由于其强大的表现力和智能循环，它在速度和内存效率方面通常可以超越我们常见的array函数。但缺点是，可能需要一段时间才能理解符号，有时需要尝试才能将其正确的应用于棘手的问题，当然熟悉之后得心应手才是使用关键。</p>\n<p>einsum以一种优雅的方式，表示各种矩阵运算，好处在于你不需要去记和使用计算框架中（TensorFlow|PyTorch|Numpy）点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法的函数名字和签名。从某种程度上解决引入不必要的张量变形或转置运算，以及可以省略的中间张量的现象。不仅如此，einsum有时可以编译到高性能代码，事实上，PyTorch最近引入的能够自动生成GPU代码并为特定输入尺寸自动调整代码的张量理解（Tensor Comprehensions）就基于类似einsum的领域特定语言。此外，可以使用opt einsum和tf einsum opt这样的项目优化einsum表达式的构造顺序。</p>\n<p>假设我们有两个多维数组A和B，现在让我们要进行如下操作：</p>\n<ul>\n<li>以特定方式将A与B相乘以创建新的乘积数组</li>\n<li>沿特定轴求和该新数组</li>\n<li>以特定顺序转置新数组的轴</li>\n</ul>\n<p>einsum帮助我们更快，更高效地执行此操作，当然，NumPy函数的组合（例如multiply，sum和transpose）也可以实现。</p>\n<h1 id=\"einsum应用\"><a href=\"#einsum应用\" class=\"headerlink\" title=\"einsum应用\"></a>einsum应用</h1><p>我们在这一节用Numpy的einsum进行讲解说明（Numpy中einsum先被开发出来，TensorFlow和PyTorch都在一定程度上参考了它），在下一节，我会将TensorFlow、PyTorch和Numpy的API都贴出来。</p>\n<p>在Numpy中，einsum使用格式字符串和任意数量的Numpy张量作为参数调用，并返回结果张量。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109145722330.png#pic_center\" alt=\"在这里插入图片描述\"><br>格式字符串包含分隔参数说明的逗号（，）和将参数说明与张量的参数分开的箭头（-&gt;）。参数说明中的数量和参数的数量必须匹配，并且必须精确地出现一个箭头，后跟一个结果说明</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109145901549.png#pic_center\" alt=\"在这里插入图片描述\"><br>格式字符串中的字符数完全等于该张量的维数。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109150049280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下面展示einsum()格式字符串的完整示例。 尝试猜测结果将是什么？注意了，0维张量（标量）对于参数和结果均有效，由空字符串“”表示。 再次提醒您，格式字符串必须仅由ASCII小写或大写字母组成。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201109150320881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们这里使用一个实际的例子来进行说明，首先我们要相乘的两个​​数组是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">A &#x3D; array([[1, 1, 1],</span><br><span class=\"line\">           [2, 2, 2],</span><br><span class=\"line\">           [5, 5, 5]])</span><br><span class=\"line\">B &#x3D; array([[0, 1, 0],</span><br><span class=\"line\">           [1, 1, 0],</span><br><span class=\"line\">           [1, 1, 1]])</span><br></pre></td></tr></table></figure>\n<p>我们的矩阵乘法np.einsum(‘ij,jk-&gt;ik’, A, B)大致如下：<br><img src=\"https://img-blog.csdnimg.cn/20201109154121134.png#pic_center\" alt=\"在这里插入图片描述\"><br>要了解输出数组的计算方法，请记住以下三个规则：</p>\n<ul>\n<li><p><strong>在输入数组中重复的字母意味着值沿这些轴相乘</strong>。乘积结果为输出数组的值。在本例中，我们使用字母 j 两次：A和B各一次。这意味着我们将A每一行与B每列相乘。这只在标记为 j 的轴在两个数组中的长度相同（或者任一数组长度为1）时才有效。</p>\n</li>\n<li><p><strong>输出中省略的字母意味着沿该轴的值将相加</strong>。在这里，j 不包含在输出数组的标签中。通过累加的方式将它从轴上除去，最终数组中的维数减少1。如果输出是’ijk’，我们得到的结果是3x3x3数组（如果我们不提供输出标签，只写箭头，则对整个数组求和）。</p>\n</li>\n<li><p><strong>我们可以按照我们喜欢的任何顺序返回未没进行累加的轴</strong>。如果我们省略箭头’-&gt;’，NumPy会将只出现一次的标签按照字母顺序排列（因此实际上’ij,jk-&gt;ik’相当于’ij,jk’）。如果我们想控制输出的样子，我们可以自己选择输出标签的顺序。例如，’ij,jk-&gt;ki’为矩阵乘法的转置。</p>\n</li>\n</ul>\n<p>现在，我们已经知道矩阵乘法是如何工作的。下图显示了如果我们不对 j 轴进行求和，而是通过写<code>np.einsum(‘ij,jk-&gt;ijk’, A, B)</code>将其包含在输出中，我们会得到什么。右边代表 j 轴已经求和：<br><img src=\"https://img-blog.csdnimg.cn/20201109154140281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>注意，由于<code>np.einsum(‘ij,jk-&gt;ik’, A, B)</code>函数不构造3维数组然后求和，它只是将总和累加到2维数组中。下面是两个表格展示了einsum如何进行各种NumPy操作。我们可以用它来熟悉符号。</p>\n<ul>\n<li>让A和B是两个形状兼容的一维数组（也就是说，我们相应的轴的长度要么相等，要么其中一个长度为1）：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>调用</th>\n<th>Numpy等式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>(‘i’, A)</td>\n<td>A</td>\n<td>返回数组A的视图</td>\n</tr>\n<tr>\n<td>(‘i-&gt;’, A)</td>\n<td>sum(A)</td>\n<td>数组A值的总和</td>\n</tr>\n<tr>\n<td>(‘i,i-&gt;i’, A, B)</td>\n<td>A * B</td>\n<td>A和B的数组元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘i,i’, A, B)</td>\n<td>inner(A, B)</td>\n<td>A和B的点积（内积）</td>\n</tr>\n<tr>\n<td>(‘i,j-&gt;ij’, A, B)</td>\n<td>outer(A, B)</td>\n<td>A和B的外积（叉积）</td>\n</tr>\n</tbody></table>\n<ul>\n<li>现在，我们A和B是与之兼容形状的两个二维数组：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>调用</th>\n<th>Numpy等式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>(‘ij’, A)</td>\n<td>A</td>\n<td>返回A的视图</td>\n</tr>\n<tr>\n<td>(‘ji’, A)</td>\n<td>A.T</td>\n<td>A的转置</td>\n</tr>\n<tr>\n<td>(‘ii-&gt;i’, A)</td>\n<td>diag(A)</td>\n<td>A的主对角线</td>\n</tr>\n<tr>\n<td>(‘ii’, A)</td>\n<td>trace(A)</td>\n<td>A的主对角线的和</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;’, A)</td>\n<td>sum(A)</td>\n<td>A的值相加</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;j’, A)</td>\n<td>sum(A, axis=0)</td>\n<td>通过A的轴竖直（列）求和</td>\n</tr>\n<tr>\n<td>(‘ij-&gt;i’, A)</td>\n<td>sum(A, axis=1)</td>\n<td>通过A的轴水平（行）求和</td>\n</tr>\n<tr>\n<td>(‘ij,ij-&gt;ij’, A, B)</td>\n<td>A * B</td>\n<td>A和B逐元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘ij,ji-&gt;ij’, A, B)</td>\n<td>A * B.T</td>\n<td>A和B的转置逐个元素依次相乘</td>\n</tr>\n<tr>\n<td>(‘ij,jk’, A, B)</td>\n<td>dot(A, B)</td>\n<td>A和B的矩阵乘法</td>\n</tr>\n<tr>\n<td>(‘ij,kj-&gt;ik’, A, B)</td>\n<td>inner(A, B)</td>\n<td>A和B点积（内积）</td>\n</tr>\n<tr>\n<td>(‘ij,kj-&gt;ijk’, A, B)</td>\n<td>A[:, None] * B</td>\n<td>A的每一行乘以B</td>\n</tr>\n<tr>\n<td>(‘ij,kl-&gt;ijkl’, A, B)</td>\n<td>A[:, :, None, None] * B</td>\n<td>A的每个值乘以B</td>\n</tr>\n</tbody></table>\n<p>当处理大量维度时，别忘了einsum允许使用省略号语法’…’。这提供了一种变量的方式标记我们不大感兴趣的轴，例如<code>np.einsum(‘…ij,ji-&gt;…’, a, b)</code>，仅将a的最后两个轴与2维数组b相乘。</p>\n<h1 id=\"TensorFlow、PyTorch和Numpy\"><a href=\"#TensorFlow、PyTorch和Numpy\" class=\"headerlink\" title=\"TensorFlow、PyTorch和Numpy\"></a>TensorFlow、PyTorch和Numpy</h1><p>通常，要将元素式方程式转换为方程式字符串，可以使用以下过程（右侧是矩阵乘法示例的中间字符串）</p>\n<ol>\n<li>原始元素式方程式：C[i,k] = sum_j A[i,j] * B[j,k]</li>\n<li>删除变量名称，方括号和逗号：ik = sum_j ij * jk</li>\n<li>将“*”替换成“,”：ik = ij, jk</li>\n<li>去掉求和符号：ik = ij, jk</li>\n<li>输出移到右边，将将“=”替换成“-&gt;”：ij,jk-&gt;ik</li>\n</ol>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/einsum?hl=en\">TensorFlow2.3：tf.einsum()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.einsum(equation, *inputs, **kwargs)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tequation：描述计算的字符串，格式与numpy.einsum相同。</span><br><span class=\"line\">\t*inputs：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">\t**kwargs：</span><br><span class=\"line\">\t\toptimize：用于使用opt_einsum查找计算路径的优化策略，可选项包括 &#39;greedy&#39;, &#39;optimal&#39;, &#39;branch-2&#39;, &#39;branch-all&#39; ,&#39;auto&#39;，默认是&#39;greedy&#39;</span><br><span class=\"line\">\t\tname：操作名称（可选）</span><br><span class=\"line\">返回值：张量，形状与方程中一致</span><br><span class=\"line\">示例：</span><br><span class=\"line\">\t# 矩阵相乘</span><br><span class=\"line\">\teinsum(&#39;ij,jk-&gt;ik&#39;, m0, m1)  # output[i,k] &#x3D; sum_j m0[i,j] * m1[j, k]</span><br><span class=\"line\">\t# 点积</span><br><span class=\"line\">\teinsum(&#39;i,i-&gt;&#39;, u, v)  # output &#x3D; sum_i u[i]*v[i]</span><br><span class=\"line\">\t# 外积</span><br><span class=\"line\">\teinsum(&#39;i,j-&gt;ij&#39;, u, v)  # output[i,j] &#x3D; u[i]*v[j]</span><br><span class=\"line\">\t# 转置</span><br><span class=\"line\">\teinsum(&#39;ij-&gt;ji&#39;, m)  # output[j,i] &#x3D; m[i,j]</span><br><span class=\"line\">\t# 主对角线的和</span><br><span class=\"line\">\teinsum(&#39;ii&#39;, m)  # output[j,i] &#x3D; trace(m) &#x3D; sum_i m[i, i]</span><br><span class=\"line\">\t# 批量矩阵相乘</span><br><span class=\"line\">\teinsum(&#39;aij,ajk-&gt;aik&#39;, s, t)  # out[a,i,k] &#x3D; sum_j s[a,i,j] * t[a, j, k]</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.einsum.html?highlight=einsum#torch.einsum\">PyTorch1.7.0：torch.einsum()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.einsum(equation, *operands)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tequation：描述计算的字符串，格式与numpy.einsum相同。</span><br><span class=\"line\">\toperands：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">示例：</span><br><span class=\"line\">torch.einsum(&#39;i,j-&gt;ij&#39;, x, y)  # 外积</span><br><span class=\"line\">torch.einsum(&#39;bn,anm,bm-&gt;ba&#39;, l, A, r) # 计算torch.nn.functional.bilinear</span><br><span class=\"line\">torch.einsum(&#39;bij,bjk-&gt;bik&#39;, As, Bs) # 批量矩阵相乘</span><br><span class=\"line\">torch.einsum(&#39;ii-&gt;i&#39;, A) # 对角线之和</span><br><span class=\"line\">torch.einsum(&#39;...ii-&gt;...i&#39;, A) # 批量对角线之和</span><br><span class=\"line\">torch.einsum(&#39;...ij-&gt;...ji&#39;, A).shape # 批量转置</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://numpy.org/doc/1.19/reference/generated/numpy.einsum.html?highlight=einsum#numpy.einsum\">Numpy1.19：numpy.einsum</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">numpy.einsum(subscripts, *operands, out&#x3D;None, dtype&#x3D;None, order&#x3D;&#39;K&#39;, casting&#x3D;&#39;safe&#39;, optimize&#x3D;False)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tsubscripts：描述计算的字符串，除非包含显式指示符“-&gt;”以及精确输出形式的下标标签，否则将执行隐式（经典的爱因斯坦求和）计算</span><br><span class=\"line\">\toperands：输入（张量），其形状应与方程保持一致。</span><br><span class=\"line\">\tout：ndarray类型（可选），如果提供，则将计算结果放入此数组中</span><br><span class=\"line\">\tdtype：&#123;data-type, None&#125;（可选），如果提供，则强制使用指定的数据类型计算。 请注意，你可能还必须提供一个更宽松的转换参数以允许进行转换。 默认为None。</span><br><span class=\"line\">\torder：&#123;‘C’, ‘F’, ‘A’, ‘K’&#125;（可选），控制输出的内存布局。其中，‘C’表示C连续的。‘F’表示它应该是Fortran连续的。如果输入全为‘F’，则‘A’表示应为‘F’，否则为‘C’。‘K’表示它应尽可能与输入尽可能靠近布局，包括任意排列的轴。</span><br><span class=\"line\">\tcasting：&#123;‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’&#125;（可选），控制可能发生的数据类型转换。 不建议将其设置为“unsafe”，因为这会积聚不利影响。其中，“no”表示完全不应该转换数据类型，“equiv”表示仅允许按照字节顺序转换，“safe”表示只允许保留值的强制类型转换。“same_kind”表示仅允许安全类型转换或同一类型（例如float64到float32）内的类型转换。“unsafe”表示可能会进行任何数据转换。</span><br><span class=\"line\">\toptimize：&#123;False, True, ‘greedy’, ‘optimal’&#125;（可选），控制优化策略，如果True将默认设置为“贪心”算法，如果是False则不会进行优化。 还接受np.einsum_path函数的显式收缩列表。</span><br></pre></td></tr></table></figure>\n<h1 id=\"其他乘法\"><a href=\"#其他乘法\" class=\"headerlink\" title=\"其他乘法\"></a>其他乘法</h1><p><strong>TensorFlow2.3</strong></p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/linalg/matmul?hl=en\">tf.linalg.matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.linalg.matmul(a, b, transpose_a&#x3D;False, transpose_b&#x3D;False, adjoint_a&#x3D;False, adjoint_b&#x3D;False,a_is_sparse&#x3D;False, b_is_sparse&#x3D;False, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：类型为float16, float32, float64, int32, complex64, complex128，并且秩大于1</span><br><span class=\"line\">\tb：和a相同类型和秩</span><br><span class=\"line\">\ttranspose_a：如果为True，a在计算前会被转置</span><br><span class=\"line\">\ttranspose_b：如果为True，b在计算前会被转置</span><br><span class=\"line\">\tadjoint_a：如果为True，a在计算前会被共轭和转置</span><br><span class=\"line\">\tadjoint_b：如果为True，b在计算前会被共轭和转置</span><br><span class=\"line\">\ta_is_sparse：如果为True，则将a视为稀疏矩阵。</span><br><span class=\"line\">\tb_is_sparse：如果为True，则将b视为稀疏矩阵。</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul?hl=en\">tf.sparse.sparse_dense_matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.sparse.sparse_dense_matmul(sp_a, b, adjoint_a&#x3D;False, adjoint_b&#x3D;False, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：SparseTensor或者是密度矩阵，并且秩为2</span><br><span class=\"line\">\tb：和a相同类型，密度矩阵或者是a：SparseTensor</span><br><span class=\"line\">\tadjoint_a：如果为True，a在计算前会被共轭和转置</span><br><span class=\"line\">\tadjoint_b：如果为True，b在计算前会被共轭和转置</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/math/multiply?hl=en\">tf.math.multiply</a><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">tf.math.multiply(x, y, name&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\ta：类型为bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128的张量</span><br><span class=\"line\">\tb：和a相同类型</span><br><span class=\"line\">\tname：操作名称（可选）</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><strong>Pytorch1.7.0</strong></p>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul\">torch.matmul</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.matmul(input, other, *, out&#x3D;None)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tinput：第一个用于乘法的张量</span><br><span class=\"line\">\tother：第二个用于乘法的张量</span><br><span class=\"line\">\tout：可选，输出张量</span><br></pre></td></tr></table></figure>\n<p><strong>Numpy1.19</strong></p>\n<ul>\n<li><a href=\"https://numpy.org/doc/1.19/reference/generated/numpy.matmul.html?highlight=matmul#numpy.matmul\">numpy.matmul()</a></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">numpy.matmul(x1, x2, &#x2F;, out&#x3D;None, *, casting&#x3D;&#39;same_kind&#39;, order&#x3D;&#39;K&#39;, dtype&#x3D;None, subok&#x3D;True[, signature, extobj])</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tx1, x2：输入数组</span><br><span class=\"line\">\tout：输出储存的位置，如果提供，则其形状必须与签名(n,k),(k,m)-&gt;(n,m)相匹配。 如果未提供或没有，则返回一个新分配的数组。</span><br><span class=\"line\">\t**kwargs：对于其他仅关键字参数，参考文档</span><br></pre></td></tr></table></figure>\n<p>参考文献</p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf?hl=en\">TensorFlow2.3</a></li>\n<li><a href=\"https://pytorch.org/docs/stable/index.html\">Pytorch1.7.0</a></li>\n<li><a href=\"https://numpy.org/doc/1.19/reference/index.html\">Numpy1.19</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/44954540\">einsum满足你一切需要：深度学习中的爱因斯坦求和约定</a></li>\n<li><a href=\"http://www.atyun.com/32288.html\">NumPy中einsum的基本介绍</a></li>\n<li><a href=\"https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/\">Einstein Summation in Numpy</a></li>\n</ul>\n","categories":["Deep-Learning"],"tags":["深度学习","TensorFlow","Pytorch","einsum"]},{"title":"看完这一篇，你就对Spring-Security略窥门径了","url":"/Spring-Boot/a86e53449298/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>开发Web应用，对页面的安全控制通常是必须的。比如：对于没有访问权限的用户需要转到登录表单页面。要实现访问控制的方法多种多样，可以通过Aop、拦截器实现，也可以通过框架实现，例如：Apache Shiro、Spring Security。我们这里要讲的Spring Security 就是一个Spring生态中关于安全方面的框架。它能够为基于Spring的企业应用系统提供声明式的安全访问控制解决方案。</p>\n<h2 id=\"默认认证用户名密码\"><a href=\"#默认认证用户名密码\" class=\"headerlink\" title=\"默认认证用户名密码\"></a>默认认证用户名密码</h2><p>项目pom.xml添加spring-boot-starter-security依赖</p>\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>重启你的应用。再次打开页面，你讲看到一个登录页面</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200308205647401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>既然跳到了登录页面，那么这个时候我们就会想，这个登录的用户名以及密码是什么呢？让我们来从SpringBoot源码寻找一下。你搜一下输出日志，会看到下面一段输出：<br><img src=\"https://img-blog.csdnimg.cn/20200308205717917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这段日志是UserDetailsServiceAutoConfiguration类里面的如下方法输出的：<br><img src=\"https://img-blog.csdnimg.cn/20200318115748850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>通过上面的这个类，我们可以看出，是SecurityProperties这个Bean管理了用户名和密码。在SecurityProperties里面的一个内部静态类User类里面，管理了默认的认证的用户名与密码。代码如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@ConfigurationProperties(</span></span><br><span class=\"line\"><span class=\"meta\">    prefix = &quot;spring.security&quot;</span></span><br><span class=\"line\"><span class=\"meta\">)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityProperties</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BASIC_AUTH_ORDER = <span class=\"number\">2147483642</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> IGNORED_ORDER = -<span class=\"number\">2147483648</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> DEFAULT_FILTER_ORDER = -<span class=\"number\">100</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SecurityProperties.Filter filter = <span class=\"keyword\">new</span> SecurityProperties.Filter();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SecurityProperties.User user = <span class=\"keyword\">new</span> SecurityProperties.User();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SecurityProperties</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SecurityProperties.<span class=\"function\">User <span class=\"title\">getUser</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.user;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SecurityProperties.<span class=\"function\">Filter <span class=\"title\">getFilter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.filter;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> String name = <span class=\"string\">&quot;user&quot;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> String password = UUID.randomUUID().toString();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> List&lt;String&gt; roles = <span class=\"keyword\">new</span> ArrayList();</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> passwordGenerated = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">User</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getName</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.name;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setName</span><span class=\"params\">(String name)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.name = name;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getPassword</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.password;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPassword</span><span class=\"params\">(String password)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.hasLength(password)) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.passwordGenerated = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.password = password;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> List&lt;String&gt; <span class=\"title\">getRoles</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.roles;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRoles</span><span class=\"params\">(List&lt;String&gt; roles)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.roles = <span class=\"keyword\">new</span> ArrayList(roles);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isPasswordGenerated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.passwordGenerated;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Filter</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> order = -<span class=\"number\">100</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> Set&lt;DispatcherType&gt; dispatcherTypes;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Filter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.dispatcherTypes = <span class=\"keyword\">new</span> HashSet(Arrays.asList(DispatcherType.ASYNC, DispatcherType.ERROR, DispatcherType.REQUEST));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getOrder</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.order;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setOrder</span><span class=\"params\">(<span class=\"keyword\">int</span> order)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.order = order;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Set&lt;DispatcherType&gt; <span class=\"title\">getDispatcherTypes</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.dispatcherTypes;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDispatcherTypes</span><span class=\"params\">(Set&lt;DispatcherType&gt; dispatcherTypes)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.dispatcherTypes = dispatcherTypes;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>综上所述，security默认的用户名是user, 默认密码是应用启动的时候，通过UUID算法随机生成的，默认的role是”USER”。当然，如果我们想简单改一下这个用户名密码，可以在<code>application.properties</code>配置你的用户名密码，例如<br><img src=\"https://img-blog.csdnimg.cn/20200318120347187.png#pic_center\" alt=\"在这里插入图片描述\"><br>当然这只是一个初级的配置，更复杂的配置，可以分不用角色，在控制范围上，能够拦截到方法级别的权限控制。</p>\n<h2 id=\"内存用户名密码认证\"><a href=\"#内存用户名密码认证\" class=\"headerlink\" title=\"内存用户名密码认证\"></a>内存用户名密码认证</h2><p>在上面的内容，我们什么都没做，就添加了spring-boot-starter-security依赖，整个应用就有了默认的认证安全机制。下面，我们来定制用户名密码。写一个继承了 WebSecurityConfigurerAdapter的配置类，具体内容如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.builders.HttpSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableWebSecurity</span></span><br><span class=\"line\"><span class=\"meta\">@EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebSecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.configure(http);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        auth.inMemoryAuthentication()</span><br><span class=\"line\">                .passwordEncoder(<span class=\"keyword\">new</span> BCryptPasswordEncoder())</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;1234567&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;USER&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>这里对上面的代码进行简要说明</strong>：</p>\n<ul>\n<li><p>Spring security 5.0中新增了多种加密方式，也改变了默认的密码格式。需要修改一下configure中的代码，我们要将前端传过来的密码进行某种方式加密，Spring Security 官方推荐的是使用<code>bcrypt</code>加密方式。<code>inMemoryAuthentication().passwordEncoder(new BCryptPasswordEncoder())</code>，这相当于登陆时用BCrypt加密方式对用户密码进行处理。以前的”<code>.password(&quot;123&quot;)</code>“ 变成了 “<code>.password(new BCryptPasswordEncoder().encode(&quot;123&quot;))</code>“，这相当于对内存中的密码进行Bcrypt编码加密。如果比对时一致，说明密码正确，才允许登陆。</p>\n</li>\n<li><p>通过 <code>@EnableWebSecurity</code>注解开启Spring Security的功能。使用<code>@EnableGlobalMethodSecurity(prePostEnabled = true)</code>这个注解，可以开启security的注解，我们可以在需要控制权限的方法上面使用<code>@PreAuthorize</code>，<code>@PreFilter</code>这些注解。</p>\n</li>\n<li><p>继承 WebSecurityConfigurerAdapter 类，并重写它的方法来设置一些web安全的细节。我们结合<code>@EnableWebSecurity</code>注解和继承WebSecurityConfigurerAdapter，来给我们的系统加上基于web的安全机制。</p>\n</li>\n<li><p>在<code>configure(HttpSecurity http)</code>方法里面，我们进入到源码中，就会看到默认的认证代码是：</p>\n</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200318121502574.png\" alt=\"在这里插入图片描述\"><br>从方法名我们基本可以看懂这些方法的功能。上面的那个默认的登录页面，就是SpringBoot默认的用户名密码认证的login页面。我们使用SpringBoot默认的配置<code>super.configure(http)</code>，它通过 <code>authorizeRequests()</code> 定义哪些URL需要被保护、哪些不需要被保护。默认配置是所有访问页面都需要认证，才可以访问。</p>\n<ul>\n<li><p>通过 <code>formLogin()</code> 定义当需要用户登录时候，转到的登录页面。</p>\n</li>\n<li><p><code>configureGlobal(AuthenticationManagerBuilder auth)</code> 方法，在内存中创建了一个用户，该用户的名称为root，密码为root，用户角色为USER。这个默认的登录页面是怎么冒出来的呢？是的，SpringBoot内置的，SpringBoot甚至给我们做好了一个极简的登录页面。这个登录页面是通过Filter实现的。具体的实现类是<code>org.springframework.security.web.authentication.ui.DefaultLoginPageGeneratingFilter</code>。同时，这个DefaultLoginPageGeneratingFilter也是SpringBoot的默认内置的Filter。</p>\n</li>\n</ul>\n<p>输入用户名，密码，点击Login。不过，我们发现，SpringBoot应用的启动日志还是打印了如下一段：<br><img src=\"https://img-blog.csdnimg.cn/20200318122300838.png\" alt=\"在这里插入图片描述\"><br>但实际上，已经使用了我们定制的用户名密码了。如果我们要配置多个用户，多个角色，可参考使用如下示例的代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        auth.inMemoryAuthentication()</span><br><span class=\"line\">                .passwordEncoder(<span class=\"keyword\">new</span> BCryptPasswordEncoder())</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;1234567&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;USER&quot;</span>)</span><br><span class=\"line\">                .and()</span><br><span class=\"line\">                .withUser(<span class=\"string\">&quot;admin1&quot;</span>)</span><br><span class=\"line\">                .password(<span class=\"keyword\">new</span> BCryptPasswordEncoder().encode(<span class=\"string\">&quot;123&quot;</span>))</span><br><span class=\"line\">                .roles(<span class=\"string\">&quot;ADMIN&quot;</span>, <span class=\"string\">&quot;USER&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"角色权限控制\"><a href=\"#角色权限控制\" class=\"headerlink\" title=\"角色权限控制\"></a>角色权限控制</h2><p>当我们的系统功能模块当需求发展到一定程度时，会不同的用户，不同角色使用我们的系统。这样就要求我们的系统可以做到，能够对不同的系统功能模块，开放给对应的拥有其访问权限的用户使用。Spring Security提供了Spring EL表达式，允许我们在定义URL路径访问(@RequestMapping)的方法上面添加注解，来控制访问权限。在标注访问权限时，根据对应的表达式返回结果，控制访问权限：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">true</span>，表示有权限</span><br><span class=\"line\">fasle，表示无权限</span><br></pre></td></tr></table></figure>\n<p>Spring Security可用表达式对象的基类是SecurityExpressionRoot。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecurityExpressionRoot</span> <span class=\"keyword\">implements</span> <span class=\"title\">SecurityExpressionOperations</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> Authentication authentication;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> AuthenticationTrustResolver trustResolver;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> RoleHierarchy roleHierarchy;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Set&lt;String&gt; roles;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String defaultRolePrefix = <span class=\"string\">&quot;ROLE_&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> permitAll = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> denyAll = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> PermissionEvaluator permissionEvaluator;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String read = <span class=\"string\">&quot;read&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String write = <span class=\"string\">&quot;write&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String create = <span class=\"string\">&quot;create&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String delete = <span class=\"string\">&quot;delete&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">final</span> String admin = <span class=\"string\">&quot;administration&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SecurityExpressionRoot</span><span class=\"params\">(Authentication authentication)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (authentication == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">&quot;Authentication object cannot be null&quot;</span>);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.authentication = authentication;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAuthority</span><span class=\"params\">(String authority)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthority(authority);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyAuthority</span><span class=\"params\">(String... authorities)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthorityName((String)<span class=\"keyword\">null</span>, authorities);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasRole</span><span class=\"params\">(String role)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyRole(role);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyRole</span><span class=\"params\">(String... roles)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.hasAnyAuthorityName(<span class=\"keyword\">this</span>.defaultRolePrefix, roles);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasAnyAuthorityName</span><span class=\"params\">(String prefix, String... roles)</span> </span>&#123;</span><br><span class=\"line\">        Set&lt;String&gt; roleSet = <span class=\"keyword\">this</span>.getAuthoritySet();</span><br><span class=\"line\">        String[] var4 = roles;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> var5 = roles.length;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> var6 = <span class=\"number\">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class=\"line\">            String role = var4[var6];</span><br><span class=\"line\">            String defaultedRole = getRoleWithDefaultPrefix(prefix, role);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (roleSet.contains(defaultedRole)) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Authentication <span class=\"title\">getAuthentication</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.authentication;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">permitAll</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">denyAll</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isAnonymous</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.trustResolver.isAnonymous(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isAuthenticated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> !<span class=\"keyword\">this</span>.isAnonymous();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isRememberMe</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.trustResolver.isRememberMe(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isFullyAuthenticated</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> !<span class=\"keyword\">this</span>.trustResolver.isAnonymous(<span class=\"keyword\">this</span>.authentication) &amp;&amp; !<span class=\"keyword\">this</span>.trustResolver.isRememberMe(<span class=\"keyword\">this</span>.authentication);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Object <span class=\"title\">getPrincipal</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.authentication.getPrincipal();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setTrustResolver</span><span class=\"params\">(AuthenticationTrustResolver trustResolver)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.trustResolver = trustResolver;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setRoleHierarchy</span><span class=\"params\">(RoleHierarchy roleHierarchy)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.roleHierarchy = roleHierarchy;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDefaultRolePrefix</span><span class=\"params\">(String defaultRolePrefix)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.defaultRolePrefix = defaultRolePrefix;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> Set&lt;String&gt; <span class=\"title\">getAuthoritySet</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.roles == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            Collection&lt;? extends GrantedAuthority&gt; userAuthorities = <span class=\"keyword\">this</span>.authentication.getAuthorities();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.roleHierarchy != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                userAuthorities = <span class=\"keyword\">this</span>.roleHierarchy.getReachableGrantedAuthorities(userAuthorities);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">this</span>.roles = AuthorityUtils.authorityListToSet(userAuthorities);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.roles;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasPermission</span><span class=\"params\">(Object target, Object permission)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.permissionEvaluator.hasPermission(<span class=\"keyword\">this</span>.authentication, target, permission);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hasPermission</span><span class=\"params\">(Object targetId, String targetType, Object permission)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.permissionEvaluator.hasPermission(<span class=\"keyword\">this</span>.authentication, (Serializable)targetId, targetType, permission);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setPermissionEvaluator</span><span class=\"params\">(PermissionEvaluator permissionEvaluator)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.permissionEvaluator = permissionEvaluator;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String <span class=\"title\">getRoleWithDefaultPrefix</span><span class=\"params\">(String defaultRolePrefix, String role)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (role == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (defaultRolePrefix != <span class=\"keyword\">null</span> &amp;&amp; defaultRolePrefix.length() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role.startsWith(defaultRolePrefix) ? role : defaultRolePrefix + role;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> role;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过阅读源码，我们可以更加深刻的理解其EL写法，并在写代码的时候正确的使用。变量defaultRolePrefix硬编码约定了role的前缀是”ROLE_”。同时，我们可以看出hasRole跟hasAnyRole是一样的。hasAnyRole是调用的<code>hasAnyAuthorityName(defaultRolePrefix, roles)</code>。所以，我们在学习一个框架或者一门技术的时候，最准确的就是源码。通过源码，我们可以更好更深入的理解技术的本质。</p>\n<p>SecurityExpressionRoot为我们提供的使用Spring EL表达式总结如下：<br>| 表达式 | 描述 |<br>|–|–|<br>|hasRole([role])    |当前用户是否拥有指定角色。|<br>|hasAnyRole([role1,role2])    |多个角色是一个以逗号进行分隔的字符串。如果当前用户拥有指定角色中的任意一个则返回true。|<br>|hasAuthority([auth])    |等同于hasRole|<br>|hasAnyAuthority([auth1,auth2])    |等同于hasAnyRole|<br>|Principle    |代表当前用户的principle对象|<br>|authentication    |直接从SecurityContext获取的当前Authentication对象|<br>|permitAll    |总是返回true，表示允许所有的|<br>|denyAll|    总是返回false，表示拒绝所有的|<br>|isAnonymous()    |当前用户是否是一个匿名用户|<br>|isRememberMe()|    表示当前用户是否是通过Remember-Me自动登录的|<br>|isAuthenticated()    |表示当前用户是否已经登录认证成功了。|<br>|isFullyAuthenticated()    |如果当前用户既不是一个匿名用户，同时又不是通过Remember-Me自动登录的，则返回true。|</p>\n<p>在Controller方法上添加@PreAuthorize这个注解，<code>value=&quot;hasRole(&#39;ADMIN&#39;)&quot;)</code>是Spring-EL expression，当表达式值为true，标识这个方法可以被调用。如果表达式值是false，标识此方法无权限访问。</p>\n<h2 id=\"在Spring-Security里面获取当前登录认证通过的用户信息\"><a href=\"#在Spring-Security里面获取当前登录认证通过的用户信息\" class=\"headerlink\" title=\"在Spring Security里面获取当前登录认证通过的用户信息\"></a>在Spring Security里面获取当前登录认证通过的用户信息</h2><p>如果我们想要在前端页面显示当前登录的用户怎么办呢？在在Spring Security里面怎样获取当前登录认证通过的用户信息？下面我们就来探讨这个问题。其实很好办。我们添加一个LoginFilter，默认拦截所有请求，把当前登录的用户放到系统session中即可。在Spring Security中，用户信息保存在SecurityContextHolder中。Spring Security使用一个Authentication对象来持有所有系统的安全认证相关的信息。这个信息的内容格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;accountNonExpired&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;accountNonLocked&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;authorities&quot;</span>:[&#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;authority&quot;</span>:<span class=\"string\">&quot;ROLE_ADMIN&quot;</span></span><br><span class=\"line\">    &#125;,&#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;authority&quot;</span>:<span class=\"string\">&quot;ROLE_USER&quot;</span></span><br><span class=\"line\">    &#125;],</span><br><span class=\"line\">    <span class=\"string\">&quot;credentialsNonExpired&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;enabled&quot;</span>:<span class=\"keyword\">true</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;username&quot;</span>:<span class=\"string\">&quot;root&quot;</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个Authentication对象信息其实就是User实体的信息，类似如下(当然，密码没放进来)。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">User</span> <span class=\"keyword\">implements</span> <span class=\"title\">UserDetails</span>, <span class=\"title\">CredentialsContainer</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String password;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> String username;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Set&lt;GrantedAuthority&gt; authorities;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> accountNonExpired;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> accountNonLocked;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> credentialsNonExpired;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> enabled;</span><br><span class=\"line\">        ....</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们可以使用下面的代码（Java）获得当前身份验证的用户的名称:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (principal <span class=\"keyword\">instanceof</span> UserDetails) &#123;</span><br><span class=\"line\">    String username = ((UserDetails)principal).getUsername();</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    String username = principal.toString();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过调用getContext()返回的对象是SecurityContext的实例对象，该实例对象保存在ThreadLocal线程本地存储中。使用Spring Security框架，通常的认证机制都是返回UserDetails实例，通过如上这种方式，我们就可以拿到认证登录的用户信息。</p>\n<h2 id=\"用数据库存储用户和角色，实现安全认证\"><a href=\"#用数据库存储用户和角色，实现安全认证\" class=\"headerlink\" title=\"用数据库存储用户和角色，实现安全认证\"></a>用数据库存储用户和角色，实现安全认证</h2><p>很多时候，我们需要的是实现一个用数据库存储用户和角色，实现系统的安全认证。为了简化讲解，本例中在权限角色上，我们简单设计两个用户角色：USER，ADMIN。我们设计页面的权限如下：</p>\n<ul>\n<li>首页/ : 所有人可访问</li>\n<li>登录页 /login: 所有人可访问</li>\n<li>普通用户权限页 /httpapi, /httpsuite: 登录后的用户都可访问</li>\n<li>管理员权限页 /httpreport ： 仅管理员可访问</li>\n<li>无权限提醒页： 当一个用户访问了其没有权限的页面，我们使用全局统一的异常处理页面提示。</li>\n</ul>\n<h3 id=\"配置Spring-Security\"><a href=\"#配置Spring-Security\" class=\"headerlink\" title=\"配置Spring Security\"></a>配置Spring Security</h3><p>我们首先使用Spring Security帮我们做登录、登出的处理，以及当用户未登录时只能访问: <a href=\"http://localhost:8080/\">http://localhost:8080/</a> 以及 <a href=\"http://localhost:8080/login\">http://localhost:8080/login</a> 两个页面。同样的，我们要写一个继承WebSecurityConfigurerAdapter的配置类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.springboot.in.action.service.LightSwordUserDetailService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Bean;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.context.annotation.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.builders.HttpSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.security.core.userdetails.UserDetailsService;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Created by jack on 2017/4/27.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"meta\">@EnableWebSecurity</span></span><br><span class=\"line\"><span class=\"meta\">@EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)</span></span><br><span class=\"line\"><span class=\"comment\">//使用@EnableGlobalMethodSecurity(prePostEnabled = true)</span></span><br><span class=\"line\"><span class=\"comment\">// 这个注解，可以开启security的注解，我们可以在需要控制权限的方法上面使用@PreAuthorize，@PreFilter这些注解。</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WebSecurityConfig</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> UserDetailsService <span class=\"title\">userDetailsService</span><span class=\"params\">()</span> </span>&#123; <span class=\"comment\">//覆盖写userDetailsService方法 (1)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> AdminUserDetailService();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * If subclassed this will potentially override subclass configure(HttpSecurity)</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> http</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(HttpSecurity http)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//super.configure(http);</span></span><br><span class=\"line\">        http.csrf().disable();</span><br><span class=\"line\"></span><br><span class=\"line\">        http.authorizeRequests()</span><br><span class=\"line\">            .antMatchers(<span class=\"string\">&quot;/&quot;</span>).permitAll()</span><br><span class=\"line\">            .antMatchers(<span class=\"string\">&quot;/amchart/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/bootstrap/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/build/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/css/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/dist/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/documentation/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/fonts/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/js/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/pages/**&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;/plugins/**&quot;</span></span><br><span class=\"line\">            ).permitAll() <span class=\"comment\">//默认不拦截静态资源的url pattern （2）</span></span><br><span class=\"line\">            .anyRequest().authenticated().and()</span><br><span class=\"line\">            .formLogin().loginPage(<span class=\"string\">&quot;/login&quot;</span>)<span class=\"comment\">// 登录url请求路径 (3)</span></span><br><span class=\"line\">            .defaultSuccessUrl(<span class=\"string\">&quot;/httpapi&quot;</span>).permitAll().and() <span class=\"comment\">// 登录成功跳转路径url(4)</span></span><br><span class=\"line\">            .logout().permitAll();</span><br><span class=\"line\"></span><br><span class=\"line\">        http.logout().logoutSuccessUrl(<span class=\"string\">&quot;/&quot;</span>); <span class=\"comment\">// 退出默认跳转页面 (5)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(AuthenticationManagerBuilder auth)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//AuthenticationManager使用我们的 Service来获取用户信息，Service可以自己写，其实就是简单的读取数据库的操作</span></span><br><span class=\"line\">        auth.userDetailsService(()); <span class=\"comment\">// （6）</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码只做了基本的配置，其中：</p>\n<ul>\n<li>覆盖写userDetailsService方法，具体的AdminUserDetailsService实现类，就是之前说的获取用户信息的service层类。</li>\n<li>默认不拦截静态资源的url pattern。我们也可以用下面的WebSecurity这个方式跳过静态资源的认证。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">configure</span><span class=\"params\">(WebSecurity web)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">    web</span><br><span class=\"line\">        .ignoring()</span><br><span class=\"line\">        .antMatchers(<span class=\"string\">&quot;/resourcesDir/**&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>跳转登录页面url请求路径为/login，我们需要定义一个Controller把路径映射到login.html。</li>\n<li>登录成功后跳转的路径为/httpapi</li>\n<li>退出后跳转到的url为/</li>\n<li>认证鉴权信息的Bean，采用我们自定义的从数据库中获取用户信息的AdminUserDetailService类。</li>\n</ul>\n<p>我们同样使用<code>@EnableGlobalMethodSecurity(prePostEnabled = true)</code>这个注解，开启security的注解，这样我们可以在需要控制权限的方法上面使用<code>@PreAuthorize</code>，<code>@PreFilter</code>这些注解。</p>\n<h3 id=\"用户退出\"><a href=\"#用户退出\" class=\"headerlink\" title=\"用户退出\"></a>用户退出</h3><p>我们在configure(HttpSecurity http)方法里面定义了任何权限都允许退出，当然SpringBoot集成Security的默认退出请求是/logout </p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">http.logout().logoutSuccessUrl(<span class=\"string\">&quot;/&quot;</span>); <span class=\"comment\">// 退出默认跳转页面 (4)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置错误处理页面\"><a href=\"#配置错误处理页面\" class=\"headerlink\" title=\"配置错误处理页面\"></a>配置错误处理页面</h3><p>访问发生错误时，跳转到系统统一异常处理页面。我们首先添加一个<code>GlobalExceptionHandlerAdvice</code>，使用<code>@ControllerAdvice</code>注解：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.&#123;ControllerAdvice, ExceptionHandler&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.context.request.WebRequest</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.servlet.ModelAndView</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Created by jack on 2017/4/27.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"><span class=\"meta\">@ControllerAdvice</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GlobalExceptionHandlerAdvice</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"meta\">@ExceptionHandler(value = Exception.class)</span><span class=\"comment\">//表示捕捉到所有的异常，你也可以捕捉一个你自定义的异常</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> ModelAndView <span class=\"title\">exception</span><span class=\"params\">(Exception exception, WebRequest request)</span></span>&#123;</span><br><span class=\"line\">        ModelAndView modelAndView = <span class=\"keyword\">new</span> ModelAndView(<span class=\"string\">&quot;/error&quot;</span>);</span><br><span class=\"line\">        modelAndView.addObject(<span class=\"string\">&quot;errorMessage&quot;</span>, exception.getMessage());</span><br><span class=\"line\">        modelAndView.addObject(<span class=\"string\">&quot;stackTrace&quot;</span>, exception.getStackTrace());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> modelAndView;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中，@ExceptionHandler(value = Exception.class)，表示捕捉到所有的异常，这里你也可以捕捉一个你自定义的异常。比如说，针对安全认证的Exception，我们可以单独定义处理。此处不再赘述。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Spring Security","Spring"]},{"title":"看完这篇恍然大悟，理解Java中的偏向锁，轻量级锁，重量级锁","url":"/Java/31c9aa976f5e/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>今天我们来聊聊 Synchronized 里面的各种锁：偏向锁、轻量级锁、重量级锁，以及三个锁之间是如何进行锁膨胀的。先来一张图来总结<br><img src=\"https://img-blog.csdnimg.cn/20200411151241193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"提前了解知识\"><a href=\"#提前了解知识\" class=\"headerlink\" title=\"提前了解知识\"></a>提前了解知识</h2><h3 id=\"锁的升级过程\"><a href=\"#锁的升级过程\" class=\"headerlink\" title=\"锁的升级过程\"></a>锁的升级过程</h3><p>锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）</p>\n<h3 id=\"Java-对象头\"><a href=\"#Java-对象头\" class=\"headerlink\" title=\"Java 对象头\"></a>Java 对象头</h3><p>因为在Java中任意对象都可以用作锁，因此必定要有一个映射关系，存储该对象以及其对应的锁信息（比如当前哪个线程持有锁，哪些线程在等待）。一种很直观的方法是，用一个全局map，来存储这个映射关系，但这样会有一些问题：需要对map做线程安全保障，不同的synchronized之间会相互影响，性能差；另外当同步对象较多时，该map可能会占用比较多的内存。所以最好的办法是将这个映射关系存储在对象头中，因为对象头本身也有一些hashcode、GC相关的数据，所以如果能将锁信息与这些信息共存在对象头中就好了。</p>\n<p>在JVM中，对象在内存中除了本身的数据外还会有个对象头，对于普通对象而言，其对象头中有两类信息：mark word和类型指针。另外对于数组而言还会有一份记录数组长度的数据。类型指针是指向该对象所属类对象的指针，mark word用于存储对象的HashCode、GC分代年龄、锁状态等信息。在32位系统上mark word长度为32bit，64位系统上长度为64bit。为了能在有限的空间里存储下更多的数据，其存储格式是不固定的，在32位系统上各状态的格式如下：<br><img src=\"https://img-blog.csdnimg.cn/20200411152443248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>可以看到锁信息也是存在于对象的mark word中的。当对象状态为偏向锁（biasable）时，mark word存储的是偏向的线程ID；当状态为轻量级锁（lightweight locked）时，mark word存储的是指向线程栈中Lock Record的指针；当状态为重量级锁（inflated）时，为指向堆中的monitor对象的指针。</p>\n<h3 id=\"全局安全点（safepoint）\"><a href=\"#全局安全点（safepoint）\" class=\"headerlink\" title=\"全局安全点（safepoint）\"></a>全局安全点（safepoint）</h3><p>safepoint这个词我们在GC中经常会提到，简单来说就是其代表了一个状态，在该状态下所有线程都是暂停的。</p>\n<h2 id=\"偏向锁\"><a href=\"#偏向锁\" class=\"headerlink\" title=\"偏向锁\"></a>偏向锁</h2><p>一个线程反复的去获取/释放一个锁，如果这个锁是轻量级锁或者重量级锁，不断的加解锁显然是没有必要的，造成了资源的浪费。于是引入了偏向锁，偏向锁在获取资源的时候会在资源对象上记录该对象是偏向该线程的，偏向锁并不会主动释放，这样每次偏向锁进入的时候都会判断该资源是否是偏向自己的，如果是偏向自己的则不需要进行额外的操作，直接可以进入同步操作。</p>\n<h3 id=\"偏向锁获取过程\"><a href=\"#偏向锁获取过程\" class=\"headerlink\" title=\"偏向锁获取过程\"></a>偏向锁获取过程</h3><ul>\n<li>访问Mark Word中偏向锁标志位是否设置成1，锁标志位是否为01——确认为可偏向状态。</li>\n<li>如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。</li>\n<li>如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。</li>\n<li>如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。</li>\n<li>执行同步代码。</li>\n</ul>\n<h3 id=\"偏向锁的释放\"><a href=\"#偏向锁的释放\" class=\"headerlink\" title=\"偏向锁的释放\"></a>偏向锁的释放</h3><p>偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点safepoint，它会首先暂停拥有偏向锁的线程A，然后判断这个线程A，此时有两种情况：<br><img src=\"https://img-blog.csdnimg.cn/20200411153633541.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"批量重偏向\"><a href=\"#批量重偏向\" class=\"headerlink\" title=\"批量重偏向\"></a>批量重偏向</h3><h4 id=\"为什么有批量重偏向\"><a href=\"#为什么有批量重偏向\" class=\"headerlink\" title=\"为什么有批量重偏向\"></a>为什么有批量重偏向</h4><p>当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到safe point时将偏向锁撤销为无锁状态或升级为轻量级/重量级锁。这个过程是要消耗一定的成本的，所以如果说运行时的场景本身存在多线程竞争的，那偏向锁的存在不仅不能提高性能，而且会导致性能下降。因此，JVM中增加了一种批量重偏向/撤销的机制。</p>\n<h4 id=\"批量重偏向的原理\"><a href=\"#批量重偏向的原理\" class=\"headerlink\" title=\"批量重偏向的原理\"></a>批量重偏向的原理</h4><ul>\n<li><p>首先引入一个概念epoch，其本质是一个时间戳，代表了偏向锁的有效性，epoch存储在可偏向对象的MarkWord中。除了对象中的epoch，对象所属的类class信息中，也会保存一个epoch值。</p>\n</li>\n<li><p>每当遇到一个全局安全点时(这里的意思是说批量重偏向没有完全替代了全局安全点，全局安全点是一直存在的)，比如要对class C 进行批量再偏向，则首先对 class C中保存的epoch进行增加操作，得到一个新的epoch_new</p>\n</li>\n<li><p>然后扫描所有持有 class C 实例的线程栈，根据线程栈的信息判断出该线程是否锁定了该对象，仅将epoch_new的值赋给被锁定的对象中，也就是现在偏向锁还在被使用的对象才会被赋值epoch_new。</p>\n</li>\n<li><p>退出安全点后，当有线程需要尝试获取偏向锁时，直接检查 class C 中存储的 epoch 值是否与目标对象中存储的 epoch 值相等， 如果不相等，则说明该对象的偏向锁已经无效了（因为（3）步骤里面已经说了只有偏向锁还在被使用的对象才会有epoch_new，这里不相等的原因是class C里面的epoch值是epoch_new,而当前对象的epoch里面的值还是epoch），此时竞争线程可以尝试对此对象重新进行偏向操作。</p>\n</li>\n</ul>\n<h2 id=\"轻量级锁\"><a href=\"#轻量级锁\" class=\"headerlink\" title=\"轻量级锁\"></a>轻量级锁</h2><h3 id=\"轻量级锁的获取过程\"><a href=\"#轻量级锁的获取过程\" class=\"headerlink\" title=\"轻量级锁的获取过程\"></a>轻量级锁的获取过程</h3><ul>\n<li><p>在代码进入同步块的时候，如果同步对象锁状态为偏向状态（就是锁标志位为“01”状态，是否为偏向锁标志位为“1”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝。官方称之为 Displaced Mark Word（所以这里我们认为Lock Record和 Displaced Mark Word其实是同一个概念）。这时候线程堆栈与对象头的状态如图所示：<br><img src=\"https://img-blog.csdnimg.cn/20200411155946401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p>拷贝对象头中的Mark Word复制到锁记录中。</p>\n</li>\n<li><p>拷贝成功后，虚拟机将使用CAS操作尝试将对象头的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向对象头的mark word。如果更新成功，则执行步骤（4），否则执行步骤（5）。</p>\n</li>\n<li><p>如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20200411160354479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p>如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，现在是重入状态，那么设置Lock Record第一部分（Displaced Mark Word）为null，起到了一个重入计数器的作用。下图为重入三次时的lock record示意图，左边为锁对象，右边为当前线程的栈帧，重入之后然后结束。接着就可以直接进入同步块继续执行。<br><img src=\"https://img-blog.csdnimg.cn/2020041116045549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>如果不是说明这个锁对象已经被其他线程抢占了，说明此时有多个线程竞争锁，那么它就会自旋等待锁，一定次数后仍未获得锁对象，说明发生了竞争，需要膨胀为重量级锁。</p>\n</li>\n</ul>\n<h3 id=\"轻量级锁的解锁过程\"><a href=\"#轻量级锁的解锁过程\" class=\"headerlink\" title=\"轻量级锁的解锁过程\"></a>轻量级锁的解锁过程</h3><ul>\n<li><p>通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word。</p>\n</li>\n<li><p>如果替换成功，整个同步过程就完成了。</p>\n</li>\n<li><p>如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程。</p>\n</li>\n</ul>\n<h2 id=\"重量级锁\"><a href=\"#重量级锁\" class=\"headerlink\" title=\"重量级锁\"></a>重量级锁</h2><h3 id=\"重量级锁加锁和释放锁机制\"><a href=\"#重量级锁加锁和释放锁机制\" class=\"headerlink\" title=\"重量级锁加锁和释放锁机制\"></a>重量级锁加锁和释放锁机制</h3><ul>\n<li>调用<code>omAlloc</code>分配一个<code>ObjectMonitor</code>对象，把锁对象头的mark word锁标志位变成 “10 ”，然后在mark word存储指向<code>ObjectMonitor</code>对象的指针</li>\n<li><code>ObjectMonitor</code>中有两个队列，<code>_WaitSet</code>和<code>_EntryList</code>，用来保存<code>ObjectWaiter</code>对象列表(每个等待锁的线程都会被封装成<code>ObjectWaiter</code>对象)，<code>_owner</code>指向持有<code>ObjectMonitor</code>对象的线程，当多个线程同时访问一段同步代码时，首先会进入 <code>_EntryList</code> 集合，当线程获取到对象的<code>monitor</code> 后进入 <code>_Owner</code> 区域并把<code>monitor</code>中的<code>owner</code>变量设置为当前线程同时<code>monitor</code>中的计数器<code>count</code>加1，若线程调用<code>wait()</code>方法，将释放当前持有的<code>monitor</code>，<code>owner</code>变量恢复为null，<code>count</code>自减1，同时该线程进入<code>WaitSet</code>集合中等待被唤醒。若当前线程执行完毕也将释放<code>monitor</code>(锁)并复位变量的值，以便其他线程进入获取<code>monitor</code>(锁)。如下图所示<br><img src=\"https://img-blog.csdnimg.cn/20200411162031746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h3 id=\"Synchronized同步代码块的底层原理\"><a href=\"#Synchronized同步代码块的底层原理\" class=\"headerlink\" title=\"Synchronized同步代码块的底层原理\"></a>Synchronized同步代码块的底层原理</h3><p>同步代码块的加锁、解锁是通过 Javac 编译器实现的，底层是借助<code>monitorenter</code>和<code>monitorerexit</code>，为了能够保证无论代码块正常执行结束 or 抛出异常结束，都能正确释放锁，Javac 编译器在编译的时候，会对<code>monitorerexit</code>进行特殊处理，举例说明：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Hello</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>) &#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;test&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过 <code>javap -c</code> 查看其编译后的字节码:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Hello</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Hello</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    Code:</span><br><span class=\"line\">       <span class=\"number\">0</span>: aload_0</span><br><span class=\"line\">       1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V</span><br><span class=\"line\">       <span class=\"number\">4</span>: <span class=\"keyword\">return</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">test</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    Code:</span><br><span class=\"line\">       <span class=\"number\">0</span>: aload_0</span><br><span class=\"line\">       <span class=\"number\">1</span>: dup</span><br><span class=\"line\">       <span class=\"number\">2</span>: astore_1</span><br><span class=\"line\">       <span class=\"number\">3</span>: monitorenter</span><br><span class=\"line\">       4: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class=\"line\">       7: ldc           #3                  // String test</span><br><span class=\"line\">       9: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span><br><span class=\"line\">      <span class=\"number\">12</span>: aload_1</span><br><span class=\"line\">      <span class=\"number\">13</span>: monitorexit</span><br><span class=\"line\">      <span class=\"number\">14</span>: goto          <span class=\"number\">22</span></span><br><span class=\"line\">      <span class=\"number\">17</span>: astore_2</span><br><span class=\"line\">      <span class=\"number\">18</span>: aload_1</span><br><span class=\"line\">      <span class=\"number\">19</span>: monitorexit</span><br><span class=\"line\">      <span class=\"number\">20</span>: aload_2</span><br><span class=\"line\">      <span class=\"number\">21</span>: athrow</span><br><span class=\"line\">      <span class=\"number\">22</span>: <span class=\"keyword\">return</span></span><br><span class=\"line\">    Exception table:</span><br><span class=\"line\">       from    to  target type</span><br><span class=\"line\">           <span class=\"number\">4</span>    <span class=\"number\">14</span>    <span class=\"number\">17</span>   any</span><br><span class=\"line\">          <span class=\"number\">17</span>    <span class=\"number\">20</span>    <span class=\"number\">17</span>   any</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>从字节码中可知同步语句块的实现使用的是<code>monitorenter</code>和<code>monitorexit</code>指令，其中<code>monitorenter</code>指令指向同步代码块的开始位置，<code>monitorexit</code>指令则指明同步代码块的结束位置，当执行<code>monitorenter</code>指令时，当前线程将试图获取mark word里面存储的<code>monitor</code>，当 <code>monitor</code>的进入计数器为 0，那线程可以成功取得<code>monitor</code>，并将计数器值设置为1，取锁成功。</p>\n<p>如果当前线程已经拥有 <code>monitor</code> 的持有权，那它可以重入这个 <code>monitor</code> ，重入时计数器的值也会加 1。倘若其他线程已经拥有<code>monitor</code>的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即<code>monitorexit</code>指令被执行，执行线程将释放 <code>monitor</code>并设置计数器值为0 ，其他线程将有机会持有 <code>monitor</code> 。</p>\n<p>值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 <code>monitorenter</code> 指令都有执行其对应 <code>monitorexit</code> 指令，而无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 <code>monitorenter</code> 和 <code>monitorexit</code> 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 <code>monitorexit</code> 指令。从上面的字节码中也可以看出有两个<code>monitorexit</code>指令，它就是异常结束时被执行的释放<code>monitor</code> 的指令。</p>\n<h3 id=\"同步方法底层原理\"><a href=\"#同步方法底层原理\" class=\"headerlink\" title=\"同步方法底层原理\"></a>同步方法底层原理</h3><p>同步方法的加锁、解锁是通过 Javac 编译器实现的，底层是借助<code>ACC_SYNCHRONIZED</code>访问标识符来实现的，代码如下所示：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Hello</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">test</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;test&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 <code>ACC_SYNCHRONIZED</code> 访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会 检查方法的 <code>ACC_SYNCHRONIZED</code>访问标志是否被设置，如果设置了，执行线程将先持有<code>monitor</code>，然后再执行方法，最后在方法完成(无论是正常完成还是非正常完成)时释放<code>monitor</code>。在方法执行期间，执行线程持有了<code>monitor</code>，其他任何线程都无法再获得同一个<code>monitor</code>。如果一个同步方法执行期间抛出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的<code>monitor</code>将在异常抛到同步方法之外时自动释放。</p>\n<p>下面我们看看字节码层面如何实现：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Hello</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Hello</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    Code:</span><br><span class=\"line\">       <span class=\"number\">0</span>: aload_0</span><br><span class=\"line\">       1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V</span><br><span class=\"line\">       <span class=\"number\">4</span>: <span class=\"keyword\">return</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title\">test</span><span class=\"params\">()</span></span>;</span><br><span class=\"line\">    Code:</span><br><span class=\"line\">       0: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class=\"line\">       3: ldc           #3                  // String test</span><br><span class=\"line\">       5: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span><br><span class=\"line\">       <span class=\"number\">8</span>: <span class=\"keyword\">return</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"锁的其他优化\"><a href=\"#锁的其他优化\" class=\"headerlink\" title=\"锁的其他优化\"></a>锁的其他优化</h2><ul>\n<li>适应性自旋（Adaptive Spinning）：从轻量级锁获取的流程中我们知道，当线程在获取轻量级锁的过程中执行CAS操作失败时，是要通过自旋来获取重量级锁的。问题在于，自旋是需要消耗CPU的，如果一直获取不到锁的话，那该线程就一直处在自旋状态，白白浪费CPU资源。解决这个问题最简单的办法就是指定自旋的次数，例如让其循环10次，如果还没获取到锁就进入阻塞状态。但是JDK采用了更聪明的方式——适应性自旋，简单来说就是线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。</li>\n<li>锁粗化（Lock Coarsening）：锁粗化的概念应该比较好理解，就是将多次连接在一起的加锁、解锁操作合并为一次，将多个连续的锁扩展成一个范围更大的锁。举个例子：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span>  <span class=\"keyword\">void</span> <span class=\"title\">lockCoarsening</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>)&#123;</span><br><span class=\"line\">        i=i+<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>)&#123;</span><br><span class=\"line\">        i=i+<span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的两个同步代码块可以变成一个</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span>  <span class=\"keyword\">void</span> <span class=\"title\">lockCoarsening</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>)&#123;</span><br><span class=\"line\">        i=i+<span class=\"number\">1</span>;</span><br><span class=\"line\">        i=i+<span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>锁消除（Lock Elimination）：锁消除即删除不必要的加锁操作的代码。比如下面的代码,下面的for循环完全可以移出来，这样可以减少加锁代码的执行过程</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span>  <span class=\"keyword\">void</span> <span class=\"title\">lockElimination</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">synchronized</span> (<span class=\"keyword\">this</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> c=<span class=\"number\">0</span>; c&lt;<span class=\"number\">1000</span>; c++)&#123;</span><br><span class=\"line\">            System.out.println(c);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        i=i+<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","categories":["Java"],"tags":["Java","线程","锁"]},{"title":"论文阅读笔记：Informer--效果远超Transformer的长序列预测模型","url":"/Paper-Reading/e914502af9e0/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting<br>原文链接：<a href=\"https://arxiv.org/pdf/2012.07436.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>在很多实际应用问题中，我们需要对长序列时间序列进行预测，例如用电使用规划。长序列时间序列预测（LSTF）要求模型具有很高的预测能力，即能够有效地捕捉输出和输入之间精确的长程相关性耦合。最近的研究表明，Transformer具有提高预测能力的潜力，但是Transformer存在一些严重的问题，如<strong>二次时间复杂度</strong>、<strong>高内存使用率</strong>以及<strong>encoder-decoder体系结构的固有限制</strong>。为了解决这些问题，本篇论文设计了一个有效的基于Transformer的LSTF模型，即Informer，它具有如下三个显著的特点：</p>\n<ul>\n<li>ProbSparse Self-Attention，在时间复杂度和内存使用率上达到了 $O(LlogL)$，在序列的依赖对齐上具有相当的性能。</li>\n<li>self-attention 提取通过将级联层输入减半来突出控制注意，并有效地处理超长的输入序列。</li>\n<li>产生式decoder虽然概念上简单，但在一个正向操作中预测长时间序列，而不是一步一步地进行，这大大提高了长序列预测的推理速度。</li>\n</ul>\n<p>在四个大规模数据集上的大量实验表明，Informer的性能明显优于现有的方法，为LSTF问题提供了一种新的解决方案。</p>\n<h1 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h1><p>在开始之前，先来感受一下LSTM在长序列中的一个例子的实验结果：</p>\n <img src=\"https://img-blog.csdnimg.cn/20210218160503616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center />\n\n<p>随着数据量的增加，很明显LSTF的主要挑战是增强预测能力以满足日益增长的序列需求，即要求模型具有</p>\n<ul>\n<li>出色的long-range对齐能力</li>\n<li>对长序列输入和输出的有效操作</li>\n</ul>\n<p>Transformer的出现，由于其自注意力机制可以减少网络信号传播路径的最大长度至理论最短 $O(1)$，从而是Transformer表现出了解决LSTF问题的巨大潜力。但是，自注意力机制的计算、内存和架构效率也成为了Transformer应用解决LSTF问题的瓶颈，因此，本论文<strong>研究Transformer是否可以提高计算、内存和架构效率，以及保持更高的预测能力？</strong></p>\n<p>首先得了解，原始Transformer应用在LSTF上的效率限制问题：</p>\n<ul>\n<li>self-attention的二次计算复杂度，self-attention机制的操作，会导致我们模型的时间复杂度为 $O(L^2)$</li>\n<li>长输入的stacking层的内存瓶颈：$J$ 个encoder/decoder的stack会导致内存的使用为 $O(J*L^2)$</li>\n<li>预测长输出的速度骤降：动态的decoding会导致step-by-step的inference非常慢</li>\n</ul>\n<p>论文中提到了许多Transformer的改进版本，如Sparse Transformer、LogSparse、Transformer、LongFormer、reformer、Linformer、Transformer-XL、Compressive Transformer等等，不过都只是局限于解决上述第一个问题，而本文提出的Informer方案同时解决了上面的三个问题，论文中研究了在self-attention机制中的稀疏性问题，本文的贡献有如下几点：</p>\n<ul>\n<li>提出Informer来成功地提高LSTF问题的预测能力，这验证了类Transformer模型的潜在价值，以捕捉长序列时间序列输出和输入之间的单个的长期依赖性；</li>\n<li>提出了ProbSparse self-attention机制来高效的替换常规的self-attention并且获得了 $O(LlogL)$ 的时间复杂度以及 $O(LlogL)$ 的内存使用率；</li>\n<li>提出了self-attention distilling操作，它大幅降低了所需的总空间复杂度 $O((2-\\epsilon)LlogL)$</li>\n<li>提出了生成式的Decoder来获取长序列的输出，这只需要一部，避免了在inference阶段的累积误差传播</li>\n</ul>\n<h1 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h1><p>在开始介绍具体细节之前，需要先给出问题的定义，在固定大小的窗口下的rolling forecasting（滚动式预测是一种随时间流逝而进行的推测）中，我们在时刻 $t$ 的输入为 $X^t={x_1^t,x_2^t,…,x_{L_z}^t|x_i^t\\in R^{d_x}}$ ，我们需要预测对应的输出序列 $Y^t={y_1^t,y_2^t,…,y_{L_y}^t|y_i^t\\in R^{d_y}}$，LSTF问题鼓励输出一个更长的输出，特征维度不再依赖于univariate case（$d_y\\geq1$）</p>\n<p><strong>Encoder-decoder结构</strong>：许多流行的模型被设计对输入表示 $X^t$ 进行编码，将 $X^t$ 编码为一个隐藏状态表示 $H^t={h_1^t,…,h_{L_h}^t}$，并且将输出的表示 $Y^t$ 解码，在推断的过程中，通过step-by-step的过程（dynamic decoding），即decoder从前一个状态 $h_k^t$ 计算一个新的隐藏状态 $h_{k+1}^t$ 以及第 $k$ 步的输出，然后对 $k+1$ 个序列进行预测 $y_{k+1}^t$</p>\n<p><strong>输入表示</strong>：为增强时间序列输入的全局位置上下文和局部时间上下文，给出了统一的输入表示，如下（更详细符号意义可参见原文附录B）：<br>$$\\chi_{feed[i]}^t=\\alpha u_i^t+PE_{(L_x\\times (t-1)+i,)}+\\sum_p[SE_{(L_x\\times (t-1)+i)}]_p$$<br>其中，$\\alpha$是在标量投影和局部投影之间平衡大小的因子，如果输入序列已经标准化过了，则推荐值为1，下图是输入表示的直观的概述：<br><img src=\"https://img-blog.csdnimg.cn/20210218172842903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"模型方法细节\"><a href=\"#模型方法细节\" class=\"headerlink\" title=\"模型方法细节\"></a>模型方法细节</h1><p>现有的时序方法预测大致分为两类：1)：经典的时间序列模型；2):RNN及其变体为代表的encoder-decoder结构的深度学习模型。Informer模型基于encoder-decoder结构，目标是解决LSTF问题，其模型结构概览图如下：<br><img src=\"https://img-blog.csdnimg.cn/20210218174325201.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"高效的Self-attention机制\"><a href=\"#高效的Self-attention机制\" class=\"headerlink\" title=\"高效的Self-attention机制\"></a>高效的Self-attention机制</h2><p>传统的self-attention输入为(query,key,value)，表示为：$A(Q,K,V)=Softmax(\\frac{QK^T}{\\sqrt{d}})$，其中 $Q\\in R^{L_Q<em>d}$，$K\\in R^{L_K</em>d}$，$V\\in R^{L_V*d}$，$d$ 是输入维度，第 $i$ 个attention被定义为kernel 平滑的概率形式：<br>$$A(q_i,K,V)=\\sum_j\\frac{k(q_i,k_j)}{\\sum_lk(1_i,k_l)}v_j=E_{p(k_j|q_i)}[v_j]\\tag{1}$$<br>self-attention需要 $O(L_QL_K)$ 的内存以及二次的点积计算代价，这是预测能力的主要缺点。先前的一些研究表明，自注意概率的分布具有潜在的稀疏性，所以在这些研究中，已经针对所有 $p(k_j|q_i)$ 设计了一些“选择性”计数策略。但是，这些方法仅限于采用启发式方法进行理论分析，并使用相同的策略来解决多头自注意的问题，这也缩小了进一步改进的范围。</p>\n<p>在论文中，首先对典型自注意的学习注意模式进行定性评估。“稀疏性” self-attention得分形成长尾分布，即少数点积对主要注意有贡献，其他点积对可以忽略。论文使用原始Transformer在ETTH数据集研究self-attention的特征图分布，如下图（选用Layer1中的Head1和Head7的分数）：<br><img src=\"https://img-blog.csdnimg.cn/2021021818492636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>那么，下一个问题是如何区分它们？</p>\n<h2 id=\"Query-Sparsity评估\"><a href=\"#Query-Sparsity评估\" class=\"headerlink\" title=\"Query Sparsity评估\"></a>Query Sparsity评估</h2><p>在上一小节的公式（1）中，第 $i$ 个查询对所有key的关注度定义为概率 $p(k_j|q_i)$，我们定义第 $i$ 个query sparsity评估为：<br>$$M(q_i,K)=ln\\sum_{j=1}^{L_k}e^{\\frac{q_ik_j^T}{\\sqrt{d}}}-\\frac{1}{L_K}\\sum_{j=1}^{L_K}\\frac{q_ik_j^T}{\\sqrt{d}}$$<br>其中，第一项 $q_i$ 是在所有key的Log-Sum-Exp(LSE)，第二项是arithmetic均值。</p>\n<ul>\n<li>ProbSparse Self-attention：<br>$$A(Q,K,V)=Softmax(\\frac{\\bar{Q}K^T}{\\sqrt{d}})V$$<br>其中 $\\bar{Q}$ 是和 $q$ 相同大小的稀疏矩阵，它仅包含稀疏评估下 $M(q,M)$ 下Top-u的queries，由采样factor $c$ 所控制，我们令$u=c\\cdot lnL_Q$， 这么做self-attention对于每个query-key lookup就只需要计算 $O(lnL_Q)$ 的内积，内存的使用包含$O(L_KlnL_Q)$，但是我们计算的时候需要计算每对的dot-product，即 $O(L_QL_K)$，同时LSE还会带来潜在的数值问题，受此影响，本文提出了query sparsity 评估的近似，即：<br>$$\\bar{M}(q_i,K)=\\underset{j}{max}{\\frac{q_ik_j^T}{\\sqrt{d}}}-\\frac{1}{L_K}\\sum_{j=1}^{L_K}\\frac{q_ik_j^T}{\\sqrt{d}}$$<br>具体证明将附录D，下图是直观的数值性示例：<br><img src=\"https://img-blog.csdnimg.cn/20210218191142392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在实践中，查询和键的输入长度通常是相等的，即 $L_Q = L_K = L$，这么做可以将时间和空间复杂度控制到$O(LlnL)$</li>\n</ul>\n<h2 id=\"Encoder：允许在内存使用限制下处理更长的顺序输入\"><a href=\"#Encoder：允许在内存使用限制下处理更长的顺序输入\" class=\"headerlink\" title=\"Encoder：允许在内存使用限制下处理更长的顺序输入\"></a>Encoder：允许在内存使用限制下处理更长的顺序输入</h2><p>Encoder设计用于提取长序列输入的鲁棒的 long-range相关性，在前面的讨论我们知道，在输入表示之后，第 $t$ 个序列输入 $X^t$ 已表示为矩阵 $X_{feed_en}^t\\in\\mathbb{R}^{L_x\\times d_{model}}$，下面是Encoder的示意图：<br><img src=\"https://img-blog.csdnimg.cn/2021021819215213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Self-attention-Distilling\"><a href=\"#Self-attention-Distilling\" class=\"headerlink\" title=\"Self-attention Distilling\"></a>Self-attention Distilling</h3><p>作为ProbSparse Self-attention的自然结果，encoder的特征映射会带来 $V$ 值的冗余组合，利用distilling对具有支配特征的优势特征进行特权化，并在下一层生成focus self-attention特征映射。它对输入的时间维度进行了锐利的修剪，如上图所示，$n$ 个头部权重矩阵（重叠的红色方块）。受扩展卷积的启发，我们的“distilling”过程从第 $j$ 层往 $j+1$ 推进：<br>$$X_{j+1}^t=MaxPoll(ELU(Conv1d([X_j^t]<em>{AB})))$$<br>其中 $[\\cdot]</em>{AB}$ 包含Multi-Head ProbSparse self-attention以及重要的attention block的操作。为了增强distilling操作的鲁棒性，我们构建了halving replicas，并通过一次删除一层（如上图）来逐步减少自关注提取层的数量，从而使它们的输出维度对齐。因此，我们将所有堆栈的输出串联起来，并得到encoder的最终隐藏表示。</p>\n<h2 id=\"Decoder：通过一个正向过程生成长序列输出\"><a href=\"#Decoder：通过一个正向过程生成长序列输出\" class=\"headerlink\" title=\"Decoder：通过一个正向过程生成长序列输出\"></a>Decoder：通过一个正向过程生成长序列输出</h2><p>此处使用标准的decoder结构，由2个一样的multihead attention层，但是，生成式inference被用来缓解速度瓶颈，我们使用下面的向量喂入decoder：<br>$$X_{feed_de}^t=Concat(X_{token}^t,X_0^t)\\in \\mathbb{R}^{(L_{token}+L_y)\\times d_{model}}$$<br>其中，$X_{token}^t\\in \\mathbb{R}^{(L_{token}+L_y)\\times d_{model}}$是start token，$X_0^t\\in\\mathbb{R}^{L_y\\times d_{model}}$ 是一个placeholder，将Masked multi-head attention应用于ProbSparse self-attention，将mask的点积设置为 $-\\infty$。它可以防止每个位置都关注未来的位置，从而避免了自回归。一个完全连接的层获得最终的输出，它的超大小取决于我们是在执行单变量预测还是在执行多变量预测。</p>\n<h3 id=\"Generative-Inference\"><a href=\"#Generative-Inference\" class=\"headerlink\" title=\"Generative Inference\"></a>Generative Inference</h3><p>我们从长序列中采样一个 $L_{token}$，这是在输出序列之前的slice。以图中预测168个点为例（7天温度预测），我们将目标序列已知的前5天的值作为“start token”，并将 $X_{feed_de}={X_{5d},X_0}$ 输入生成式推断Decoder。$X_0$ 包含目标序列的时间戳，即目标周的上下文。注意，我们提出的decoder通过一个前向过程预测所有输出，并且不存在耗时的“dynamic decoding”。</p>\n<p><strong>选用MSE 损失函数作为最终的Loss。</strong></p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><ul>\n<li>看实验结果之前，我们先来看看实验的模型组件的详细信息：<img src=\"https://img-blog.csdnimg.cn/20210218215245328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>ProbSparse self-attention实现伪代码<br><img src=\"https://img-blog.csdnimg.cn/20210218215353811.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>超参微调范围：<br><img src=\"https://img-blog.csdnimg.cn/20210218215818912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>各个模型在ETT数据集上的实验对比<br><img src=\"https://img-blog.csdnimg.cn/20210218215927364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>实验效果如下，从中我们可以发现论文所提出的模型Informer极大地提高了所有数据集的推理效果（最后一列的获胜计数），并且在不断增长的预测范围内，它们的预测误差平稳而缓慢地上升。同时，query sparsity假设在很多数据集上是成立的，Informer在很多数据集上远好于LSTM和ERNN。<br><img src=\"https://img-blog.csdnimg.cn/20210218220013822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>参数敏感性实验如下，从下图中，我们发现：（1）Input Length中，当预测短序列（如48）时，最初增加编码器/解码器的输入长度会降低性能，但进一步增加会导致MSE下降，因为它会带来重复的短期模式。然而，在预测中，输入时间越长，平均误差越低：信息者的参数敏感性。长序列（如168）。因为较长的编码器输入可能包含更多的依赖项。（2）Sampling Factor中，我们验证了冗余点积的查询稀疏性假设，实践中，我们把sample factor设置为5即可，即 $c=5$ 。（3）Number of Layer Stacking中，Longer stack对输入更敏感，部分原因是接收到的长期信息较多<br><img src=\"https://img-blog.csdnimg.cn/20210218220432402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>解耦实验：从下表中我们发现，（1）ProbSparse self-attention机制的效果：ProbSparse self-attention的效果更好，而且可以节省很多内存消耗（2）self-attention distilling：是值得使用的，尤其是对长序列进行预测的时候（3）generative stype decoderL：它证明了decoder能够捕获任意输出之间的长依赖关系，避免了误差的积累；<br><img src=\"https://img-blog.csdnimg.cn/20210218220919205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>计算高效性：（1）在训练阶段，在基于Transformer的方法中，Informer获得了最佳的训练效率。（2）在测试阶段，我们的方法比其他生成式decoder方法要快得多。</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20210218221133592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文研究了长序列时间序列预测问题，提出了长序列预测的Informer方法。具体地：</p>\n<ul>\n<li>设计了ProbSparse self-attention和提取操作来处理vanilla Transformer中二次时间复杂度和二次内存使用的挑战。</li>\n<li>generative decoder缓解了传统编解码结构的局限性。</li>\n<li>通过对真实数据的实验，验证了Informer对提高预测能力的有效性</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["Transformer","Paper","Informer"]},{"title":"论文阅读笔记：各种Optimizer梯度下降优化算法回顾和总结","url":"/Paper-Reading/d9b52c160292/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：An overview of gradient descent optimization algorithms<br>原文链接：<a href=\"https://arxiv.org/pdf/1609.04747.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>不管是使用PyTorch还是TensorFlow，用多了Optimizer优化器封装好的函数，对其内部使用的优化算法却没有仔细研究过，也很难对其优点和缺点进行实用的解释。所以打算以这一篇论文为主线并结合多篇优秀博文，回顾和总结目前主流的优化算法，对于没有深入了解过的算法，正好借这个机会学习一下。</p>\n<h1 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h1><p>当前使用的许多优化算法，是对梯度下降法的衍生和优化。在微积分中，对多元函数的参数求 $\\theta$ 偏导数，把求得的各个参数的导数以向量的形式写出来就是梯度。梯度就是函数变化最快的地方。梯度下降是迭代法的一种，在求解机器学习算法的模型参数 $\\theta$ 时，即无约束问题时，梯度下降是最常采用的方法之一。 </p>\n<p>这里定义一个通用的思路框架，方便我们后面理解各算法之间的关系和改进。首先定义待优化参数 $\\theta$ ，目标函数 $J(\\theta)$，学习率为 $\\eta$，然后我们进行迭代优化，假设当前的epoch为 $t$ ，则有：</p>\n<ul>\n<li>计算目标函数关于当前参数的梯度： $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$</li>\n<li>根据历史梯度计算一阶动量和二阶动量：$m_t=\\phi(g_1,g_2,…,g_t);V_t=\\psi(g_1,g_2,…,g_t)$，</li>\n<li>计算当前时刻的下降梯度： $\\triangledown_t=\\eta\\cdot \\frac{m_t}{\\sqrt{V_t}}$</li>\n<li>根据下降梯度进行更新： $\\theta_{t+1} = \\theta_t -\\triangledown_t$</li>\n</ul>\n<p>其中，$\\theta_{t+1}$为下一个时刻的参数，$\\theta_t$为当前时刻 $t$ 参数，后面的描述我们都将结合这个框架来进行。</p>\n<p>这里提一下一些概念：</p>\n<ul>\n<li>鞍点：一个光滑函数的鞍点邻域的曲线，曲面，或超曲面，都位于这点的切线的不同边。例如这个二维图形，像个马鞍：在x-轴方向往上曲，在y-轴方向往下曲，鞍点就是（0，0）。<br><img src=\"https://img-blog.csdnimg.cn/20210111155320118.png#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>指数加权平均、偏差修正：可参见这篇文章：<a href=\"https://www.cnblogs.com/guoyaohua/p/8544835.html\">什么是指数加权平均、偏差修正？</a></li>\n</ul>\n<h1 id=\"Gradient-Descent（GD）\"><a href=\"#Gradient-Descent（GD）\" class=\"headerlink\" title=\"Gradient Descent（GD）\"></a>Gradient Descent（GD）</h1><p>在GD中没有动量的概念，也就是说在上述框架中：$m_t=g_t；V_t=I^2$，则我们在当前时刻需要下降的梯度就是 $\\triangledown_t=\\eta\\cdot g_t$ ，则使用梯度下降法更新参数为（假设当前样本为 $(x_i,y_i)$，每当样本输入时，参数即进行更新）：<br>$$\\theta_{t+1}=\\theta_t-\\triangledown_t=\\theta_t-\\eta\\cdot g_t=\\theta_t-\\eta\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t,x_i,y_i)$$</p>\n<p>梯度下降算法中，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步，更形象的如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210111142747267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>标准的梯度下降主要有两个缺点：</p>\n<ul>\n<li>训练速度慢：在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本，会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。</li>\n<li>容易陷入局部最优解：由于是在有限视距内寻找下山的反向，当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点，落入鞍点，梯度为0，使得模型参数不在继续更新。</li>\n</ul>\n<h1 id=\"Batch-Gradient-Descent（BGD）\"><a href=\"#Batch-Gradient-Descent（BGD）\" class=\"headerlink\" title=\"Batch Gradient Descent（BGD）\"></a>Batch Gradient Descent（BGD）</h1><p>BGD相对于标准GD进行了改进，改进的地方通过它的名字应该也能看出来，也就是不再是想标准GD一样，对每个样本输入都进行参数更新，而是针对一个批量的数据输入进行参数更新。我们假设<strong>批量训练样本总数</strong>为 $n$，样本为 ${(x_1,y_1),..,(x_n, y_n)}$  ，则在第 $i$ 对样本 $(x_i,y_i)$ 上损失函数关于参数的梯度为 $\\triangledown_\\theta J_i(\\theta, x_i, y_i)$ , 则使用BGD更新参数为：<br>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\frac{1}{n}\\cdot\\sum_{i=1}^{n}\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$<br>从上面的公式我们可以看到，BGD其实是在一个批量的样本数据中，求取该批量样本梯度的均值来更新参数，即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数，这样就会大大加快训练速度，但是还是不够，我们接着往下看。</p>\n<h1 id=\"Stochastic-Gradient-Descent（SGD）\"><a href=\"#Stochastic-Gradient-Descent（SGD）\" class=\"headerlink\" title=\"Stochastic Gradient Descent（SGD）\"></a>Stochastic Gradient Descent（SGD）</h1><p>随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本 $(x_i,y_i)$ 计算其梯度，参数更新公式为：</p>\n<p>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$</p>\n<p>公式看起来和上面标准GD一样，但是注意了，这里的样本是从批量中随机选取一个，而标准GD是所有的输入样本都进行计算。可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大（如下图），更容易从一个局部最优跳到另一个局部最优，准确度下降。<br><img src=\"https://img-blog.csdnimg.cn/20210111150957576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>论文中提到，当缓慢降低学习率时，SGD会显示与BGD相同的收敛行为，几乎一定会收敛到局部（非凸优化）或全局最小值（凸优化）。</p>\n<p>SGD的优点：</p>\n<ul>\n<li>虽然看起来SGD波动非常大，会走很多弯路，但是对梯度的要求很低（计算梯度快），而且对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。</li>\n<li>应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。</li>\n</ul>\n<p>SGD的缺点：</p>\n<ul>\n<li>SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确（次要）。</li>\n<li>SGD也没能单独克服局部最优解的问题（主要）。</li>\n</ul>\n<h1 id=\"Mini-batch-Gradient-Descent（MBGD，也叫作SGD）\"><a href=\"#Mini-batch-Gradient-Descent（MBGD，也叫作SGD）\" class=\"headerlink\" title=\"Mini-batch Gradient Descent（MBGD，也叫作SGD）\"></a>Mini-batch Gradient Descent（MBGD，也叫作SGD）</h1><p>小批量梯度下降法就是结合BGD和SGD的折中，对于含有 $n$ 个训练样本的数据集，每次参数更新，选择一个大小为 $m(m&lt;n)$的mini-batch数据样本计算其梯度，其参数更新公式如下：<br>$$\\theta_{t+1}=\\theta_t-\\eta\\cdot\\frac{1}{m}\\cdot\\sum_{i=x}^{i=x+m-1}\\triangledown_{\\theta_t} J_i(\\theta_t, x_i, y_i)$$<br>小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法。常用的小批量尺寸范围在50到256之间，但可能因不同的应用而异。</p>\n<p>MBGD的缺点：</p>\n<ul>\n<li>Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点）。对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点所有维度的梯度都接近于0，SGD 很容易被困在这里（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是BGD的训练集全集带入，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动）。</li>\n<li>SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新， 且learning rate会随着更新的次数逐渐变小。</li>\n</ul>\n<h1 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h1><p>momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。从这里开始，我们引入一阶动量的概念（在mini-batch SGD的基础之上），也就是说，在最开始说的框架中， $m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot g_t$，而 $V_t=I^2$ 不变，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$        $$\\theta_{t+1}=\\theta_t-m_{t+1}$$</p>\n<p>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 $\\frac{1}{(1-\\beta_1)}$ 个时刻的梯度向量和的平均值（移动平均是啥看最上面的文章）。也就是说，$t$ 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $\\beta_1$ 的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡，在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210111162859514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>动量主要解决SGD的两个问题：</p>\n<ul>\n<li>随机梯度的方法（引入的噪声）</li>\n<li>Hessian矩阵病态问题（可以理解为SGD在收敛过程中和正确梯度相比来回摆动比较大的问题）。</li>\n</ul>\n<h1 id=\"Nesterov-Accelerated-Gradient\"><a href=\"#Nesterov-Accelerated-Gradient\" class=\"headerlink\" title=\"Nesterov Accelerated Gradient\"></a>Nesterov Accelerated Gradient</h1><p>牛顿加速梯度（NAG, Nesterov accelerated gradient）算法，是Momentum动量算法的变种。momentum保留了上一时刻的梯度  $\\triangledown_\\theta J(\\theta)$  ，对其没有进行任何改变，NAG是momentum的改进，在梯度更新时做一个矫正，具体做法就是在当前的梯度上添加上一时刻的动量 $\\beta_1\\cdot m_t$ ，梯度改变为  $\\triangledown_\\theta J(\\theta-\\beta_1\\cdot m_t)$  ，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J(\\theta_t-\\beta_1\\cdot m_t)$$    $$\\theta_{t+1}=\\theta_t-m_{t+1}$$</p>\n<p>加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。 下图是momentum和nesterrov的对比表述图如下：<br><img src=\"https://img-blog.csdnimg.cn/20210111164437433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Nesterov动量梯度的计算在模型参数施加当前速度之后，因此可以理解为往标准动量中添加了一个校正因子。在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从 $O(1/k)$ (k步后)改进到 $O(1/k^2)$，然而，在随机梯度情况下，Nesterov动量对收敛率的作用却不是很大。</p>\n<p>Momentum和Nexterov都是为了使梯度更新更灵活。但是人工设计的学习率总是有些生硬，下面介绍几种自适应学习率的方法。</p>\n<h1 id=\"Adagrad\"><a href=\"#Adagrad\" class=\"headerlink\" title=\"Adagrad\"></a>Adagrad</h1><p>Adagrad其实是对学习率进行了一个约束，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。而该方法中开始使用二阶动量，才意味着“自适应学习率”优化算法时代的到来。</p>\n<p>我们前面都没有好好的讨论二阶动量，二阶动量是个啥？它是用来度量历史更新频率的，二阶动量是迄今为止所有梯度值的平方和，即 $V_t = \\sum_{i=1}^tg_t^2$，在最上面的框架中 $\\triangledown_t=\\eta\\cdot \\frac{m_t}{\\sqrt{V_t}}$（在这里$m_t=I$）， 也就是说，我们的学习率现在是 $\\frac{\\eta}{\\sqrt{V_t+\\epsilon}}$（一般为了避免分母为0，会在分母上加一个小的平滑项 $\\epsilon$），从这里我们就会发现 $\\sqrt{V_t+\\epsilon}$ 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小，这一方法在稀疏数据场景下表现非常好，参数更新公式如下：<br>$$V_t = \\sum_{i=1}^tg_t^2$$   $$\\theta_{t+1}=\\theta_t-\\eta\\frac{1}{\\sqrt{V_t+\\epsilon}}$$</p>\n<p>细心的小伙伴应该会发现Adagrad还是存在一个很明显的缺点：</p>\n<ul>\n<li>仍需要手工设置一个全局学习率 $\\eta$ , 如果 $\\eta$ 设置过大的话，会使regularizer过于敏感，对梯度的调节太大</li>\n<li>中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习</li>\n</ul>\n<h1 id=\"Adadelta\"><a href=\"#Adadelta\" class=\"headerlink\" title=\"Adadelta\"></a>Adadelta</h1><p>由于AdaGrad调整学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度，即Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值（指数移动平均值），这就避免了二阶动量持续累积、导致训练过程提前结束的问题了，参数更新公式如下：<br>$$V_t = \\beta_2\\cdot V_{t-1} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$   $$\\theta_{t+1}=\\theta_t-\\eta\\frac{1}{\\sqrt{V_t+\\epsilon}}$$</p>\n<p>观察上面的参数更新公式，我们发现还是依赖于全局学习率 $\\eta$ ，但是原作者在此基础之上做出了一定的处理，上式经过牛顿迭代法之后，得到Adadelta最终迭代公式如下式，其中 $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$：<br>$$E[g_t^2]<em>t=\\rho\\cdot E[g_t^2]_{t-1}+(1-\\rho)\\cdot g_t^2$$    $$\\triangledown_t=\\frac{\\sum</em>{i=1}^{t-1}\\triangle\\theta_r}{\\sqrt{E[g_t^2]_t+\\epsilon}}$$</p>\n<p><strong>此时可以看出Adadelta已经不依赖全局learning rate了</strong>，Adadelta有如下特点：</p>\n<ul>\n<li>训练初中期，加速效果不错，很快</li>\n<li>训练后期，反复在局部最小值附近抖动</li>\n</ul>\n<h1 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h1><p>RMSProp算法修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好。设定参数：全局初始率 $\\eta$ , 默认设为0.001，decay rate $\\rho$ ，默认设置为0.9，一个极小的常量 $\\epsilon$ ，通常为10e-6，参数更新公式如下，其中 $g_t = \\triangledown_{\\theta_t} J(\\theta_t)$：<br>$$E[g_t^2]_t=\\rho\\cdot E[g_t^2]_{t-1}+(1-\\rho)\\cdot g_t^2$$    $$\\triangledown_t=\\frac{\\eta}{\\sqrt{E[g_t^2]_t+\\epsilon}}\\cdot g_t$$</p>\n<ul>\n<li>其实RMSprop依然依赖于全局学习率 $\\eta$</li>\n<li>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</li>\n<li>适合处理非平稳目标(包括季节性和周期性)——对于RNN效果很好</li>\n</ul>\n<h1 id=\"Adaptive-Moment-Estimation（Adam）\"><a href=\"#Adaptive-Moment-Estimation（Adam）\" class=\"headerlink\" title=\"Adaptive Moment Estimation（Adam）\"></a>Adaptive Moment Estimation（Adam）</h1><p>其实有了前面的方法，Adam和Nadam的出现就很理所当然的了，因为它们结合了前面方法的一阶动量和二阶动量。我们看到，SGD-M和NAG在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量，参数更新公式如下（按照最开始总结的计算框架）：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2\\cdot V_{t} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>通常情况下，默认值为$\\beta_1=0.9$、$\\beta_2=0.999$ 和 $\\epsilon=10^{-8}$，Adam通常被认为对超参数的选择相当鲁棒，特点如下：</p>\n<ul>\n<li>Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。</li>\n<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>\n<li>为不同的参数计算不同的自适应学习率</li>\n<li>也适用于大多非凸优化问题——适用于大数据集和高维空间。</li>\n</ul>\n<h1 id=\"AdaMax\"><a href=\"#AdaMax\" class=\"headerlink\" title=\"AdaMax\"></a>AdaMax</h1><p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围，即使用无穷范式，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+(1-\\beta_1)\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2^\\infty\\cdot V_{t} + (1-\\beta_2^\\infty)(\\triangledown_{\\theta_t} J(\\theta_t))^\\infty=max(\\beta_2\\cdot V_t, |\\triangledown_{\\theta_t}J(\\theta_t)|)$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>通常情况下，默认值为$\\beta_1=0.9$、$\\beta_2=0.999$ 和 $\\eta=0.002$</p>\n<h1 id=\"Nadam\"><a href=\"#Nadam\" class=\"headerlink\" title=\"Nadam\"></a>Nadam</h1><p>其实如果说要集成所有方法的优点于一身的话，Nadam应该就是了，Adam遗漏了啥？没错，就是Nesterov项，我们在Adam的基础上，加上Nesterov项就是Nadam了，参数更新公式如下：<br>$$m_{t+1}=\\beta_1\\cdot m_{t}+\\frac{(1-\\beta_1)}{(1-\\beta_1^t)}\\cdot \\triangledown_{\\theta_t} J_i(\\theta_t)$$      $$V_{t+1} = \\beta_2\\cdot V_{t} + (1-\\beta_2)(\\triangledown_{\\theta_t} J(\\theta_t))^2$$     $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_{t+1}}{\\sqrt{V_{t+1}+\\epsilon}}$$</p>\n<p>可以看出，Nadam对学习率有更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在使用带动量的RMSprop或Adam的问题上，使用Nadam可以取得更好的结果。</p>\n<p>来张直观的动态图展示上述优化算法的效果：</p>\n<ul>\n<li>下图描述了在一个曲面上，6种优化器的表现：<br><img src=\"https://img-blog.csdnimg.cn/20210111213136496.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>下图在一个存在鞍点的曲面，比较6中优化器的性能表现：<br><img src=\"https://img-blog.csdnimg.cn/20210111212630400.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>下图图比较了6种优化器收敛到目标点（五角星）的运行过程<br><img src=\"https://img-blog.csdnimg.cn/20210111212612731.gif#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>那种优化器最好？该选择哪种优化算法？目前还没能够达达成共识。Schaul et al (2014)展示了许多优化算法在大量学习任务上极具价值的比较。虽然结果表明，具有自适应学习率的优化器表现的很鲁棒，不分伯仲，但是没有哪种算法能够脱颖而出。</p>\n<p>目前，最流行并且使用很高的优化器（算法）包括SGD、具有动量的SGD、RMSprop、具有动量的RMSProp、AdaDelta和Adam。在实际应用中，选择哪种优化器应结合具体问题；同时，也优化器的选择也取决于使用者对优化器的熟悉程度（比如参数的调节等等）。</p>\n<ul>\n<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>\n<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>\n<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>\n<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>\n<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>\n<li>如果验证损失较长时间没有得到改善，可以停止训练。</li>\n<li>添加梯度噪声（高斯分布$N(0,\\sigma_t^2)$）到参数更新，可使网络对不良初始化更加健壮，并有助于训练特别深而复杂的网络。</li>\n</ul>\n<p><em>参考文献</em>：</p>\n<ul>\n<li><a href=\"https://ruder.io/optimizing-gradient-descent/\">An overview of gradient descent optimization algorithms</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/22252270\">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></li>\n<li><a href=\"https://github.com/snnclsr/visualize_optimizers\">visualize_optimizers</a></li>\n<li><a href=\"https://lossfunctions.tumblr.com/\">lossfunctions</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/55150256\">优化算法Optimizer比较和总结</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32230623\">一个框架看懂优化算法之异同 SGD/AdaGrad/Adam</a></li>\n<li><a href=\"https://www.cnblogs.com/guoyaohua/p/8542554.html\">深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）</a></li>\n<li><a href=\"https://blog.csdn.net/weixin_40170902/article/details/80092628\">机器学习：各种优化器Optimizer的总结与比较</a></li>\n<li><a href=\"https://blog.csdn.net/muyu709287760/article/details/62531509#%E4%B8%89%E7%A7%8Dgradient-descent%E5%AF%B9%E6%AF%94\">optimizer优化算法总结</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["深度学习","TensorFlow","机器学习","梯度下降","优化算法","Optimizer"]},{"title":"论文阅读笔记：大名鼎鼎的BERT模型","url":"/Paper-Reading/1fc44f71c5c6/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>原文链接：<a href=\"https://arxiv.org/pdf/1810.04805.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>本文介绍的language representation model就是大名鼎鼎的BERT，其模型结构是利用Transformer的双向Encoder表示。BERT有个明显的特点就是，它通过在所有层的左侧和右侧上下文中共同进行条件预处理，从而在未标记的文本中预训练深层双向表示。</p>\n<p>现有两种将预训练语言表示应用于下游任务的策略：<strong>feature-based</strong>和<strong>fine-tuning</strong>，基于特征的方法，比较典型的就是ELMo，使用特定于任务的架构，其中包括预训练的表示形式作为附加功能。微调的方式，众所周知的是GPT了，这种方式通过引入最少的特定于任务的参数，并通过简单地微调所有预训练的参数来对下游任务进行训练。两种方法在预训练过程中具有相同的目标功能，它们使用<strong>单向语言模型</strong>学习通用语言表示形式。而BERT作为预训练模型，就是定位于通过微调的方式，不过和其他模型有个区别就是（前面我加粗字体），BERT是双向结合上下文的架构，论文里面说双向有利于句子级任务。</p>\n<p>BERT的特别之处：</p>\n<ul>\n<li>BERT受到完型填空任务的启发，通过使用一个“masked language model”(MLM)预训练目标来减轻上面提到的单向约束问题。MLM随机masks掉input中的一些tokens，目标是从这些tokens的上下文中预测出它们在原始词汇表中的id。与从左到右的语言模型预训练不同，MLM目标使表示形式能够融合左右上下文，这使我们可以预训练深度双向Transformer。</li>\n<li>除了MLM，作者还使用了一个“next sentence prediction”任务，连带的预训练text-pair表征</li>\n</ul>\n<h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><ul>\n<li>深度双向：深度双向和浅度双向的区别在于，后者仅仅是将分开训练好的left-to-right和right-to-left的表征简单的串联，而前者是一起训练得到的。</li>\n<li>feature-based: 又称feature-extraction 特征提取。就是用预训练好的网络在新样本上提取出相关的特征，然后将这些特征输入一个新的分类器，从头开始训练的过程。也就是说在训练的过程中，网络的特征提取层是被冻结的，只有后面的密集链接分类器部分是可以参与训练的。</li>\n<li>fine-tuning: 微调。和feature-based的区别是，训练好新的分类器后，还要解冻特征提取层的顶部的几层，然后和分类器再次进行联合训练。之所以称为微调，就是因为在预训练好的参数上进行训练更新的参数，比预训练好的参数的变化相对小，这个相对是指相对于不采用预训练模型参数来初始化下游任务的模型参数的情况。也有一种情况，如果你有大量的数据样本可以训练，那么就可以解冻所有的特征提取层，全部的参数都参与训练，但由于是基于预训练的模型参数，所以仍然比随机初始化的方式训练全部的参数要快的多。对于作者团队使用BERT模型在下游任务的微调时，就采用了解冻所有层，微调所有参数的方法。</li>\n<li>warmup:学习率热身。规定前多少个热身步骤内，对学习率采取逐步递增的过程。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1></li>\n<li><strong>无监督的基于特征的方法</strong>：ELMo和它的前身从不同的维度概括了传统的词嵌入研究。它们从left-to-right和right-to-left语言模型中提取上下文敏感的特征。每个token(单词、符号等)的上下文表征是通过<strong>串联left-to-right和right-to-left</strong>的表征得到的。Melamud等人在2016年提出了使用LSTMs模型通过一个预测单词左右上下文的任务来学习上下文表征。与ELMo类似，他们的模型也是基于feature-based方法，并且没有深度双向</li>\n<li><strong>无监督的微调方法</strong>：与feature-based方法一样，该方向刚开始只是在未标记的文本上预训练词嵌入参数(无监督学习)。最近，句子和文档等生成上下文token表征的编码器已经从未标记的文本中预训练出来，并且通过fine-tuned的方式用在下游任务中。</li>\n<li><strong>监督数据的迁移学习</strong>：也有工作展示了从大数据集的监督任务的做迁移学习的有效性，就像自然语言推理(NLI)，和机器翻译。</li>\n</ul>\n<h1 id=\"具体实现结构\"><a href=\"#具体实现结构\" class=\"headerlink\" title=\"具体实现结构\"></a>具体实现结构</h1><p>框架分为两个步骤：</p>\n<ul>\n<li>pre-training</li>\n<li>fine-tuning</li>\n</ul>\n<p>在预训练期间，BERT模型在不同任务的未标记数据上进行训练。微调的时候，BERT模型用预训练好的参数进行初始化，并且是基于下游任务的有标签的数据来训练的。问答领域的例子作为本节一个运行示例，如下：<br><img src=\"https://img-blog.csdnimg.cn/2020102911482744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>BERT的一个特性是其跨不同任务的统一体系结构，即在预训练架构和下游的架构之间的差异很小。BERT是基于原始Transformer实现中的Encoder（Transformer的那篇文章我也有阅读笔记，分别在<a href=\"https://dengbocong.blog.csdn.net/article/details/108639979\">CSDN</a>和<a href=\"https://zhuanlan.zhihu.com/p/250946855\">知乎</a>发布），改造成多层双向Transformer Encoder。<strong>BERT Transformer使用双向self-attention，而GPT Transformer 使用带约束的self-attention，每个token只能注意到它左边的上下文。</strong></p>\n<blockquote>\n<p>本文中，$L$表示层数，$H$表示每个隐藏单元的维数大小，$A$表示self-attention头数。BERT有2种参数，分别是BERT($base$，$L=12$, $H=768$, $A=12$, $Total Parameters=110M$)和BERT($large$，$L=24$, $H=1024$, $A=16$, $Total Parameters=340M$)。</p>\n</blockquote>\n<p>使用BERT做各种下游任务，输入表征可以在一个token序列里清楚的表示一个句子或者一对句子(比如&lt;Question,Answer&gt;)。在整个工作中，“句子”可以是任意连续的文本范围，而不是实际的语言句子。论文中BERT关于输入输出的操作：</p>\n<ul>\n<li>使用大小为30000的WordPiece作为词嵌入层</li>\n<li>句子对打包成一个序列</li>\n<li>每个序列的首个token总是一个特定的classification token([CLS])，这个token对应的最后的隐藏状态被用作分类任务的聚合序列表征。</li>\n<li>区分句子对中句子的方法有两种：<ul>\n<li>通过分隔符[SEP]</li>\n<li>模型架构中添加了一个经过学习的嵌入(learned embedding)到每个token，以表示它是属于句子A或者句子B。</li>\n</ul>\n</li>\n</ul>\n<p>如上面图1中，$E$表示输入的词嵌入，$C$表示最后隐藏层的[CLS]的向量，$T_i$ 表示第 $i$ 个输入token在最后隐藏层的向量。对于给定的token，其输入表示是通过将相应的token，segment和position embeddings求和而构造的，如下图。<br><img src=\"https://img-blog.csdnimg.cn/2020102913010227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Pre-training-BERT\"><a href=\"#Pre-training-BERT\" class=\"headerlink\" title=\"Pre-training BERT\"></a>Pre-training BERT</h3><ul>\n<li><p> <strong>Masked LM（MLM–掩盖式语言模型–Cloze任务）</strong>：本文作者直觉上认为，深层双向模型比left-to-right模型或left-to-right模型和right-to-left模型的浅层连接更强大。标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件将使模型“间接的看到自己”。为了训练深度双向表示，做法是简单的随机mask一些百分比的输入tokens，然后预测那些被mask掉的tokens。在实验中，作者为每个序列随机mask掉了15%的WordPiece tokens。注意了，<strong>BERT只预测被mask掉的词，而不是重建完整的输入</strong>。但由于[MASK]符号作为token不会出现在微调阶段，所以要想办法让那些被mask掉的词的原本的表征也被模型学习到，所以这里需要采用一些策略，如下：<br><img src=\"https://img-blog.csdnimg.cn/20201029193110597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p><strong>Next Sentence Prediction（NSP）</strong>：许多下游任务，比如问答，自然语言推理等，需要基于对两个句子之间的关系的理解，而这种关系不能直接通过语言建模来获取到。为了训练一个可以理解句子间关系的模型，做法是为一个二分类的下一个句子预测任务进行了预训练。特别是，当为每个预测样例选择一个句子对A和B，50%的时间B是A后面的下一个句子(标记为IsNext)， 50%的时间B是语料库中的一个随机句子(标记为NotNext)。在图1中，C用来预测下一个句子（NSP）。尽管简单，但是该方法QA和NLI任务都非常有帮助。<br><img src=\"https://img-blog.csdnimg.cn/20201029210744426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</li>\n<li><p><strong>Pre-training data</strong>：预训练语料使用了BooksCorpus(800M words)，English Wikipedia(2500M words) 。之所以使用这种文档级语料而不是使用语句级语料，就是为了提取长连续序列。</p>\n</li>\n</ul>\n<h3 id=\"Fine-tuning-BERT\"><a href=\"#Fine-tuning-BERT\" class=\"headerlink\" title=\"Fine-tuning BERT\"></a>Fine-tuning BERT</h3><p>对于涉及到文本对的应用，常见的模式是先分辨编码文本对中的文本，然后应用双向交叉的注意力。BERT使用self-attention机制统一了这两个步骤，BERT使用self-attention编码一个串联的文本对，其过程中就包含了2个句子之间的双向交叉注意力。</p>\n<ul>\n<li>输入端：句子A和句子B可以是：（1）释义句子对（2）假设条件句子对（3）问答句子对 （4）文本分类或序列标注中的text-∅对。</li>\n<li>输出端：token表征喂给一个针对token级别的任务的输出层，序列标注和问答是类似的，而[CLS]表征喂给一个分类器输出层，比如情感分析。</li>\n</ul>\n<p>BERT在不同任务上微调的图解<br><img src=\"https://img-blog.csdnimg.cn/20201029195812498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><h3 id=\"GLUE\"><a href=\"#GLUE\" class=\"headerlink\" title=\"GLUE\"></a>GLUE</h3><p>在所有的GLUE任务上，作者使用了$batch-size=32,epochs=3$。对于每个任务，都通过开发集的验证来选择了最佳的微调学习率(在$5e- 5$，$4e - 5$，$3e -5$和$2e-5$之间)。另外，对于BERT的large模型，作者发现微调有时候在小数据集上不稳定，所以随机重启了几次，并选择了开发集上表现最佳的模型。<br><img src=\"https://img-blog.csdnimg.cn/20201029201521980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>在模型架构方面，除了注意力掩盖之外，BERT base版本的模型架构和OpenAI GPT几乎相同。</li>\n<li>BERT large 版本明显比base版本要表现的更好。</li>\n</ul>\n<h3 id=\"SQuAD-v1-1\"><a href=\"#SQuAD-v1-1\" class=\"headerlink\" title=\"SQuAD v1.1\"></a>SQuAD v1.1</h3><p>标准的SQuAD v1.1是一个100k的问答对集合，给定一个问题和一篇短文，以及对应的答案，任务是预测出短文中的答案文本span。图1所示，在问答任务中，作者将输入问题和短文表示成一个序列，其中，使用A嵌入表示问题，B嵌入表示短文。在微调的时候，作者引入一个start向量S，和一个end向量E，维数都为H。</p>\n<ul>\n<li>answer span的起始词 $i$ 的概率计算公式（答案末尾词的概率表示原理一样。）：$P_i=\\frac{e^{S\\cdot T_i}}{\\sum_je^{S\\cdot T_j}}$。</li>\n<li>位置 $i$ 到位置 $j$ 的候选span的分数定义如下：$S\\cdot T_i+E\\cdot T_j$。</li>\n</ul>\n<p>并将满足 $j&gt;i$ 的最大得分的span最为预测结果。训练目标是正确的开始和结束位置的对数似然估计的和。<br><img src=\"https://img-blog.csdnimg.cn/20201029204255261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"SQuAD-2-0\"><a href=\"#SQuAD-2-0\" class=\"headerlink\" title=\"SQuAD 2.0\"></a>SQuAD 2.0</h3><p><img src=\"https://img-blog.csdnimg.cn/20201029204512621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"SWAG\"><a href=\"#SWAG\" class=\"headerlink\" title=\"SWAG\"></a>SWAG</h3><p>The Situations With Adversarial Generations (SWAG)数据集包含113k个句子对完整示例，用于评估基于常识的推理。给定一个句子，任务是从四个选项中选择出最有可能是对的的continuation(延续/扩展)。在微调的时候，作者构造了4个输入序列，每个包含给定句子A的序列和continuation(句子B)。引入的唯一特定于任务的参数是一个向量，它与[CLS]token做点积，得到每个选项的分数，该分数会通过一个softmax层来归一化。<br><img src=\"https://img-blog.csdnimg.cn/20201029204522481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Ablation-Studies（消融研究）\"><a href=\"#Ablation-Studies（消融研究）\" class=\"headerlink\" title=\"Ablation Studies（消融研究）\"></a>Ablation Studies（消融研究）</h1><p>简单而言就是通过控制变量法证明算法的有效性。</p>\n<h3 id=\"预训练任务的影响\"><a href=\"#预训练任务的影响\" class=\"headerlink\" title=\"预训练任务的影响\"></a>预训练任务的影响</h3><p>通过去掉NSP后，对比BERT的双向表征和Left-to-Right表征，作者得证明了有NSP更好，且双向表征更有效。通过引入一个双向的LSTM，作者证明了BILSTM比Left-to-Right能得到更好的结果，但是仍然没有BERT的base版本效果好，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201029205352261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>ELMo是分别训练LTR和RTL的方式，以下列举了其不如BERT的地方：</p>\n<ul>\n<li>ELMo这种分别训练在串联的方式比单个双向模型代价高的两倍</li>\n<li>对于QA这样的任务而言不直观，因为RTL模型将无法确定问题的答案</li>\n<li>绝对不如深度双向模型强大，因为深度双向模型可以在每一层使用左右上下文</li>\n</ul>\n<h3 id=\"模型大小的影响\"><a href=\"#模型大小的影响\" class=\"headerlink\" title=\"模型大小的影响\"></a>模型大小的影响</h3><p>论文中训练了一些不同层数、隐藏单元数、注意力头的BERT模型，但使用相同的超参数和训练过程。<br><img src=\"https://img-blog.csdnimg.cn/20201029205942994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"训练步数的影响\"><a href=\"#训练步数的影响\" class=\"headerlink\" title=\"训练步数的影响\"></a>训练步数的影响</h3><p>从如下图总结出，BERT是需要巨大的预训练量级(128,000 words/batch * 1000,000 steps)进行训练的。还有就是虽然MLM预训练收敛速度比LTR慢（因为每个batch中只有15%的单词被预测，而不是所有单词都参与），但是准确度超过了LTR模型。<br><img src=\"https://img-blog.csdnimg.cn/20201029211329960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"不同Masking过程的影响\"><a href=\"#不同Masking过程的影响\" class=\"headerlink\" title=\"不同Masking过程的影响\"></a>不同Masking过程的影响</h3><p><img src=\"https://img-blog.csdnimg.cn/20201029211959996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"BERT用于feature-based方法\"><a href=\"#BERT用于feature-based方法\" class=\"headerlink\" title=\"BERT用于feature-based方法\"></a>BERT用于feature-based方法</h3><p>相对于fine-tuning的方式而言，feature-based的方式也有着其关键的优势。首先，不是所有的任务都可以轻易的表示成Trasformer encoder 架构，所以会有需要添加一个基于特定任务的模型架构的需求。其次，预先计算一次训练数据的昂贵表示，然后在此表示之上使用更便宜的模型运行许多实验，这对计算有很大的好处。<br><img src=\"https://img-blog.csdnimg.cn/20201029210454454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>BERT的大名想必早就知道了，了解BERT相关的结构以及威力还有有必要的，必备不时之需。BERT和GPT相反，使用的Transformer的Encoder（可见Transformer有多牛逼）。BERT的主要贡献是进一步将这些发现推广到深层双向架构，使得相同的预训练模型可以成功应对一组广泛的NLP任务。附录的相关信息我也直接穿插在上面各节的论述中，然后最后补充一下一个BERT,ELMo,OpenAI GPT模型架构对比图<br><img src=\"https://img-blog.csdnimg.cn/20201029210915719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<ul>\n<li>BERT使用了双向的Transformer架构</li>\n<li>OpenAI GPT使用了left-to-right的Transformer</li>\n<li>ELMo分别使用了left-to-right和right-to-left进行独立训练，然后将输出拼接起来，为下游任务提供序列特征<br>上面的三个模型架构中，只有BERT模型的表征在每一层都联合考虑到了左边和右边的上下文信息。</li>\n</ul>\n<p>除了架构不同，另外的区别在于BERT和OpenAI GPT是基于fine-tuning的方法，而ELMo是基于feature-based的方法。</p>\n","categories":["Paper-Reading"],"tags":["深度学习","Transformer","NLP","BERT"]},{"title":"论文阅读笔记：大模型指导小模型--ProjectionNet的联合框架","url":"/Paper-Reading/86116a512b8e/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections<br>原文链接：<a href=\"https://arxiv.org/pdf/1708.00630.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>众所周知，深度学习网络通常很大，包括涉及许多层的参数，并经过大量数据训练，以学习可用于在推理时预测输出的有用表示形式，所以为了达到高效率，我们使用分布式计算达到目标，这将需要多个CPU内核或图形处理单元（GPU）。随着移动端设备的普及，我们自然而然的想将这些模型应用到移动端设备上，但是，与在云上运行的高性能群集不同，这些设备在低功耗模式下运行，并且存在显着的内存限制，所以如果还是使用老方法，完全是行不通的。</p>\n<p>即使将模型部署在云端，通过网络连接的方式进行使用，也会涉及到连接性问题（数据无法发送到服务器）或隐私原因（某些数据类型和处理需要限制在某些范围内），而且在许多实际场景中，将计算密集型操作从设备委派给云并不可行。还有就是模型压缩，降低浮点精度等等手段去缩减模型体积，其实在某些情况下，达不到应用场景的精度需要。所以需要有一个学习高效，具有低内存占用量的设备上机器学习模型的能力，这些模型可以直接在设备上运行以进行推理，并且计算成本较低</p>\n<p>论文中介绍了一种叫ProjectionNet的联合框架，可以为不同机器学习模型架构训练轻量的设备端模型。其使用复杂的前馈/循环架构（就像 LSTM）作为训练模型，联合一个简单的投影（projection）架构——其中包含动态投影操作以及一些窄带全连接层。整个架构使用反向传播在 TensorFlow 上进行端到端训练，在训练完成后，我们就可以直接使用紧凑的 ProjectionNet 进行推理了。通过这种方法，我们可以训练尺寸很小的 ProjectionNet 模型，兼顾小尺寸（比常规模型小几个数量级）与高性能，在一些视觉和语言分类任务中达到满意的效果。</p>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>许多相关工作通过在有限的大小或内存限制下学习有效的模型，比如简单的字典查找、特征修剪或散列的神经网络压缩的技术，以及降低的数字精度、矢量量化、网络的二值化策略、权重共享来实现神经网络的紧凑表示。这些方法大多数旨在通过使用低秩分解或哈希技巧对连接进行分组来利用网络权重中的冗余。</p>\n<p>相反，本论文中建议学习一个简单的基于投影（projection）的网络，该网络可以有效地编码中间网络表示形式（即隐藏单元）和所涉及的操作，而不是权重。同时还为设备模型引入了新的训练范例，其中简单网络经过耦合和联合训练可以模仿现有的深度网络，而且该深度网络非常灵活，可以根据不同的体系结构或任务进行自定义。</p>\n<h1 id=\"神经投影网络（Neural-Projection-Networks）\"><a href=\"#神经投影网络（Neural-Projection-Networks）\" class=\"headerlink\" title=\"神经投影网络（Neural Projection Networks）\"></a>神经投影网络（Neural Projection Networks）</h1><h2 id=\"ProjectionNets\"><a href=\"#ProjectionNets\" class=\"headerlink\" title=\"ProjectionNets\"></a>ProjectionNets</h2><p>神经网络是一类非线性模型，用于学习从输入 $\\vec{x}_i$ 到输出 $y_i$ 的映射，其中 $\\vec{x}_i$ 表示输入特征向量或序列（在递归神经网络的情况下），而 $y_i$ 是分类任务的输出类别或预测的序列。通常，这些网络由多层隐藏的单元或神经元组成，并在一对层之间建立连接。例如，在完全连接的前馈神经网络中，经过训练的加权连接或网络参数的数量为 $O(n^2)$，其中 $n$ 是每层隐藏单元的数量。而论文提出的联合优化框架，该架构结合了projection网络和trainer网络进行联合训练。如下图：<br><img src=\"https://img-blog.csdnimg.cn/2020111520400533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上图说明了神经投影网络架构，使用前馈NN作为trainer网络，这个耦合网络进行联合训练以优化组合损耗函数（公式1）：<br>$$L(\\theta,p)=\\lambda_1\\cdot L_\\theta(.)+\\lambda_2 \\cdot L^p(.)+\\lambda_3 \\cdot \\hat{L^p}(.)$$<br>其中$L_\\theta(.)$，$L^p(.)$和$\\hat{L^p}(.)$，对应于以下定义的两个网络的损失函数（公式2）：<br>$$L_\\theta(.)=\\sum_{i\\in N}D(h_\\theta(\\vec{x}_i), \\hat{y_i})$$  $$L^p(.)=\\sum_{i\\in N}D(h^p(\\vec{x}<em>i),h_\\theta(\\vec{x}_i))$$  $$\\hat{L^p}(.)=\\sum</em>{i\\in N}D(h^p(\\vec{x}_i), \\hat{y_i})$$<br>其中，$N$ 表示数据集中训练实例的数量， $\\vec{x}_i$ 表示前馈网络中的输入特征向量或RNN中的序列输入，而 $\\hat{y_i}$ 表示用于网络训练的真实输出类别。$h_\\theta(\\vec{x}_i)$，表示训练器网络中隐藏单元的参数化表示，将 $\\vec{x}_i$ 转换为输出预测 $y_i$，同样的，$h^p(\\vec{x}_i)$ 表示将输入转换为相应预测 $y_i^p$ 的投影网络参数。我们在两个网络的最后一层应用softmax激活来计算预测 $y_i$ 和 $y_i^p$。</p>\n<p>D表示距离函数，作为损失函数用于计算预测误差，误差分为三个部分：训练器预测误差，投影模拟误差和投影预测误差。减少第一个误差会得到更好的训练器网络，而减少后两个会反过来学习更好的投影网络，该网络更简单，但预测能力大致相同。实际上，我们在所有实验中对 $D(.)$ 使用交叉熵。</p>\n<p>对于等式2中的投影 $L^p$，我们遵循一种蒸馏方法来优化 $D(.)$ ，因为它已经显示出比仅在标签 $\\hat{y_i}$ 上训练的模型具有更好的泛化能力。$\\lambda_1$，$\\lambda_2$和 $\\lambda_3$ 是影响这些不同类型误差之间权衡的超参数，这些是在一个小的保留开发集上进行调整的，在我们的实验中，我们将它们设置为$\\lambda_1=1.0$，$\\lambda_2=0.1$和 $\\lambda_3=1.0$ 。</p>\n<ul>\n<li>**Trainer Network (θ)**：训练器模型是一个完整的神经网络（前馈，RNN或CNN），其选择灵活，取决于任务。图1演示了使用前馈网络的训练器，但可以与LSTM RNN（我们稍后介绍）或其他深度神经网络互换。对于图中所示的网络，层 $l_{k+1}$ 中 $h_\\theta(.)$ 的激活计算如下（公式3）：<br>$$A_{\\theta_{l_{k+1}}}=\\sigma(W_{\\theta_{l_{k+1}}}\\cdot A_{\\theta_{l_k}}+B_{\\theta_{l_{k+1}}})$$<br>其中，其中 $\\sigma$ 是除最后一个层以外应用于每一层的ReLU激活函数，$A$ 表示计算得出的隐藏单元的激活值。该网络中的 权重/偏差 参数 $W_\\theta$，$B_\\theta$ 的数量可以任意大，因为这只会在训练阶段使用，而这可以通过使用具有CPU或GPU的高性能分布式计算来有效地完成。</li>\n<li>**Projection Network ( p )**：投影模型是一个简单的网络，对一组有效的计算操作进行编码，这些操作将直接在设备上进行推断。模型本身定义了一组有效的“投影”函数 $\\mathbb{P}(\\vec{x}_i)$，将每个输入实例 $\\vec{x}_i$ 投影到不同的空间 $\\Omega_\\mathbb{P}$ ，然后在该空间中执行学习以将其映射到相应的输出 $y_i^p$。我们使用简化的投影网络，几乎没有操作，如图1所示。输入 $\\vec{x}_i$ 使用一系列 $T$ 投影函数 $\\mathbb{P}^1,…,\\mathbb{P}^T$ 进行转换，然后再进行单层激活（公式4和公式5）。<br>$$\\vec{x}_i^p=\\mathbb{P}^1(\\vec{x}_i),…,\\mathbb{P}^T(\\vec{x}_i)$$  $$y_i^p=softmax(W^p\\cdot \\vec{x}_i^p + B^p)$$<br>投影转换使用预先计算的参数化方法，即在学习过程中未对其进行训练，并且将其输出连接起来以形成用于后续操作的隐藏单元。在训练期间，较简单的投影网络将学习选择和应用特定的投影操作 $\\mathbb{P}^j$（通过激活），这些操作对于给定任务更具预测性。可以堆叠连接到该网络中间层的其他层，以实现投影的非线性组合。</li>\n</ul>\n<p>投影模型是与训练器共同训练的，并学会模仿整个训练器网络进行的预测，训练器预测网具有更多的参数，因此具有更大的预测能力。一旦学习完成，就从投影网络中提取变换函数 $\\mathbb{P}(.)$ 和相应的训练权重 $W^p$，$B^p$，以创建一个轻量级模型，并将其转移到设备。</p>\n<p>在我们的设置中，选择投影矩阵 $\\mathbb{P}$ 以及表示投影空间 $ΩP$ 会对计算成本和模型大小产生直接影响。我们建议利用局部敏感哈希（LSH）的修改版本，作为有效的随机投影方法来定义 $\\mathbb{P}(.)$ 。结合起来，我们使用 $1^d$ 表示 $\\Omega_\\mathbb{P}$ ，即网络的隐藏单元本身使用投影的位向量表示。与整个网络相比，这在参数的数量和大小方面都大大降低了内存占用量。我们在下面重点介绍此方法的一些关键属性：</p>\n<ul>\n<li>与典型的机器学习方法不同，不需要依靠预设的词汇表或特征空间，典型的机器学习方法采用较小的词汇表作为缩放机制。例如，LSTM RNN模型通常应用修剪，并在输入编码步骤中使用较小且固定大小的词汇表来降低模型的复杂性。</li>\n<li>所提出的学习方法可有效地缩放到大数据大小和高维空间。这对于涉及稀疏高维特征空间的自然语言应用程序特别有用。对于密集的特征空间（例如图像像素），可以在不依赖大量参数的情况下有效地近似现有操作（例如全连接层（甚至卷积））进行预测。在限制存储需求下，这种操作还可以与投影功能结合使用，以产生更复杂的投影网络。</li>\n<li> $\\mathbb{P}(x_i)$ 的计算与训练数据的大小无关。</li>\n<li>我们确保 $\\mathbb{P}(.)$ 可以高效地进行即时计算，以便在设备上进行推理。</li>\n</ul>\n<p>接下来，将更详细地描述投影方法和相关的操作。</p>\n<h2 id=\"局部敏感投影网络（Locality-Sensitive-Projection-Network）\"><a href=\"#局部敏感投影网络（Locality-Sensitive-Projection-Network）\" class=\"headerlink\" title=\"局部敏感投影网络（Locality Sensitive Projection Network）\"></a>局部敏感投影网络（Locality Sensitive Projection Network）</h2><p>前面描述的投影网络依赖于一组转换函数 $\\mathbb{P}$，这些函数将输入 $\\vec{x}_i$ 投影到隐藏的单位表示 $\\Omega_\\mathbb{P}$ 中。可以使用不同类型的函数来执行公式4中概述的投影操作。一种可能性是使用通过word2vec或类似技术得到预训练的特征嵌入矩阵，模型 $\\mathbb{P}$ 作为 $\\vec{x}_i$ 中特征的嵌入查找，然后进行诸如矢量平均之类的聚合操作。</p>\n<p>相反，我们在此步骤中采用了有效的随机投影方法，我们使用局部敏感哈希（LSH）来建模基础的投影操作，LSH通常用作诸如聚类之类的应用程序的降维技术。我们在Projection Nets中使用LSH的动机是，它允许我们将类似的输入 $\\vec{x}_i$ 或中间网络层投影到接近度量空间的隐藏单位向量中。这使我们能够转换输入并学习有效而紧凑的网络表示形式，该表示形式仅取决于数据的固有维数（即观察到的特征），而不是实例数或实际数据矢量的维数（即所有特征或词汇量）。我们通过二进制散列函数来实现。</p>\n<ul>\n<li><strong>定理1</strong>：对于 $\\vec{x}_i,\\vec{x}_j\\in \\mathbb{R}^n$ 和从 $\\mathbb{R}^n$ 上的球对称分布绘制的矢量 $\\mathbb{P}_k$，内积符号与矢量之间的角度 $\\measuredangle(\\vec{x}_i,\\vec{x}_j)$ 之间的关系可以表示为：<br>$$\\measuredangle(\\vec{x}_i,\\vec{x}_j)=\\pi Pr{sgn[&lt;\\vec{x}_i,\\mathbb{P}_k&gt;]\\neq sgn[&lt;\\vec{x}_j,\\mathbb{P}_k&gt;]}$$<br>此属性适用于简单的几何，即每当投影矩阵 $\\mathbb{P}$ 的行向量落入单位向量之间在 $\\vec{x}_i$ 和 $\\vec{x}_j$ 方向上的角度之内时，它们将产生相反的符号。与包含 $\\vec{x}_i\\vec{x}_j$ 的平面正交的任何投影矢量都将无效。由于可以使用内积来确定附近的参数表示形式，所以 $&lt;\\vec{x}_i,\\vec{x}_j&gt;=||\\vec{x}_i||\\cdot ||\\vec{x}_j||\\cdot cos\\measuredangle(\\vec{x}_i,\\vec{x}_j)$，因此我们可以通过使用矢量的签名来有效地建模和存储网络隐藏的激活单元矢量。</li>\n<li><strong>计算投影</strong>：遵循上述属性，我们反复使用二进制哈希，然后将 $\\mathbb{P}$ 中的投影向量应用于将输入 $\\vec{x}<em>i$ 转换为由 $\\mathbb{P}_k(\\vec{x}_i)\\in {0,1}^d$ 表示的二进制哈希表示，其中 $[\\mathbb{P}_k(\\vec{x}_i)] :=sgn[&lt;\\vec{x}_i,\\mathbb{P}_k&gt;]$ 。使得 d-bit 矢量表示，对应于每个投影行 $\\mathbb{P}</em>{k=1…d}$ 的一位。投影矩阵 $\\mathbb{P}$ 在训练和推理之前是固定的，请注意，我们无需显式存储随机投影矢量 $\\mathbb{P}_k$ ，因为我们可以使用哈希函数动态计算它们，而不用调用随机数生成器。此外，这还使我们能够执行在观察到的特征尺寸上呈线性的投影操作，而不是对高维数据而言可能过大的整体特征尺寸，从而节省了内存和计算成本。二进制表示很重要，因为与训练器网络相比，这使得投影网络参数表示的非常紧凑，从而大大减小了模型大小。注意，其他方法，例如量化或权重共享，可以叠加在此方法之上，以在内存减少方面提供较小的进一步收益。</li>\n<li><strong>投影参数</strong>：实际上，我们采用 $T$ 个不同的投影函数 $\\mathbb{P}^{j=1…T}$ ，如图1所示。每一个都产生 d-bit 向量，将其连接起来以形成等式4中的预测激活单元 $\\vec{x}_i^p$。$T$ 和 $d$ 取决于为 $\\mathbb{P}$ 指定的投影网络参数配置，并且可以调整为在预测质量和模型大小之间进行权衡。</li>\n</ul>\n<h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2><p>如前所述，我们使用紧凑的位单元来表示投影网络。在训练过程中，该网络学习在投影的位空间 $\\Omega_\\mathbb{P}$ 中彼此相邻的点沿相同方向移动梯度。梯度的方向和大小由可访问更多参数集和更复杂架构的训练器网络确定。使用反向传播对两个网络进行联合训练。尽管有联合优化目标，但在高性能CPU或GPU上进行分布式计算时，随机梯度下降可以有效地进行训练。经过训练后，这两个网络将解耦并用于不同的目的。 训练器模型可以部署在使用标准神经网络的任何地方。提取更简单的投影网络模型权重以及变换函数 $\\mathbb{P}(.)$ ，以创建轻量级模型，并将其推送到设备。</p>\n<ul>\n<li>复杂度：推理的总体复杂度为 $O(n\\cdot T\\cdot d)$，其中 $n$ 是观察到的特征大小（<strong>不是</strong>总词汇量大小），它在输入大小上呈线性关系，$d$ 是为每个投影矢量 $\\mathbb{P}_k$ 指定的LSH位数 ，$T$ 是 $\\mathbb{P}$ 中使用的投影函数的数量。在此设置中，投影推断模型所需的模型大小（根据参数的数量）和存储空间为 $O(T\\cdot d)$。</li>\n</ul>\n<p>作为位向量表示 $\\Omega_\\mathbb{P}$ 的替代，投影矩阵 $\\mathbb{P}$ 可以用来生成投影网络中隐藏单元的稀疏表示。每个 d-bit 块都可以编码为整数而不是位向量。这样会导致整体参数空间为 $O(T\\cdot 2^d)$（反而更大），但是对于实际学习的参数数量很少，并且可以通过有效的稀疏查找操作进行推理的应用程序仍然是有益的。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>下表显示了基线（MNIST手写数字识别）的结果以及与具有不同大小 $(T,d)$ 的ProjectionNet模型的比较。结果表明，小型的ProjectionNet的压缩率高达388x，可以实现92.3％的高精度，而内存占用量明显更大的基准可以达到98.9％。此外，ProjectionNet模型能够实现模型尺寸的进一步减小（最大2000x-3500x），同时对top-1的预测的精度约为70-80％，而对top-3的预测的精度约为90-94％。<br><img src=\"https://img-blog.csdnimg.cn/20201115230624884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表显示了ProjectionNets与基准（ CIFAR-100图像分类）之间的结果比较。 如前所述，此任务比MNIST更复杂，因此精度数较低。<br><img src=\"https://img-blog.csdnimg.cn/20201115231337199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表使用神经投影网络和基线进行语义意图分类的结果。表中显示，使用LSTM RNN训练的ProjectionNet达到82.3％的precision@ 1，与基线LSTM相比仅下降了15％，但减少了内存占用和计算量（与LSTM展开步骤相比）。<br><img src=\"https://img-blog.csdnimg.cn/20201115231834990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在MNIST的识别任务中，80-100个神经投影bits足以得到70-80％的精度解决该任务，将其增加到720bits可达到92.3％的精度，通过使用更深的投影进一步提高了精度网络。对于涉及序列输入的语义分类的语言任务，需要720个神经投影bits才能达到82.3％的top-1精度。</p>\n<p>下图显示了MNIST和CIFAR-100任务的图。 该图表明，可以使用简单的100位ProjectionNet进行MNIST分类，从而简洁地获得具有3-5M参数的3层前馈网络的预测能力（比率=〜0.8）。 需要恢复90％以上的基础深度网络质量。<br><img src=\"https://img-blog.csdnimg.cn/20201115232738807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文引入了一种新的神经投影方法来训练轻量级神经网络模型，从而以较低的计算和内存成本在设备上执行有效的推理。论文展示了这种方法在模型大小和深度网络体系结构变化方面的灵活性。第3节末尾讨论了该框架的一些可能的将来扩展。除了深度学习之外，还可以将此框架应用于其他类型的学习场景中的轻量级模型训练。例如，训练范例可以更改为半监督或无监督设置。可以修改训练器模型本身，以合并在图形或概率图形模型（而非深度神经网络）上定义的结构化损失函数。下图展示了使用图优化损失函数学习轻量模型的端到端投影图方法，可以使用大规模分布图算法甚至神经图方法有效地训练它们。<br><img src=\"https://img-blog.csdnimg.cn/20201115233402806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Paper-Reading"],"tags":["深度学习","神经网络","ProjectionNet","移动端模型","部署"]},{"title":"论文阅读笔记：看完也许能进一步了解Batch-Normalization","url":"/Paper-Reading/c766928db46f/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<br>原文链接：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>训练深度神经网络非常复杂，因为在训练过程中，随着先前各层的参数发生变化，各层输入的分布也会发生变化，导致调参工作要做的很小心，训练更加困难，论文中将这种现象称为“internal covariate shift”，而Batch Normalization正式用来解决深度神经网络中internal covariate shift现象的方法。有关covariate shift的内容，可以参阅我另一篇<a href=\"https://zhuanlan.zhihu.com/p/339719861\">论文阅读笔记</a>。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>Batch Normalization是在每个mini-batch进行归一化操作，并将归一化操作作为模型体系结构的一部分，使用BN可以获得如下的好处：</p>\n<ul>\n<li><strong>可以使用更大的学习率</strong>，训练过程更加稳定，极大提高了训练速度。</li>\n<li><strong>可以将bias置为0</strong>，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。</li>\n<li><strong>对权重初始化不再敏感</strong>，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差也会放缩同样的倍数，相除抵消。</li>\n<li><strong>对权重的尺度不再敏感</strong>。</li>\n<li><strong>深层网络可以使用sigmoid和tanh了</strong>，BN抑制了梯度消失。</li>\n<li><strong>Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。</strong></li>\n</ul>\n<p>我们从梯度计算开始看起，如在SGD中是优化参数 $\\theta$，从而最小化损失，如下公式：<br>$$\\theta=arg\\underset{\\theta}{min}\\frac{1}{N}\\sum_{i=1}^{N}l(x_i,\\theta)$$<br>其中，$x_1…x_N$是训练数据集。使用SGD，训练将逐步进行，并且在每个步骤中，我们考虑大小为 $m$ 的mini-batch，即$x_1…m$，通过计算$\\frac{1}{m}\\frac{\\partial(x_i,\\theta)}{\\partial\\theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算 $m$ 次效率更高。</p>\n<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p>\n<h1 id=\"BN之前的一些减少Covariate-Shift的方法\"><a href=\"#BN之前的一些减少Covariate-Shift的方法\" class=\"headerlink\" title=\"BN之前的一些减少Covariate Shift的方法\"></a>BN之前的一些减少Covariate Shift的方法</h1><p>对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除Internal Covariate Shift的不良影响。那么如何消除呢？考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数，但这样会弱化梯度下降步骤。</p>\n<p>例如：例如，考虑一个层，其输入u加上学习到的偏置 $b$，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\\hat x=x - E[x]$，$x = u+b$，$X={x_{1\\ldots N}}$ 是训练集上$x$ 值的集合，$E[x] = \\frac{1}{N}\\sum_{i=1}^N x_i$。如果梯度下降步骤忽略了 $E[x]$ 对 $b$的依赖，那它将更新$b\\leftarrow b+\\Delta b$，其中$\\Delta b\\propto -\\partial{\\ell}/\\partial{\\hat x}$。然后 $u+(b+\\Delta b) -E[u+(b+\\Delta b)] = u+b-E[u+b]$。因此，结合 $b$ 的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，$b$ 将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。在最初的实验中，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p>\n<p>总结而言就是使用白话来缓解ICS问题，白化是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：</p>\n<ul>\n<li>使得输入特征分布具有相同的均值与方差，其中PCA白化保证了所有特征分布均值为0，方差为1，而ZCA白化则保证了所有特征分布均值为0，方差相同。</li>\n<li>去除特征之间的相关性。</li>\n</ul>\n<p><strong>通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛。但是白话过程的计算成本太高，并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作，这未免过于奢侈。而且白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力，底层网络学习到的参数信息会被白化操作丢失掉。</strong></p>\n<h1 id=\"BN算法描述\"><a href=\"#BN算法描述\" class=\"headerlink\" title=\"BN算法描述\"></a>BN算法描述</h1><p>文中使用了类似z-score的归一化方式：每一维度减去自身均值，再除以自身标准差，由于使用的是随机梯度下降法，这些均值和方差也只能在当前迭代的batch中计算，故作者给这个算法命名为Batch Normalization。BN变换的算法如下所示，其中，为了数值稳定，$\\epsilon$ 是一个加到小批量数据方差上的常量。<br><img src=\"https://img-blog.csdnimg.cn/20201228154526445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以将上面的算法总结为两步：</p>\n<ul>\n<li><strong>Standardizatio</strong>n：首先对 $m$ 个 $x$ 进行Standardization，得到 zero mean unit variance的分布 $\\hat{x}$。</li>\n<li><strong>scale and shift</strong>：然后再对 $\\hat{x}$ 进行scale and shift，缩放并平移到新的分布 $y$，具有新的均值 $\\beta$ 方差 $\\gamma$。</li>\n</ul>\n<p>更形象一点，假设BN层有 $d$ 个输入节点，则 $x$ 可构成 $d\\times m$大小的矩阵 $X$，BN层相当于通过行操作将其映射为另一个 $d\\times m$ 大小的矩阵$Y$，如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228154941139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>将2个过程写在一个公式里如下：<br>$$y_i^{(b)}=BN(x_i)^{(b)}=\\gamma (\\frac{x_i^{(b)}-\\mu(x_i)}{\\sqrt{\\sigma(x_i)^2+\\epsilon}})+\\beta$$</p>\n<p>其中，$x_i^{(b)}$ 表示输入当前batch的b-th样本时该层i-th输入节点的值，$x_i$ 为 $[x_i^{(1)},x_i^{(2)},…,x_i^{(m)}]$ 构成的行向量，长度为batch size $m$，$\\mu$和$\\sigma$为该行的均值和标准差，$\\epsilon$ 为防止除零引入的极小量（可忽略），$\\gamma$和$\\beta$为该行的scale和shift参数，可知</p>\n<ul>\n<li>$\\mu$ 和 $\\sigma$ 为当前行的统计量，不可学习。</li>\n<li>$\\gamma$ 和 $\\beta$ 为待学习的scale和shift参数，用于控制 $y_i$ 的方差和均值。</li>\n<li>BN层中，$x_i$ 和 $x_j$ 之间不存在信息交流 $(i\\neq j)$</li>\n</ul>\n<p>可见，**无论xi原本的均值和方差是多少，通过BatchNorm后其均值和方差分别变为待学习的 $\\gamma$ 和 $\\beta$**。为什么需要 $\\gamma$ 和 $\\beta$ 的可训练参数？Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，单纯通过让每一层的输入分布均值为0，方差为1，而不做缩放和移位，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。</p>\n<blockquote>\n<p>在训练初期，分界面还在剧烈变化时，计算出的参数不稳定，所以退而求其次，<strong>在 $Wx+b$ 之后，ReLU激活层前面进行归一化</strong>。因为初始的 $W$ 是从标准高斯分布中采样得到的，而 $W$ 中元素的数量远大于 $x$，$Wx+b$ 每维的均值本身就接近 $0$、方差接近 $1$，所以在 $Wx+b$ 后使用Batch Normalization能得到更稳定的结果，如下图所示：</p>\n</blockquote>\n<p><img src=\"https://img-blog.csdnimg.cn/20201228154233138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Batch-Normalization的反向传播\"><a href=\"#Batch-Normalization的反向传播\" class=\"headerlink\" title=\"Batch Normalization的反向传播\"></a>Batch Normalization的反向传播</h1><p>讲反向传播之前，我们先来简单的写一下正向传递的代码，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">batchnorm_forward</span>(<span class=\"params\">x, gamma, beta, eps</span>):</span></span><br><span class=\"line\">    N, D = x.shape</span><br><span class=\"line\">    <span class=\"comment\"># 第一步：计算平均</span></span><br><span class=\"line\">    mu = <span class=\"number\">1.</span>/N * np.<span class=\"built_in\">sum</span>(x, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 第二步：每个训练样本减去平均</span></span><br><span class=\"line\">    xmu = x - mu</span><br><span class=\"line\">    <span class=\"comment\"># 第三步：计算分母</span></span><br><span class=\"line\">    sq = xmu ** <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"comment\"># 第四步：计算方差</span></span><br><span class=\"line\">    var = <span class=\"number\">1.</span>/N * np.<span class=\"built_in\">sum</span>(sq, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 第五步：加上eps保证数值稳定性，然后计算开方</span></span><br><span class=\"line\">    sqrtvar = np.sqrt(var + eps)</span><br><span class=\"line\">    <span class=\"comment\"># 第六步：倒转sqrtvar</span></span><br><span class=\"line\">    ivar = <span class=\"number\">1.</span>/sqrtvar</span><br><span class=\"line\">    <span class=\"comment\"># 第七步：计算归一化</span></span><br><span class=\"line\">    xhat = xmu * ivar</span><br><span class=\"line\">    <span class=\"comment\"># 第八步：加上两个参数</span></span><br><span class=\"line\">    gammax = gamma * xhat</span><br><span class=\"line\">    out = gammax + beta</span><br><span class=\"line\">    <span class=\"comment\"># cache储存计算反向传递所需要的一些内容</span></span><br><span class=\"line\">    cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out, cache</span><br></pre></td></tr></table></figure>\n<p>我们都知道，对于目前的神经网络计算框架，一个层要想加入到网络中，要保证其是可微的，即可以求梯度。BatchNorm的梯度该如何求取？反向传播求梯度只需抓住一个关键点，如果一个变量对另一个变量有影响，那么他们之间就存在偏导数，找到直接相关的变量，再配合链式法则，公式就很容易写出了。<br><img src=\"https://img-blog.csdnimg.cn/20201228161718394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>根据反向传播的顺序，首先求取损失 $l$ 对BN层输出 $y_i$ 的偏导 $\\frac{\\partial l}{\\partial y_i}$，然后是对可学习参数的偏导 $\\frac{\\partial l}{\\partial \\gamma}$ 和 $\\frac{\\partial l}{\\partial \\beta}$，用于对参数进行更新，想继续回传的话还需要求对输入 $x$ 偏导，于是引出对变量 $\\mu$、$\\sigma^2$ 和 $\\hat{x}$ 的偏导，根据链式法则再求这些变量对 $x$ 的偏导，计算图如下：<br><img src=\"https://img-blog.csdnimg.cn/20201228205034787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>通过链式法则，我们可以对上面的正向传递的代码进行运算，得到反向传播的代码，如下（结合代码理解更方便）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">batchnorm_backward</span>(<span class=\"params\">dout, cache</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 展开存储在cache中的变量</span></span><br><span class=\"line\">    xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache</span><br><span class=\"line\">    <span class=\"comment\"># 获得输入输出的维度</span></span><br><span class=\"line\">    N, D = dout.shape</span><br><span class=\"line\">    dbeta = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dgammax = dout</span><br><span class=\"line\">    dgamma = np.<span class=\"built_in\">sum</span>(dgammax * xhat, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dxhat = dgammax * gamma</span><br><span class=\"line\">    divar = np.<span class=\"built_in\">sum</span>(dxhat * xmu, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dxmu1 = dxhat * ivar</span><br><span class=\"line\">    dsqrtvar = -<span class=\"number\">1.</span>/(sqrtvar ** <span class=\"number\">2</span>) * divar</span><br><span class=\"line\">    dvar = <span class=\"number\">0.5</span> * <span class=\"number\">1.</span> / np.sqrt(var + eps) * dsqrtvar</span><br><span class=\"line\">    dsq = <span class=\"number\">1.</span> / N * np.ones((N, D)) * dvar</span><br><span class=\"line\">    dxmu2 = <span class=\"number\">2</span> * xmu * dsq</span><br><span class=\"line\">    dx1 = (dxmu1 + dxmu2)</span><br><span class=\"line\">    dmu = -<span class=\"number\">1</span> * np.<span class=\"built_in\">sum</span>(dxmu1+dxmu2, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    dx2 = <span class=\"number\">1.</span> / N * np.ones((N, D)) * dmu</span><br><span class=\"line\">    dx = dx1 + dx2</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>\n<h1 id=\"Batch-Normalization的预测阶段\"><a href=\"#Batch-Normalization的预测阶段\" class=\"headerlink\" title=\"Batch Normalization的预测阶段\"></a>Batch Normalization的预测阶段</h1><p>在预测阶段，所有参数的取值是固定的，对BN层而言，意味着$\\mu$、$\\sigma$、$\\gamma$、$\\beta$ 都是固定值。$\\gamma$和$\\beta$ 比较好理解，随着训练结束，两者最终收敛，预测阶段使用训练结束时的值即可。对于 $\\mu$ 和 $\\sigma$，在训练阶段，它们为当前mini batch的统计量，随着输入batch的不同， $\\mu$ 和 $\\sigma$ 一直在变化。在预测阶段，输入数据可能只有1条，该使用哪个 $\\mu$ 和 $\\sigma$ ，或者说，每个BN层的 $\\mu$ 和 $\\sigma$ 该如何取值？可以采用训练收敛最后几批mini batch的 $\\mu$ 和 $\\sigma$ 的期望，作为预测阶段的 $\\mu$ 和 $\\sigma$ ，如下所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228162916396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>因为Standardization和scale and shift均为线性变换，在预测阶段所有参数均固定的情况下，参数可以合并成 $y=kx+b$ 的形式，如上图中行号11所示。</p>\n<p>这里多说一句，BN在卷积中使用时，1个卷积核产生1个feature map，1个feature map有1对 $\\gamma$和$\\beta$ 参数，同一batch同channel的feature map共享同一对 $\\gamma$和$\\beta$ 参数，若卷积层有 $n$ 个卷积核，则有 $n$ 对 $\\gamma$和$\\beta$ 参数。</p>\n<p>对于测试集均值和方差已经不是针对某一个Batch了，而是针对整个数据集而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录每一个Batch的均值和方差，以便训练完成之后按照下式计算整体的均值和方差：<br>$$E[x]\\leftarrow E_\\beta[\\mu_\\beta]$$    $$Var[x]\\leftarrow \\frac{m}{m-1}E_\\beta[\\sigma_\\beta^2]$$<br> 上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是行号11所示。</p>\n<h1 id=\"网络inference阶段conv层和BN层的融合\"><a href=\"#网络inference阶段conv层和BN层的融合\" class=\"headerlink\" title=\"网络inference阶段conv层和BN层的融合\"></a>网络inference阶段conv层和BN层的融合</h1><p>现在很多的网络结构都将BN层直接放在卷积层和激活层之间，这种做法可以在网络的inference阶段，将BN层的运算直接嵌入到卷积层中，减少运算量，提升网络的运行速度。在inference阶段，已知某层卷积层的kernel参数 $w$， $b$ ，以及输入 $x$ ，紧随其后的BN层参数（已学习到）：尺度参数 $\\gamma$ 、偏移参数 $\\beta$ 、以及样本均值 $\\hat{\\mu}$ 和标准差 $\\hat{\\sigma}$ 。</p>\n<ul>\n<li>卷积层输出为：$w<em>x+b$<br>bn层输出为： $\\gamma \\frac{w</em>x+b-\\hat{\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta$<br>bn层的输出可以化为如下形式：<br>$$\\gamma \\frac{w*x+b-\\hat{\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta=(\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}w)<em>x+\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}b-\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}\\hat{\\mu}+\\beta=kw</em>x+b^{‘}$$<br>所以，在inference阶段，如果BN直接跟在卷积层后，可以将BN层直接嵌入到卷积层的计算中，相当于将卷积核缩放一定倍数，并对偏置进行一定改变。</li>\n</ul>\n<p>将BN层融合到卷积层中，就相当于对卷积核进行一定的修改，并没有增加卷积层的计算量，同时整个BN层的计算量都省去了。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>下图是使用三层全连接层，在每层之后添加BN以及无添加的实验对比：<br><img src=\"https://img-blog.csdnimg.cn/20201228170635438.png#pic_center\" alt=\"在这里插入图片描述\"><br>下图是训练步和精度的实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201228172923853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图是使用BN在Inception上的相关实验结果：<br><img src=\"https://img-blog.csdnimg.cn/20201228195302919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"关于BN的几个讨论\"><a href=\"#关于BN的几个讨论\" class=\"headerlink\" title=\"关于BN的几个讨论\"></a>关于BN的几个讨论</h2><ul>\n<li>没有scale and shift过程可不可以？<br>BatchNorm有两个过程，Standardization和scale and shift，前者是机器学习常用的数据预处理技术，在浅层模型中，只需对数据进行Standardization即可，Batch Normalization可不可以只有Standardization呢？答案是可以，但网络的表达能力会下降。直觉上理解，浅层模型中，只需要模型适应数据分布即可。对深度神经网络，每层的输入分布和权重要相互协调，强制把分布限制在zero mean unit variance并不见得是最好的选择，加入参数 $\\gamma$和$\\beta$ ，对输入进行scale and shift，有利于分布与权重的相互协调，特别地，令 $\\gamma=1$和$\\beta=0$ 等价于只用Standardization，令 $\\gamma=\\sigma$和$\\beta=\\mu$ 等价于没有BN层，scale and shift涵盖了这2种特殊情况，在训练过程中决定什么样的分布是适合的，所以使用scale and shift增强了网络的表达能力。表达能力更强，在实践中性能就会更好吗？并不见得，就像曾经参数越多不见得性能越好一样。在<a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a>中，作者实验发现没有scale and shift性能可能还更好一些，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20201228195444340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li>BN层放在ReLU前面还是后面？实验表明，放在前后的差异似乎不大，甚至放在ReLU后还好一些（如上图），放在ReLU后相当于直接对每层的输入进行归一化，这与浅层模型的Standardization是一致的。<a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a>中有很多组合实验结果，可以看看。BN究竟应该放在激活的前面还是后面？以及，BN与其他变量，如激活函数、初始化方法、dropout等，如何组合才是最优？可能只有直觉和经验性的指导意见，具体问题的具体答案可能还是得实验说了算</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>Batch Normalization的加速作用体现在两个方面：一是归一化了每层和每维度的scale，所以可以整体使用一个较高的学习率，而不必像以前那样迁就小scale的维度；二是归一化后使得更多的权重分界面落在了数据中，降低了overfit的可能性，因此一些防止overfit但会降低速度的方法，例如dropout和权重衰减就可以不使用或者降低其权重。</p>\n<h1 id=\"写在最后\"><a href=\"#写在最后\" class=\"headerlink\" title=\"写在最后\"></a>写在最后</h1><p>BN层的有效性已有目共睹，但为什么有效可能还需要进一步研究，还需要进一步研究，这里整理了一些关于BN为什么有效的论文，贴在这：</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1805.11604.pdf\">How Does Batch Normalization Help Optimization?</a>：<strong>BN层让损失函数更平滑</strong>。<br>论文中通过分析训练过程中每步梯度方向上步长变化引起的损失变化范围、梯度幅值的变化范围、光滑度的变化，认为添加BN层后，损失函数的landscape(loss surface)变得更平滑，相比高低不平上下起伏的loss surface，平滑loss surface的梯度预测性更好，可以选取较大的步长。如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/20201228201948418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n<li><a href=\"https://arxiv.org/pdf/1612.04010.pdf\">An empirical analysis of the optimization of deep network loss surfaces</a>：<strong>BN更有利于梯度下降</strong>。<br>论文中绘制了VGG和NIN网络在有无BN层的情况下，loss surface的差异，包含初始点位置以及不同优化算法最终收敛到的local minima位置，如下图所示。没有BN层的，其loss surface存在较大的高原，有BN层的则没有高原，而是山峰，因此更容易下降。<br><img src=\"https://img-blog.csdnimg.cn/2020122820245825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></li>\n</ul>\n<p>参考文献：</p>\n<ul>\n<li><a href=\"https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\">caffenet-benchmark-batchnorm</a></li>\n<li><a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\">Understanding the backward pass through Batch Normalization Layer</a></li>\n<li><a href=\"https://abay.tech/blog/2018/07/01/why-does-batch-normalization-work/\">Why Does Batch Normalization Work?</a></li>\n<li><a href=\"https://www.cnblogs.com/shine-lee/p/11989612.html\">Batch Normalization详解</a></li>\n<li><a href=\"https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b\">Batch Normalization — What the hey</a></li>\n<li><a href=\"http://gradientscience.org/batchnorm/\">How does Batch Normalization Help Optimization?</a></li>\n<li><a href=\"https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/\">How does Batch Normalization Help Optimization?</a></li>\n</ul>\n","categories":["Paper-Reading"],"tags":["计算图","Covariate Shift","机器学习","Batch-Normalization"]},{"title":"论文阅读笔记：端到端可训练的面向任务的对话系统","url":"/Paper-Reading/79c2e935e8d0/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：A Network-based End-to-End Trainable Task-oriented Dialogue System<br>原文链接：<a href=\"https://arxiv.org/pdf/1604.04562.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和实现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>论文作者将对话建模成一个seq2seq的映射问题，该seq2seq框架以对话历史数据（通过belief tracker建模）和数据库查询结果（通过Database Operator得到结果）作为支撑。</p>\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>教会机器完成与人自然交流的任务是充满挑战性的，当前，开发面向任务的对话系统需要创建多个组件，通常这涉及大量的手工制作或获取昂贵的标记数据集以解决每个组件的统计学习问题。在这项工作中，我们介绍了基于神经网络的文本输入，文本输出的端到端可训练的面向目标的对话系统，以及一种基于pipeline的Wizard-of-Oz框架的收集对话数据的新方法。这种方法使我们能够轻松开发对话系统，而无需对手头的任务做太多假设。结果表明，该模型可以自然地与人类交谈，同时帮助他们完成餐馆搜索领域的任务。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>建立面向任务的对话系统（例如酒店预订或技术支持服务）很困难，因为它是针对特定应用的，并且训练数据的可用性通常有限。为了缓解这个问题，最近的面向任务的对话系统设计的机器学习方法已经将该问题作为部分可观察到的马尔可夫决策过程（POMDP）进行了研究，目的是使用强化学习（RL）进行训练，通过与真实用户的互动在线对话策略。然而，语言理解和语言生成模块仍然依赖于监督学习，因此需要语料库来训练。此外，为了使RL易于处理，必须仔细设计状态和动作空间，这可能会限制模型的表达能力和可学习性，而且训练此类模型所需的奖励方法难以设计且难以在运行时进行衡量。</p>\n<p>另一方面，从序列到序列学习激发了一些努力来构建端到端可训练的，非任务导向的对话系统。该系列方法将对话视为目标序列转导问题的来源，应用编码器网络将用户查询编码为代表其语义的分布矢量，然后使用解码器网络以生成每个系统响应，这些模型通常需要大量数据来训练。它们允许创建有效的聊天机器人类型的系统，但是它们缺乏支持领域特定任务的任何功能，例如，能够与数据库进行交互，并将有用的信息汇总到他们的系统中回应。</p>\n<p>在这项工作中，我们通过平衡两个研究方向的优势和劣势，为面向任务的对话系统提出了一种基于神经网络的模型。</p>\n<ul>\n<li>该模型是端到端可训练的，但仍模块化连接</li>\n<li>它并不能直接为用户目标建模，但是尽管如此，它仍然可以通过在每一步提供<strong>relevant</strong>且<strong>appropriate</strong>的响应来学习完成所需的任务</li>\n<li>它具有数据库（DB）属性（槽-值对）的显式表示形式，用于实现较高的任务成功率，但具有用户意图的分布表示（对话行为）允许模棱两可的输入</li>\n<li>并且它使用了词法分解和权重绑定策略来减少训练模型所需的数据，但是如果有更多数据可用，它仍然保持较高的自由度。</li>\n</ul>\n<h1 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h1><p>模型结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/2020100322492021.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在每个回合中，系统都会从用户那里获得token序列作为输入，并将其转换为两个内部表示形式：</p>\n<ul>\n<li>由 intent network生成的分布表示</li>\n<li>由一组belief trackers生成的称为belief state的槽值对上的概率分布</li>\n</ul>\n<p>然后数据库operator挑选belief state中最可能的值以形成对数据库的查询，策略网络将搜索结果、意图表示和信念状态进行转换和组合，以形成代表下一个系统动作的单个向量。然后，该系统动作向量用于调节响应生成网络，该网络以骨架(skeletal)形式中的token生成所需的系统token输出。然后，通过将数据库实体的实际值代入骨架句结构来形成最终的系统响应。</p>\n<blockquote>\n<p>具体而言，在每一轮对话中，通过Intent Network得到一个用户输入的向量表征，通过Belief Tracker得到一个slot-value的概率分布，随后database operator针对概率最大的slot-value在数据库中进行查询，得到的结果将会和Intent Network输出的向量表征以及Belief Tracker输出的slot-value分布概率共同输入给policy network，随后获得一个向量，该向量表征了该系统的下一个action，然后该action被输入到Generation Network中产生回复。</p>\n</blockquote>\n<p>每个组件的详细说明如下。</p>\n<h3 id=\"Intent-Network\"><a href=\"#Intent-Network\" class=\"headerlink\" title=\"Intent Network\"></a>Intent Network</h3><p>Intent Network可以看作是序列到序列学习框架中的编码器，其工作是在 $t$回合， 将输入tokens为 $w_0^t，w_1^t,…,w_N^t$ 的序列编码为分布向量表示 $z_t$。通常，使用长短期记忆（LSTM）网络，其中最后一个时间步中隐藏层 $z_t^N$ 被表示为：<br>$$z_t=z_t^N=LSTM(w_0^1,w_1^t,…w_N^t)$$<br>或者，可以使用卷积神经网络（CNN）代替LSTM作为编码器<br>$$z_t=CNN(w_0^1,w_1^t,…w_N^t)$$<br>本文中都进行探讨。</p>\n<h3 id=\"Belief-Trackers\"><a href=\"#Belief-Trackers\" class=\"headerlink\" title=\"Belief Trackers\"></a>Belief Trackers</h3><p><img src=\"https://img-blog.csdnimg.cn/20201004101353442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>Belief Tracker也叫做Dialogue State Tracker，它的详细结构如上图所示，包含两个主要结构：</p>\n<ul>\n<li>Delexicalised CNN</li>\n<li>Jordan-type RNN</li>\n</ul>\n<p>在Delexicalised CNN中（<strong>delexicalised主要指句子中同一类型的实体词都被替换成该类型的统一符号，以在slot类型中共享参数</strong>），在当前对话轮次 $t$ 的用户输入 $u$，以及上一轮次系统的回复 $m$，分别通过该CNN结构后的输出进行concatenation，只不过需要注意的是，在各自的CNN结构中，除了使用CNN的最终输出外，也会利用各自输入句子中被delexicalised处的浅层CNN特征（为了保证各层卷积的输出能够与输入句子长度对应一致，在每一层卷积输入的首尾两端进行padding），如果当前句子没有被delexicalised的词则进行padding。<br>$$f_{v,cnn}^t=CNN_{s,v}^{(u)}(u_t)⊕CNN_{s,v}^{(m)}(m_{t-1})$$<br>在Jordan-type RNN中可以看到，和普通RNN结构不同，Jordan-type RNN更为简单，没有输入层，直接将上一个时刻的输出，以及来自于Delexicalised CNN的输出进行concatenation后当做RNN的隐藏层状态，并通过softmax后得到当前时刻的输出，具体计算过程公式如下所示：<br>$$f_v^t=f_{v,cnn}^t⊕p_v^{t-1}⊕p_∅^{t-1}$$ $$g_v^t=w_s \\cdot sigmoid(W_sf_v^t+b_s)+b_s^{‘}$$ $$p_v^t=\\frac{exp(g_v^t)}{exp(g_{∅,s})+\\sum_{v’∈V_s}exp(g_{v’}^t)}$$</p>\n<p>其中concat到Jordan-type RNN隐藏层的除了CNN的输出外，还有两个概率，一个是上一轮的该槽位取值某个 $v$ 的概率大小，另一个是直到当前轮次 $t$ 时，用户还未提及该槽位，也可以用一个概率大小来表征，直接在第三个公式中利用分母中多余的一个参数代替普通的 $g(v)$ 计算即可（这样的话，该槽对应所有可能取值的概率之和，以及用户未提及该槽的概率，才能够使得所有概率之和为1）。</p>\n<p>特别需要注意的是，论文中采用的方法是，先针对每个具体的task构建一个本体库$G$，它是一个知识图谱，在这个知识图谱中，定义了该task下存在的各种可能槽位以及槽位对应的可能取值。而槽位分为两种类型：informable slot和requestable slot，前者是用户用来限定查询范围的一些信息（比如订餐task中的食物类型，价格区间等等），后者是用户想要咨询的信息（比如询问地址和电话等，那么地址和电话此时便是一个requestable slot）。此后针对该本体知识图谱$G$中的每一个槽位$s$，有两种处理办法：</p>\n<ul>\n<li>对于informable slot，每一个槽位$s$都有一个专门的Jordan type RNN进行跟踪。例如针对食物类型有一个专门的RNN进行跟踪，在跟踪过程中，每一个轮次$t$都会计算一次RNN在当前时刻$t$的输出，用以更新食物类型这个槽位上所有可能取值的概率分布</li>\n<li>对于requestable slot，因为不需要跟踪，并未使用RNN结构，然而原文未做详细解读，个人猜测就是每个时刻做一个简单的二分类，输出一个binary distribution，大概意思就是用户当前是否向系统请求了该槽位的信息</li>\n</ul>\n<h3 id=\"Database-Operator\"><a href=\"#Database-Operator\" class=\"headerlink\" title=\"Database Operator\"></a>Database Operator</h3><p>通过Belief Tracker后，可以针对所有informable slot的所有可能取值，通过下式形成一个查询语句 $q$（不过个人在这里有些疑问，按照下式的意思，大概是针对每一个槽位都取一个概率最大的值，并将所有informable slot全部合并形成一个 $query$，这样的话，岂不就会在每一轮的查询语句中，都包含了所有的informable slot，只不过每一轮的查询语句 $q$ 中各个槽位的具体取值不一样而已。如果是这样个人感觉不太合理，如果不是这样那是否公式的argmax应该放到取合集符号的外边来呢？），使用查询语句在数据库中进行查询后，会得到一个针对数据库中实体的一个 $01$ 向量（类似于bag-of-words中，该向量的每一位表示数据库中的某个实体，如果命中了数据库中的某个实体，则该位置1）。<br>$$\\bigcup_{s’\\in S_1 }{\\underset{v}{argmax}p_{s’}^t}$$<br>此外，如果查询结果只要有命中（即向量x不全为0），则这里还会维护一个DB pointer，它将随机指向一个命中的实体。并且根据每轮查询的结果进行动态更新。这个pointer主要是在后面生成网络中用来显示一个有意义的句子（因为生成的句子是类似于模板一样的，并没有显示具体的实体信息）。</p>\n<h3 id=\"Policy-network\"><a href=\"#Policy-network\" class=\"headerlink\" title=\"Policy network\"></a>Policy network</h3><p>在该模块中，实际上和强化学习中的policy action还有点不一样，这里的policy实际上就是一个融合另外三个模块，并输出的一个向量。公式如下：<br>$$o_t=tanh(W_{zo}z_t+W_{po}\\hat{p}<em>t+W</em>{xo}\\hat{x}_t)$$</p>\n<ul>\n<li>$z$ 便是intent network输出的向量。</li>\n<li>其中对于belief tracker模块的处理如下（也就是将各个informable slot的概率进行concatenation）$\\hat{p}<em>t=\\oplus</em>{s\\in G}\\hat{p}_s^t$，而针对每一个具体的slot，它也是一个向量，其由三部分组成：（该slot下各个可能取值的概率之和，用户表示对当前槽位无所谓的概率，用户到当前轮次并未提及该槽位的概率）。这里为什么要针对每一个slot下的各个可能取值的概率大小进行求和，就是因为在generation network中，对于槽位信息很重要，但是对于每个槽位下的可能取值则不重要（因为无论是输入的句子还是生成的句子都是delexicalised）</li>\n<li>对于database operator的输出而言，同样的，对于查询语句得到的结果能够查询到的实体个数很重要，但是具体查询到的是什么实体并不重要。因此最后 $x$ 便转化为了一个6位的one-hot向量，每一位分别表示没有查询到任意实体，查询结果命中一个实体，查询结果命中两个实体，…，查询结果命中等于或超过五个实体。</li>\n</ul>\n<h3 id=\"Generation-Network\"><a href=\"#Generation-Network\" class=\"headerlink\" title=\"Generation Network\"></a>Generation Network</h3><p>生成网络就是一个普通的decoder，只不过在输入中加入Policy Network输出的action vector，也就是向量 $o$，公式如下：<br>$$P(w_{j+1}^t|w_j^t,h_{j-1}^t,o_t)=LSTM_j(w_j^t,h_{j-1}^t,o_t)$$<br>在每一个时刻输出一个token，该token有三种可能：</p>\n<ul>\n<li>一个正常的词；</li>\n<li>一个delexicalised slot name，在最终输出的时候会将其替换为实际的一个有意义的词，比如&lt;s.food&gt;会替换为”food”或”type of food”；</li>\n<li>一个delexicalised slot value，在最终输出的时候会将其替换为在Database Operator中DB pointer维护的一个实体。</li>\n</ul>\n<h1 id=\"Wizard-of-Oz-Data-Collection\"><a href=\"#Wizard-of-Oz-Data-Collection\" class=\"headerlink\" title=\"Wizard-of-Oz Data Collection\"></a>Wizard-of-Oz Data Collection</h1><p>Wizard-of-Oz数据集搜集范式这个就不做描述和介绍了，应该已经很熟悉了。</p>\n<h1 id=\"Empirical-Experiment\"><a href=\"#Empirical-Experiment\" class=\"headerlink\" title=\"Empirical Experiment\"></a>Empirical Experiment</h1><ul>\n<li>使用交叉熵预先训练belief tracker的参数<br>$$L_1(\\Theta_b)=-{\\sum}_t{\\sum}_s(y_s^t)^Tlogp_s^t$$<br>其中，y代表真实label。对于完整的模型，我们有三个informable追踪器（食品，价格范围，面积）和七个requestable追踪器（地址，电话，邮编，姓名，以及三个slot）。</li>\n<li>在固定了tracker的参数之后，使用来自生成网络语言模型的交叉熵损失函数对模型的其余部分进行训练<br>$$L_1(\\Theta_{/b})=-{\\sum}_t{\\sum}_j(y_j^t)^Tlogp_j^t$$<br>其中$y_j^t$和$p_j^t$分别是在第 $t$ 轮decoder第 $j$ 步的时候输出的target token和预测 token。我们将每个对话视为一个批次，并使用随机梯度下降和小的l2正则化项来训练模型。</li>\n</ul>\n<p>收集的语料库按3：1：1的比例划分为训练，验证和测试集。early stopping是基于正则化的验证集来实现的，并且梯度裁剪被设置为1.所有隐藏层大小被设置为50，并且所有权重在-0.3和0.3之间被随机初始化，包括字嵌入。输入和输出的词汇大小大约为500，其中可以灵活化的去掉罕见单词和可以被delexicalisation的单词。我们对实验中的所有CNN使用了三个卷积层，并且所有滤波器大小都设置为3.池化操作仅在最后的卷积层之后应用。</p>\n<blockquote>\n<p>梯度裁剪是一种在非常深度的网络（通常是循环神经网络）中用于防止梯度爆炸（exploding gradient）的技术。执行梯度裁剪的方法有很多，但常见的一种是当参数矢量的 L2 范数（L2 norm）超过一个特定阈值时对参数矢量的梯度进行标准化，这个特定阈值根据函数：新梯度=梯度* 阈值/L2范数（梯度）<br>{new_gradients = gradients * threshold / l2_norm(gradients)}确定。</p>\n</blockquote>\n<p>在该评估中，模型使用了三个评估指标：</p>\n<ul>\n<li>BLEU评分（on top-1 and top-5 candidates）：我们使用实体值替换进行lexicalising之前，在模板输出句子上计算BLEU分数。</li>\n<li>实体匹配率：通过确定每个对话结束时实际选择的实体是否与用户指定的任务相匹配来计算实体匹配率。我们通过确定每个对话结束时实际选择的实体是否与指定给用户的任务相匹配来计算实体匹配率。 如果（1）所提供的实体匹配，并且（2）系统回答来自用户的所有相关信息请求（例如，地址是什么），则对话被标记为成功。</li>\n<li>客观任务成功率。</li>\n</ul>\n<p><strong>下表是对tracker的评估结果</strong><br><img src=\"https://img-blog.csdnimg.cn/2020100412021267.png#pic_center\" alt=\"在这里插入图片描述\"><br><strong>下表是基于语料的评估结果</strong><br><img src=\"https://img-blog.csdnimg.cn/2020100412030771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们使用t-SNE生成一个降维视图，该视图嵌入了前三个生成的输出词（完整模型，不注意）嵌入，绘制和标记，该图如下图所示。<br><img src=\"https://img-blog.csdnimg.cn/2020100416220381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下表是认为评估的结果<br><img src=\"https://img-blog.csdnimg.cn/20201004162227849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们还对NN模型和由handcrafted 语义分析器，基于规则的策略和信念跟踪器以及基于模板的生成器组成的人工模块化基准系统（HDC）进行比较。 结果如下表：<br><img src=\"https://img-blog.csdnimg.cn/20201004162258400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"Conclusions-and-Future-Work\"><a href=\"#Conclusions-and-Future-Work\" class=\"headerlink\" title=\"Conclusions and Future Work\"></a>Conclusions and Future Work</h1><p>目前的模型是一个基于文本的对话系统，它不能直接处理噪声语音识别输入，也不能在用户不确定时要求用户确认。 事实上，这种类型的模型在多大程度上可以扩展到更大更广的领域，这仍然是希望在今后的工作中追求的一个悬而未决的问题。</p>\n","categories":["Paper-Reading"],"tags":["对话系统","Paper","seq2seq","任务对话"]},{"title":"论文阅读笔记：类似于Multi-head注意力的Multi-Cast-Attention网络","url":"/Paper-Reading/dd0da68f3f17/","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote>\n<p>标题：Multi-Cast Attention Networks for Retrieval-based Question Answering and Response Prediction<br>原文链接：<a href=\"https://arxiv.org/pdf/1806.00778.pdf\">Link</a><br>Github：<a href=\"https://github.com/DengBoCong/nlp-paper\">NLP相关Paper笔记和代码复现</a><br>说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>\n</blockquote>\n<p>在基于检索式对话系统中，总体上可以分为关键的两个部分，分别是文本特征表示和文本相关性建模，核心思想就是通过评分模型，输出一个排序的候选响应列表，并在其中选择最佳响应（也就是所谓的神经排序模型）。说到文本相关性，我们就很容易想到当下非常受欢迎的注意力机制，注意力的关键思想是仅提取对预测有用的最相关信息，在文本数据的上下文中，注意力学习根据文档中的单词和子短语的重要性来对它们进行加权，有关深度学习中注意力机制的相关总结和复现可以看我这一篇文章：<a href=\"https://zhuanlan.zhihu.com/p/338193410\">NLP中遇到的各类Attention结构汇总以及代码复现</a>。</p>\n<p>论文中提到，注意力机制通常情况下用来作为特征提取器，它的行为可以被认为是一种动态的pooling形式，因为它学习选择和组合不同的词来形成最终的文档表示。而本文中换了一种思路，将attention作为一种特征增强方法（也就是说Attention的目的不是组合学习，而是为后续层提供提示特征），这怎么理解呢？不着急，我们接着往下看。</p>\n<p>通常情况下，我们只在一个句子上施加一次注意力，然后将学习到的表示传递给后续的预测网络进行学习，在我以前学习和接触的模型结构中，通常只是使用一次Attention。即使在使用多个Attention的结构中，也通常使用串联来融合表示形式，这样做有一个很明显的缺点就是会使得表示大小成倍增大，从而在后续层中产生成本。</p>\n<p>使用多种注意力机制可以显著提高性能，比如Co-Attention 和 Intra-Attention（Self-Attention）中，每种Attention都为query-document对提供了不同的视图，可以学习用于预测的高质量表示。例如，在Co-Attention机制中，利用max-pooling基于单词对另一文本序列的最大贡献来提取特征，利用mean-pooling计算其对整个句子的贡献，利用alignment-based pooling将语义相似的子短语对齐在一起。Co-Attention的论文：<a href=\"https://arxiv.org/pdf/1611.01604.pdf\">DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING</a></p>\n<p>综上所述，论文主要解决两个方面的问题：</p>\n<ul>\n<li>消除调用任意k次注意力机制所需架构工程的需要，且不会产生任何后果。</li>\n<li>通过多次注意力调用建模多个视图以提高性能，即多播注意力(Multi-Cast Attention)。</li>\n</ul>\n<p>类似Multi-head Attention，通过多次投射Co-attention，每次返回一个压缩的标量特征，重新附加到原始的单词表示上。压缩函数可以实现多个注意力调用的可扩展投射，旨在不仅为后续层提供全局知识而且还有跨句子知识的特征。当将这些增强嵌入传递到组合编码器（例如LSTM编码器）时，LSTM可以从该提示中获益并相应地改变其表示学习过程。</p>\n<h1 id=\"Multi-Cast-Attention-Networks（MCAN）结构\"><a href=\"#Multi-Cast-Attention-Networks（MCAN）结构\" class=\"headerlink\" title=\"Multi-Cast Attention Networks（MCAN）结构\"></a>Multi-Cast Attention Networks（MCAN）结构</h1><p>首先定义网络的输入，即两个序列，分别是query $q$和document $d$，这两个输入基本可以概括QA或响应预测的输入，下图是QA检索结构图：<br><img src=\"https://img-blog.csdnimg.cn/20210204130339123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Input-Encoder\"><a href=\"#Input-Encoder\" class=\"headerlink\" title=\"Input Encoder\"></a>Input Encoder</h2><p>document和query输入作为one-hot编码向量。词向量层通过 $W_e\\in R^{d\\times |V|}$ 参数化，将每个单词转换为密集的单词表示 $w\\in R^d$， $V$ 是词汇表中所有单词的集合。每个词嵌入向量都将通过Highway Encoder（<a href=\"https://arxiv.org/pdf/1505.00387.pdf\">Highway Encoder</a>原文，即通过一种控制穿过神经网络的信息流的闸门机制为神经网络提供通路，让信息穿过后却没有损失，将这种通路称为information highways，也就是说，highway networks主要解决的问题是网络深度加深、梯度信息回流受阻造成网络训练困难的问题）。</p>\n<p>highway网络是门控非线性变换层，它控制后续层的信息流。许多工作都采用一种训练过的投影层来代替原始词向量。这不仅节省了计算成本，还减少了可训练参数的数量。本文将此投影层扩展为使用highway编码器，可以解释为数据驱动的词滤波器，它们可以参数化地了解哪些词对于任务具有重要性和重要性。例如，删除通常对预测没有多大贡献的停用词和单词。与自然门控的循环模型类似，highway编码器层控制每个单词流入下一层多少信息。设 $H(.)$ 和 $T(.)$ 是单层的，分别用ReLU和Sigmoid激活函数进行变换。单个highway网络层定义为：<br>$$y=H(x,W_H)\\cdot T(x,W_T)+(1-T(x,W_T))\\cdot x$$<br>其中，$W_H,W_T\\in R^{r\\times d}$，前一项表示输入信息被转换的部分，后一项原来信息中保留的部分。</p>\n<h2 id=\"Co-Attention\"><a href=\"#Co-Attention\" class=\"headerlink\" title=\"Co-Attention\"></a>Co-Attention</h2><p>Co-Attention 是一种成对注意力机制，它能够共同关注文本序列对。本文引入了四种注意力变体，即max-pooling，mean-pooling，alignment-pooling和intra-attention（self attention）。Co-Attention的第一步是学习两个序列中每个单词之间的一个近似（或相似）矩阵。采用以下公式来学习近似矩阵：<br>$$s_{ij}=F(q_i)^TF(d_j)$$<br>其中 $F(.)$ 是诸如多层感知器（MLP）之类的函数。Co-Attention形式也可以为 $s_{ij}=q_i^TMd_j$ 和 $s_{ij}=F([q_i;d_j])$</p>\n<ul>\n<li>抽取式Co-Attention最常见变体是max-pooling co-attention，基于每个词在其他文本序列上的最大影响来关注每个词:<br>$$q^{‘}=Soft(\\underset{col}{max}(s))^Tq \\ and \\ d^{‘}=Soft(\\underset{row}{max}(s))^Td$$<br>其中，$q^{‘},d^{‘}$ 分别是 $q$ 和 $d$ 的co-attentive表示，$Soft(.)$ 表示softmax操作</li>\n<li>和max-pooling co-attention相似，我们也可以取行和列层次的平均池化矩阵：<br>$$q^{‘}=Soft(\\underset{col}{mean}(s))^Tq \\ and \\ d^{‘}=Soft(\\underset{row}{mean}(s))^Td$$<br>mean-pooling co-attention则是基于每个词在其他文本上的总体影响来关注每个词（一般是更合适的选择），当然max和mean之间的选择一般可以作为超参数进行实验调整。</li>\n<li>Alignment-Pooling，Soft alignment-based pooling也被用于学习co-attentive表示。和标准co-attention最主要区别在于，标准co-attention只是学会对重要单词进行加权和评分，而soft alignment重新调整序列对，用下述公式学习co-attention的表示：<br>$$d_i^{‘}:=\\sum_{j=1}^{l_q}\\frac{exp(s_{ij})}{\\sum_{K=1}^{l_q}exp(s_{ik})}q_j\\ and \\ q_j^{‘}:=\\sum_{i=1}^{l_q}\\frac{exp(s_{ij})}{\\sum_{K=1}^{l_d}exp(s_{kj})}d_i$$<br>其中 $d_i^{‘}$ 是 $q$ 中与 $d_i$ 软对齐的子短语。直观地说， $d_i^{‘}$ 是 ${q_j}_{j=1}^{l_q}$ 上的加权和，在 $q$ 上选择最相关的部分表示 $d_i$。</li>\n<li> Intra-Attention（Self-Attention）可以学习长期依赖性的表示，这通常被表述为关于其自身的co-attention（或alignment）操作。在这种情况下，本文将intra-attention同时分别应用于document和query。intra-attention函数定义为（为了符号简单，这里将文档和查询称为 $x$ 而不是 $q$ 或 $d$）：<br>$$x_i^{‘}:=\\sum_{j=1}^l\\frac{exp(s_{ij})}{\\sum_{K=1}^lexp(s_{ik})}x_j$$<br>其中 $x_i^{‘}$ 是 $x_j$ 的内部注意力表示。</li>\n</ul>\n<h2 id=\"Multi-Cast-Attention\"><a href=\"#Multi-Cast-Attention\" class=\"headerlink\" title=\"Multi-Cast Attention\"></a>Multi-Cast Attention</h2><p>在上面的讨论，我们很容易发现每种Attention机制都为模型提供了不同的视角，且Attention是通过重新加权或重新调整来改变原始表示，大多数神经架构仅使用一种类型的co-attention或alignment函数 ，这不仅需要调整模型架构，并且可能错过使用co-attention的多种变体带来的好处， 因此，本文模型将每个注意力操作视为word-level特征，然后使用如下几个操作进行特征表示：</p>\n<ul>\n<li>Casted Attention：设 $x$ 为 $q$ 或 $d$ ，$\\bar{x}$ 是应用attention后x的表示，则co-attention操作的注意力特征是：<br>$$f_c=F_c([\\bar{x};x])$$   $$f_m=F_c(\\bar{x}\\odot x)$$    $$f_s=F_c(\\bar{x}-x)$$<br>其中 $\\odot$ 是Hadamard乘积，$[. ; .]$ 是连接运算符。$F_c(.)$ 是用于将特征减少到标量的压缩函数。 通过比较在co-attention之前和之后的表示来模拟co-attention的影响，其中，这里使用多个比较运算符（减法，连接和乘法运算符）是为了捕捉多个视角。</li>\n<li>Compression Function：上面提到的 $F_c(.)$ 的压缩函数原理简单而直观，即作者不希望使用高维向量来膨胀后续图层，从而产生后续图层的参数开销，因此本文研究了三种压缩函数的用法，它们能够将 $n$ 维向量减少到标量：<ul>\n<li>Sum（SM）函数是一个非参数化函数，它对整个向量求和，并输出标量：<br>$$F(x)=\\sum_i^nx_i,\\forall x_i\\in x$$</li>\n<li>Neural Network（NN）是一个全连接层，按如下方式转换每个n维特征向量：<br>$$F(x)=ReLU(W_c(x)+b_c)$$</li>\n<li>Factorization Machines（FM），即因子分解机是一种通用机器学习技术，接受实值特征向量 $x\\in R^n$ 并返回标量输出<br>$$F(x)=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^n\\sum_{j=i+1}^n&lt;v_i,v_j&gt;x_ix_j$$<br>FM是表达模型，使用分解参数捕获特征之间的成对相互作用， $k$ 是FM模型的因子数。</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>注意到，模型不会在多个注意力投射中共享参数，因为每个注意力都旨在为不同的视角建模。实验分别在MCAN（SM），MCAN（NN）和MCAN（FM）下展示了上述变体的结果。</p>\n</blockquote>\n<ul>\n<li>Multi-Cast：架构背后的关键思想是促进 $k$ 个注意力投射，每个投射都用一个实值注意力特征来增强原始词向量，对于每个query-document对，应用Co-Attention with mean-pooling，Co-Attention with max-Pooling和Co-Attention with alignment-pooling。 此外，将Intra-Attention分别单独应用于query和document。 每个注意力投射产生三个标量（每个单词），它们与词向量连接在一起。最终的投射特征向量是 $z\\in R^{12}$。 因此，对于每个单词 $w_i$，新的表示成为 $[w_i;z_i]$ 。</li>\n</ul>\n<h2 id=\"LSTM-Encoder\"><a href=\"#LSTM-Encoder\" class=\"headerlink\" title=\"LSTM Encoder\"></a>LSTM Encoder</h2><p>将带有casted attetnion的单词表示 $\\bar{w_1},\\bar{w_2},…\\bar{w_l}$ 传递到序列encoder层，采用标准的LSTM编码器：<br>$$h_i=LSTM(u,i),\\forall_i\\in [1,…l]$$<br>其中 $l$ 代表序列的最大长度，LSTM在document和query之间共享权重，关键思想是LSTM encoder通过使用非线性变换作为门控函数来学习表示序列依赖性的表示。因此，在该层之前引入attention作为特征的关键思想是它为LSTM encoder提供了带有信息的提示，例如长期和全局句子知识和句子对（document 和 query）之间的信息。</p>\n<p>Pooling Operation：最后，在每个句子的隐藏状态 ${h_1…h_l}$ 上应用池化函数，将序列转换为固定维度的表示：$h=MeanMax[h_1…h_l]$。采用MeanMax pooling，它将mean pooling和max pooling的结果连接在一起。我们发现这样比单独使用max pooling或mean pooling更好。</p>\n<h2 id=\"Prediction-Layer和Optimization\"><a href=\"#Prediction-Layer和Optimization\" class=\"headerlink\" title=\"Prediction Layer和Optimization\"></a>Prediction Layer和Optimization</h2><p>最后，给定 document-query 对的固定维度表示，将它们的连接传递到一个两层 $h$ 维highway网络中，模型的最终预测层计算如下：<br>$$y_{out}=H_2(H_1([x_q;x_d;x_q\\odot x_d;x_q-x_d]))$$<br>其中，$H_1(.),H_2(.)$ 是具有ReLU激活的highway网络层，然后将输出传递到最终线性softmax层：<br>$$y_{pred}=softmax(W_F\\cdot y_{out}+b_F)$$<br>然后用带有L2范式的标准多分类交叉熵loss函数训练：<br>$$J(\\theta)=-\\sum_{i=1}^N[y_ilog\\hat{y}<em>i+(1-y_i)log(1-\\hat{y}_i)]+\\lambda||\\theta||</em>{L2}$$<br>其中，$\\theta$ 是网络的参数，$\\hat{y}$ 是网络的输出，$\\lambda$ 是 $||\\theta||_{L2}$ L2正则化的权重。</p>\n<h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>首先需要知道实验的目的，如下：</p>\n<ul>\n<li>论文提出的方法能否在问题回答和对话建模任务上达到最先进的性能？ 与完善的baseline相比有哪些相对改进？</li>\n<li>模型结构设计对性能有什么影响？使用LSTM来对casted特征进行学习是否有必要？  co-attention的变体是否都对整体模型性能有所贡献？</li>\n<li>能否解释所提出模型的内部运作方式？ 我们可以解释casted attention特征吗？</li>\n</ul>\n<h2 id=\"Dialogue-Prediction\"><a href=\"#Dialogue-Prediction\" class=\"headerlink\" title=\"Dialogue Prediction\"></a>Dialogue Prediction</h2><p>在这个任务中，评估模型是否能够成功预测对话中的下一个回复。使用Ubuntu对话语料库进行实验，其中训练集由一百万个message-response对组成，正负样本1：1。召回率@ k（Rn@K），表示在 $n$ 个response候选的前 $k$ 个结果中是否存在ground truth，论文使用了四个评估指标分别是 $R2@ 1$，$R10@1$，$R10@2$ 和 $R10@5$。</p>\n<p>相关参数设置：MCAN中的LSTM encoder的 $d=100$ ，使用学习率 $3\\times 10^{-4}$ 的Adam优化器对MCAN进行优化，处理词嵌入层以外的所有层都使用采样率为 $0.2$ 的dropout，序列最大为 $50$，同时使用预训练的 GloVe 作为词嵌入模型。</p>\n<p>所有指标的改善比KEHNN好 $5％-9％$。 比AP-LSTM和MV-LSTM的 $R10@1$ 性能提升了 $15％$。总体而言，MCAN（FM）和MCAN（NN）在性能方面具有可比性， MCAN（SM）略低于MCAN（FM）和MCAN（NN），如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210204232737757.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Factoid-Question-Answering\"><a href=\"#Factoid-Question-Answering\" class=\"headerlink\" title=\"Factoid Question Answering\"></a>Factoid Question Answering</h2><p>Factoid Question Answering是回答基于事实的问题的任务，在此任务中，目标是为给定问题提供的答案的排序列表。TREC（文本检索会议）的QA数据集，TrecQA是QA最广泛评估和长期作为标准的数据集之一，评估指标使用MAP（mean average precision）和MRR（mean reciprocal rank）。</p>\n<p>相关参数设置：MCAN中的LSTM encoder的 $d=300$ ，使用学习率 $3\\times 10^{-4}$ 的Adam优化器对MCAN进行优化，L2正则化设置$10^{-6}$，处理词嵌入层以外的所有层都使用采样率为 $0.2$ 的dropout，序列长度取最大序列长度，同时使用预训练的 $300d$ 的GloVe 作为词嵌入模型，使用10 factors的FM模型。</p>\n<p>所有MCAN变体都优于所有现有的最先进模型。其中MCAN（FM）是目前在这个广泛研究的数据集上表现最好的模型，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210204234016105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Community-Question-Answering-cQA\"><a href=\"#Community-Question-Answering-cQA\" class=\"headerlink\" title=\"Community Question Answering (cQA)\"></a>Community Question Answering (cQA)</h2><p>此任务涉及在社区论坛中对答案进行排名，与factoid QA不同，答案通常是主观的而不是事实的，而且，答案长度也要长得多。使用QatarLiving数据集，这是一个来自SemEval-2016 Task 3 Subtask的经过充分研究的基准数据集A（cQA），已被广泛用作cQA最新的神经网络模型的基准。包括36000个训练对，2400个开发对和3600个测试对。在这个数据集中，每个问题有十个答案，标记为“正向”和“负向”。评估指标使用Precision@1（P@1）和Mean Average Precision （MAP）</p>\n<p>MCAN模型在此数据集上实现了最先进的性能。就P@1指标而言，MCAN（FM）相对于AI-CNN的改善在MAP方面为4.1％和1.1％。相对于CTRN模型，MCAN（FM）也取得了有竞争力的结果，如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210204234440805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Tweet-Reply-Prediction\"><a href=\"#Tweet-Reply-Prediction\" class=\"headerlink\" title=\"Tweet Reply Prediction\"></a>Tweet Reply Prediction</h2><p>使用来自Kaggle的顾客支持数据集，这个数据集中包含对知名品牌的Tweet-Response对，评估指标使用MRR (Mean reciprocal rank)和Precision@1 (accuracy)，实验结果如下图：<br><img src=\"https://img-blog.csdnimg.cn/20210204234717160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Ablation-Analysis\"><a href=\"#Ablation-Analysis\" class=\"headerlink\" title=\"Ablation Analysis\"></a>Ablation Analysis</h2><p><img src=\"https://img-blog.csdnimg.cn/20210204234810211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"模型可解释性分析\"><a href=\"#模型可解释性分析\" class=\"headerlink\" title=\"模型可解释性分析\"></a>模型可解释性分析</h1><p>通过观察 casted attention特征列出了一些观察结果，下面使用训练了带有FM压缩的MCAN模型，并提取了word-level  casted attention特征，这些特征称为 $f_i$，其中 $i\\in[1，12]$。 $f_1,f_2,f_3$ 是从alignment pooling生成的， $f_4,f_5,f6$ 和 $f_7,f_8,f_9$ 分别从最大和平均co-attention中生成， $f_10,f_11,f_12$ 是从intra-attention生成的。</p>\n<p>下图显示TrecQA测试集中的正和负QA对：<br><img src=\"https://img-blog.csdnimg.cn/20210204235513465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>下图显示使用max-pooling attention和mean-pooling attention的casted attention特征：<br><img src=\"https://img-blog.csdnimg.cn/20210204235632415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文主要贡献有以下几点：</p>\n<ul>\n<li>首次提出了一种新的思路，不是将attention作为pooling操作，而是作为一种特征增强方式使用，即casted attention。提出了一种用于通用序列对建模的整体架构，称为多播注意力网络Multi-Cast Attention Networks（MCAN）。这是一种新的注意力机制和通用模型架构，用于对话建模和问答系统领域中的排序任务，这种方法执行一系列soft-attention操作，每次返回一个压缩的标量特征，重新附加到原始的单词表示上。关键思想是为后续编码器层提供实值特征，旨在改进表示学习过程。这种设计有几个优点，例如，它允许投射任意数量的注意力机制，允许多种注意力类型（例如，co-attention, intra-attention）和注意力变体（例如，alignment-pooling, max-pooling, mean-pooling）同时执行。这不仅消除了调整co-attention层的昂贵需求，而且还为从业者提供了更大的可解释性。</li>\n<li>根据四个基准任务评估提出的模型，即对话回复预测（Ubuntu对话语料库），Factoid问答（TrecQA），社区问答（来自SemEval 2016的QatarLiving论坛）和推特回复预测（客户支持）。在Ubuntu对话语料库中，MCAN的表现优于现有的最先进模型9％。MCAN在经过充分研究的TrecQA数据集上也取得了0.838 (MAP) 和0.904 (MRR) 的最佳表现评分。</li>\n<li>对提出的MCAN模型的内部工作进行全面而深入的分析。实验表明，多播注意力特征是可解释的。</li>\n</ul>\n","categories":["Paper-Reading"],"tags":["Attention","Paper","检索式对话","问答系统"]},{"title":"详细SpringBoot教程之Web开发（二）","url":"/Spring-Boot/22741562e341/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"默认访问首页\"><a href=\"#默认访问首页\" class=\"headerlink\" title=\"默认访问首页\"></a>默认访问首页</h2><p>开始正式讲解之前呢，先把整个过程的项目源码贴出来，方便边看边学<a href=\"https://github.com/DengBoCong/demo\">项目GitHub地址</a>，然后我们在讲解的过程中还会用到Thymeleaf相关语法，可以边打开我的另一篇博文当参考手册，直接点击那篇博文的目录，可以快速导航到对应语法的示例，<a href=\"https://blog.csdn.net/DBC_121/article/details/104480132\">模板引擎Thymeleaf？来这一篇就够用了</a></p>\n<p>接上一篇博文创建的项目，我们先在properties配置文件中添加一个这个配置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#/接随便啥名，我这里用了dbc</span></span><br><span class=\"line\">server.servlet.context-path=/dbc</span><br></pre></td></tr></table></figure>\n<p>意思是应用的上下文路径，也可以称为项目路径，是构成url地址的一部分。也就是说server.servlet.context-path不配置时，默认为 / ，如：localhost:8080/xxxxxx。当server.servlet.context-path有配置时，比如 /dbc，此时的访问方式为localhost:8080/dbc/xxxxxx，这样我们就配置好了默认首页路径。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/2020022516124813.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"国际化\"><a href=\"#国际化\" class=\"headerlink\" title=\"国际化\"></a>国际化</h2><p>接着我们再来编写国际化配置文件，在SpringBoot中使用ResouceBundleMessageSource管理国际化配置文件，如果在放在以前的SpringMVC中，我们使用了ResouceBundleMessageSource之后，需要在页面中使用fmt：message取出国际化配置文件，我们我们在SpringBoot中就当然不需要这样来写啦，哈哈哈。在SpringBoot中怎么使用呢，具体步骤如下</p>\n<h3 id=\"第一步\"><a href=\"#第一步\" class=\"headerlink\" title=\"第一步\"></a>第一步</h3><p>我们在Resources目录下，创建一个i18n的包，然后创建三个国际化的properties配置文件<br><img src=\"https://img-blog.csdnimg.cn/20200225123311248.png#pic_center\" alt=\"在这里插入图片描述\"><br>如图，创建了三个国际化的properties配置文件之后，Idea会检测到我们是创建了国际化配置文件，会自动帮我们创建国际化配置文件视图。</p>\n<p>接下来我们随便点进一个国际化配置文件，这里我点击进入login.properties中，然后可以点击Resources Bundle进入，如下<br><img src=\"https://img-blog.csdnimg.cn/20200225124159783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>是不是很方便，然后我们这里总共设置了五条国际化的属性，意思一下就好了，不用写那么多，我们知道怎么用就可以了，哈哈哈，如下。<br><img src=\"https://img-blog.csdnimg.cn/20200225160712916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"第二步\"><a href=\"#第二步\" class=\"headerlink\" title=\"第二步\"></a>第二步</h3><p>SpringBoot自动配置好了管理国际化资源的文件的组件，即ResourceBundleMessageSource，所以我们只要在properties配置文件中启用指定国际化文件就可以了，配置如下<br><img src=\"https://img-blog.csdnimg.cn/20200225161330353.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"第三步\"><a href=\"#第三步\" class=\"headerlink\" title=\"第三步\"></a>第三步</h3><p>去页面获取国际化的值，我们在HTML中使用Thymeleaf语法进行获取<br><img src=\"https://img-blog.csdnimg.cn/20200225162802273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里还是要说明一下，如果出现了乱码，是因为properties默认是ASCII编码，我们Idea是UTF-8编码，所以会乱码，怎么设置之前的博文有说，这里我就再强调一下，省的你们翻前面的博文。<br><img src=\"https://img-blog.csdnimg.cn/20200225162956577.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200225163043506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"第四步\"><a href=\"#第四步\" class=\"headerlink\" title=\"第四步\"></a>第四步</h3><p>现在我们已经设置好了国际化，怎么测试呢？我们可以切换浏览器国际化信息用进行测试，项目会自动检测浏览器语言环境，然后更换，我这里使用Chrome举例，点开Chrome的设置-&gt;高级，然后更换，如下<br><img src=\"https://img-blog.csdnimg.cn/20200225163325775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>为什么会自动检测呢？我们啥都没有配置，我这里简单的说一下按原理，也就是SpringBoot有一个国际化Locale（区域信息对象）以及一个LocaleResolver（获取区域信息对象），这个就是默认的根据请求头里面的，会根据请求头里面的Accept-language的值进行读取配置值。<br><img src=\"https://img-blog.csdnimg.cn/20200225163809860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然，如果我们不想让浏览器自动检测，而是通过我们点击按钮进行更换语言，像下面这样。<br><img src=\"https://img-blog.csdnimg.cn/2020022516475413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们可以自己写一个实现LocaleResolver接口的类，我们就可以在链接上携带区域语言信息，不过要注意了，如果我们自己实现了LocaleResolver接口，默认的检测浏览器语言就不生效了，就算我们更换浏览器语言也没有用，我们自己编写的国际化解析器如下<br><img src=\"https://img-blog.csdnimg.cn/20200225164422290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>写完了之后记得在主配置文件MyMvcConfig中注册一下我们编写的国际化解析器组件<br><img src=\"https://img-blog.csdnimg.cn/20200225164520275.png#pic_center\" alt=\"在这里插入图片描述\"><br>HTML的按钮发出相关请求，在请求后面带上标记属性，如下<br><img src=\"https://img-blog.csdnimg.cn/20200225165230925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后现在就可以重启测试了，我们点击按钮之后，就会在连接上出现属性值，然后更换语言<a href=\"http://localhost:8080/index.html?l=zh_CN%E5%92%8Chttp://localhost:8080/index.html?l=en_US\">http://localhost:8080/index.html?l=zh_CN和http://localhost:8080/index.html?l=en_US</a></p>\n<h2 id=\"登录进入主页\"><a href=\"#登录进入主页\" class=\"headerlink\" title=\"登录进入主页\"></a>登录进入主页</h2><p>我们在登录页面的内容如下，有几个输入框，用来输入账号和密码的，然后我们写入之后发送请求，请求的接口是/user/login<br><img src=\"https://img-blog.csdnimg.cn/20200225165634815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里我们把/user/login的Controller写出来<br><img src=\"https://img-blog.csdnimg.cn/20200225165951484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在开发中可能会遇到缓存影响我们刷新页面开发的情况，我们在可以在配置文件中将Thymeleaf的默认缓存给关掉<br><img src=\"https://img-blog.csdnimg.cn/20200225165801865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>现在就可以重新启动我们的项目了，然后发现可以进入到主页<br><img src=\"https://img-blog.csdnimg.cn/20200225172738930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"拦截器\"><a href=\"#拦截器\" class=\"headerlink\" title=\"拦截器\"></a>拦截器</h2><p>我们弄好了登录，这个时候就会想，如果我们不通过正常的登录，而是直接通过主页的连接进入到主页，这样子我们的登录界面还有什么意义呢？所以这里我们需要对所有请求进行拦截，写一个LoginHandlerInterceptor实现HandlerInterceptor的接口的拦截器<br><img src=\"https://img-blog.csdnimg.cn/20200225170407494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>和LocaleResolver一样，我们编写了自己的拦截器之后呢，我们还是需要在主配置文件中进行注册我们的拦截器组件，顺便设置一些我们不需要进行拦截的请求路径<br><img src=\"https://img-blog.csdnimg.cn/20200225170906490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"用户列表功能\"><a href=\"#用户列表功能\" class=\"headerlink\" title=\"用户列表功能\"></a>用户列表功能</h2><p>用户列表的功能效果如下，我们可以显示用户的信息，然后进行增删改查的基本操作，所以我们需要创建Dao层和实体类，代码内容如下<br><img src=\"https://img-blog.csdnimg.cn/20200225174213636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">import com.example.demo.Entities.Department;</span><br><span class=\"line\">import org.springframework.stereotype.Repository;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.util.Collection;</span><br><span class=\"line\">import java.util.Map;</span><br><span class=\"line\">import java.util.HashMap;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * @program: demo</span><br><span class=\"line\"> * @description:</span><br><span class=\"line\"> * @author: DBC</span><br><span class=\"line\"> * @create: 2020-02-24 22:54</span><br><span class=\"line\"> **/</span><br><span class=\"line\">@Repository</span><br><span class=\"line\">public class DepartmentDao &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    private static Map&lt;Integer, Department&gt; departments = null;</span><br><span class=\"line\"></span><br><span class=\"line\">    static&#123;</span><br><span class=\"line\">        departments = new HashMap&lt;Integer, Department&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">        departments.put(101, new Department(101, <span class=\"string\">&quot;D-AA&quot;</span>));</span><br><span class=\"line\">        departments.put(102, new Department(102, <span class=\"string\">&quot;D-BB&quot;</span>));</span><br><span class=\"line\">        departments.put(103, new Department(103, <span class=\"string\">&quot;D-CC&quot;</span>));</span><br><span class=\"line\">        departments.put(104, new Department(104, <span class=\"string\">&quot;D-DD&quot;</span>));</span><br><span class=\"line\">        departments.put(105, new Department(105, <span class=\"string\">&quot;D-EE&quot;</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public Collection&lt;Department&gt; <span class=\"function\"><span class=\"title\">getDepartments</span></span>()&#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> departments.values();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public Department getDepartment(Integer id)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> departments.get(id);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">import com.example.demo.Entities.Department;</span><br><span class=\"line\">import com.example.demo.Entities.Employee;</span><br><span class=\"line\">import org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\">import org.springframework.stereotype.Repository;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.util.Collection;</span><br><span class=\"line\">import java.util.Map;</span><br><span class=\"line\">import java.util.HashMap;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * @program: demo</span><br><span class=\"line\"> * @description:</span><br><span class=\"line\"> * @author: DBC</span><br><span class=\"line\"> * @create: 2020-02-24 22:56</span><br><span class=\"line\"> **/</span><br><span class=\"line\">@Repository</span><br><span class=\"line\">public class EmployeeDao &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    private static Map&lt;Integer, Employee&gt; employees = null;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private DepartmentDao departmentDao;</span><br><span class=\"line\"></span><br><span class=\"line\">    static&#123;</span><br><span class=\"line\">        employees = new HashMap&lt;Integer, Employee&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">        employees.put(1001, new Employee(1001, <span class=\"string\">&quot;E-AA&quot;</span>, <span class=\"string\">&quot;aa@163.com&quot;</span>, 1, new Department(101, <span class=\"string\">&quot;D-AA&quot;</span>)));</span><br><span class=\"line\">        employees.put(1002, new Employee(1002, <span class=\"string\">&quot;E-BB&quot;</span>, <span class=\"string\">&quot;bb@163.com&quot;</span>, 1, new Department(102, <span class=\"string\">&quot;D-BB&quot;</span>)));</span><br><span class=\"line\">        employees.put(1003, new Employee(1003, <span class=\"string\">&quot;E-CC&quot;</span>, <span class=\"string\">&quot;cc@163.com&quot;</span>, 0, new Department(103, <span class=\"string\">&quot;D-CC&quot;</span>)));</span><br><span class=\"line\">        employees.put(1004, new Employee(1004, <span class=\"string\">&quot;E-DD&quot;</span>, <span class=\"string\">&quot;dd@163.com&quot;</span>, 0, new Department(104, <span class=\"string\">&quot;D-DD&quot;</span>)));</span><br><span class=\"line\">        employees.put(1005, new Employee(1005, <span class=\"string\">&quot;E-EE&quot;</span>, <span class=\"string\">&quot;ee@163.com&quot;</span>, 1, new Department(105, <span class=\"string\">&quot;D-EE&quot;</span>)));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    private static Integer initId = 1006;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void save(Employee employee)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(employee.getId() == null)&#123;</span><br><span class=\"line\">            employee.setId(initId++);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        employee.setDepartment(departmentDao.getDepartment(employee.getDepartment().getId()));</span><br><span class=\"line\">        employees.put(employee.getId(), employee);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //查询所有员工</span><br><span class=\"line\">    public Collection&lt;Employee&gt; <span class=\"function\"><span class=\"title\">getAll</span></span>()&#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> employees.values();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public Employee get(Integer id)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> employees.get(id);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void delete(Integer id)&#123;</span><br><span class=\"line\">        employees.remove(id);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">import com.example.demo.Dao.DepartmentDao;</span><br><span class=\"line\">import com.example.demo.Dao.EmployeeDao;</span><br><span class=\"line\">import com.example.demo.Entities.Department;</span><br><span class=\"line\">import com.example.demo.Entities.Employee;</span><br><span class=\"line\">import org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\">import org.springframework.stereotype.Controller;</span><br><span class=\"line\">import org.springframework.ui.Model;</span><br><span class=\"line\">import org.springframework.web.bind.annotation.*;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.util.Collection;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * @program: demo</span><br><span class=\"line\"> * @description:</span><br><span class=\"line\"> * @author: DBC</span><br><span class=\"line\"> * @create: 2020-02-24 23:14</span><br><span class=\"line\"> **/</span><br><span class=\"line\">@Controller</span><br><span class=\"line\">public class EmployeeController &#123;</span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    EmployeeDao employeeDao;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    DepartmentDao departmentDao;</span><br><span class=\"line\"></span><br><span class=\"line\">    //查询所有员工返回列表页面</span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/emps&quot;</span>)</span><br><span class=\"line\">    public String  list(Model model)&#123;</span><br><span class=\"line\">        Collection&lt;Employee&gt; employees = employeeDao.getAll();</span><br><span class=\"line\"></span><br><span class=\"line\">        //放在请求域中</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;emps&quot;</span>,employees);</span><br><span class=\"line\">        // thymeleaf默认就会拼串</span><br><span class=\"line\">        // classpath:/templates/xxxx.html</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;emp/list&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //来到员工添加页面</span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/emp&quot;</span>)</span><br><span class=\"line\">    public String toAddPage(Model model)&#123;</span><br><span class=\"line\">        //来到添加页面,查出所有的部门，在页面显示</span><br><span class=\"line\">        Collection&lt;Department&gt; departments = departmentDao.getDepartments();</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;depts&quot;</span>,departments);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;emp/add&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //员工添加</span><br><span class=\"line\">    //SpringMVC自动将请求参数和入参对象的属性进行一一绑定；要求请求参数的名字和javaBean入参的对象里面的属性名是一样的</span><br><span class=\"line\">    @PostMapping(<span class=\"string\">&quot;/emp&quot;</span>)</span><br><span class=\"line\">    public String addEmp(Employee employee)&#123;</span><br><span class=\"line\">        //来到员工列表页面</span><br><span class=\"line\"></span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;保存的员工信息：&quot;</span>+employee);</span><br><span class=\"line\">        //保存员工</span><br><span class=\"line\">        employeeDao.save(employee);</span><br><span class=\"line\">        // redirect: 表示重定向到一个地址  /代表当前项目路径</span><br><span class=\"line\">        // forward: 表示转发到一个地址</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;redirect:/emps&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //来到修改页面，查出当前员工，在页面回显</span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/emp/&#123;id&#125;&quot;</span>)</span><br><span class=\"line\">    public String toEditPage(@PathVariable(<span class=\"string\">&quot;id&quot;</span>) Integer id,Model model)&#123;</span><br><span class=\"line\">        Employee employee = employeeDao.get(id);</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;emp&quot;</span>,employee);</span><br><span class=\"line\"></span><br><span class=\"line\">        //页面要显示所有的部门列表</span><br><span class=\"line\">        Collection&lt;Department&gt; departments = departmentDao.getDepartments();</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;depts&quot;</span>,departments);</span><br><span class=\"line\">        //回到修改页面(add是一个修改添加二合一的页面);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;emp/add&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //员工修改；需要提交员工id；</span><br><span class=\"line\">    @PutMapping(<span class=\"string\">&quot;/emp&quot;</span>)</span><br><span class=\"line\">    public String updateEmployee(Employee employee)&#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;修改的员工数据：&quot;</span>+employee);</span><br><span class=\"line\">        employeeDao.save(employee);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;redirect:/emps&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //员工删除</span><br><span class=\"line\">    @DeleteMapping(<span class=\"string\">&quot;/emp/&#123;id&#125;&quot;</span>)</span><br><span class=\"line\">    public String deleteEmployee(@PathVariable(<span class=\"string\">&quot;id&quot;</span>) Integer id)&#123;</span><br><span class=\"line\">        employeeDao.delete(id);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;redirect:/emps&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里顺便说一下，我们在提交日期格式的字符串的时候，SpringBoot有着自己默认的字符串转换日期格式的方法，而这个默认方法是需要日期通过“/”进行分离的，如果我们先想要通过“-”进行分离，那么我们还是需要在主配置文件中设置日期格式的配置，如下<br><img src=\"https://img-blog.csdnimg.cn/2020022517510785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"HiddenHttpMethodFilter\"><a href=\"#HiddenHttpMethodFilter\" class=\"headerlink\" title=\"HiddenHttpMethodFilter\"></a>HiddenHttpMethodFilter</h2><ul>\n<li>SpringMVC中配置HiddenHttpMethodFilter;（SpringBoot自动配置好的）</li>\n<li>页面创建一个post表单</li>\n<li>创建一个input项，name=”_method”;值就是我们指定的请求方式</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200225182117145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>像上面这样子指定请求的方式，通过这种方式，我们就可以使用同一个路径请求进行不同的处理操作<br><img src=\"https://img-blog.csdnimg.cn/20200225185900240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>这里我们大致上是已经将SpringBoot开发流程走了一遍，不过还有一些web开发相关的操作需要了解，比如错误请求等等，将在下一篇博文中进行讲解。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Web"]},{"title":"详细SpringBoot教程之缓存开发","url":"/Spring-Boot/608eed3fec6d/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"为啥用缓存\"><a href=\"#为啥用缓存\" class=\"headerlink\" title=\"为啥用缓存\"></a>为啥用缓存</h2><p>缓存在开发中是一个必不可少的优化点，关于缓存优化了很多点，比如在加载一些数据比较多的场景中,会大量使用缓存机制提高接口响应速度,间接提升用户体验。当然对于缓存的使用也有需要注意的地方，比如它如果处理不好，没有用好比如LRU这种策略，没有及时更新数据库的数据就会导致数据产生滞后，进而产生用户的误读，或者疑惑。不过关于这一切，SpringBoot已经提供给我们很便捷的开发工具。</p>\n<h2 id=\"JSR107\"><a href=\"#JSR107\" class=\"headerlink\" title=\"JSR107\"></a>JSR107</h2><p>在正式讲解缓存之前呢，我想先说说JSR107注解标准，这是个啥呢，简单来说就是对于缓存的接口，Java Caching定义了5个核心接口，分别是CachingProvider, CacheManager, Cache, Entry 和 Expiry。 </p>\n<ul>\n<li>CachingProvider定义了创建、配置、获取、管理和控制多个CacheManager。一个应用可以在运行期访问多个CachingProvider。 </li>\n<li>CacheManager定义了创建、配置、获取、管理和控制多个唯一命名的Cache，这些Cache 存在于CacheManager的上下文中。一个CacheManager仅被一个CachingProvider所拥有。 </li>\n<li>Cache是一个类似Map的数据结构并临时存储以Key为索引的值。一个Cache仅被一个 CacheManager所拥有。 </li>\n<li>Entry是一个存储在Cache中的key-value对。 </li>\n<li>Expiry 每一个存储在Cache中的条目有一个定义的有效期。一旦超过这个时间，条目为过期 的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。</li>\n</ul>\n<p>他们的关系大致像下面这样<br><img src=\"https://img-blog.csdnimg.cn/20200229125119881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"Spring缓存抽象\"><a href=\"#Spring缓存抽象\" class=\"headerlink\" title=\"Spring缓存抽象\"></a>Spring缓存抽象</h2><p>Spring从3.1开始定义了org.springframework.cache.Cache 和org.springframework.cache.CacheManager接口来统一不同的缓存技术，并支持使用JCache（JSR-107）注解简化我们开发。</p>\n<ul>\n<li>Cache接口为缓存的组件规范定义，包含缓存的各种操作集合； </li>\n<li>Cache接口下Spring提供了各种xxxCache的实现；如RedisCache，EhCacheCache , ConcurrentMapCache等；</li>\n<li>每次调用需要缓存功能的方法时，Spring会检查检查指定参数的指定的目标方法是否已经被调用过，如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户，下次调用直接从缓存中获取。</li>\n<li>使用Spring缓存抽象时我们需要关注两点，第一是确定方法需要被缓存以及他们的缓存策略，第二是从缓存中读取之前缓存存储的数据</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200229162941293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"SpringBoot开启注解\"><a href=\"#SpringBoot开启注解\" class=\"headerlink\" title=\"SpringBoot开启注解\"></a>SpringBoot开启注解</h2><p>1.1:搭建SpringBoot环境</p>\n<p>在idea中，搭建一个SpringBoot是很简单的。接下来我简单说一下步骤：</p>\n<p>我们还是先使用Idea向导创建一个带web模块的项目，这个之前的博文有说，很基本的操作，然后我们就可以开始编写代码了。首先我们需要在主程序上加入可使用缓存注解，如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@SpringBootApplication</span><br><span class=\"line\">@EnableAutoConfiguration</span><br><span class=\"line\">@EnableCaching</span><br><span class=\"line\">public class SpringbootcacheApplication &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        SpringApplication.run(SpringbootcacheApplication.class, args);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>主要是@EnableCaching用于开启缓存注解的驱动，否则后面使用的缓存都是无效的，开启了之后就可以使用了，没错，就是这么简单，我们只需要在我们需要用到缓存的地方使用缓存注解就可以了，下面我们来了解一下有哪些缓存注解。</p>\n<h2 id=\"常用缓存注解\"><a href=\"#常用缓存注解\" class=\"headerlink\" title=\"常用缓存注解\"></a>常用缓存注解</h2><h3 id=\"CacheConfig\"><a href=\"#CacheConfig\" class=\"headerlink\" title=\"@CacheConfig\"></a>@CacheConfig</h3><p>这个注解的的主要作用就是全局配置缓存，比如配置缓存的名字（cacheNames),只需要在类上配置一次，下面的方法就默认以全局配置为主，不需要二次配置，节省了部分代码。</p>\n<h3 id=\"Cacheable\"><a href=\"#Cacheable\" class=\"headerlink\" title=\"@Cacheable\"></a>@Cacheable</h3><p>这个注解是最重要的，主要实现的功能再进行一个读操作的时候。就是先从缓存中查询，如果查找不到，就会走数据库的执行方法，这是缓存的注解最重要的一个方法，基本上我们的所有缓存实现都要依赖于它。</p>\n<h3 id=\"CacheEvict\"><a href=\"#CacheEvict\" class=\"headerlink\" title=\"@CacheEvict\"></a>@CacheEvict</h3><p>这个注解主要是配合@Cacheable一起使用的，它的主要作用就是清除缓存，当方法进行一些更新、删除操作的时候，这个时候就要删除缓存。如果不删除缓存，就会出现读取不到最新缓存的情况，拿到的数据都是过期的。它可以指定缓存的key和conditon，它有一个重要的属性叫做allEntries默认是false，也可以指定为true,主要作用就是清除所有的缓存，而不以指定的key为主。</p>\n<h3 id=\"CachePut\"><a href=\"#CachePut\" class=\"headerlink\" title=\"@CachePut\"></a>@CachePut</h3><p>这个注解它总是会把数据缓存，而不会去每次做检查它是否存在，相比之下它的使用场景就比较少，毕竟我们希望并不是每次都把所有的数据都给查出来，我们还是希望能找到缓存的数据，直接返回，这样能提升我们的软件效率。</p>\n<h3 id=\"cache\"><a href=\"#cache\" class=\"headerlink\" title=\"@cache\"></a>@cache</h3><p>这个注解它是上面的注解的综合体，包含上面的三个注解（cacheable、cachePut、CacheEvict），可以使用这一个注解来包含上面的所有的注解，看源码如下<br><img src=\"https://img-blog.csdnimg.cn/20200228170220904.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>上面的注解总结如下表格：<br><img src=\"https://img-blog.csdnimg.cn/20200228170236581.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>我这里把上面介绍到的注解的相关属性，列一张表放出来放，这样方便理解<br><img src=\"https://img-blog.csdnimg.cn/20200229164202441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"使用实例\"><a href=\"#使用实例\" class=\"headerlink\" title=\"使用实例\"></a>使用实例</h2><p>首先我们在前文博文的基础上，建立数据库（如何建看前面博文，记得创建项目的时候勾选Mybatis），我们来新建一个表，含义为文章，下面的示例将会在这张表中进行操作，所使用的框架为SSM+SpringBoot，下面是创建数据库的表。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Artile (</span><br><span class=\"line\">`id`  int(11) NOT NULL AUTO_INCREMENT ,</span><br><span class=\"line\">`title`  varchar(30) CHARACTER SET gbk COLLATE gbk_chinese_ci NULL DEFAULT NULL ,</span><br><span class=\"line\">`author`  varchar(30) CHARACTER SET gbk COLLATE gbk_chinese_ci NULL DEFAULT NULL ,</span><br><span class=\"line\">`content`  mediumtext CHARACTER SET gbk COLLATE gbk_chinese_ci NULL ,</span><br><span class=\"line\">`file_name`  varchar(30) CHARACTER SET gbk COLLATE gbk_chinese_ci NULL DEFAULT NULL ,</span><br><span class=\"line\">`state`  smallint(2) NULL DEFAULT 1 COMMENT <span class=\"string\">&#x27;状态&#x27;</span> ,</span><br><span class=\"line\">PRIMARY KEY (`id`)</span><br><span class=\"line\">)</span><br><span class=\"line\">ENGINE=InnoDB</span><br><span class=\"line\">DEFAULT CHARACTER SET=gbk COLLATE=gbk_chinese_ci</span><br><span class=\"line\">AUTO_INCREMENT=11</span><br><span class=\"line\">ROW_FORMAT=COMPACT</span><br></pre></td></tr></table></figure>\n<p>接着我们创建Mapper层，主要就是对Article进行增删改查的业务操作，映射到具体的xml的sql里（映射的原理我再前面博文的数据访问中有说过，当然，我们也可以直接在mapper层中写入@Select注解来写sql语句），然后用service去调用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">public interface ArticleMapper &#123;</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 插入一篇文章</span><br><span class=\"line\">\t * @param title</span><br><span class=\"line\">\t * @param author</span><br><span class=\"line\">\t * @param content</span><br><span class=\"line\">\t * @param fileName</span><br><span class=\"line\">\t * @<span class=\"built_in\">return</span></span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Integer addArticle(@Param(<span class=\"string\">&quot;title&quot;</span>) String  title,@Param(<span class=\"string\">&quot;author&quot;</span>)String author,</span><br><span class=\"line\">\t                              @Param(<span class=\"string\">&quot;content&quot;</span>)String content,@Param(<span class=\"string\">&quot;fileName&quot;</span>)String fileName);</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据id获取文章</span><br><span class=\"line\">\t * @param id</span><br><span class=\"line\">\t * @<span class=\"built_in\">return</span></span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Article getArticleById(@Param(<span class=\"string\">&quot;id&quot;</span>) Integer id);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 更新content</span><br><span class=\"line\">\t * @param content</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Integer updateContentById(@Param(<span class=\"string\">&quot;content&quot;</span>)String content,@Param(<span class=\"string\">&quot;id&quot;</span>)Integer id);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据id删除文章</span><br><span class=\"line\">\t * @param id</span><br><span class=\"line\">\t * @<span class=\"built_in\">return</span></span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Integer removeArticleById(@Param(<span class=\"string\">&quot;id&quot;</span>)Integer id);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 获得上一次插入的id</span><br><span class=\"line\">\t * @<span class=\"built_in\">return</span></span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Integer getLastInertId();</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后就是service层，主要需要注意的是我们上述讲述的缓存注解都是基于service层(不能放在contoller和dao层)，首先我们在类上配置一个CacheConfig，然后配置一个cacheNames，那么下面的方法都是以这个缓存名字作为默认值，他们的缓存名字都是这个，不必进行额外的配置。当进行select查询方法的时候，我们配置上@Cacheable，并指定key，这样除了第一次之外，我们都会把结果缓存起来，以后的结果都会把这个缓存直接返回。而当进行更新数据（删除或者更新操作）的时候，使用@CacheEvict来清除缓存，防止调用@Cacheabel的时候没有更新缓存</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@Service</span><br><span class=\"line\">@CacheConfig(cacheNames = <span class=\"string\">&quot;articleCache&quot;</span>)</span><br><span class=\"line\">public class ArticleService &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    private AtomicInteger count =new AtomicInteger(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private ArticleMapper articleMapper;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 增加一篇文章 每次就进行缓存</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @CachePut</span><br><span class=\"line\">    public Integer addArticle(Article article)&#123;</span><br><span class=\"line\">        Integer result = articleMapper.addArticle(article.getTitle(), article.getAuthor(), article.getContent(), article.getFileName());</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (result&gt;0) &#123;</span><br><span class=\"line\">            Integer lastInertId = articleMapper.getLastInertId();</span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;--执行增加操作--id:&quot;</span> + lastInertId);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 获取文章  以传入的id为键，当state为0的时候不进行缓存</span><br><span class=\"line\">     * @param id 文章id</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @Cacheable(key = <span class=\"string\">&quot;#id&quot;</span>,unless = <span class=\"string\">&quot;#result.state==0&quot;</span>)</span><br><span class=\"line\">    public Article getArticle(Integer id) &#123;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            //模拟耗时操作</span><br><span class=\"line\">            Thread.sleep(5000);</span><br><span class=\"line\">        &#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        final Article artcile = articleMapper.getArticleById(id);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;--执行数据库查询操作&quot;</span>+count.incrementAndGet()+<span class=\"string\">&quot;次&quot;</span>+<span class=\"string\">&quot;id:&quot;</span>+id);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> artcile;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 通过id更新内容 清除以id作为键的缓存</span><br><span class=\"line\">     *</span><br><span class=\"line\">     * @param id</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @CacheEvict(key = <span class=\"string\">&quot;#id&quot;</span>)</span><br><span class=\"line\">    public Integer updateContentById(String contetnt, Integer id) &#123;</span><br><span class=\"line\">        Integer result = articleMapper.updateContentById(contetnt, id);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;--执行更新操作id:--&quot;</span>+id);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 通过id移除文章</span><br><span class=\"line\">     * @param id  清除以id作为键的缓存</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @CacheEvict(key = <span class=\"string\">&quot;#id&quot;</span>)</span><br><span class=\"line\">    public Integer removeArticleById(Integer id)&#123;</span><br><span class=\"line\">        final Integer result = articleMapper.removeArticleById(id);</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;执行删除操作,id:&quot;</span>+id);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接着编写controller层，主要是接受客户端的请求，我们配置了@RestController表示它是一个rest风格的应用程序，在收到add请求会增加一条数据，get请求会查询一条数据，resh会更新一条数据，rem会删除一条数据</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RestController</span><br><span class=\"line\">@ComponentScan(basePackages = &#123;<span class=\"string\">&quot;com.wyq.controller&quot;</span>, <span class=\"string\">&quot;com.wyq.service&quot;</span>&#125;)</span><br><span class=\"line\">@MapperScan(basePackages = &#123;<span class=\"string\">&quot;com.wyq.dao&quot;</span>&#125;)</span><br><span class=\"line\">public class ArticleController &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private ArticleService articleService;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    ArticleMapper articleMapper;</span><br><span class=\"line\"></span><br><span class=\"line\">    @PostMapping(<span class=\"string\">&quot;/add&quot;</span>)</span><br><span class=\"line\">    public ResultVo addArticle(@RequestBody Article article) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        System.out.println(article.toString());</span><br><span class=\"line\">        Integer result = articleService.addArticle(article);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (result &gt;= 0) &#123;</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.success(result);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> ResultVo.fail();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/get&quot;</span>)</span><br><span class=\"line\">    public ResultVo getArticle(@RequestParam(<span class=\"string\">&quot;id&quot;</span>) Integer id) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        Long start = System.currentTimeMillis();</span><br><span class=\"line\">        Article article = articleService.getArticle(id);</span><br><span class=\"line\">        Long end = System.currentTimeMillis();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;耗时：&quot;</span>+(end-start));</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (null != article)</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.success(article);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> ResultVo.fail();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 更新一篇文章</span><br><span class=\"line\">     *</span><br><span class=\"line\">     * @param contetnt</span><br><span class=\"line\">     * @param id</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/resh&quot;</span>)</span><br><span class=\"line\">    public ResultVo update(@RequestParam(<span class=\"string\">&quot;content&quot;</span>) String contetnt, @RequestParam(<span class=\"string\">&quot;id&quot;</span>) Integer id) &#123;</span><br><span class=\"line\">        final Integer result = articleService.updateContentById(contetnt, id);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (result &gt; 0) &#123;</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.success(result);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.fail();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * 删除一篇文章</span><br><span class=\"line\">     *</span><br><span class=\"line\">     * @param id</span><br><span class=\"line\">     * @<span class=\"built_in\">return</span></span><br><span class=\"line\">     */</span><br><span class=\"line\">    @GetMapping(<span class=\"string\">&quot;/rem&quot;</span>)</span><br><span class=\"line\">    public ResultVo remove(@RequestParam(<span class=\"string\">&quot;id&quot;</span>) Integer id) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        final Integer result = articleService.removeArticleById(id);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (result &gt; 0) &#123;</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.success(result);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">return</span> ResultVo.fail();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试实例\"><a href=\"#测试实例\" class=\"headerlink\" title=\"测试实例\"></a>测试实例</h2><p>这里使用postman模拟接口请求，首先我们来增加一篇文章：请求add接口：<br><img src=\"https://img-blog.csdnimg.cn/20200228170304272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>后台返回表示成功：<br><img src=\"https://img-blog.csdnimg.cn/20200228170315191.png#pic_center\" alt=\"在这里插入图片描述\"><br>我看到后台数据库已经插入了数据，它的id是11<br><img src=\"https://img-blog.csdnimg.cn/2020022817032885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>执行查询操作，在查询操作中，getArticle，我使用线程睡眠的方式，模拟了5秒的时间来处理耗时性业务，第一次请求肯定会查询数据库，理论上第二次请求，将会走缓存，我们来测试一下:首先执行查询操作<br><img src=\"https://img-blog.csdnimg.cn/20200228170340870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>接口响应成功，再看一下后台打印：表示执行了一次查询操作，耗时5078秒<br><img src=\"https://img-blog.csdnimg.cn/20200228170356172.png#pic_center\" alt=\"在这里插入图片描述\"><br>好，重点来了，我们再次请求接口看看会返回什么？理论上，将不会走数据库执行操作，并且耗时会大大减少：与上面的比对，这次没有打印执行数据库查询操作，证明没有走数据库，并且耗时只有5ms，成功了！缓存发挥作用，从5078秒减小到5秒！大大提升了响应速度，哈哈！<br><img src=\"https://img-blog.csdnimg.cn/2020022817040773.png#pic_center\" alt=\"在这里插入图片描述\"><br>更新操作，当我们进行修改操作的时候，我们希望缓存的数据被清空：看接口返回值成功了，再看数据库<br><img src=\"https://img-blog.csdnimg.cn/2020022817041966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200228170429672.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>后台控制台打印：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">--执行更新操作id:--11</span><br></pre></td></tr></table></figure>\n<p> 趁热打铁，我们再次请求三次查询接口，看看会返回什么？每次都会返回这样的结果，但是我的直观感受就是第一次最慢，第二次、第三次返回都很快<br> <img src=\"https://img-blog.csdnimg.cn/20200229170335399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br> 再看看后台打印了什么？执行id为11的数据库查询操作，这是因为缓存被清空了，所以它又走数据库了（获得最新数据），然后后面的查询都会走缓存！很明显，实验成功！<br> <img src=\"https://img-blog.csdnimg.cn/20200229170348794.png#pic_center\" alt=\"在这里插入图片描述\"><br>删除操作。同理，在删除操作中，执行了一次删除，那么缓存也会被清空，查询的时候会再次走数据库，这里就不给具体实验效果了。</p>\n<h2 id=\"整合redis实现缓存\"><a href=\"#整合redis实现缓存\" class=\"headerlink\" title=\"整合redis实现缓存\"></a>整合redis实现缓存</h2><p>SpringBoot整合redis其实非常简单（好吧SpringBoot整合啥都很简单），我们只要引入spring-boot-starter-data-redis，然后在application.yml中配置redis连接地址，接着通过使用RestTemplate操作redis，下面介绍Redis操作缓存的相关方法</p>\n<ul>\n<li>redisTemplate.opsForValue();//操作字符串</li>\n<li>redisTemplate.opsForHash();//操作hash</li>\n<li>redisTemplate.opsForList();//操作list</li>\n<li>redisTemplate.opsForSet();//操作set</li>\n<li>redisTemplate.opsForZSet();//操作有序set</li>\n<li>配置缓存、CacheManagerCustomizers</li>\n<li>测试使用缓存、切换缓存、 CompositeCacheManager</li>\n</ul>\n<h2 id=\"下一篇\"><a href=\"#下一篇\" class=\"headerlink\" title=\"下一篇\"></a>下一篇</h2><p>本篇博客介绍了springBoot中缓存的一些使用方法，如何在开发中使用缓存？怎样合理的使用都是值得我们学习的地方，本篇博客只是探讨的spring的注解缓存，相对来说比较简单。希望起到抛砖引玉的作用，下一篇博文我们将要介绍SpringBoot使用消息队列。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","缓存开发"]},{"title":"详细SpringBoot教程之配置文件（二）","url":"/Spring-Boot/45a96b0cb018/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>鉴于有人留言说想要学习SpringBoot相关的知识，我这里打算写一个SpringBoot系列的相关博文，目标呢是想让看了这一系列博文的同学呢，能够对SpringBoot略窥门径，这一系列的博文初步都定下来包括SpringBoot介绍、入门、配置、日志相关、web开发、数据访问、结合docker、缓存、消息队列、检索、任务安全、分布式等等一系列的博文，工作量很大，是个漫长的过程，每一步我都尽量详细，配上截图说明，也希望对看的同学真的有用。<br><strong>单纯就是想分享技术博文，还想说一句就是，如果觉得有用，请点个关注、给个赞吧，也算对我来说是个宽慰，毕竟也得掉不少头发，嘿嘿嘿</strong></p>\n<h2 id=\"系列文章传送条\"><a href=\"#系列文章传送条\" class=\"headerlink\" title=\"系列文章传送条\"></a>系列文章传送条</h2><p><a href=\"https://blog.csdn.net/DBC_121/article/details/104383089\">详细SpringBoot教程之入门（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104429074\">详细SpringBoot教程之入门（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104446690\">详细SpringBoot教程之配置文件（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104448353\">详细SpringBoot教程之配置文件（二）</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104456941\">详细SpringBoot教程之日志框架</a><br><a href=\"https://dengbocong.blog.csdn.net/article/details/104473765\">详细SpringBoot教程之Web开发（一）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503158\">详细SpringBoot教程之Web开发（二）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104503231\">详细SpringBoot教程之Web开发（三）</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104527730\">详细SpringBoot教程之数据访问</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104541113\">详细SpringBoot教程之启动配置原理</a><br><a href=\"https://blog.csdn.net/DBC_121/article/details/104559440\">详细SpringBoot教程之缓存开发</a></p>\n<h2 id=\"配置映射的另一种方式\"><a href=\"#配置映射的另一种方式\" class=\"headerlink\" title=\"配置映射的另一种方式\"></a>配置映射的另一种方式</h2><p>上一篇博文我们进行了配置文件的编写，映射，注入以及如何进行测试相关的操作，在这里我们再来介绍另一种配置文件的映射注入，即使用其他注解的方式进行注入，我们这里要使用的注解是@Value，@Value是Spring的底层注解。</p>\n<p>我们回想一下我们以前写SpringMvc的配置文件的时候，我们在配置文件中使用bean进行配置，然后进行相关映射，还记得不，不记得我这里写出一个bean配置回忆一下，内容如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;bean class=<span class=\"string\">&quot;Person&quot;</span>&gt;</span><br><span class=\"line\">\t&lt;property name=<span class=\"string\">&quot;lastName&quot;</span> value=<span class=\"string\">&quot;字面量/环境变量/#&#123;获取值&#125;等等&quot;</span>&gt;&lt;/property&gt;</span><br><span class=\"line\">&lt;bean/&gt;</span><br></pre></td></tr></table></figure>\n<p>发现没有，以前的方式是我们通过一个一个给属性赋值，比如这里我们给lastName赋值，值就是value指定的。可能你已经意识到了，没错，@Value的作用其实和这里的value一样，相当于Spring帮我们简化了，只需要通过一个注解就能赋值，不过也是一个一个赋值哦，如何使用以及运行结果如下<br><img src=\"https://img-blog.csdnimg.cn/20200222185701552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>发现没有，结果依旧可以获取值，所以以为这我们依旧配置成功。</p>\n<h2 id=\"ConfigurationProperties和-Value比较\"><a href=\"#ConfigurationProperties和-Value比较\" class=\"headerlink\" title=\"@ConfigurationProperties和@Value比较\"></a>@ConfigurationProperties和@Value比较</h2><p>我们上一篇博文使用的是@ConfigurationProperties映射yml或者properties配置文件，这篇博文前面我们举例使用@Value进行配置，那么这两者有什么区别呢？首先我这里给一个比较官方的区别<br>|  | @ConfigurationProperties | @Value |<br>|–|–| – |<br>| 功能 | 批量注入配置文件中的属性 |  一个个指定 |<br>| 松散绑定| 支持 |  不支持 |<br>| SpELl | 不支持 |  支持 |<br>| JSR30数据校验| 支持 |  不支持 |<br>| 复杂类型封装| 支持 |  不支持 |</p>\n<p>那么这里来解释一下什么是松散绑定呢？其实我们前面已经接触过了，只是不知道那是松散语法而已。比如我们有一个属性时lastName，按道理我们在配置文件中的属性名和Person类中的属性名需要一直对不对，不然没办法映射，但是在松散语法中，lastName和last-name是认为同一个，这种写法就是松散写法，也就是说，我们在配置文件中，person.lastName和person.last-name两种写法都可以，但是@Value是不支持松散写法的，属性名必须一致。</p>\n<p>还有就是JSR30数据校验，其实我们以前写Spring项目的时候基本都用过，就是校验器，比如对属性是否符合邮箱格式进行校验，不符合提示错误等，而配置文件注入值数据校验，只能使用ConfigurationProperties，如下<br><img src=\"https://img-blog.csdnimg.cn/20200222190916689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>所以如果说我们只是在某个业务逻辑中需要获取一下配置文件中的某项值，可以使用@Value比较简洁。而如果说我们专门编写一个javaBean来和配置文件进行映射，我们就直接使用ConfigurationProperties。</p>\n<h2 id=\"PropertySource和-ImportResource比较\"><a href=\"#PropertySource和-ImportResource比较\" class=\"headerlink\" title=\"@PropertySource和@ImportResource比较\"></a>@PropertySource和@ImportResource比较</h2><h3 id=\"PropertySource\"><a href=\"#PropertySource\" class=\"headerlink\" title=\"@PropertySource\"></a>@PropertySource</h3><p>我们前面在Person类上直接使用@ConfigurationProperties或者@Value进行映射，SpringBoot都是默认到主配置文件中寻找。而@PropertySource就是用来加载指定的配置文件。</p>\n<p>怎么理解呢？比如我们的application.properties和application.Yml是全局配置文件，我们原先把配置内容都是写到全局配置文件中的，然后我们使用@ConfigurationProperties或者@Value进行映射，也就是说@ConfigurationProperties或者@Value是默认映射全局配置文件的，那么如果项目一旦很大，我们不可能所有配置都写到全局配置文件中，所以这个时候我们需要创建一些其他的配置文件，然后指定映射。</p>\n<p>可能直接讲不容易理解，下面我举个例子，我们在resources下面创建一个person.properties，然后把application.properties里面的内容粘贴到person.properties中，这个时候我们通过@PropertySource，将Person类和person.properties进行映射，如下图。<br><img src=\"https://img-blog.csdnimg.cn/20200222225018617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"ImportResource\"><a href=\"#ImportResource\" class=\"headerlink\" title=\"@ImportResource\"></a>@ImportResource</h3><p>@ImportResource是用来导入原先Spring的配置文件，让配置文件里面的内容生效，也就是说，如果想按照以前SpringMVC的那种出入配置，我们可以使用这个注解来完成。</p>\n<p>为了更好的理解，我们先在resourcs下创建一个bean.xml（就是以前Spring的配置文件），然后按照以前的方式写一个bean，如下内容<br><img src=\"https://img-blog.csdnimg.cn/20200222225927353.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后我们新写一个测试类，使用ApplicationContext容器，判断容器中是否注入了helloService，内容如下<br><img src=\"https://img-blog.csdnimg.cn/20200222230301813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>然后启动测试类，运行之后查看发现helloService没有被注入，日志输出如下<br><img src=\"https://img-blog.csdnimg.cn/20200222230447176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这个时候就可以用到我们的ImportResource了，SpringBoot里面没有Spring的配置文件，我们自己编写的Spring配置文件也不能自动识别，想让Spring的配置文件生效，加载进容器，就可以使用@ImportResource标注在一个配置类上，这里我们在主配置类上标注如下，再次运行测试类<br><img src=\"https://img-blog.csdnimg.cn/20200222230800355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222230839128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>当然我们实际开发中不可能像这样使用注解一个一个导入，而我们SpringBoot推荐给容器中添加组件的方式，也就是使用全注解的方式。怎么理解呢？就是我们专门创建一些配置类，用来同意管理项目中所有的配置注入。举个例子，我们专门创建一个配置类，如下创建一个config包，然后创建一个MyAppConfig类，内容如下<br><img src=\"https://img-blog.csdnimg.cn/20200222231125479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>我们现在来测试一下，注意了，测试之前先把主配置类里面的@ImportResource去掉，我不使用@ImportResource注解，启动测试类，helloService注入依旧成功。</p>\n<h2 id=\"配置文件占位符\"><a href=\"#配置文件占位符\" class=\"headerlink\" title=\"配置文件占位符\"></a>配置文件占位符</h2><p>RandomValuePropertySource：配置文件中可以使用随机数如</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$&#123;random.value&#125;</span></span><br><span class=\"line\"><span class=\"variable\">$&#123;random.int&#125;</span></span><br><span class=\"line\"><span class=\"variable\">$&#123;random.long&#125;</span></span><br><span class=\"line\"><span class=\"variable\">$&#123;random.int(10)&#125;</span></span><br><span class=\"line\"><span class=\"variable\">$&#123;random.int[1024,65536]&#125;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>还有属性配置占位符</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">app.name=MyApp</span><br><span class=\"line\">app.description=<span class=\"variable\">$&#123;app.name&#125;</span> is a Spring Boot application</span><br></pre></td></tr></table></figure>\n<p>可以在配置文件中引用前面配置过的属性（优先级前面配置过的这里都能使用）而可以通过${app.name:默认值}来指定找不到属性时的默认值，还是一样，我在配置文件中举个例子帮助理解<br><img src=\"https://img-blog.csdnimg.cn/2020022223162311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"profile快速切换配置\"><a href=\"#profile快速切换配置\" class=\"headerlink\" title=\"profile快速切换配置\"></a>profile快速切换配置</h2><p>profile是Spring对不同环境提供不同配置功能的支持，可以通过激活、指定参数等方式快速切换环境，通俗点理解就是，开发环境的配置，生产环境的配置，测试环境的配置分开，使用profile可以进行快速切换</p>\n<h3 id=\"激活方式（即切换方式）\"><a href=\"#激活方式（即切换方式）\" class=\"headerlink\" title=\"激活方式（即切换方式）\"></a>激活方式（即切换方式）</h3><ul>\n<li>命令行： –spring.profiles.active=dev  （打包之后，使用命令行执行的时候，指定参数）</li>\n<li>配置文件：spring.profiles.active=dev  </li>\n<li>Jvm参数 -Dspring.profiles.active=dev  （在Idea修改参数）</li>\n</ul>\n<h3 id=\"多profile文件形式\"><a href=\"#多profile文件形式\" class=\"headerlink\" title=\"多profile文件形式\"></a>多profile文件形式</h3><p>有了激活方式，我们来看怎么使用，首先我们创建三种配置（包括原来的application.properties在内的三个），一般的配置文件格式：application-{profile}.properties。比如application-dev.properties、application-prod.properties。创建之后，我们可以在另外两个配置文件中配置我们想要的配置，然后在主配置文件中进行激活即可，如下<br><img src=\"https://img-blog.csdnimg.cn/20200222232029197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"多profile文档块模式\"><a href=\"#多profile文档块模式\" class=\"headerlink\" title=\"多profile文档块模式\"></a>多profile文档块模式</h3><p>上面是通过properties进行多profile的配置，我们也可以使用yml来进行多profile的配置，yml的配置更简洁，不需要额外创建像application-dev.properties、application-prod.properties这样的配置文件，只需要在主yml配置文件中，使用“—”的文档块模式，如下图<br><img src=\"https://img-blog.csdnimg.cn/20200222232546294.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"配置文件加载位置\"><a href=\"#配置文件加载位置\" class=\"headerlink\" title=\"配置文件加载位置\"></a>配置文件加载位置</h2><p>SpringBoot启动会扫描以下位置的application.properties或者application.yml文件作为SpringBoot的默认配置文件，也就是说我们可以更改这两个配置文件的位置</p>\n<ul>\n<li>file:./config/</li>\n<li>file:./</li>\n<li>classpath:/config/</li>\n<li>classpath:/<br>说明一下classpath是在resources目录位置，file是在项目根目录的位置，以上是按照优先级从高到低的顺序，所有位置的文件都会被加载，高优先级配置内容会覆盖低优先级配置内容，配置文件的内容是互补的，也就是说高优先级中没有的配置，如果低优先级有的话一样会生效，但是如果高优先级的配置中已经有了一项配置，低优先级中相同的配置就会被覆盖掉。</li>\n</ul>\n<p>我们也可以通过配置spring.config.location来改变默认配置，也就是说配置文件可以放在任意的位置，网络上或者电脑桌面任意位置，把位置路径赋给location就可以了，这样对于打包后的项目，使用命令行做运维</p>\n<h2 id=\"外部配置的加载顺序\"><a href=\"#外部配置的加载顺序\" class=\"headerlink\" title=\"外部配置的加载顺序\"></a>外部配置的加载顺序</h2><ul>\n<li>SpringBoot支持多种外部配置方式</li>\n<li>命令行参数</li>\n<li>来自java:comp/env的JNDI属性</li>\n<li>Java系统属性（System.getProperties）</li>\n<li>操作系统环境变量</li>\n<li>RandomValuePropertySource配置的random.*属性值</li>\n<li>Jar包外的application-{profile}.properties或application.yml（带spring.profile）配置文件</li>\n<li>Jar包内的application-{profile}.properties或application.yml（带spring.profile）配置文件</li>\n<li>Jar包外的application.properties或application.yml（不带spring.profile）配置文件</li>\n<li>Jar包外的application.properties或application.yml（不带spring.profile）配置文件</li>\n<li>@Configuration注解类上的@PropertySource</li>\n<li>通过SpringApplication.setDefaultProperties指定的默认属性</li>\n</ul>\n<p>也就是说，除了我们项目运行配置的文件，我们可以在任何外部环境中，继续配置相关文件进行运行，及时项目已经被打包了，我们也可以使用相关命令行指令，运行在某个位置我们的配置文件，从而启动项目，非常方便运维对项目进行维护而不需要重新打包项目。</p>\n<h2 id=\"自动配置原理\"><a href=\"#自动配置原理\" class=\"headerlink\" title=\"自动配置原理\"></a>自动配置原理</h2><p>前面我们其实有提高过自动配置类，即@EnableAutoConfiguration，不过只是带过了他的作用，没有深入讨论，这里我们进行深入讨论，首先先放出所有自动配置文件属性的参考，<a href=\"https://docs.spring.io/spring-boot/docs/2.2.4.RELEASE/reference/html/appendix-application-properties.html#common-application-properties\">官网文档位置</a></p>\n<p>然后，我们这里通过观察HttpEncodingAutoConfiguration的这个自动配置来讲解，可以在如下位置中找到<br><img src=\"https://img-blog.csdnimg.cn/20200222233725486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>打开HttpEncodingAutoConfiguration，进入到里面，我们逐个进行分析<br><img src=\"https://img-blog.csdnimg.cn/2020022223331585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>点进HttpProperties.class我们会惊奇的发现，里面也是使用@ConfigurationProperties(prefix = “spring.http”)进行配置，这和我们之前在application.properties中手动配置相关组件的时候，使用的是一样的方式，所以能不能体会到，其实我们可以自行配置更多组件，并进行封装<br><img src=\"https://img-blog.csdnimg.cn/20200222233919920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222234124377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>这里我们就可以总结一下自动配置文件的通用模式（即通过命名分辨）</p>\n<ul>\n<li>xxxAutoConfiguration:自动配置类</li>\n<li>xxxProperties：属性配置类<br>也就是说，我们yml/properties文件中能配置的值就来源于属性配置类</li>\n</ul>\n<p>@EnableAutoConfiguration 的作用是利用AutoConfigurationImportSelector给容器中导入一些组件，详细的可以查看SpringFactoriesLoader.loadFactoryNames<br>意思是扫描所有jar包类路径下，META-INF/spring.factories。把扫描到的这些文件的内容装成properties对象，然后从properties中获取到EnableAutoConfiguration.class类对应的值，然后把它们添加到容器中，也就是将类路径下META-INF/spring.factories里面配置的所有EnableAutoConfiguration的值加入到容器中，类似下面这些自动配置类</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.cloud.CloudServiceConnectorsAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveElasticsearchRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveRestClientAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.jdbc.JdbcRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.mongo.MongoReactiveRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\\</span><br><span class=\"line\">org.springframework.boot.autoconfigure.data.redis.RedisReactiveAutoConfiguration,\\</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>所有在配置文件中能配置的属性都是在xxxProperties类中封装这，配置文件能配置什么就可以参照某个功能对应的这个属性类，根据当前不同条件判断，决定这个配置是否生效。SpringBoot启动会加载大量的自动配置类，如果我们需要的功能有没有SpringBoot默认写好的自动配置类，那我们就手动引入或配置，在开发中，我们看这个自动配置类中到底配置了哪些组件，只要我们要用的组件存在，我们就不需要再来配置了。给容器中自动配置类添加组件的时候，会从properties类中获取某些属性，我们就可以在配置文件中指定配置属性的值。</p>\n<h3 id=\"细节\"><a href=\"#细节\" class=\"headerlink\" title=\"细节\"></a>细节</h3><ul>\n<li>@Conditional由Spring提供，而在Spring Boot中衍生出了以下相关的注解：</li>\n<li>@ConditionalOnBean：当容器中有指定Bean的条件下。</li>\n<li>@ConditionalOnClass：当classpath类路径下有指定类的条件下。</li>\n<li>@ConditionalOnCloudPlatform：当指定的云平台处于active状态时。</li>\n<li>@ConditionalOnExpression：基于SpEL表达式的条件判断。</li>\n<li>@ConditionalOnJava：基于JVM版本作为判断条件。</li>\n<li>@ConditionalOnJndi：在JNDI存在的条件下查找指定的位置。</li>\n<li>@ConditionalOnMissingBean：当容器里没有指定Bean的条件。</li>\n<li>@ConditionalOnMissingClass：当类路径下没有指定类的条件下。</li>\n<li>@ConditionalOnNotWebApplication：当项目不是一个Web项目的条件下。</li>\n<li>@ConditionalOnProperty：当指定的属性有指定的值的条件下。</li>\n<li>@ConditionalOnResource：类路径是否有指定的值。</li>\n<li>@ConditionalOnSingleCandidate：当指定的Bean在容器中只有一个，或者有多个但是指定了首选的Bean。</li>\n<li>@ConditionalOnWebApplication：当项目是一个Web项目的条件下。<br>以上组合注解均位于spring-boot-autoconfigure jar包下的org.springframework.boot.autoconfigure.condition包</li>\n</ul>\n<h3 id=\"Debug查看详细的自动配置报告\"><a href=\"#Debug查看详细的自动配置报告\" class=\"headerlink\" title=\"Debug查看详细的自动配置报告\"></a>Debug查看详细的自动配置报告</h3><p>SpringBoot的debug功能还是非常有帮助的，比如它可以帮助我们查看项目加载注入了哪些自动配置类，从而使得我们不用一个类一个类点入查看。SpringBoot启动的时候，加载主配置类，进而开启了自动配置功能@EnableAutoConfiguration。</p>\n<p>因此，我们可以在配置文件中开启debug=true，来控制打印自动配置报告，这样我们就可以很方便的知道自动配置类生效</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20200222234549747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222234606754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/20200222234617504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","配置文件"]},{"title":"彻底理解AbstractQueuedSynchronizer（二）","url":"/Java/acf870f6fb09/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>在分析 Java 并发包 java.util.concurrent 源码的时候，少不了需要了解 AbstractQueuedSynchronizer（以下简写AQS）这个抽象类，因为它是 Java 并发包的基础工具类，是实现 ReentrantLock、CountDownLatch、Semaphore、FutureTask 等类的基础。</p>\n<p>我本人在研究AQS的时候，寻找了许多资料，过程中遇到了大神的这篇<a href=\"https://www.javadoop.com/post/AbstractQueuedSynchronizer-2\">文章</a>，写的非常清晰，内容很充实，所以我就打算引用这篇文章的内容，并在需要的地方加入我个人的观点和理解，也就是站在大神的肩膀上学习。有兴趣的可以直接前往查看原文，非常值得一看。</p>\n<h2 id=\"公平锁和非公平锁\"><a href=\"#公平锁和非公平锁\" class=\"headerlink\" title=\"公平锁和非公平锁\"></a>公平锁和非公平锁</h2><p>ReentrantLock 默认采用非公平锁，除非你在构造方法中传入参数 true 。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ReentrantLock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 默认非公平锁</span></span><br><span class=\"line\">    sync = <span class=\"keyword\">new</span> NonfairSync();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ReentrantLock</span><span class=\"params\">(<span class=\"keyword\">boolean</span> fair)</span> </span>&#123;</span><br><span class=\"line\">    sync = fair ? <span class=\"keyword\">new</span> FairSync() : <span class=\"keyword\">new</span> NonfairSync();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>公平锁的 lock 方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FairSync</span> <span class=\"keyword\">extends</span> <span class=\"title\">Sync</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        acquire(<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// AbstractQueuedSynchronizer.acquire(int arg)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquire</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class=\"line\">            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class=\"line\">            selfInterrupt();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Thread current = Thread.currentThread();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> c = getState();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!hasQueuedPredecessors() &amp;&amp;</span><br><span class=\"line\">                compareAndSetState(<span class=\"number\">0</span>, acquires)) &#123;</span><br><span class=\"line\">                setExclusiveOwnerThread(current);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> nextc = c + acquires;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (nextc &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(<span class=\"string\">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class=\"line\">            setState(nextc);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>非公平锁的 lock 方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NonfairSync</span> <span class=\"keyword\">extends</span> <span class=\"title\">Sync</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (compareAndSetState(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">            setExclusiveOwnerThread(Thread.currentThread());</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            acquire(<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// AbstractQueuedSynchronizer.acquire(int arg)</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquire</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class=\"line\">            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class=\"line\">            selfInterrupt();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">tryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nonfairTryAcquire(acquires);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Performs non-fair tryLock.  tryAcquire is implemented in</span></span><br><span class=\"line\"><span class=\"comment\"> * subclasses, but both need nonfair try for trylock method.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">nonfairTryAcquire</span><span class=\"params\">(<span class=\"keyword\">int</span> acquires)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Thread current = Thread.currentThread();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> c = getState();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 这里没有对阻塞队列进行判断</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (compareAndSetState(<span class=\"number\">0</span>, acquires)) &#123;</span><br><span class=\"line\">            setExclusiveOwnerThread(current);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> nextc = c + acquires;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (nextc &lt; <span class=\"number\">0</span>) <span class=\"comment\">// overflow</span></span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> Error(<span class=\"string\">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class=\"line\">        setState(nextc);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>总结：公平锁和非公平锁只有两处不同：</p>\n<ul>\n<li>非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。</li>\n<li>非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。</li>\n</ul>\n<p>公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。</p>\n<p>相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。</p>\n<h2 id=\"Condition\"><a href=\"#Condition\" class=\"headerlink\" title=\"Condition\"></a>Condition</h2><p>我们先来看看 Condition 的使用场景，Condition 经常可以用在生产者-消费者的场景中，请看 Doug Lea 给出的这个例子：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.locks.Condition;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.locks.Lock;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.locks.ReentrantLock;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BoundedBuffer</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Lock lock = <span class=\"keyword\">new</span> ReentrantLock();</span><br><span class=\"line\">    <span class=\"comment\">// condition 依赖于 lock 来产生</span></span><br><span class=\"line\">    <span class=\"keyword\">final</span> Condition notFull = lock.newCondition();</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Condition notEmpty = lock.newCondition();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">final</span> Object[] items = <span class=\"keyword\">new</span> Object[<span class=\"number\">100</span>];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> putptr, takeptr, count;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 生产</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">put</span><span class=\"params\">(Object x)</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">        lock.lock();</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (count == items.length)</span><br><span class=\"line\">                notFull.await();  <span class=\"comment\">// 队列已满，等待，直到 not full 才能继续生产</span></span><br><span class=\"line\">            items[putptr] = x;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (++putptr == items.length) putptr = <span class=\"number\">0</span>;</span><br><span class=\"line\">            ++count;</span><br><span class=\"line\">            notEmpty.signal(); <span class=\"comment\">// 生产成功，队列已经 not empty 了，发个通知出去</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            lock.unlock();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 消费</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Object <span class=\"title\">take</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">        lock.lock();</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (count == <span class=\"number\">0</span>)</span><br><span class=\"line\">                notEmpty.await(); <span class=\"comment\">// 队列为空，等待，直到队列 not empty，才能继续消费</span></span><br><span class=\"line\">            Object x = items[takeptr];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (++takeptr == items.length) takeptr = <span class=\"number\">0</span>;</span><br><span class=\"line\">            --count;</span><br><span class=\"line\">            notFull.signal(); <span class=\"comment\">// 被我消费掉一个，队列 not full 了，发个通知出去</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> x;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            lock.unlock();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>1、我们可以看到，在使用 condition 时，必须先持有相应的锁。这个和 Object 类中的方法有相似的语义，需要先持有某个对象的监视器锁才可以执行 wait(), notify() 或 notifyAll() 方法。<br>2、ArrayBlockingQueue 采用这种方式实现了生产者-消费者，所以请只把这个例子当做学习例子，实际生产中可以直接使用 ArrayBlockingQueue</p>\n</blockquote>\n<p>我们常用 obj.wait()，obj.notify() 或 obj.notifyAll() 来实现相似的功能，但是，它们是基于对象的监视器锁的。而这里说的 Condition 是基于 ReentrantLock 实现的，而 ReentrantLock 是依赖于 AbstractQueuedSynchronizer 实现的。</p>\n<p>在往下看之前，读者心里要有一个整体的概念。condition 是依赖于 ReentrantLock  的，不管是调用 await 进入等待还是 signal 唤醒，都必须获取到锁才能进行操作。</p>\n<p>每个 ReentrantLock  实例可以通过调用多次 newCondition 产生多个 ConditionObject 的实例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> ConditionObject <span class=\"title\">newCondition</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 实例化一个 ConditionObject</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ConditionObject();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们首先来看下我们关注的 Condition 的实现类 AbstractQueuedSynchronizer 类中的 ConditionObject。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ConditionObject</span> <span class=\"keyword\">implements</span> <span class=\"title\">Condition</span>, <span class=\"title\">java</span>.<span class=\"title\">io</span>.<span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">1173984872572414699L</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 条件队列的第一个节点</span></span><br><span class=\"line\">          <span class=\"comment\">// 不要管这里的关键字 transient，是不参与序列化的意思</span></span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> Node firstWaiter;</span><br><span class=\"line\">        <span class=\"comment\">// 条件队列的最后一个节点</span></span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> Node lastWaiter;</span><br><span class=\"line\">        ......</span><br></pre></td></tr></table></figure>\n<p>在上一篇介绍 AQS 的时候，我们有一个阻塞队列，用于保存等待获取锁的线程的队列。这里我们引入另一个概念，叫条件队列（condition queue），我画了一张简单的图用来说明这个。</p>\n<blockquote>\n<p>这里的阻塞队列如果叫做同步队列（sync queue）其实比较贴切，不过为了和前篇呼应，我就继续使用阻塞队列了。记住这里的两个概念，阻塞队列和条件队列。</p>\n</blockquote>\n<p><img src=\"https://img-blog.csdnimg.cn/20200416200040881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>基本上，把这张图看懂，你也就知道 condition 的处理流程了。所以，我先简单解释下这图，然后再具体地解释代码实现。</p>\n<ul>\n<li>条件队列和阻塞队列的节点，都是 Node 的实例，因为条件队列的节点是需要转移到阻塞队列中去的；</li>\n<li>我们知道一个 ReentrantLock 实例可以通过多次调用 newCondition() 来产生多个 Condition 实例，这里对应 condition1 和 condition2。注意，ConditionObject 只有两个属性 firstWaiter 和 lastWaiter；</li>\n<li>每个 condition 有一个关联的条件队列，如线程 1 调用 condition1.await() 方法即可将当前线程 1 包装成 Node 后加入到条件队列中，然后阻塞在这里，不继续往下执行，条件队列是一个单向链表；</li>\n<li>调用condition1.signal() 触发一次唤醒，此时唤醒的是队头，会将condition1 对应的条件队列的 firstWaiter（队头） 移到阻塞队列的队尾，等待获取锁，获取锁后 await 方法才能返回，继续往下执行。</li>\n</ul>\n<p>上面的 2-&gt;3-&gt;4 描述了一个最简单的流程，没有考虑中断、signalAll、还有带有超时参数的 await 方法等，不过把这里弄懂是这节的主要目的。同时，从图中也可以很直观地看出，哪些操作是线程安全的，哪些操作是线程不安全的。 这个图看懂后，下面的代码分析就简单了。接下来，我们一步步按照流程来走代码分析，我们先来看看 wait 方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 首先，这个方法是可被中断的，不可被中断的是另一个方法 awaitUninterruptibly()</span></span><br><span class=\"line\"><span class=\"comment\">// 这个方法会阻塞，直到调用 signal 方法（指 signal() 和 signalAll()，下同），或被中断</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">await</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 老规矩，既然该方法要响应中断，那么在最开始就判断中断状态</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.interrupted())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 添加到 condition 的条件队列中</span></span><br><span class=\"line\">    Node node = addConditionWaiter();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 释放锁，返回值是释放锁之前的 state 值</span></span><br><span class=\"line\">    <span class=\"comment\">// await() 之前，当前线程是必须持有锁的，这里肯定要释放掉</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> savedState = fullyRelease(node);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">int</span> interruptMode = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 这里退出循环有两种情况，之后再仔细分析</span></span><br><span class=\"line\">    <span class=\"comment\">// 1. isOnSyncQueue(node) 返回 true，即当前 node 已经转移到阻塞队列了</span></span><br><span class=\"line\">    <span class=\"comment\">// 2. checkInterruptWhileWaiting(node) != 0 会到 break，然后退出循环，代表的是线程中断</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class=\"line\">        LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 被唤醒后，将进入阻塞队列，等待获取锁</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class=\"line\">        interruptMode = REINTERRUPT;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node.nextWaiter != <span class=\"keyword\">null</span>) <span class=\"comment\">// clean up if cancelled</span></span><br><span class=\"line\">        unlinkCancelledWaiters();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (interruptMode != <span class=\"number\">0</span>)</span><br><span class=\"line\">        reportInterruptAfterWait(interruptMode);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其实，我大体上也把整个 await 过程说得十之八九了，下面我们分步把上面的几个点用源码说清楚。</p>\n<h3 id=\"将节点加入到条件队列\"><a href=\"#将节点加入到条件队列\" class=\"headerlink\" title=\"将节点加入到条件队列\"></a>将节点加入到条件队列</h3><p>addConditionWaiter() 是将当前节点加入到条件队列，看图我们知道，这种条件队列内的操作是线程安全的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 将当前线程对应的节点入队，插入队尾</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Node <span class=\"title\">addConditionWaiter</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node t = lastWaiter;</span><br><span class=\"line\">    <span class=\"comment\">// 如果条件队列的最后一个节点取消了，将其清除出去</span></span><br><span class=\"line\">    <span class=\"comment\">// 为什么这里把 waitStatus 不等于 Node.CONDITION，就判定为该节点发生了取消排队？</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (t != <span class=\"keyword\">null</span> &amp;&amp; t.waitStatus != Node.CONDITION) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 这个方法会遍历整个条件队列，然后会将已取消的所有节点清除出队列</span></span><br><span class=\"line\">        unlinkCancelledWaiters();</span><br><span class=\"line\">        t = lastWaiter;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// node 在初始化的时候，指定 waitStatus 为 Node.CONDITION</span></span><br><span class=\"line\">    Node node = <span class=\"keyword\">new</span> Node(Thread.currentThread(), Node.CONDITION);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// t 此时是 lastWaiter，队尾</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果队列为空</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (t == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        firstWaiter = node;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        t.nextWaiter = node;</span><br><span class=\"line\">    lastWaiter = node;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> node;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的这块代码很简单，就是将当前线程进入到条件队列的队尾。在addWaiter 方法中，有一个 unlinkCancelledWaiters() 方法，该方法用于清除队列中已经取消等待的节点。当 await 的时候如果发生了取消操作（这点之后会说），或者是在节点入队的时候，发现最后一个节点是被取消的，会调用一次这个方法。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 等待队列是一个单向链表，遍历链表将已经取消等待的节点清除出去</span></span><br><span class=\"line\"><span class=\"comment\">// 纯属链表操作，很好理解，看不懂多看几遍就可以了</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">unlinkCancelledWaiters</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node t = firstWaiter;</span><br><span class=\"line\">    Node trail = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (t != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        Node next = t.nextWaiter;</span><br><span class=\"line\">        <span class=\"comment\">// 如果节点的状态不是 Node.CONDITION 的话，这个节点就是被取消的</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t.waitStatus != Node.CONDITION) &#123;</span><br><span class=\"line\">            t.nextWaiter = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (trail == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">                firstWaiter = next;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                trail.nextWaiter = next;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (next == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">                lastWaiter = trail;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            trail = t;</span><br><span class=\"line\">        t = next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"完全释放独占锁\"><a href=\"#完全释放独占锁\" class=\"headerlink\" title=\"完全释放独占锁\"></a>完全释放独占锁</h3><p>回到 wait 方法，节点入队了以后，会调用 int savedState = fullyRelease(node); 方法释放锁，注意，这里是完全释放独占锁（fully release），因为 ReentrantLock 是可以重入的。</p>\n<blockquote>\n<p>考虑一下这里的 savedState。如果在 condition1.await() 之前，假设线程先执行了 2 次 lock() 操作，那么 state 为 2，我们理解为该线程持有 2 把锁，这里 await() 方法必须将 state 设置为 0，然后再进入挂起状态，这样其他线程才能持有锁。当它被唤醒的时候，它需要重新持有 2 把锁，才能继续下去。</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 首先，我们要先观察到返回值 savedState 代表 release 之前的 state 值</span></span><br><span class=\"line\"><span class=\"comment\">// 对于最简单的操作：先 lock.lock()，然后 condition1.await()。</span></span><br><span class=\"line\"><span class=\"comment\">//         那么 state 经过这个方法由 1 变为 0，锁释放，此方法返回 1</span></span><br><span class=\"line\"><span class=\"comment\">//         相应的，如果 lock 重入了 n 次，savedState == n</span></span><br><span class=\"line\"><span class=\"comment\">// 如果这个方法失败，会将节点设置为&quot;取消&quot;状态，并抛出异常 IllegalMonitorStateException</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">int</span> <span class=\"title\">fullyRelease</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> failed = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> savedState = getState();</span><br><span class=\"line\">        <span class=\"comment\">// 这里使用了当前的 state 作为 release 的参数，也就是完全释放掉锁，将 state 置为 0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (release(savedState)) &#123;</span><br><span class=\"line\">            failed = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> savedState;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalMonitorStateException();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (failed)</span><br><span class=\"line\">            node.waitStatus = Node.CANCELLED;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>考虑一下，如果一个线程在不持有 lock 的基础上，就去调用 condition1.await() 方法，它能进入条件队列，但是在上面的这个方法中，由于它不持有锁，release(savedState) 这个方法肯定要返回 false，进入到异常分支，然后进入 finally 块设置 node.waitStatus = Node.CANCELLED，这个已经入队的节点之后会被后继的节点”请出去“。</p>\n</blockquote>\n<h3 id=\"等待进入阻塞队列\"><a href=\"#等待进入阻塞队列\" class=\"headerlink\" title=\"等待进入阻塞队列\"></a>等待进入阻塞队列</h3><p>释放掉锁以后，接下来是这段，这边会自旋，如果发现自己还没到阻塞队列，那么挂起，等待被转移到阻塞队列。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> interruptMode = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"comment\">// 如果不在阻塞队列中，注意了，是阻塞队列</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 线程挂起</span></span><br><span class=\"line\">    LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里可以先不用看了，等看到它什么时候被 unpark 再说</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>isOnSyncQueue(Node node) 用于判断节点是否已经转移到阻塞队列了：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 在节点入条件队列的时候，初始化时设置了 waitStatus = Node.CONDITION</span></span><br><span class=\"line\"><span class=\"comment\">// 前面我提到，signal 的时候需要将节点从条件队列移到阻塞队列，</span></span><br><span class=\"line\"><span class=\"comment\">// 这个方法就是判断 node 是否已经移动到阻塞队列了</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isOnSyncQueue</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 移动过去的时候，node 的 waitStatus 会置为 0，这个之后在说 signal 方法的时候会说到</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果 waitStatus 还是 Node.CONDITION，也就是 -2，那肯定就是还在条件队列中</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果 node 的前驱 prev 指向还是 null，说明肯定没有在 阻塞队列(prev是阻塞队列链表中使用的)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node.waitStatus == Node.CONDITION || node.prev == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 如果 node 已经有后继节点 next 的时候，那肯定是在阻塞队列了</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node.next != <span class=\"keyword\">null</span>) </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 下面这个方法从阻塞队列的队尾开始从后往前遍历找，如果找到相等的，说明在阻塞队列，否则就是不在阻塞队列</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 可以通过判断 node.prev() != null 来推断出 node 在阻塞队列吗？答案是：不能。</span></span><br><span class=\"line\">    <span class=\"comment\">// 这个可以看上篇 AQS 的入队方法，首先设置的是 node.prev 指向 tail，</span></span><br><span class=\"line\">    <span class=\"comment\">// 然后是 CAS 操作将自己设置为新的 tail，可是这次的 CAS 是可能失败的。</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> findNodeFromTail(node);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从阻塞队列的队尾往前遍历，如果找到，返回 true</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">boolean</span> <span class=\"title\">findNodeFromTail</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    Node t = tail;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t == node)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">        t = t.prev;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>回到前面的循环，isOnSyncQueue(node) 返回 false 的话，那么进到 LockSupport.park(this); 这里线程挂起。</p>\n<h3 id=\"signal-唤醒线程，转移到阻塞队列\"><a href=\"#signal-唤醒线程，转移到阻塞队列\" class=\"headerlink\" title=\"signal 唤醒线程，转移到阻塞队列\"></a>signal 唤醒线程，转移到阻塞队列</h3><p>为了大家理解，这里我们先看唤醒操作，因为刚刚到 LockSupport.park(this); 把线程挂起了，等待唤醒。唤醒操作通常由另一个线程来操作，就像生产者-消费者模式中，如果线程因为等待消费而挂起，那么当生产者生产了一个东西后，会调用 signal 唤醒正在等待的线程来消费。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 唤醒等待了最久的线程</span></span><br><span class=\"line\"><span class=\"comment\">// 其实就是，将这个线程对应的 node 从条件队列转移到阻塞队列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">signal</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 调用 signal 方法的线程必须持有当前的独占锁</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!isHeldExclusively())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalMonitorStateException();</span><br><span class=\"line\">    Node first = firstWaiter;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (first != <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        doSignal(first);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从条件队列队头往后遍历，找出第一个需要转移的 node</span></span><br><span class=\"line\"><span class=\"comment\">// 因为前面我们说过，有些线程会取消排队，但是可能还在队列中</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">doSignal</span><span class=\"params\">(Node first)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">// 将 firstWaiter 指向 first 节点后面的第一个，因为 first 节点马上要离开了</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果将 first 移除后，后面没有节点在等待了，那么需要将 lastWaiter 置为 null</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> ( (firstWaiter = first.nextWaiter) == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">            lastWaiter = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 因为 first 马上要被移到阻塞队列了，和条件队列的链接关系在这里断掉</span></span><br><span class=\"line\">        first.nextWaiter = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">while</span> (!transferForSignal(first) &amp;&amp;</span><br><span class=\"line\">             (first = firstWaiter) != <span class=\"keyword\">null</span>);</span><br><span class=\"line\">      <span class=\"comment\">// 这里 while 循环，如果 first 转移不成功，那么选择 first 后面的第一个节点进行转移，依此类推</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将节点从条件队列转移到阻塞队列</span></span><br><span class=\"line\"><span class=\"comment\">// true 代表成功转移</span></span><br><span class=\"line\"><span class=\"comment\">// false 代表在 signal 之前，节点已经取消了</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">transferForSignal</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// CAS 如果失败，说明此 node 的 waitStatus 已不是 Node.CONDITION，说明节点已经取消，</span></span><br><span class=\"line\">    <span class=\"comment\">// 既然已经取消，也就不需要转移了，方法返回，转移后面一个节点</span></span><br><span class=\"line\">    <span class=\"comment\">// 否则，将 waitStatus 置为 0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!compareAndSetWaitStatus(node, Node.CONDITION, <span class=\"number\">0</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// enq(node): 自旋进入阻塞队列的队尾</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意，这里的返回值 p 是 node 在阻塞队列的前驱节点</span></span><br><span class=\"line\">    Node p = enq(node);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> ws = p.waitStatus;</span><br><span class=\"line\">    <span class=\"comment\">// ws &gt; 0 说明 node 在阻塞队列中的前驱节点取消了等待锁，直接唤醒 node 对应的线程。唤醒之后会怎么样，后面再解释</span></span><br><span class=\"line\">    <span class=\"comment\">// 如果 ws &lt;= 0, 那么 compareAndSetWaitStatus 将会被调用，上篇介绍的时候说过，节点入队后，需要把前驱节点的状态设为 Node.SIGNAL(-1)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ws &gt; <span class=\"number\">0</span> || !compareAndSetWaitStatus(p, ws, Node.SIGNAL))</span><br><span class=\"line\">        <span class=\"comment\">// 如果前驱节点取消或者 CAS 失败，会进到这里唤醒线程，之后的操作看下一节</span></span><br><span class=\"line\">        LockSupport.unpark(node.thread);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>正常情况下，ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL) 这句中，ws &lt;= 0，而且 compareAndSetWaitStatus(p, ws, Node.SIGNAL) 会返回 true，所以一般也不会进去 if 语句块中唤醒 node 对应的线程。然后这个方法返回 true，也就意味着 signal 方法结束了，节点进入了阻塞队列。假设发生了阻塞队列中的前驱节点取消等待，或者 CAS 失败，只要唤醒线程，让其进到下一步即可。</p>\n<h3 id=\"唤醒后检查中断状态\"><a href=\"#唤醒后检查中断状态\" class=\"headerlink\" title=\"唤醒后检查中断状态\"></a>唤醒后检查中断状态</h3><p>上一步 signal 之后，我们的线程由条件队列转移到了阻塞队列，之后就准备获取锁了。只要重新获取到锁了以后，继续往下执行。等线程从挂起中恢复过来，继续往下看</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> interruptMode = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 线程挂起</span></span><br><span class=\"line\">    LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>先解释下 interruptMode。interruptMode 可以取值为 REINTERRUPT（1），THROW_IE（-1），0</p>\n<ul>\n<li>REINTERRUPT： 代表 await 返回的时候，需要重新设置中断状态</li>\n<li>THROW_IE： 代表 await 返回的时候，需要抛出 InterruptedException 异常</li>\n<li>0 ：说明在 await 期间，没有发生中断</li>\n</ul>\n<p>有以下三种情况会让 LockSupport.park(this); 这句返回继续往下执行：</p>\n<ul>\n<li>常规路径。signal -&gt; 转移节点到阻塞队列 -&gt; 获取了锁（unpark）</li>\n<li>线程中断。在 park 的时候，另外一个线程对这个线程进行了中断</li>\n<li>signal 的时候我们说过，转移以后的前驱节点取消了，或者对前驱节点的CAS操作失败了</li>\n<li>假唤醒。这个也是存在的，和 Object.wait() 类似，都有这个问题</li>\n</ul>\n<p>线程唤醒后第一步是调用 checkInterruptWhileWaiting(node) 这个方法，此方法用于判断是否在线程挂起期间发生了中断，如果发生了中断，是 signal 调用之前中断的，还是 signal 之后发生的中断。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 1. 如果在 signal 之前已经中断，返回 THROW_IE</span></span><br><span class=\"line\"><span class=\"comment\">// 2. 如果是 signal 之后中断，返回 REINTERRUPT</span></span><br><span class=\"line\"><span class=\"comment\">// 3. 没有发生中断，返回 0</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">checkInterruptWhileWaiting</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Thread.interrupted() ?</span><br><span class=\"line\">        (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) :</span><br><span class=\"line\">        <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Thread.interrupted()：如果当前线程已经处于中断状态，那么该方法返回 true，同时将中断状态重置为 false，所以，才有后续的 重新中断（REINTERRUPT） 的使用。</p>\n</blockquote>\n<p>看看怎么判断是 signal 之前还是之后发生的中断：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 只有线程处于中断状态，才会调用此方法</span></span><br><span class=\"line\"><span class=\"comment\">// 如果需要的话，将这个已经取消等待的节点转移到阻塞队列</span></span><br><span class=\"line\"><span class=\"comment\">// 返回 true：如果此线程在 signal 之前被取消，</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">transferAfterCancelledWait</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 用 CAS 将节点状态设置为 0 </span></span><br><span class=\"line\">    <span class=\"comment\">// 如果这步 CAS 成功，说明是 signal 方法之前发生的中断，因为如果 signal 先发生的话，signal 中会将 waitStatus 设置为 0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (compareAndSetWaitStatus(node, Node.CONDITION, <span class=\"number\">0</span>)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 将节点放入阻塞队列</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里我们看到，即使中断了，依然会转移到阻塞队列</span></span><br><span class=\"line\">        enq(node);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 到这里是因为 CAS 失败，肯定是因为 signal 方法已经将 waitStatus 设置为了 0</span></span><br><span class=\"line\">    <span class=\"comment\">// signal 方法会将节点转移到阻塞队列，但是可能还没完成，这边自旋等待其完成</span></span><br><span class=\"line\">    <span class=\"comment\">// 当然，这种事情还是比较少的吧：signal 调用之后，没完成转移之前，发生了中断</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (!isOnSyncQueue(node))</span><br><span class=\"line\">        Thread.yield();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里再说一遍，即使发生了中断，节点依然会转移到阻塞队列。到这里，大家应该都知道这个 while 循环怎么退出了吧。要么中断，要么转移成功。这里描绘了一个场景，本来有个线程，它是排在条件队列的后面的，但是因为它被中断了，那么它会被唤醒，然后它发现自己不是被 signal 的那个，但是它会自己主动去进入到阻塞队列。</p>\n<h3 id=\"获取独占锁\"><a href=\"#获取独占锁\" class=\"headerlink\" title=\"获取独占锁\"></a>获取独占锁</h3><p>while 循环出来以后，下面是这段代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class=\"line\">    interruptMode = REINTERRUPT;</span><br></pre></td></tr></table></figure>\n<p>由于 while 出来后，我们确定节点已经进入了阻塞队列，准备获取锁。这里的 acquireQueued(node, savedState) 的第一个参数 node 之前已经经过 enq(node) 进入了队列，参数 savedState 是之前释放锁前的 state，这个方法返回的时候，代表当前线程获取了锁，而且 state == savedState了。注意，前面我们说过，不管有没有发生中断，都会进入到阻塞队列，而 acquireQueued(node, savedState) 的返回值就是代表线程是否被中断。如果返回 true，说明被中断了，而且 interruptMode != THROW_IE，说明在 signal 之前就发生中断了，这里将 interruptMode 设置为 REINTERRUPT，用于待会重新中断。继续往下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (node.nextWaiter != <span class=\"keyword\">null</span>) <span class=\"comment\">// clean up if cancelled</span></span><br><span class=\"line\">    unlinkCancelledWaiters();</span><br><span class=\"line\"><span class=\"keyword\">if</span> (interruptMode != <span class=\"number\">0</span>)</span><br><span class=\"line\">    reportInterruptAfterWait(interruptMode);</span><br></pre></td></tr></table></figure>\n<p>本着一丝不苟的精神，这边说说 node.nextWaiter != null 怎么满足。我前面也说了 signal 的时候会将节点转移到阻塞队列，有一步是 node.nextWaiter = null，将断开节点和条件队列的联系。可是，在判断发生中断的情况下，是 signal 之前还是之后发生的？ 这部分的时候，我也介绍了，如果 signal 之前就中断了，也需要将节点进行转移到阻塞队列，这部分转移的时候，是没有设置 node.nextWaiter = null 的。之前我们说过，如果有节点取消，也会调用 unlinkCancelledWaiters 这个方法，就是这里了。</p>\n<h3 id=\"处理中断状态\"><a href=\"#处理中断状态\" class=\"headerlink\" title=\"处理中断状态\"></a>处理中断状态</h3><p>到这里，我们终于可以好好说下这个 interruptMode 干嘛用了。</p>\n<ul>\n<li>0：什么都不做，没有被中断过；</li>\n<li>THROW_IE：await 方法抛出 InterruptedException 异常，因为它代表在 await() 期间发生了中断；</li>\n<li>REINTERRUPT：重新中断当前线程，因为它代表 await() 期间没有被中断，而是 signal() 以后发生的中断</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">reportInterruptAfterWait</span><span class=\"params\">(<span class=\"keyword\">int</span> interruptMode)</span></span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (interruptMode == THROW_IE)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (interruptMode == REINTERRUPT)</span><br><span class=\"line\">        selfInterrupt();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个中断状态这部分内容，大家应该都理解了吧，不理解的话，多看几遍就是了。</p>\n<h3 id=\"带超时机制的-await\"><a href=\"#带超时机制的-await\" class=\"headerlink\" title=\"带超时机制的 await\"></a>带超时机制的 await</h3><p>经过前面的 7 步，整个 ConditionObject 类基本上都分析完了，接下来简单分析下带超时机制的 await 方法。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> <span class=\"title\">awaitNanos</span><span class=\"params\">(<span class=\"keyword\">long</span> nanosTimeout)</span> </span></span><br><span class=\"line\"><span class=\"function\">                  <span class=\"keyword\">throws</span> InterruptedException</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">awaitUntil</span><span class=\"params\">(Date deadline)</span></span></span><br><span class=\"line\"><span class=\"function\">                <span class=\"keyword\">throws</span> InterruptedException</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">await</span><span class=\"params\">(<span class=\"keyword\">long</span> time, TimeUnit unit)</span></span></span><br><span class=\"line\"><span class=\"function\">                <span class=\"keyword\">throws</span> InterruptedException</span></span><br></pre></td></tr></table></figure>\n<p>这三个方法都差不多，我们就挑一个出来看看吧：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">await</span><span class=\"params\">(<span class=\"keyword\">long</span> time, TimeUnit unit)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 等待这么多纳秒</span></span><br><span class=\"line\">    <span class=\"keyword\">long</span> nanosTimeout = unit.toNanos(time);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.interrupted())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\">    Node node = addConditionWaiter();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> savedState = fullyRelease(node);</span><br><span class=\"line\">    <span class=\"comment\">// 当前时间 + 等待时长 = 过期时间</span></span><br><span class=\"line\">    <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> deadline = System.nanoTime() + nanosTimeout;</span><br><span class=\"line\">    <span class=\"comment\">// 用于返回 await 是否超时</span></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> timedout = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> interruptMode = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 时间到啦</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (nanosTimeout &lt;= <span class=\"number\">0L</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这里因为要 break 取消等待了。取消等待的话一定要调用 transferAfterCancelledWait(node) 这个方法</span></span><br><span class=\"line\">            <span class=\"comment\">// 如果这个方法返回 true，在这个方法内，将节点转移到阻塞队列成功</span></span><br><span class=\"line\">            <span class=\"comment\">// 返回 false 的话，说明 signal 已经发生，signal 方法将节点转移了。也就是说没有超时嘛</span></span><br><span class=\"line\">            timedout = transferAfterCancelledWait(node);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// spinForTimeoutThreshold 的值是 1000 纳秒，也就是 1 毫秒</span></span><br><span class=\"line\">        <span class=\"comment\">// 也就是说，如果不到 1 毫秒了，那就不要选择 parkNanos 了，自旋的性能反而更好</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (nanosTimeout &gt;= spinForTimeoutThreshold)</span><br><span class=\"line\">            LockSupport.parkNanos(<span class=\"keyword\">this</span>, nanosTimeout);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 得到剩余时间</span></span><br><span class=\"line\">        nanosTimeout = deadline - System.nanoTime();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class=\"line\">        interruptMode = REINTERRUPT;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node.nextWaiter != <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        unlinkCancelledWaiters();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (interruptMode != <span class=\"number\">0</span>)</span><br><span class=\"line\">        reportInterruptAfterWait(interruptMode);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> !timedout;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>超时的思路还是很简单的，不带超时参数的 await 是 park，然后等待别人唤醒。而现在就是调用 parkNanos 方法来休眠指定的时间，醒来后判断是否 signal 调用了，调用了就是没有超时，否则就是超时了。超时的话，自己来进行转移到阻塞队列，然后抢锁。</p>\n<h3 id=\"不抛出-InterruptedException-的-await\"><a href=\"#不抛出-InterruptedException-的-await\" class=\"headerlink\" title=\"不抛出 InterruptedException 的 await\"></a>不抛出 InterruptedException 的 await</h3><p>关于 Condition 最后一小节了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">awaitUninterruptibly</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Node node = addConditionWaiter();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> savedState = fullyRelease(node);</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> interrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class=\"line\">        LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (Thread.interrupted())</span><br><span class=\"line\">            interrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (acquireQueued(node, savedState) || interrupted)</span><br><span class=\"line\">        selfInterrupt();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>很简单，贴一下代码大家就都懂了，我就不废话了。</p>\n<h2 id=\"AbstractQueuedSynchronizer-独占锁的取消排队\"><a href=\"#AbstractQueuedSynchronizer-独占锁的取消排队\" class=\"headerlink\" title=\"AbstractQueuedSynchronizer 独占锁的取消排队\"></a>AbstractQueuedSynchronizer 独占锁的取消排队</h2><p>这篇文章说的是 AbstractQueuedSynchronizer，只不过好像 Condition 说太多了，赶紧把思路拉回来。接下来，我想说说怎么取消对锁的竞争？上篇文章提到过，最重要的方法是这个，我们要在这里面找答案：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">acquireQueued</span><span class=\"params\">(<span class=\"keyword\">final</span> Node node, <span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> failed = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> interrupted = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> Node p = node.predecessor();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class=\"line\">                setHead(node);</span><br><span class=\"line\">                p.next = <span class=\"keyword\">null</span>; <span class=\"comment\">// help GC</span></span><br><span class=\"line\">                failed = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> interrupted;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class=\"line\">                parkAndCheckInterrupt())</span><br><span class=\"line\">                interrupted = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (failed)</span><br><span class=\"line\">            cancelAcquire(node);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>首先，到这个方法的时候，节点一定是入队成功的。我把 parkAndCheckInterrupt() 代码贴过来：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">boolean</span> <span class=\"title\">parkAndCheckInterrupt</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    LockSupport.park(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Thread.interrupted();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这两段代码联系起来看，是不是就清楚了。如果我们要取消一个线程的排队，我们需要在另外一个线程中对其进行中断。比如某线程调用 lock() 老久不返回，我想中断它。一旦对其进行中断，此线程会从 LockSupport.park(this); 中唤醒，然后 Thread.interrupted(); 返回 true。我们发现一个问题，即使是中断唤醒了这个线程，也就只是设置了 interrupted = true 然后继续下一次循环。而且，由于 Thread.interrupted();  会清除中断状态，第二次进 parkAndCheckInterrupt 的时候，返回会是 false。所以，我们要看到，在这个方法中，interrupted 只是用来记录是否发生了中断，然后用于方法返回值，其他没有做任何相关事情。所以，我们看外层方法怎么处理 acquireQueued 返回 false 的情况。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquire</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class=\"line\">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class=\"line\">        selfInterrupt();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">selfInterrupt</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Thread.currentThread().interrupt();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所以说，lock() 方法处理中断的方法就是，你中断归中断，我抢锁还是照样抢锁，几乎没关系，只是我抢到锁了以后，设置线程的中断状态而已，也不抛出任何异常出来。调用者获取锁后，可以去检查是否发生过中断，也可以不理会。</p>\n<h2 id=\"分割线\"><a href=\"#分割线\" class=\"headerlink\" title=\"分割线\"></a>分割线</h2><p>来条分割线。有没有被骗的感觉，我说了一大堆，可是和取消没有任何关系啊。我们来看 ReentrantLock 的另一个 lock 方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">lockInterruptibly</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    sync.acquireInterruptibly(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>方法上多了个 throws InterruptedException ，经过前面那么多知识的铺垫，这里我就不再啰里啰嗦了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquireInterruptibly</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.interrupted())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!tryAcquire(arg))</span><br><span class=\"line\">        doAcquireInterruptibly(arg);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>继续往里：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">doAcquireInterruptibly</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> Node node = addWaiter(Node.EXCLUSIVE);</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> failed = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> Node p = node.predecessor();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class=\"line\">                setHead(node);</span><br><span class=\"line\">                p.next = <span class=\"keyword\">null</span>; <span class=\"comment\">// help GC</span></span><br><span class=\"line\">                failed = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">                <span class=\"keyword\">return</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class=\"line\">                parkAndCheckInterrupt())</span><br><span class=\"line\">                <span class=\"comment\">// 就是这里了，一旦异常，马上结束这个方法，抛出异常。</span></span><br><span class=\"line\">                <span class=\"comment\">// 这里不再只是标记这个方法的返回值代表中断状态</span></span><br><span class=\"line\">                <span class=\"comment\">// 而是直接抛出异常，而且外层也不捕获，一直往外抛到 lockInterruptibly</span></span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果通过 InterruptedException 异常出去，那么 failed 就是 true 了</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (failed)</span><br><span class=\"line\">            cancelAcquire(node);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>既然到这里了，顺便说说 cancelAcquire 这个方法吧：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">cancelAcquire</span><span class=\"params\">(Node node)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// Ignore if node doesn&#x27;t exist</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    node.thread = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"comment\">// Skip cancelled predecessors</span></span><br><span class=\"line\">    <span class=\"comment\">// 找一个合适的前驱。其实就是将它前面的队列中已经取消的节点都”请出去“</span></span><br><span class=\"line\">    Node pred = node.prev;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (pred.waitStatus &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">        node.prev = pred = pred.prev;</span><br><span class=\"line\">    <span class=\"comment\">// predNext is the apparent node to unsplice. CASes below will</span></span><br><span class=\"line\">    <span class=\"comment\">// fail if not, in which case, we lost race vs another cancel</span></span><br><span class=\"line\">    <span class=\"comment\">// or signal, so no further action is necessary.</span></span><br><span class=\"line\">    Node predNext = pred.next;</span><br><span class=\"line\">    <span class=\"comment\">// Can use unconditional write instead of CAS here.</span></span><br><span class=\"line\">    <span class=\"comment\">// After this atomic step, other Nodes can skip past us.</span></span><br><span class=\"line\">    <span class=\"comment\">// Before, we are free of interference from other threads.</span></span><br><span class=\"line\">    node.waitStatus = Node.CANCELLED;</span><br><span class=\"line\">    <span class=\"comment\">// If we are the tail, remove ourselves.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123;</span><br><span class=\"line\">        compareAndSetNext(pred, predNext, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// If successor needs signal, try to set pred&#x27;s next-link</span></span><br><span class=\"line\">        <span class=\"comment\">// so it will get one. Otherwise wake it up to propagate.</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> ws;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pred != head &amp;&amp;</span><br><span class=\"line\">            ((ws = pred.waitStatus) == Node.SIGNAL ||</span><br><span class=\"line\">             (ws &lt;= <span class=\"number\">0</span> &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp;</span><br><span class=\"line\">            pred.thread != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            Node next = node.next;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (next != <span class=\"keyword\">null</span> &amp;&amp; next.waitStatus &lt;= <span class=\"number\">0</span>)</span><br><span class=\"line\">                compareAndSetNext(pred, predNext, next);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            unparkSuccessor(node);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        node.next = node; <span class=\"comment\">// help GC</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其实这个方法没什么好说的，一行行看下去就是了，节点取消，只要把 waitStatus 设置为 Node.CANCELLED，会有非常多的情况被从阻塞队列中请出去，主动或被动。</p>\n<h2 id=\"再说-java-线程中断和-InterruptedException-异常\"><a href=\"#再说-java-线程中断和-InterruptedException-异常\" class=\"headerlink\" title=\"再说 java 线程中断和 InterruptedException 异常\"></a>再说 java 线程中断和 InterruptedException 异常</h2><p>在之前的文章中，我们接触了大量的中断，这边算是个总结吧。如果你完全熟悉中断了，没有必要再看这节，本节为新手而写。</p>\n<h3 id=\"线程中断\"><a href=\"#线程中断\" class=\"headerlink\" title=\"线程中断\"></a>线程中断</h3><p>首先，我们要明白，中断不是类似 linux 里面的命令 kill -9 pid，不是说我们中断某个线程，这个线程就停止运行了。中断代表线程状态，每个线程都关联了一个中断状态，是一个 true 或 false 的 boolean 值，初始值为 false。<strong>Java 中的中断和操作系统的中断还不一样，这里就按照状态来理解吧，不要和操作系统的中断联系在一起</strong>。关于中断状态，我们需要重点关注 Thread 类中的以下几个方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isInterrupted</span><span class=\"params\">()</span> </span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Thread 中的静态方法，检测调用这个方法的线程是否已经中断</span></span><br><span class=\"line\"><span class=\"comment\">// 注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false</span></span><br><span class=\"line\"><span class=\"comment\">// 所以，如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">boolean</span> <span class=\"title\">interrupted</span><span class=\"params\">()</span> </span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Thread 类中的实例方法，用于设置一个线程的中断状态为 true</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">interrupt</span><span class=\"params\">()</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>我们说中断一个线程，其实就是设置了线程的 interrupted status 为 true，至于说被中断的线程怎么处理这个状态，那是那个线程自己的事。如以下代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (!Thread.interrupted()) &#123;</span><br><span class=\"line\">   doWork();</span><br><span class=\"line\">   System.out.println(<span class=\"string\">&quot;我做完一件事了，准备做下一件，如果没有其他线程中断我的话&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>这种代码就是会响应中断的，它会在干活的时候先判断下中断状态，不过，除了 JDK 源码外，其他用中断的场景还是比较少的，毕竟 JDK 源码非常讲究。</strong></p>\n<p>当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。如果线程处于以下三种情况，那么当线程被中断的时候，能自动感知到：</p>\n<ul>\n<li>来自 Object 类的 wait()、wait(long)、wait(long, int)<br>来自 Thread 类的 join()、join(long)、join(long, int)、sleep(long)、sleep(long, int)。这几个方法的相同之处是，方法上都有: throws InterruptedException 。如果线程阻塞在这些方法上（我们知道，这些方法会让当前线程阻塞），这个时候如果其他线程对这个线程进行了中断，那么这个线程会从这些方法中立即返回，抛出 InterruptedException 异常，同时重置中断状态为 false。</li>\n<li>实现了 InterruptibleChannel 接口的类中的一些 I/O 阻塞操作，如 DatagramChannel 中的 connect 方法和 receive 方法等。如果线程阻塞在这里，中断线程会导致这些方法抛出 ClosedByInterruptException 并重置中断状态。</li>\n<li>Selector 中的 select 方法，参考下我写的 NIO 的文章。一旦中断，方法立即返回</li>\n</ul>\n<p>对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。</p>\n<h3 id=\"InterruptedException-概述\"><a href=\"#InterruptedException-概述\" class=\"headerlink\" title=\"InterruptedException 概述\"></a>InterruptedException 概述</h3><p>它是一个特殊的异常，不是说 JVM 对其有特殊的处理，而是它的使用场景比较特殊。通常，我们可以看到，像 Object 中的 wait() 方法，ReentrantLock 中的 lockInterruptibly() 方法，Thread 中的 sleep() 方法等等，这些方法都带有 throws InterruptedException，我们通常称这些方法为阻塞方法（blocking method）。</p>\n<p>阻塞方法一个很明显的特征是，它们需要花费比较长的时间（不是绝对的，只是说明时间不可控），还有它们的方法结束返回往往依赖于外部条件，如 wait 方法依赖于其他线程的 notify，lock 方法依赖于其他线程的 unlock等等。</p>\n<p>当我们看到方法上带有 throws InterruptedException 时，我们就要知道，这个方法应该是阻塞方法，我们如果希望它能早点返回的话，我们往往可以通过中断来实现。 </p>\n<p>除了几个特殊类（如 Object，Thread等）外，感知中断并提前返回是通过轮询中断状态来实现的。我们自己需要写可中断的方法的时候，就是通过在合适的时机（通常在循环的开始处）去判断线程的中断状态，然后做相应的操作（通常是方法直接返回或者抛出异常）。当然，我们也要看到，如果我们一次循环花的时间比较长的话，那么就需要比较长的时间才能感知到线程中断了。</p>\n<h3 id=\"处理中断\"><a href=\"#处理中断\" class=\"headerlink\" title=\"处理中断\"></a>处理中断</h3><p>一旦中断发生，我们接收到了这个信息，然后怎么去处理中断呢？本小节将简单分析这个问题。我们经常会这么写代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    Thread.sleep(<span class=\"number\">10000</span>);</span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// ignore</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// go on </span></span><br></pre></td></tr></table></figure>\n<p>当 sleep 结束继续往下执行的时候，我们往往都不知道这块代码是真的 sleep 了 10 秒，还是只休眠了 1 秒就被中断了。这个代码的问题在于，我们将这个异常信息吞掉了。（对于 sleep 方法，我相信大部分情况下，我们都不在意是否是中断了，这里是举例）。AQS 的做法很值得我们借鉴，我们知道 ReentrantLock 有两种 lock 方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">lock</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    sync.lock();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">lockInterruptibly</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    sync.acquireInterruptibly(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>前面我们提到过，lock() 方法不响应中断。如果 thread1 调用了 lock() 方法，过了很久还没抢到锁，这个时候 thread2 对其进行了中断，thread1 是不响应这个请求的，它会继续抢锁，当然它不会把“被中断”这个信息扔掉。我们可以看以下代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">acquire</span><span class=\"params\">(<span class=\"keyword\">int</span> arg)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class=\"line\">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class=\"line\">        <span class=\"comment\">// 我们看到，这里也没做任何特殊处理，就是记录下来中断状态。</span></span><br><span class=\"line\">        <span class=\"comment\">// 这样，如果外层方法需要去检测的时候，至少我们没有把这个信息丢了</span></span><br><span class=\"line\">        selfInterrupt();<span class=\"comment\">// Thread.currentThread().interrupt();</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>而对于 lockInterruptibly() 方法，因为其方法上面有 throws InterruptedException ，这个信号告诉我们，如果我们要取消线程抢锁，直接中断这个线程即可，它会立即返回，抛出 InterruptedException 异常。</p>\n<p>在并发包中，有非常多的这种处理中断的例子，提供两个方法，分别为响应中断和不响应中断，对于不响应中断的方法，记录中断而不是丢失这个信息。如 Condition 中的两个方法就是这样的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">await</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException</span>;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">awaitUninterruptibly</span><span class=\"params\">()</span></span>;</span><br></pre></td></tr></table></figure>\n<p>通常，如果方法会抛出 InterruptedException 异常，往往方法体的第一句就是：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> <span class=\"keyword\">void</span> <span class=\"title\">await</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> InterruptedException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Thread.interrupted())</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> InterruptedException();</span><br><span class=\"line\">     ...... </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>熟练使用中断，对于我们写出优雅的代码是有帮助的，也有助于我们分析别人的源码。</p>\n","categories":["Java"],"tags":["Java","AbstractQueuedSynchronizer"]},{"title":"模板引擎Thymeleaf？来这一篇就够用了","url":"/Spring-Boot/2f14d053285f/","content":"<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>模板页面中的 html 上需要声明 Thymeleaf 的命名空间，具体代码如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;html xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>这样比如我们在Idea中编写ThymeLeaf代码的时候，就会有相关的代码提示。</p>\n<h2 id=\"th-text和th-utext\"><a href=\"#th-text和th-utext\" class=\"headerlink\" title=\"th:text和th:utext\"></a>th:text和th:utext</h2><p>这两个标签用于文本内容的显示操作，他们功能相近，只有区别于会不会解析。</p>\n<ul>\n<li>th:text 进行文本替换 不会解析html</li>\n<li>th:utext 进行文本替换 会解析html</li>\n</ul>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><p>我们编写一个这样的代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/th&quot;</span>)</span><br><span class=\"line\">public String th(Model model)&#123;</span><br><span class=\"line\">    String msg = <span class=\"string\">&quot;&lt;h1&gt;我是h1&lt;/h1&gt;&quot;</span>;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;msg&quot;</span>,msg);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/th&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于<strong>th:text</strong>，在HTML页面中这样编写，如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;text标签：  + <span class=\"variable\">$&#123;msg&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>因为text不会进行解析，将会获得如下结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;text标签：&lt;h1&gt;我是h1&lt;/h1&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>对于<strong>th:utext</strong>在HTML页面中这样编写，如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:utext=<span class=\"string\">&quot;utext标签： + <span class=\"variable\">$&#123;msg&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>因为utext会进行解析，将会获得如下结果<br><img src=\"https://img-blog.csdnimg.cn/20200224164718658.png#pic_center\" alt=\"在这里插入图片描述\"><br>当然，这里提示一下的是，使用 + 和 | | 效果是一样的，如下代码所示</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:utext=<span class=\"string\">&quot;utext标签： + <span class=\"variable\">$&#123;msg&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:utext=<span class=\"string\">&quot;|utext标签： <span class=\"variable\">$&#123;msg&#125;</span>|&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"字符串拼接\"><a href=\"#字符串拼接\" class=\"headerlink\" title=\"字符串拼接\"></a>字符串拼接</h2><p>拼接字符串通过 + 或者 | 进行拼接，请求我们先编写一个这样的代码，以及使用下面的HTML模板</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/th&quot;</span>)</span><br><span class=\"line\">public String th(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;a&quot;</span>,1);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;b&quot;</span>,2);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/th&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;a&#125;</span>+<span class=\"variable\">$&#123;b&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;|<span class=\"variable\">$&#123;a&#125;</span> <span class=\"variable\">$&#123;b&#125;</span>|&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板三</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;a&#125;</span> &gt; <span class=\"variable\">$&#123;b&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>所得结果如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;p&gt;1 2&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#结果三</span></span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>我们换过一个java代码，以及使用下面的HTML模板</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/th&quot;</span>)</span><br><span class=\"line\">public String th(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;flag&quot;</span>,<span class=\"literal\">true</span>);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/th&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;!<span class=\"variable\">$&#123;flag&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>得到如下结果：<code>&lt;p&gt;false&lt;/p&gt;</code></p>\n<h2 id=\"…-和-…-表达式\"><a href=\"#…-和-…-表达式\" class=\"headerlink\" title=\"*{…}和 ${…}表达式\"></a>*{…}和 ${…}表达式</h2><p>一般情况下 *{…} 和 ${…}是一样的，但是 *{…} 一般和 th:object 一起使用来完成对象属性的简写，我们来举个例子，用如下java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/th&quot;</span>)</span><br><span class=\"line\">public String th(Model model)&#123;</span><br><span class=\"line\">    User user = new User(<span class=\"string\">&quot;ljk&quot;</span>，18);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;user&quot;</span>,user);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/th&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板操作</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#使用 $&#123;...&#125;操作</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user.name&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user.age&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#使用 *&#123;...&#125;操作</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;*&#123;user.name&#125;&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;*&#123;user.age&#125;&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>两个获得的结果都是如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;ljk&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;18&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>我们这里来试试使用 *{…}特有操作</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:object=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user&#125;</span>&quot;</span> &gt;</span><br><span class=\"line\">    &lt;p th:text=<span class=\"string\">&quot;*&#123;name&#125;&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">    &lt;p th:text=<span class=\"string\">&quot;*&#123;age&#125;&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>会得到如下结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;ljk&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;18&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"…-表达式\"><a href=\"#…-表达式\" class=\"headerlink\" title=\"#{…}表达式\"></a>#{…}表达式</h2><p>用于国际化message.properties 属性读取，定义message_zh_CN.properties 配置文件<br><img src=\"https://img-blog.csdnimg.cn/20200224171021264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/2020022417104743.png#pic_center\" alt=\"在这里插入图片描述\"><br>定义国际化处理转换处理类</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@Configuration</span><br><span class=\"line\">public class LocaleResolverConfig &#123;</span><br><span class=\"line\">    @Bean(name=<span class=\"string\">&quot;localeResolver&quot;</span>)</span><br><span class=\"line\">    public LocaleResolver <span class=\"function\"><span class=\"title\">localeResolverBean</span></span>() &#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> new SessionLocaleResolver();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>定义国际化处理的controller</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@Controller</span><br><span class=\"line\">public class ProductController &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    @Autowired</span><br><span class=\"line\">    private LocaleResolver localeResolver;</span><br><span class=\"line\">    private  ProductService productService = new ProductService();</span><br><span class=\"line\"></span><br><span class=\"line\">    @RequestMapping(<span class=\"string\">&quot;/&quot;</span>)</span><br><span class=\"line\">    public String useT(Model model,HttpServletRequest request,HttpServletResponse response) &#123;</span><br><span class=\"line\">        //设置访问用户信息到session</span><br><span class=\"line\">        request.getSession(<span class=\"literal\">true</span>).setAttribute(<span class=\"string\">&quot;user&quot;</span>, new User(<span class=\"string\">&quot;我是&quot;</span>, <span class=\"string\">&quot;哈哈&quot;</span>, <span class=\"string\">&quot;CHINA&quot;</span>, null));</span><br><span class=\"line\">        localeResolver.setLocale(request,response,Locale.CHINA);</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;productList&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果没有定义 message_en_US.properties 和message_zh_CN.properties 会默认取message.properties中的信息。如果 Locale = Locale.CHINA 就取 message_zh_CN.properties。如果 Locale = Locale.US 就取 message_en_US.properties。我们使用如下模板代码测试一下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:utext=<span class=\"string\">&quot;#&#123;home.welcome(<span class=\"variable\">$&#123;session.user.name&#125;</span>)&#125;&quot;</span>&gt;Welcome to our grocery store, Sebastian!&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>得到如下结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">欢迎来到我们的杂货店，我是哈哈</span><br></pre></td></tr></table></figure>\n<h2 id=\"…-片段表达式\"><a href=\"#…-片段表达式\" class=\"headerlink\" title=\"~{…}片段表达式\"></a>~{…}片段表达式</h2><p>这个一般和模版布局的语法一起使用，具体使用方式请看下面模版布局的教程。</p>\n<h2 id=\"…-链接网址表达式\"><a href=\"#…-链接网址表达式\" class=\"headerlink\" title=\"@{…}链接网址表达式\"></a>@{…}链接网址表达式</h2><p>一般和 th:href、th:src进行结合使用，用于显示Web 应用中的URL链接。通过@{…}表达式，Thymeleaf 可以帮助我们拼接上web应用访问的全路径，同时我们可以通过（）进行参数的拼接</p>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;img th:src=<span class=\"string\">&quot;@&#123;/images/gtvglogo.png&#125;&quot;</span>  /&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;a th:href=<span class=\"string\">&quot;@&#123;/product/comments(prodId=<span class=\"variable\">$&#123;prod.id&#125;</span>)&#125;&quot;</span> &gt;查看&lt;/a&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板三</span></span><br><span class=\"line\">&lt;a th:href=<span class=\"string\">&quot;@&#123;/product/comments(prodId=<span class=\"variable\">$&#123;prod.id&#125;</span>,prodId2=<span class=\"variable\">$&#123;prod.id&#125;</span>)&#125;&quot;</span> &gt;查看&lt;/a&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;img src=<span class=\"string\">&quot;/sbe/images/gtvglogo.png&quot;</span>&gt;</span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;a href=<span class=\"string\">&quot;/sbe/product/comments?prodId=2&quot;</span>&gt;查看&lt;/a&gt;</span><br><span class=\"line\"><span class=\"comment\">#结果三</span></span><br><span class=\"line\">&lt;a href=<span class=\"string\">&quot;/sbe/product/comments?prodId=2&amp;amp;prodId2=2&quot;</span>&gt;查看&lt;/a&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-if和th-unless\"><a href=\"#th-if和th-unless\" class=\"headerlink\" title=\"th:if和th:unless\"></a>th:if和th:unless</h2><ul>\n<li>th:if 当条件为true则显示。</li>\n<li>th:unless 当条件为false 则显示。</li>\n</ul>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thif&quot;</span>)</span><br><span class=\"line\">public String thif(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;flag&quot;</span>,<span class=\"literal\">true</span>);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thif&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:<span class=\"keyword\">if</span>=<span class=\"string\">&quot;<span class=\"variable\">$&#123;flag&#125;</span>&quot;</span>&gt;<span class=\"keyword\">if</span>判断&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p th:unless=<span class=\"string\">&quot;!<span class=\"variable\">$&#123;flag&#125;</span>&quot;</span>&gt;unless 判断&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"keyword\">if</span>判断&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;unless 判断&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"switch\"><a href=\"#switch\" class=\"headerlink\" title=\"switch\"></a>switch</h2><ul>\n<li>th:switch 我们可以通过switch来完成类似的条件表达式的操作。</li>\n</ul>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thswitch&quot;</span>)</span><br><span class=\"line\">public String thswitch(Model model)&#123;</span><br><span class=\"line\">    User user = new User(<span class=\"string\">&quot;ljk&quot;</span>,23);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;user&quot;</span>,user);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thswitch&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:switch=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user.name&#125;</span>&quot;</span>&gt;</span><br><span class=\"line\">   &lt;p th:<span class=\"keyword\">case</span>=<span class=\"string\">&quot;&#x27;ljk&#x27;&quot;</span>&gt;User is  ljk&lt;/p&gt;</span><br><span class=\"line\">   &lt;p th:<span class=\"keyword\">case</span>=<span class=\"string\">&quot;ljk1&quot;</span>&gt;User is ljk1&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;&lt;p&gt; User is ljk&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"for循环\"><a href=\"#for循环\" class=\"headerlink\" title=\"for循环\"></a>for循环</h2><ul>\n<li>th:each 遍历集合</li>\n</ul>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/theach&quot;</span>)</span><br><span class=\"line\">public String theach(Model model)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;User&gt; userList = new ArrayList&lt;User&gt;();</span><br><span class=\"line\">    User user1 = new User(<span class=\"string\">&quot;ljk&quot;</span>,18);</span><br><span class=\"line\">    User user2 = new User(<span class=\"string\">&quot;ljk2&quot;</span>,19);</span><br><span class=\"line\">    User user3 = new User(<span class=\"string\">&quot;ljk3&quot;</span>,20);</span><br><span class=\"line\">    User user4 = new User(<span class=\"string\">&quot;lj4&quot;</span>,21);</span><br><span class=\"line\">    userList.add(user1);</span><br><span class=\"line\">    userList.add(user2);</span><br><span class=\"line\">    userList.add(user3);</span><br><span class=\"line\">    userList.add(user4);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;userList&quot;</span>,userList);</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;String&gt; strList = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">    strList.add(<span class=\"string\">&quot;ljk&quot;</span>);</span><br><span class=\"line\">    strList.add(<span class=\"string\">&quot;ljk2&quot;</span>);</span><br><span class=\"line\">    strList.add(<span class=\"string\">&quot;ljk3&quot;</span>);</span><br><span class=\"line\">    strList.add(<span class=\"string\">&quot;lj4&quot;</span>);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;strList&quot;</span>,strList);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/theach&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"> &lt;table&gt;</span><br><span class=\"line\">  &lt;thead&gt;</span><br><span class=\"line\">    &lt;tr&gt;</span><br><span class=\"line\">      &lt;th&gt;用户名称&lt;/th&gt;</span><br><span class=\"line\">      &lt;th&gt;用户年龄&lt;/th&gt;</span><br><span class=\"line\">    &lt;/tr&gt;</span><br><span class=\"line\">  &lt;/thead&gt;</span><br><span class=\"line\">  &lt;tbody&gt;</span><br><span class=\"line\">    &lt;tr th:each=<span class=\"string\">&quot;user : <span class=\"variable\">$&#123;userList&#125;</span>&quot;</span> th:class=<span class=\"string\">&quot;<span class=\"variable\">$&#123;userStat.odd&#125;</span>? &#x27;odd&#x27;&quot;</span>&gt;</span><br><span class=\"line\">      &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user.name&#125;</span>&quot;</span>&gt;Onions&lt;/td&gt;</span><br><span class=\"line\">      &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;user.age&#125;</span>&quot;</span>&gt;2.41&lt;/td&gt;</span><br><span class=\"line\">    &lt;/tr&gt;</span><br><span class=\"line\">  &lt;/tbody&gt;</span><br><span class=\"line\">&lt;/table&gt;</span><br><span class=\"line\">----------------------------------------------------------------------</span><br><span class=\"line\">&lt;table&gt;</span><br><span class=\"line\">  &lt;thead&gt;</span><br><span class=\"line\">    &lt;tr&gt;</span><br><span class=\"line\">      &lt;th&gt;用户名称&lt;/th&gt;</span><br><span class=\"line\">    &lt;/tr&gt;</span><br><span class=\"line\">  &lt;/thead&gt;</span><br><span class=\"line\">  &lt;tbody&gt;</span><br><span class=\"line\">    &lt;tr th:each=<span class=\"string\">&quot;str : <span class=\"variable\">$&#123;strList&#125;</span>&quot;</span> th:class=<span class=\"string\">&quot;<span class=\"variable\">$&#123;strStat.odd&#125;</span>? &#x27;odd&#x27;&quot;</span>&gt;</span><br><span class=\"line\">      &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;str&#125;</span>&quot;</span>&gt;Onions&lt;/td&gt;</span><br><span class=\"line\">    &lt;/tr&gt;</span><br><span class=\"line\">  &lt;/tbody&gt;</span><br><span class=\"line\">&lt;/table&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：<br><img src=\"https://img-blog.csdnimg.cn/20200224172840700.png#pic_center\" alt=\"在这里插入图片描述\"><br>这里解释一下<code>th:class=&quot;$&#123;strStat.odd&#125;? &#39;odd&#39;&quot;</code>，我们可以通过便利的变量名+Stat 来获取索引 是否是第一个或最后一个等。<br>便利的变量名+Stat称作状态变量，其属性有：</p>\n<ul>\n<li>index:当前迭代对象的迭代索引，从0开始，这是索引属性；</li>\n<li>count:当前迭代对象的迭代索引，从1开始，这个是统计属性；</li>\n<li>size:迭代变量元素的总量，这是被迭代对象的大小属性；</li>\n<li>current:当前迭代变量；</li>\n<li>even/odd:布尔值，当前循环是否是偶数/奇数（从0开始计算）；</li>\n<li>first:布尔值，当前循环是否是第一个；</li>\n<li>last:布尔值，当前循环是否是最后一个</li>\n</ul>\n<h2 id=\"th-href\"><a href=\"#th-href\" class=\"headerlink\" title=\"th:href\"></a>th:href</h2><p>用于声明在a 标签上的href属性的链接 该语法会和@{..} 表达式一起使用。<br>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thhref&quot;</span>)</span><br><span class=\"line\">public String thhref(Model model)&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thhref&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板页面</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;a href=<span class=\"string\">&quot;../home.html&quot;</span> th:href=<span class=\"string\">&quot;@&#123;/&#125;&quot;</span>&gt;返回首页&lt;/a&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;a href=<span class=\"string\">&quot;/sbe/&quot;</span>&gt;返回首页&lt;/a&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-class\"><a href=\"#th-class\" class=\"headerlink\" title=\"th:class\"></a>th:class</h2><p>用于声明在标签上class 属性信息。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thclass&quot;</span>)</span><br><span class=\"line\">public String thclass(Model model)&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thclass&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:class=<span class=\"string\">&quot; &#x27;even&#x27;? &#x27;even&#x27; : &#x27;odd&#x27;&quot;</span> th:text=<span class=\"string\">&quot; &#x27;even&#x27;? &#x27;even&#x27; : &#x27;odd&#x27;&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p class=<span class=\"string\">&quot;even&quot;</span>&gt;even&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-attr\"><a href=\"#th-attr\" class=\"headerlink\" title=\"th:attr\"></a>th:attr</h2><p>用于声明html中或自定义属性信息。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thattr&quot;</span>)</span><br><span class=\"line\">public String thattr(Model model)&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thattr&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;img  th:attr=<span class=\"string\">&quot;src=@&#123;/images/gtvglogo.png&#125;&quot;</span> /&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;img src=<span class=\"string\">&quot;/sbe/images/gtvglogo.png&quot;</span>&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-value\"><a href=\"#th-value\" class=\"headerlink\" title=\"th:value\"></a>th:value</h2><p>用于声明html中value属性信息。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thvalue&quot;</span>)</span><br><span class=\"line\">public String thvalue(Model model)&#123;</span><br><span class=\"line\">  model.addAttribute(<span class=\"string\">&quot;name&quot;</span>, <span class=\"string\">&quot;ljk&quot;</span>);</span><br><span class=\"line\">  <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thvalue&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;input <span class=\"built_in\">type</span>=<span class=\"string\">&quot;text&quot;</span> th:value=<span class=\"string\">&quot;<span class=\"variable\">$&#123;name&#125;</span>&quot;</span> /&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;input <span class=\"built_in\">type</span>=<span class=\"string\">&quot;text&quot;</span> value=<span class=\"string\">&quot;ljk&quot;</span>&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-action\"><a href=\"#th-action\" class=\"headerlink\" title=\"th:action\"></a>th:action</h2><p>用于声明html from标签中action属性信息。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thaction&quot;</span>)</span><br><span class=\"line\">  public String thaction(Model model)&#123;</span><br><span class=\"line\">  <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thaction&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;form action=<span class=\"string\">&quot;subscribe.html&quot;</span> th:action=<span class=\"string\">&quot;@&#123;/subscribe&#125;&quot;</span>&gt;</span><br><span class=\"line\">    &lt;input <span class=\"built_in\">type</span>=<span class=\"string\">&quot;text&quot;</span> name=<span class=\"string\">&quot;name&quot;</span> value=<span class=\"string\">&quot;abc&quot;</span>/&gt;</span><br><span class=\"line\">&lt;/form&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;form action=<span class=\"string\">&quot;/sbe/subscribe&quot;</span>&gt;</span><br><span class=\"line\">    &lt;input <span class=\"built_in\">type</span>=<span class=\"string\">&quot;text&quot;</span> name=<span class=\"string\">&quot;name&quot;</span> value=<span class=\"string\">&quot;abc&quot;</span>&gt;</span><br><span class=\"line\">&lt;/form&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-id\"><a href=\"#th-id\" class=\"headerlink\" title=\"th:id\"></a>th:id</h2><p>用于声明htm id属性信息。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thid&quot;</span>)</span><br><span class=\"line\">public String thid(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;id&quot;</span>, 123);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thid&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:id=<span class=\"string\">&quot;<span class=\"variable\">$&#123;id&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p id=<span class=\"string\">&quot;123&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-inline\"><a href=\"#th-inline\" class=\"headerlink\" title=\"th:inline\"></a>th:inline</h2><p>JavaScript内联 操作使用的语法，具体请参考下面内联操作相关介绍</p>\n<h2 id=\"th-onclick\"><a href=\"#th-onclick\" class=\"headerlink\" title=\"th:onclick\"></a>th:onclick</h2><p>用于声明htm 中的onclick事件。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thonclick&quot;</span>)</span><br><span class=\"line\">public String honclick(Model model)&#123;</span><br><span class=\"line\">  <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thonclick&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">&lt;title&gt;Insert title here&lt;/title&gt;</span><br><span class=\"line\">&lt;script <span class=\"built_in\">type</span>=<span class=\"string\">&quot;text/javascript&quot;</span>&gt;</span><br><span class=\"line\">    <span class=\"keyword\">function</span> <span class=\"function\"><span class=\"title\">showUserInfo</span></span>()&#123;</span><br><span class=\"line\">        alert(<span class=\"string\">&quot;i am zhuoqianmingyue!&quot;</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">   &lt;p th:onclick=<span class=\"string\">&quot;&#x27;showUserInfo()&#x27;&quot;</span>&gt;点我&lt;/p&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p onclick=<span class=\"string\">&quot;showUserInfo()&quot;</span>&gt;点我&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-selected\"><a href=\"#th-selected\" class=\"headerlink\" title=\"th:selected\"></a>th:selected</h2><p>用于声明htm 中的selected属性信息。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thselected&quot;</span>)</span><br><span class=\"line\">public String thselected(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;sex&quot;</span>, 1);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thselected&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;select&gt;</span><br><span class=\"line\">    &lt;option name=<span class=\"string\">&quot;sex&quot;</span>&gt;&lt;/option&gt;</span><br><span class=\"line\">    &lt;option th:selected=<span class=\"string\">&quot;1 == <span class=\"variable\">$&#123;sex&#125;</span>&quot;</span>&gt;男&lt;/option&gt;</span><br><span class=\"line\">    &lt;option th:selected=<span class=\"string\">&quot;0 == <span class=\"variable\">$&#123;sex&#125;</span>&quot;</span>&gt;女&lt;/option&gt;</span><br><span class=\"line\">&lt;/select&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;select&gt;</span><br><span class=\"line\">&lt;option name=<span class=\"string\">&quot;sex&quot;</span>&gt;&lt;/option&gt;</span><br><span class=\"line\">    &lt;option selected=<span class=\"string\">&quot;selected&quot;</span>&gt;男&lt;/option&gt;</span><br><span class=\"line\">    &lt;option&gt;女&lt;/option&gt;</span><br><span class=\"line\">&lt;/select&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-src\"><a href=\"#th-src\" class=\"headerlink\" title=\"th:src\"></a>th:src</h2><p>用于声明htm 中的img中src属性信息。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thsrc&quot;</span>)</span><br><span class=\"line\">public String thsrc(Model model)&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thsrc&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;img  title=<span class=\"string\">&quot;GTVG logo&quot;</span> th:src=<span class=\"string\">&quot;@&#123;/images/gtvglogo.png&#125;&quot;</span> /&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;img title=<span class=\"string\">&quot;GTVG logo&quot;</span> src=<span class=\"string\">&quot;/sbe/images/gtvglogo.png&quot;</span>&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-style\"><a href=\"#th-style\" class=\"headerlink\" title=\"th:style\"></a>th:style</h2><p>用于声明htm中的标签 css的样式信息。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">RequestMapping(<span class=\"string\">&quot;/thstyle&quot;</span>)</span><br><span class=\"line\">public String thstyle(Model model)&#123;</span><br><span class=\"line\">  model.addAttribute(<span class=\"string\">&quot;isShow&quot;</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">  <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thstyle&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:style=<span class=\"string\">&quot;&#x27;display:&#x27; + @&#123;(<span class=\"variable\">$&#123;isShow&#125;</span> ? &#x27;none&#x27; : &#x27;block&#x27;)&#125; + &#x27;&#x27;&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p style=<span class=\"string\">&quot;display:none&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"th-with\"><a href=\"#th-with\" class=\"headerlink\" title=\"th:with\"></a>th:with</h2><p>用于thymeleaf 模版页面中局部变量定义的使用。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#代码一</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thwith&quot;</span>)</span><br><span class=\"line\">public String thwith(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;today&quot;</span>, new Date());</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thwith&quot;</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#代码二</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/thwith&quot;</span>)</span><br><span class=\"line\">public String thwith(Model model)&#123;</span><br><span class=\"line\">    List&lt;User&gt; users = new ArrayList&lt;User&gt;();</span><br><span class=\"line\">    users.add(new User(<span class=\"string\">&quot;ljk&quot;</span>,18));</span><br><span class=\"line\">    users.add(new User(<span class=\"string\">&quot;ljk2&quot;</span>,18));</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;users&quot;</span>,users);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/thwith&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:with=<span class=\"string\">&quot;df=&#x27;dd/MMM/yyyy HH:mm&#x27;&quot;</span>&gt;</span><br><span class=\"line\">  Today is: &lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.format(today,df)&#125;</span>&quot;</span>&gt;13 February 2011&lt;/span&gt;</span><br><span class=\"line\">&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;div th:with=<span class=\"string\">&quot;firstEle=<span class=\"variable\">$&#123;users[0]&#125;</span>&quot;</span>&gt;</span><br><span class=\"line\">    &lt;p&gt;第一个用户的名称是： </span><br><span class=\"line\">    \t&lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;firstEle.name&#125;</span>&quot;</span>&gt;&lt;/span&gt;.</span><br><span class=\"line\">    &lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;span&gt;02/02/2020 06:52&lt;/span&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">    &lt;p&gt;</span><br><span class=\"line\">      第一个用户的名称是： &lt;span&gt;ljk&lt;/span&gt;.</span><br><span class=\"line\">    &lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>还有一种用法是在模版布局中带参数的引用片段中使用方式如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:replace=<span class=\"string\">&quot;::frag&quot;</span> th:with=<span class=\"string\">&quot;onevar=<span class=\"variable\">$&#123;value1&#125;</span>,twovar=<span class=\"variable\">$&#123;value2&#125;</span>&quot;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>具体演示请参考模版布局中的介绍。</p>\n<h2 id=\"Elvis运算符\"><a href=\"#Elvis运算符\" class=\"headerlink\" title=\"Elvis运算符\"></a>Elvis运算符</h2><p>Elvis运算可以理解成简单的判断是否为null的三元运算的简写，如果值为null显示默认值，如果不为null 则显示原有的值。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#代码一</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/elvis&quot;</span>)</span><br><span class=\"line\">public String elvis(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;age&quot;</span>, null);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/elvis&quot;</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#代码二</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/elvis&quot;</span>)</span><br><span class=\"line\">public String elvis(Model model)&#123;</span><br><span class=\"line\">  model.addAttribute(<span class=\"string\">&quot;age2&quot;</span>, 18);</span><br><span class=\"line\">  <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/elvis&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\"> &lt;p&gt;Age: &lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;age&#125;</span>?: &#x27;年龄为nll&#x27;&quot;</span>&gt;&lt;/span&gt;&lt;/p&gt;</span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p&gt;Age2: &lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;age2&#125;</span>?: &#x27;年龄为nll&#x27;&quot;</span>&gt;&lt;/span&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;p&gt;Age: &lt;span&gt;年龄为nll&lt;/span&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;p&gt;Age2: &lt;span&gt;18&lt;/span&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"三元表达式\"><a href=\"#三元表达式\" class=\"headerlink\" title=\"三元表达式\"></a>三元表达式</h2><p>我们可以在thymeleaf 的语法中使用三元表达式 具体使用方法是在th:x 中通过 表达式？1选项：2选项。</p>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#代码一</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/threeElementOperation&quot;</span>)</span><br><span class=\"line\">public String threeElementOperation(Model model)&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/threeElementOperation&quot;</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#代码二</span></span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/threeElementOperation&quot;</span>)</span><br><span class=\"line\">public String threeElementOperation(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;name&quot;</span>, <span class=\"string\">&quot;ljk&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/threeElementOperation&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:class=<span class=\"string\">&quot; &#x27;even&#x27;? &#x27;even&#x27; : &#x27;odd&#x27;&quot;</span> th:text=<span class=\"string\">&quot; &#x27;even&#x27;? &#x27;even&#x27; : &#x27;odd&#x27;&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p th:value=<span class=\"string\">&quot;<span class=\"variable\">$&#123;name eq &#x27;ljk&#x27; ? &#x27;帅哥&#x27;:&#x27;丑男&#x27;&#125;</span>&quot;</span> th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;name eq &#x27;ljk&#x27; ? &#x27;帅哥&#x27;:&#x27;丑男&#x27;&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;p class=<span class=\"string\">&quot;even&quot;</span>&gt;even&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\"> &lt;p value=<span class=\"string\">&quot;帅哥&quot;</span>&gt;帅哥&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p><strong>条件表达式操作字符：</strong></p>\n<ul>\n<li>gt：great than（大于）</li>\n<li>ge：great equal（大于等于）</li>\n<li>eq：equal（等于）</li>\n<li>lt：less than（小于）</li>\n<li>le：less equal（小于等于）</li>\n<li>ne：not equal（不等于）</li>\n</ul>\n<h2 id=\"No-Operation（-）\"><a href=\"#No-Operation（-）\" class=\"headerlink\" title=\"No-Operation（_）\"></a>No-Operation（_）</h2><p>Elvis运算符 的一种特殊简写操作，当显示的值为null 是就什么都不做。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/noOperation&quot;</span>)</span><br><span class=\"line\">public String noOperation(Model model)&#123;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;name&quot;</span>, null);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/noOperation&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;name&#125;</span> ?: _&quot;</span>&gt;no user authenticated&lt;/span&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;span&gt;no user authenticated&lt;/span&gt;</span><br></pre></td></tr></table></figure>\n<p>标准方言中存在以下固定值布尔属性：<br>|th:async|    th:autofocus    |th:autoplay|<br>|–|–| – |<br>|th:checked    |th:controls|    th:declare|<br>|th:default|    th:defer|    th:disabled|<br>|th:formnovalidate    |th:hidden    |th:ismap|<br>|th:loop|    th:multiple    |th:novalidate|<br>|th:nowrap|    th:open|    th:pubdate|<br>|th:readonly    |th:required|    th:reversed|<br>|th:scoped|    th:seamless|    th:selected|</p>\n<p>针对特定的HTML5属性：<br>|  |  |  |<br>|–|–| – |<br>|th:abbr    |th:accept|    th:accept-charset|<br>|th:accesskey    |th:action|    th:align|<br>|th:alt    |th:archive|    th:audio|<br>|th:autocomplete    |th:axis|    th:background|<br>|th:bgcolor|    th:border|    th:cellpadding|<br>|th:cellspacing|    th:challenge|    th:charset|<br>|th:cite    |th:class    |th:classid|<br>|th:codebase|    th:codetype|    th:cols|<br>|th:colspan|    th:compact|    th:content|<br>|th:contenteditable|    th:contextmenu|    th:data|<br>|th:datetime    |th:dir|    th:draggable|<br>|th:dropzone    |th:enctype|    th:for|<br>|th:form    |th:formaction    |th:formenctype|<br>|th:formmethod|    th:formtarget|    th:fragment|<br>|th:frame    |th:frameborder|    th:headers|<br>|th:height    |th:high    |th:href|<br>|th:hreflang    |th:hspace|    th:http-equiv|<br>|th:icon|    th:id|    th:inline|<br>|th:keytype    |th:kind    |th:label|<br>|th:lang|    th:list    |th:longdesc|<br>|th:low|    th:manifest|    th:marginheight|<br>|th:marginwidth|    th:max|    th:maxlength|<br>|th:media|    th:method    |th:min|<br>|th:name|    th:onabort|    th:onafterprint|<br>|th:onbeforeprint    |th:onbeforeunload    |th:onblur|<br>|th:oncanplay|    th:oncanplaythrough|    th:onchange|<br>|th:onclick|    th:oncontextmenu|    th:ondblclick|<br>|th:ondrag    |th:ondragend|    th:ondragenter|<br>|th:ondragleave|    th:ondragover|    th:ondragstart|<br>|th:ondrop    |th:ondurationchange    |th:onemptied|<br>|th:onended    |th:onerror    |th:onfocus|<br>|th:onformchange|    th:onforminput    |th:onhashchange|<br>|th:oninput    |th:oninvalid|    th:onkeydown|<br>|th:onkeypress    |th:onkeyup|    th:onload|<br>|th:onloadeddata    |th:onloadedmetadata    |th:onloadstart|<br>|th:onmessage    |th:onmousedown    |th:onmousemove|<br>|th:onmouseout    |th:onmouseover    |th:onmouseup|<br>|th:onmousewheel    |th:onoffline|    th:ononline|<br>|th:onpause    |th:onplay|    th:onplaying|<br>|th:onpopstate    |th:onprogress|    th:onratechange|<br>|th:onreadystatechange    |th:onredo    |th:onreset|<br>|th:onresize|    th:onscroll|    th:onseeked|<br>|th:onseeking|    th:onselect    |th:onshow|<br>|th:onstalled|    th:onstorage    |th:onsubmit|<br>|th:onsuspend    |th:ontimeupdate|    th:onundo|<br>|th:onunload|    th:onvolumechange|    th:onwaiting|<br>|th:optimum    |th:pattern    |th:placeholder|<br>|th:poster    |th:preload    |th:radiogroup|<br>|th:rel    |th:rev    |th:rows|<br>|th:rowspan|    th:rules|    th:sandbox|<br>|th:scheme|    th:scope    |th:scrolling|<br>|th:size    |th:sizes|    th:span|<br>|th:spellcheck    |th:src|    th:srclang|<br>|th:standby|    th:start|    th:step|<br>|th:style    |th:summary    |th:tabindex|<br>|th:target    |th:title    |th:type|<br>|th:usemap    |th:value|    th:valuetype|<br>|th:vspace|    th:width|    th:wrap|<br>|th:xmlbase|    th:xmllang|    th:xmlspace|</p>\n<h2 id=\"内联\"><a href=\"#内联\" class=\"headerlink\" title=\"内联\"></a>内联</h2><p>如何使用内连操作，我们可以通过在父标签声明 th:inline=”text” 来开启内联操作。当然如果想整个页面使用可以直接声明在body上即可。具体使用方式如下面代码所示。</p>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:inline=<span class=\"string\">&quot;text&quot;</span>&gt;</span><br><span class=\"line\">\t&lt;p&gt;Hello, [[<span class=\"variable\">$&#123;user.name&#125;</span>]]!&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>结果内容如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">\t&lt;p&gt;Hello,zhuoqianmingyue!&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>这样的操作和使用th:text是等同的。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">\t&lt;p th:text=<span class=\"string\">&quot;Hello,+<span class=\"variable\">$&#123;user.name&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>[[…]]对应于th:text，[(…)]对应于th:utext</p>\n<h3 id=\"禁用内联操作\"><a href=\"#禁用内联操作\" class=\"headerlink\" title=\"禁用内联操作\"></a>禁用内联操作</h3><p>这我们可以通过在父标签或者本标签上声明th:inline=”none”来禁用内联的操作，如下面代码所示：<br>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:inline=<span class=\"string\">&quot;none&quot;</span>&gt;A double array looks like this: [[1, 2, 3], [4, 5]]!&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;A double array looks like this: [[1, 2, 3], [4, 5]]!&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"JavaScript内联\"><a href=\"#JavaScript内联\" class=\"headerlink\" title=\"JavaScript内联\"></a>JavaScript内联</h3><p>如果我们想在JavaScript 中使用内联操作，需要在 script 标签上声明 th:inline=”javascript” 然后我们就可以 script 标签中使用内联操作了。具体使用方式如下面代码所示：<br>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;script th:inline=<span class=\"string\">&quot;javascript&quot;</span>&gt;</span><br><span class=\"line\">    var username = [[<span class=\"variable\">$&#123;user.name&#125;</span>]];</span><br><span class=\"line\">&lt;/script&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;script th:inline=<span class=\"string\">&quot;javascript&quot;</span>&gt;</span><br><span class=\"line\">    var username = <span class=\"string\">&quot;zhuoqianmingyue&quot;</span>;</span><br><span class=\"line\">&lt;/script&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"CSS内联\"><a href=\"#CSS内联\" class=\"headerlink\" title=\"CSS内联\"></a>CSS内联</h3><p>我们可以通过在 style 标签上声明 th:inline=”css” 来开启在css中使用内联的操作，具体操作方式如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;style th:inline=<span class=\"string\">&quot;css&quot;</span>&gt;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&lt;/style&gt;</span><br></pre></td></tr></table></figure>\n<p>例如，假设我们将两个变量设置为两个不同的String值：classname = ‘main_elems’以及align = ‘center’<br>我们可以像以下一样使用它们：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;style th:inline=<span class=\"string\">&quot;css&quot;</span>&gt;</span><br><span class=\"line\">    .[[<span class=\"variable\">$&#123;classname&#125;</span>]] &#123;</span><br><span class=\"line\">      text-align: [[<span class=\"variable\">$&#123;align&#125;</span>]];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;style th:inline=<span class=\"string\">&quot;css&quot;</span>&gt;</span><br><span class=\"line\">    .main_elems &#123;</span><br><span class=\"line\">      text-align: center;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&lt;/style&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"模板布局\"><a href=\"#模板布局\" class=\"headerlink\" title=\"模板布局\"></a>模板布局</h2><h3 id=\"定义引用片段代码\"><a href=\"#定义引用片段代码\" class=\"headerlink\" title=\"定义引用片段代码\"></a>定义引用片段代码</h3><p>SpringBoot2.0 使用模版模版布局需要先引入 thymeleaf的 thymeleaf-layout-dialect依赖</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;nz.net.ultraq.thymeleaf&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;thymeleaf-layout-dialect&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>定义footer.html页面 该页面就是我们的引用片段代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=<span class=\"string\">&quot;en&quot;</span> xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    &lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">    &lt;title&gt;Title&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">    &lt;div th:fragment=<span class=\"string\">&quot;copy&quot;</span>&gt;</span><br><span class=\"line\">        &amp;copy; 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">    &lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>我们可以通过 th:fragment 来定义引用片段，然后可以在其他页面进行引用。定义引用页面 index.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=<span class=\"string\">&quot;en&quot;</span> xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    &lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">    &lt;title&gt;Title&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;div th:insert=<span class=\"string\">&quot;~&#123;footer :: copy&#125;&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>通过 th:insert 和 ~{…}片段引用表达式 进行引入footer.html中定义的片段,定义访问index页面的 controller</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@Controller</span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/layout&quot;</span>)</span><br><span class=\"line\">public class LayOutController &#123;</span><br><span class=\"line\">    @RequestMapping(<span class=\"string\">&quot;/index&quot;</span>)</span><br><span class=\"line\">    public String <span class=\"function\"><span class=\"title\">index</span></span>()&#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;/layout/index&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">  &lt;div&gt;</span><br><span class=\"line\">      © 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">  &lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>如下面的代码2种方式的写法是一致的。如果你觉得~{footer :: copy}写法比较麻烦可以采用简写的方式footer :: copy。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:insert=<span class=\"string\">&quot;footer :: copy&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div th:insert=<span class=\"string\">&quot;~&#123;footer :: copy&#125;&quot;</span>&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"通过id属性来声明片段\"><a href=\"#通过id属性来声明片段\" class=\"headerlink\" title=\"通过id属性来声明片段\"></a>通过id属性来声明片段</h3><p>我们可以通过 th:fragment 来定义引用片段，但是我们也可以通过在引用片段代码上声明id属性的方式进行片段的引用，具体操作方式如下：定义引用片段代码模版页面 footer.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=<span class=\"string\">&quot;en&quot;</span> xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    &lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">    &lt;title&gt;Title&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;div id=<span class=\"string\">&quot;copy-section&quot;</span> &gt;</span><br><span class=\"line\">    &amp;copy; 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>引用引用片段的模版页面：index.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=<span class=\"string\">&quot;en&quot;</span> xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    &lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">    &lt;title&gt;Title&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;div th:insert=<span class=\"string\">&quot;~&#123;footer :: #copy-section&#125;&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">&lt;div id=<span class=\"string\">&quot;copy-section&quot;</span>&gt;</span><br><span class=\"line\">    © 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>footer :: #copy-section和~{footer :: #copy-section} 结果是一致的。</p>\n<h3 id=\"th-insert和th-replace（和th-include）之间的区别\"><a href=\"#th-insert和th-replace（和th-include）之间的区别\" class=\"headerlink\" title=\"th:insert和th:replace（和th:include）之间的区别\"></a>th:insert和th:replace（和th:include）之间的区别</h3><ul>\n<li>th:insert 是最简单的：他会将使用th:insert的标签 和引用片段的内容都显示出来</li>\n<li>th:replace 插入引用片段的标签和内容</li>\n<li>th:include类似于th:insert，只插入此片段的内容。</li>\n</ul>\n<h4 id=\"th-insert\"><a href=\"#th-insert\" class=\"headerlink\" title=\"th:insert\"></a>th:insert</h4><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@Controller</span><br><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/layout&quot;</span>)</span><br><span class=\"line\">public class LayoutController &#123;</span><br><span class=\"line\">    @RequestMapping(<span class=\"string\">&quot;/index2&quot;</span>)</span><br><span class=\"line\">    public String index2(Model model) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> <span class=\"string\">&quot;/layout/index2&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>声明引用片段模版页面：footer2.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">&lt;title&gt;Insert title here&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;footer th:fragment=<span class=\"string\">&quot;copy&quot;</span>&gt;</span><br><span class=\"line\">  &amp;copy; 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/footer&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>引用片段模版页面：index2.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">&lt;title&gt;Insert title here&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;div th:insert=<span class=\"string\">&quot;footer2 :: copy&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div th:replace=<span class=\"string\">&quot;footer2 :: copy&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div th:include=<span class=\"string\">&quot;footer2:: copy&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#th:insert 结果：</span></span><br><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">&lt;footer&gt;</span><br><span class=\"line\">  © 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/footer&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#th:replace结果：</span></span><br><span class=\"line\">&lt;footer&gt;</span><br><span class=\"line\">  © 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/footer&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#th:include结果：</span></span><br><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">  © 2011 The Good Thymes Virtual Grocery</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"带参数的引用片段\"><a href=\"#带参数的引用片段\" class=\"headerlink\" title=\"带参数的引用片段\"></a>带参数的引用片段</h3><p>定义引用片段代码模版页面 footer.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:th=<span class=\"string\">&quot;http://www.thymeleaf.org&quot;</span>&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">&lt;title&gt;Insert title here&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;div th:fragment=<span class=\"string\">&quot;frag (onevar,twovar)&quot;</span>&gt;</span><br><span class=\"line\">    &lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;onevar&#125;</span> + &#x27; - &#x27; + <span class=\"variable\">$&#123;twovar&#125;</span>&quot;</span>&gt;...&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>引用引用片段的模版页面：index.html</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span><br><span class=\"line\">&lt;title&gt;Insert title here&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">    &lt;div th:insert=<span class=\"string\">&quot;footer :: frag(&#x27;a&#x27;,&#x27;b&#x27;)&quot;</span>&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">&lt;div&gt;</span><br><span class=\"line\">    &lt;p&gt;a - b&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>th:insert=”footer ::frag (onevar=’a’,twovar=’b’)” 和th:insert=”footer :: frag(‘a’,’b’)效果是相等的。还有另一种写法就是使用th:with<br>th:insert=”::frag” th:with=”onevar=’a’,twovar=’b’”</p>\n<h3 id=\"删除模版片段\"><a href=\"#删除模版片段\" class=\"headerlink\" title=\"删除模版片段\"></a>删除模版片段</h3><p>我们为了方便通过直接查看下面的页面 productList.html （主要是为了作为原型页面进行查看）我们需要添加一些模拟数据。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;table&gt;</span><br><span class=\"line\">  &lt;tr&gt;</span><br><span class=\"line\">    &lt;th&gt;NAME&lt;/th&gt;</span><br><span class=\"line\">    &lt;th&gt;PRICE&lt;/th&gt;</span><br><span class=\"line\">    &lt;th&gt;IN STOCK&lt;/th&gt;</span><br><span class=\"line\">    &lt;th&gt;COMMENTS&lt;/th&gt;</span><br><span class=\"line\">  &lt;/tr&gt;</span><br><span class=\"line\">  &lt;tr th:each=<span class=\"string\">&quot;prod : <span class=\"variable\">$&#123;prods&#125;</span>&quot;</span> th:class=<span class=\"string\">&quot;<span class=\"variable\">$&#123;prodStat.odd&#125;</span>? &#x27;odd&#x27;&quot;</span>&gt;</span><br><span class=\"line\">    &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;prod.name&#125;</span>&quot;</span>&gt;Onions&lt;/td&gt;</span><br><span class=\"line\">    &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;prod.price&#125;</span>&quot;</span>&gt;2.41&lt;/td&gt;</span><br><span class=\"line\">    &lt;td th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;prod.inStock&#125;</span>? #&#123;true&#125; : #&#123;false&#125;&quot;</span>&gt;yes&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;</span><br><span class=\"line\">      &lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.size(prod.comments)&#125;</span>&quot;</span>&gt;2&lt;/span&gt; comment/s</span><br><span class=\"line\">      &lt;a href=<span class=\"string\">&quot;comments.html&quot;</span> </span><br><span class=\"line\">         th:href=<span class=\"string\">&quot;@&#123;/product/comments(prodId=<span class=\"variable\">$&#123;prod.id&#125;</span>)&#125;&quot;</span> </span><br><span class=\"line\">         th:unless=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.isEmpty(prod.comments)&#125;</span>&quot;</span>&gt;view&lt;/a&gt;</span><br><span class=\"line\">    &lt;/td&gt;</span><br><span class=\"line\">  &lt;/tr&gt;</span><br><span class=\"line\">  &lt;tr class=<span class=\"string\">&quot;odd&quot;</span>&gt;</span><br><span class=\"line\">    &lt;td&gt;Blue Lettuce&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;9.55&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;no&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;</span><br><span class=\"line\">      &lt;span&gt;0&lt;/span&gt; comment/s</span><br><span class=\"line\">    &lt;/td&gt;</span><br><span class=\"line\">  &lt;/tr&gt;</span><br><span class=\"line\">  &lt;tr&gt;</span><br><span class=\"line\">    &lt;td&gt;Mild Cinnamon&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;1.99&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;yes&lt;/td&gt;</span><br><span class=\"line\">    &lt;td&gt;</span><br><span class=\"line\">      &lt;span&gt;3&lt;/span&gt; comment/s</span><br><span class=\"line\">      &lt;a href=<span class=\"string\">&quot;comments.html&quot;</span>&gt;view&lt;/a&gt;</span><br><span class=\"line\">    &lt;/td&gt;</span><br><span class=\"line\">  &lt;/tr&gt;</span><br><span class=\"line\">&lt;/table&gt;</span><br></pre></td></tr></table></figure>\n<p>在上面的代码中模拟数据的代码，但是我们通过正常的controller访问该页面的时候会显示出下面的模拟数据。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;tr class=<span class=\"string\">&quot;odd&quot;</span>&gt;</span><br><span class=\"line\">   &lt;td&gt;Blue Lettuce&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;9.55&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;no&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;</span><br><span class=\"line\">     &lt;span&gt;0&lt;/span&gt; comment/s</span><br><span class=\"line\">   &lt;/td&gt;</span><br><span class=\"line\"> &lt;/tr&gt;</span><br><span class=\"line\"> &lt;tr&gt;</span><br><span class=\"line\">   &lt;td&gt;Mild Cinnamon&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;1.99&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;yes&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;</span><br><span class=\"line\">     &lt;span&gt;3&lt;/span&gt; comment/s</span><br><span class=\"line\">     &lt;a href=<span class=\"string\">&quot;comments.html&quot;</span>&gt;view&lt;/a&gt;</span><br><span class=\"line\">   &lt;/td&gt;</span><br><span class=\"line\"> &lt;/tr&gt;</span><br></pre></td></tr></table></figure>\n<p>thymeleaf 为我们提供了 th:remove 帮助我们解决这个问题：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;tr class=<span class=\"string\">&quot;odd&quot;</span> th:remove=<span class=\"string\">&quot;all&quot;</span>&gt;</span><br><span class=\"line\">   &lt;td&gt;Blue Lettuce&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;9.55&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;no&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;</span><br><span class=\"line\">     &lt;span&gt;0&lt;/span&gt; comment/s</span><br><span class=\"line\">   &lt;/td&gt;</span><br><span class=\"line\"> &lt;/tr&gt;</span><br><span class=\"line\"> &lt;tr th:remove=<span class=\"string\">&quot;all&quot;</span>&gt;</span><br><span class=\"line\">   &lt;td&gt;Mild Cinnamon&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;1.99&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;yes&lt;/td&gt;</span><br><span class=\"line\">   &lt;td&gt;</span><br><span class=\"line\">     &lt;span&gt;3&lt;/span&gt; comment/s</span><br><span class=\"line\">     &lt;a href=<span class=\"string\">&quot;comments.html&quot;</span>&gt;view&lt;/a&gt;</span><br><span class=\"line\">   &lt;/td&gt;</span><br><span class=\"line\"> &lt;/tr&gt;</span><br></pre></td></tr></table></figure>\n<p>我们在模拟数据上声明th:remove=”all” 后在此通过url访问 没有了我们之前的模拟数据</p>\n<p><strong>all属性中的这个值是什么意思？th:remove可以根据其价值以五种不同的方式表现：</strong></p>\n<ul>\n<li>all：删除包含标记及其所有子标记。</li>\n<li>body：不要删除包含标记，但删除其所有子标记。</li>\n<li>tag：删除包含标记，但不删除其子项。</li>\n<li>all-but-first：删除除第一个之外的所有包含标记的子项。</li>\n<li>none： 没做什么。此值对于动态评估很有用。当我们知道没有属性的含义后我们可以通过在 声明一次即可，无需在通过定义多个 th:remove=”all”</li>\n</ul>\n<h2 id=\"预定义的工具对象\"><a href=\"#预定义的工具对象\" class=\"headerlink\" title=\"预定义的工具对象\"></a>预定义的工具对象</h2><h3 id=\"dates\"><a href=\"#dates\" class=\"headerlink\" title=\"dates\"></a>dates</h3><p>处理日期数据 生成，转换，获取日期的具体天数 年数。<br>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/dates&quot;</span>)</span><br><span class=\"line\">public String dates(Model model) throws ParseException&#123;</span><br><span class=\"line\">    Date date = new Date();</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;date&quot;</span>,date);</span><br><span class=\"line\"></span><br><span class=\"line\">    String dateStr = <span class=\"string\">&quot;2018-05-30&quot;</span>;</span><br><span class=\"line\">    SimpleDateFormat sdf = new SimpleDateFormat(<span class=\"string\">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class=\"line\">    Date date2 =  sdf.parse(dateStr); </span><br><span class=\"line\">    Date[] datesArray = new Date[2];</span><br><span class=\"line\">    datesArray[0] = date;</span><br><span class=\"line\">    datesArray[1] = date2;</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;datesArray&quot;</span>,datesArray);</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;Date&gt; datesList = new ArrayList&lt;Date&gt;();</span><br><span class=\"line\">    datesList.add(date);</span><br><span class=\"line\">    datesList.add(date2);</span><br><span class=\"line\">    model.addAttribute(<span class=\"string\">&quot;datesList&quot;</span>,datesList);</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/dates&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"format操作\"><a href=\"#format操作\" class=\"headerlink\" title=\"format操作\"></a>format操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#代码一</span></span><br><span class=\"line\">Date date = new Date();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#代码三</span></span><br><span class=\"line\">Date[] datesArray = new Date[2];</span><br><span class=\"line\">datesArray[0] = date;</span><br><span class=\"line\">datesArray[1] = date2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#代码四</span></span><br><span class=\"line\">List&lt;Date&gt; datesList = new ArrayList&lt;Date&gt;();</span><br><span class=\"line\">datesList.add(date);</span><br><span class=\"line\">datesList.add(date2);</span><br><span class=\"line\">model.addAttribute(<span class=\"string\">&quot;datesList&quot;</span>,datesList);</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.format(date)&#125;</span>&quot;</span>&gt;4564546&lt;/span&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;span th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.format(date, &#x27;dd/MMM/yyyy HH:mm&#x27;)&#125;</span>&quot;</span>&gt;4564546&lt;/span&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板三</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.format(datesArray, &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板四</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.listFormat(datesList, &#x27;dd/MMM/yyyy HH:mm&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;span&gt;2020年1月30日 上午10时03分24秒 &lt;/span&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">结果二</span><br><span class=\"line\">&lt;span&gt;30/一月/2020 10:03 &lt;/span&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果三</span></span><br><span class=\"line\">&lt;p&gt;2019-05-30 10:03&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果四</span></span><br><span class=\"line\">&lt;p&gt;[30/五月/2019 10:03, 30/五月/2018 00:00]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"获取日期属性操作\"><a href=\"#获取日期属性操作\" class=\"headerlink\" title=\"获取日期属性操作\"></a>获取日期属性操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#代码一</span></span><br><span class=\"line\">Date date = new Date();</span><br></pre></td></tr></table></figure>\n<p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.day(date)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.month(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板三</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.monthName(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板四</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.monthNameShort(date)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板五</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.year(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板六</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.dayOfWeek(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板七</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.dayOfWeekName(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板八</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.dayOfWeekNameShort(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板九</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.hour(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板十</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.minute(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板十一</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.second(date)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板十二</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.millisecond(date)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;p&gt;30&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;p&gt;5&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果三</span></span><br><span class=\"line\">&lt;p&gt;五月&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果四</span></span><br><span class=\"line\">&lt;p&gt;五月&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果五</span></span><br><span class=\"line\">&lt;p&gt;2019&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果六</span></span><br><span class=\"line\">&lt;p&gt;5&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果七</span></span><br><span class=\"line\">&lt;p&gt;星期四&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果七</span></span><br><span class=\"line\">&lt;p&gt;星期四&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果八</span></span><br><span class=\"line\">&lt;p&gt;10&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果九</span></span><br><span class=\"line\">&lt;p&gt;10&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果十</span></span><br><span class=\"line\">&lt;p&gt;45&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果十一</span></span><br><span class=\"line\">&lt;p&gt;853&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"生成日期操作\"><a href=\"#生成日期操作\" class=\"headerlink\" title=\"生成日期操作\"></a>生成日期操作</h3><p>模版页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#模板一</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.createNow()&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板二</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.format(#dates.createNow())&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板三</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.create(&#x27;2019&#x27;,&#x27;05&#x27;,&#x27;30&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板四</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.create(&#x27;2019&#x27;,&#x27;05&#x27;,&#x27;31&#x27;,&#x27;10&#x27;,&#x27;18&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板五</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.create(&#x27;2019&#x27;,&#x27;05&#x27;,&#x27;30&#x27;,&#x27;10&#x27;,&#x27;18&#x27;,&#x27;34&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#模板六</span></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#dates.createToday()&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#结果一</span></span><br><span class=\"line\">&lt;p&gt;Thu May 30 10:15:55 CST 2019&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果二</span></span><br><span class=\"line\">&lt;p&gt;2019年5月30日 上午10时15分55秒&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果三</span></span><br><span class=\"line\">&lt;p&gt;Thu May 30 00:00:00 CST 2019&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果四</span></span><br><span class=\"line\">&lt;p&gt;Fri May 31 10:18:00 CST 2019&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果五</span></span><br><span class=\"line\">&lt;p&gt;Thu May 30 10:18:34 CST 2019&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#结果六</span></span><br><span class=\"line\">&lt;p&gt;Thu May 30 00:00:00 CST 2019&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"numbers\"><a href=\"#numbers\" class=\"headerlink\" title=\"numbers\"></a>numbers</h2><p>处理数字数据的转换。包括:</p>\n<ul>\n<li>对不够位数的数字进行补0（formatInteger ）</li>\n<li>设置千位分隔符（formatInteger）</li>\n<li>精确小数点（formatDecimal ）</li>\n<li>设置百分号（formatPercent ）</li>\n<li>生成数组（sequence ）<br>代码演示：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/numbers&quot;</span>)</span><br><span class=\"line\">public String numbers(Model model) throws ParseException&#123;</span><br><span class=\"line\">    <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/numbers&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"数字进行补0操作\"><a href=\"#数字进行补0操作\" class=\"headerlink\" title=\"数字进行补0操作\"></a>数字进行补0操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;123&#x27;,4)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;123&#x27;,3)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;123&#x27;,2)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;0123&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;123&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;123&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">  @RequestMapping(<span class=\"string\">&quot;/numbers&quot;</span>)</span><br><span class=\"line\">  public String numbers(Model model) throws ParseException&#123;</span><br><span class=\"line\">      List&lt;Integer&gt; numList = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">      numList.add(1);</span><br><span class=\"line\">      numList.add(12);</span><br><span class=\"line\">      numList.add(13);</span><br><span class=\"line\">      model.addAttribute(<span class=\"string\">&quot;numList&quot;</span>,numList);</span><br><span class=\"line\">      <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/numbers&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.listFormatInteger(numList,3)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[001, 012, 013]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"千位分隔符操作\"><a href=\"#千位分隔符操作\" class=\"headerlink\" title=\"千位分隔符操作\"></a>千位分隔符操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,2,&#x27;POINT&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,6,&#x27;POINT&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,7,&#x27;POINT&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,2,&#x27;COMMA&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,2,&#x27;WHITESPACE&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,2,&#x27;NONE&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatInteger(&#x27;1000&#x27;,2,&#x27;DEFAULT&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;1.000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;001.000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;0.001.000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;1,000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;1 000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;1000&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;1,000&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"精确小数点操作\"><a href=\"#精确小数点操作\" class=\"headerlink\" title=\"精确小数点操作\"></a>精确小数点操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatDecimal(&#x27;10.123&#x27;,3,2)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatDecimal(&#x27;1000.123&#x27;,5,&#x27;POINT&#x27;,2,&#x27;COMMA&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;010.12&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;01.000,12&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"钱显示符号操作\"><a href=\"#钱显示符号操作\" class=\"headerlink\" title=\"钱显示符号操作\"></a>钱显示符号操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatCurrency(&#x27;1000&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;￥1,000.00&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"百分比操作\"><a href=\"#百分比操作\" class=\"headerlink\" title=\"百分比操作\"></a>百分比操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatPercent(&#x27;0.2&#x27;,2, 4)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#numbers.formatPercent(&#x27;0.2&#x27;,3, 2)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;20.0000%&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;020.00%&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"生成数组操作\"><a href=\"#生成数组操作\" class=\"headerlink\" title=\"生成数组操作\"></a>生成数组操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div th:each=<span class=\"string\">&quot;num : <span class=\"variable\">$&#123;#numbers.sequence(0,4)&#125;</span>&quot;</span> &gt;</span><br><span class=\"line\">          &lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;num&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;div th:each=<span class=\"string\">&quot;num : <span class=\"variable\">$&#123;#numbers.sequence(0,4,1)&#125;</span>&quot;</span> &gt;</span><br><span class=\"line\">         &lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;num&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;div th:each=<span class=\"string\">&quot;num : <span class=\"variable\">$&#123;#numbers.sequence(0,10,2)&#125;</span>&quot;</span> &gt;</span><br><span class=\"line\">         &lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;num&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div&gt;&lt;p&gt;0&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;1&lt;/p&gt;&lt;/div&gt; </span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;3&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;4&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;0&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;1&lt;/p&gt;&lt;/div&gt; </span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;3&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;4&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;0&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div&gt;&lt;p&gt;4&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"strings\"><a href=\"#strings\" class=\"headerlink\" title=\"strings\"></a>strings</h2><p>处理String的相关操作，包括:</p>\n<ul>\n<li>字符串转换（toString）</li>\n<li>检查字符串是否为空（isEmpty）</li>\n<li>字符串是为空替换操作（defaultString）</li>\n<li>检查字符串中是否包含某个字符串（contains + containsIgnoreCase）</li>\n<li>检查字符串是以片段开头还是结尾（startsWith endsWith）</li>\n<li>截取（substring substringAfter）</li>\n<li>替换（replace）</li>\n<li>追加（prepend append）</li>\n<li>变更大小写（toUpperCase toLowerCase）</li>\n<li>拆分和组合字符串（arrayJoin arraySplit）</li>\n<li>去空格（trim）</li>\n<li>缩写文本（abbreviate）</li>\n<li>字符串连接（concat）</li>\n</ul>\n<p>java 代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/strings&quot;</span>)</span><br><span class=\"line\">    public String strings(Model model)&#123;</span><br><span class=\"line\">        Object object = <span class=\"string\">&quot;123&quot;</span>;</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;object&quot;</span>,object);</span><br><span class=\"line\"></span><br><span class=\"line\">        List&lt;Integer&gt; numList = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">        numList.add(1);</span><br><span class=\"line\">        numList.add(12);</span><br><span class=\"line\">        numList.add(13);</span><br><span class=\"line\">        model.addAttribute(<span class=\"string\">&quot;numList&quot;</span>,numList);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Object object = <span class=\"string\">&quot;123&quot;</span>;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;object&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;123&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"toString操作\"><a href=\"#toString操作\" class=\"headerlink\" title=\"toString操作\"></a>toString操作</h3><p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Object object = <span class=\"string\">&quot;123&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">List&lt;Integer&gt; numList = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">    numList.add(1);</span><br><span class=\"line\">    numList.add(12);</span><br><span class=\"line\">    numList.add(13);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.toString(object)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.toString(numList)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;123&lt;/p&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;p&gt;[1, 12, 13]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"isEmpty操作\"><a href=\"#isEmpty操作\" class=\"headerlink\" title=\"isEmpty操作\"></a>isEmpty操作</h3><p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">String name = null;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.isEmpty(name)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt; nameList = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">nameList.add(<span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">nameList.add(null);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.listIsEmpty(nameList)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[<span class=\"literal\">false</span>, <span class=\"literal\">true</span>]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Set&lt;String&gt; nameSet = new HashSet&lt;String&gt;();</span><br><span class=\"line\">nameSet.add(null);</span><br><span class=\"line\">nameSet.add(<span class=\"string\">&quot;1&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.setIsEmpty(nameSet)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[<span class=\"literal\">true</span>, <span class=\"literal\">false</span>]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"defaultString操作\"><a href=\"#defaultString操作\" class=\"headerlink\" title=\"defaultString操作\"></a>defaultString操作</h3><p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">String name = null;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.defaultString(text,&#x27;该值为null&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;该值为null&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>Java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt; nameList = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">nameList.add(<span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">nameList.add(null);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.listDefaultString(textList,&#x27;该值为null&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[abc, 该值为null]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"contains操作\"><a href=\"#contains操作\" class=\"headerlink\" title=\"contains操作\"></a>contains操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.contains(&#x27;abcez&#x27;,&#x27;ez&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.containsIgnoreCase(&#x27;abcEZ&#x27;,&#x27;ez&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"startsWith-endsWith-操作\"><a href=\"#startsWith-endsWith-操作\" class=\"headerlink\" title=\"startsWith endsWith 操作\"></a>startsWith endsWith 操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.startsWith(&#x27;Donabcez&#x27;,&#x27;Don&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.endsWith(&#x27;Donabcezn&#x27;,&#x27;n&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"indexOf操作\"><a href=\"#indexOf操作\" class=\"headerlink\" title=\"indexOf操作\"></a>indexOf操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.indexOf(&#x27;abcefg&#x27;,&#x27;e&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"substring操作\"><a href=\"#substring操作\" class=\"headerlink\" title=\"substring操作\"></a>substring操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.substring(&#x27;abcefg&#x27;,3,5)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;ef&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"replace操作\"><a href=\"#replace操作\" class=\"headerlink\" title=\"replace操作\"></a>replace操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.replace(&#x27;lasabce&#x27;,&#x27;las&#x27;,&#x27;ler&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;lerabce&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"prepend操作\"><a href=\"#prepend操作\" class=\"headerlink\" title=\"prepend操作\"></a>prepend操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.prepend(&#x27;abc&#x27;,&#x27;012&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;012abc&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"append操作\"><a href=\"#append操作\" class=\"headerlink\" title=\"append操作\"></a>append操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.append(&#x27;abc&#x27;,&#x27;456&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;abc456&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"toUpperCase操作\"><a href=\"#toUpperCase操作\" class=\"headerlink\" title=\"toUpperCase操作\"></a>toUpperCase操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.toUpperCase(&#x27;abc&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;ABC&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"toLowerCase操作\"><a href=\"#toLowerCase操作\" class=\"headerlink\" title=\"toLowerCase操作\"></a>toLowerCase操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.toLowerCase(&#x27;ABC&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;abc&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"length操作\"><a href=\"#length操作\" class=\"headerlink\" title=\"length操作\"></a>length操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.length(&#x27;abc&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"trim操作\"><a href=\"#trim操作\" class=\"headerlink\" title=\"trim操作\"></a>trim操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.trim(&#x27; abc &#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;abc&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"abbreviate操作\"><a href=\"#abbreviate操作\" class=\"headerlink\" title=\"abbreviate操作\"></a>abbreviate操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#strings.abbreviate(&#x27;12345678910&#x27;,10)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;1234567...&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"objects\"><a href=\"#objects\" class=\"headerlink\" title=\"#objects\"></a>#objects</h2><p>处理Object对象的操作 包含obj不为空返回改值如果为空返回默认值（nullSafe）<br>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/objects&quot;</span>)</span><br><span class=\"line\">public String objects(Model model)&#123;</span><br><span class=\"line\">  Object obj = null;</span><br><span class=\"line\">  model.addAttribute(<span class=\"string\">&quot;obj&quot;</span>,obj);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#objects.nullSafe(obj,&#x27;该对象为null&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;该对象为null&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"bools\"><a href=\"#bools\" class=\"headerlink\" title=\"#bools\"></a>#bools</h2><p>判断对象是否为ture或者是否为false的操作。</p>\n<ul>\n<li>数字 1 为 ture , 0 为 false;</li>\n<li>“on” 为 true, “off” 为false;</li>\n<li>“true” 为true, “false”为 false;</li>\n</ul>\n<h3 id=\"isTrue操作\"><a href=\"#isTrue操作\" class=\"headerlink\" title=\"isTrue操作\"></a>isTrue操作</h3><p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(true)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(false)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(&#x27;on&#x27;)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(&#x27;off&#x27;)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(&#x27;true&#x27;)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(&#x27;false&#x27;)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(1)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#bools.isTrue(0)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"arrays\"><a href=\"#arrays\" class=\"headerlink\" title=\"#arrays\"></a>#arrays</h2><p>处理数组的相关操作的内置对象，包含：</p>\n<ul>\n<li>转换数组 toStringArray toIntegerArray</li>\n<li>获取数组的长度（length ）</li>\n<li>判断数组是否为空（isEmpty ）</li>\n<li>是否包含某个元素（contains）</li>\n<li>是否包含一批元素（containsAll）</li>\n<li>其中 toStringArray 等操作接受的是Object对象，containsAll 接受+ 一批元素支持数组和集合的参数。</li>\n</ul>\n<h3 id=\"toStringArray操作\"><a href=\"#toStringArray操作\" class=\"headerlink\" title=\"toStringArray操作\"></a>toStringArray操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/arrays&quot;</span>)</span><br><span class=\"line\">public String arrays(Model model)&#123;</span><br><span class=\"line\">  List&lt;String&gt; object = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">  object.add(<span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">  object.add(<span class=\"string\">&quot;2&quot;</span>);</span><br><span class=\"line\">  model.addAttribute(<span class=\"string\">&quot;object&quot;</span>,object);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#arrays.toStringArray(object)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[Ljava.lang.String;@3cca655d&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"length操作-1\"><a href=\"#length操作-1\" class=\"headerlink\" title=\"length操作\"></a>length操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer[] array = &#123;1,2,3&#125;;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#arrays.length(array)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"isEmpty操作-1\"><a href=\"#isEmpty操作-1\" class=\"headerlink\" title=\"isEmpty操作\"></a>isEmpty操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer[] array = &#123;1,2,3&#125;;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#arrays.isEmpty(array)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"contains操作-1\"><a href=\"#contains操作-1\" class=\"headerlink\" title=\"contains操作\"></a>contains操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer[] array = &#123;1,2,3&#125;;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#arrays.contains(array,1)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"containsAll操作\"><a href=\"#containsAll操作\" class=\"headerlink\" title=\"containsAll操作\"></a>containsAll操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer[] array = &#123;1,2,3&#125;;</span><br><span class=\"line\">Integer[] array2 = &#123;1,3&#125;;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#arrays.containsAll(array,array2)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"lists\"><a href=\"#lists\" class=\"headerlink\" title=\"#lists\"></a>#lists</h2><p>处理 list 相关操作的内置对象，包括：</p>\n<ul>\n<li>计算长度(size)</li>\n<li>检查list是否为空(isEmpty)</li>\n<li>检查元素是否包含在list中(contains,containsAll)</li>\n<li>对给定list的副本排序(sort)<br>java代码</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/lists&quot;</span>)</span><br><span class=\"line\">public String lists(Model model)&#123;</span><br><span class=\"line\">   List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">   list.add(1);</span><br><span class=\"line\">   list.add(3);</span><br><span class=\"line\">   list.add(2);</span><br><span class=\"line\">   model.addAttribute(<span class=\"string\">&quot;list&quot;</span>,list);</span><br><span class=\"line\"> ｝</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.size(list)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(2);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.isEmpty(list)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(2);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.contains(list, 1)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(2);</span><br><span class=\"line\">List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list2.add(1);</span><br><span class=\"line\">list2.add(2);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<!-- elements 可以是数组 集合 list -->\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.containsAll(list,list2)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(2);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.sort(list)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[1, 2, 3]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"sets\"><a href=\"#sets\" class=\"headerlink\" title=\"#sets\"></a>#sets</h2><p>处理 set 相关操作的内置对象，包括：</p>\n<ul>\n<li>转换为Set(toSet)</li>\n<li>计算长度(size)</li>\n<li>检查set是否为空(isEmpty)</li>\n<li>检查元素是否包含在set中 (contains,containsAll)</li>\n</ul>\n<h3 id=\"size操作\"><a href=\"#size操作\" class=\"headerlink\" title=\"size操作\"></a>size操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/sets&quot;</span>)</span><br><span class=\"line\">public String sets(Model model)&#123;</span><br><span class=\"line\">   Set&lt;Integer&gt; <span class=\"built_in\">set</span> = new HashSet&lt;Integer&gt;();</span><br><span class=\"line\">   set.add(1);</span><br><span class=\"line\">   set.add(2);</span><br><span class=\"line\">   set.add(3);</span><br><span class=\"line\">   set.add(4);</span><br><span class=\"line\">   model.addAttribute(<span class=\"string\">&quot;set&quot;</span>,<span class=\"built_in\">set</span>);</span><br><span class=\"line\"> ｝</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#sets.size(set)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"isEmpty-操作\"><a href=\"#isEmpty-操作\" class=\"headerlink\" title=\"isEmpty 操作\"></a>isEmpty 操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Set&lt;Integer&gt; <span class=\"built_in\">set</span> = new HashSet&lt;Integer&gt;();</span><br><span class=\"line\">  set.add(1);</span><br><span class=\"line\">  set.add(2);</span><br><span class=\"line\">  set.add(3);</span><br><span class=\"line\">  set.add(4);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#sets.isEmpty(set)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"contains操作-2\"><a href=\"#contains操作-2\" class=\"headerlink\" title=\"contains操作\"></a>contains操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Set&lt;Integer&gt; <span class=\"built_in\">set</span> = new HashSet&lt;Integer&gt;();</span><br><span class=\"line\">  set.add(1);</span><br><span class=\"line\">  set.add(2);</span><br><span class=\"line\">  set.add(3);</span><br><span class=\"line\">  set.add(4);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#sets.contains(set, 1)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"containsAll操作-1\"><a href=\"#containsAll操作-1\" class=\"headerlink\" title=\"containsAll操作\"></a>containsAll操作</h3><p>java代码</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Set&lt;Integer&gt; <span class=\"built_in\">set</span> = new HashSet&lt;Integer&gt;();</span><br><span class=\"line\">  set.add(1);</span><br><span class=\"line\">  set.add(2);</span><br><span class=\"line\">  set.add(3);</span><br><span class=\"line\">  set.add(4);</span><br><span class=\"line\"></span><br><span class=\"line\">Integer[] elements = &#123;1,2&#125;;</span><br><span class=\"line\">model.addAttribute(<span class=\"string\">&quot;elements&quot;</span>,elements);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#sets.containsAll(set, elements)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"sort操作\"><a href=\"#sort操作\" class=\"headerlink\" title=\"sort操作\"></a>sort操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Set&lt;Integer&gt; <span class=\"built_in\">set</span> = new HashSet&lt;Integer&gt;();</span><br><span class=\"line\">  set.add(1);</span><br><span class=\"line\">  set.add(2);</span><br><span class=\"line\">  set.add(3);</span><br><span class=\"line\">  set.add(4);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#lists.sort(list)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;[1, 2, 3]&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"maps\"><a href=\"#maps\" class=\"headerlink\" title=\"#maps\"></a>#maps</h2><p>处理 map相关操作的内置对象，包括：</p>\n<ul>\n<li>计算长度(size)</li>\n<li>检查map是否为空(isEmpty)</li>\n<li>检查映射中是否包含键或值(containsKey,containsAllKeys,containsValue)</li>\n</ul>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/maps&quot;</span>)</span><br><span class=\"line\">public String maps(Model model)&#123;</span><br><span class=\"line\">   Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br><span class=\"line\">   model.addAttribute(<span class=\"string\">&quot;map&quot;</span>,map);</span><br><span class=\"line\">｝</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.size(map)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;3&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.isEmpty(map)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">false</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">  map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.containsKey(map, &#x27;1&#x27;)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br><span class=\"line\">String[] keys = &#123;<span class=\"string\">&quot;1&quot;</span>,<span class=\"string\">&quot;2&quot;</span>&#125;;</span><br><span class=\"line\">model.addAttribute(<span class=\"string\">&quot;keys&quot;</span>,keys);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<!-- keys 可以是数组可以是集合 -->\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.containsAllKeys(map, keys)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.containsValue(map, 2)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;1&quot;</span>,1);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;2&quot;</span>,2);</span><br><span class=\"line\">   map.put(<span class=\"string\">&quot;3&quot;</span>,3);</span><br><span class=\"line\">Integer[] values = &#123;1,2&#125;;</span><br><span class=\"line\">model.addAttribute(<span class=\"string\">&quot;values&quot;</span>,values);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<!-- values 可以是数组可以是集合 -->\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#maps.containsAllValues(map, values)&#125;</span>&quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;<span class=\"literal\">true</span>&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"aggregates\"><a href=\"#aggregates\" class=\"headerlink\" title=\"#aggregates\"></a>#aggregates</h2><p>用户处理集合或者数组的一些统计操作，包括：</p>\n<ul>\n<li>求和(sum)</li>\n<li>求平均值(avg)</li>\n<li>处理包装类型或基本类型的数组或集合</li>\n<li>求和操作</li>\n</ul>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">@RequestMapping(<span class=\"string\">&quot;/aggregates&quot;</span>)</span><br><span class=\"line\">public String aggregates(Model model)&#123;</span><br><span class=\"line\">   Integer[] array = &#123;1,2,3,4&#125;;</span><br><span class=\"line\">   model.addAttribute(<span class=\"string\">&quot;array&quot;</span>,array);</span><br><span class=\"line\">   <span class=\"built_in\">return</span> <span class=\"string\">&quot;/course/aggregates&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#aggregates.sum(array)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;10&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(2);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(4);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#aggregates.sum(list)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;10&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"求平均值操作\"><a href=\"#求平均值操作\" class=\"headerlink\" title=\"求平均值操作\"></a>求平均值操作</h3><p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">Integer[] array = &#123;1,2,3,4&#125;;</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#aggregates.avg(array)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;2.5&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>java代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();</span><br><span class=\"line\">list.add(1);</span><br><span class=\"line\">list.add(2);</span><br><span class=\"line\">list.add(3);</span><br><span class=\"line\">list.add(4);</span><br></pre></td></tr></table></figure>\n<p>模板代码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p th:text=<span class=\"string\">&quot;<span class=\"variable\">$&#123;#aggregates.avg(list)&#125;</span> &quot;</span>&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<p>结果页面：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;p&gt;2.5&lt;/p&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><p>本文主要介绍 Thymeleaf 的基础用法、内联、模板布局、预定义的工具对象。整体来看Thymeleaf 使用语法还是很强大的。</p>\n","categories":["Spring-Boot"],"tags":["Sprint Boot","Thymeleaf","模板引擎"]}]