<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="stylesheet" href="/css/search.css">
<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/search.js"></script>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #5cb0ff; /*进度条颜色*/
        height: 2px;
    }
    .pace .pace-progress-inner {
        box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>

    <meta name="author" content="DengBoCong">


    <meta name="subtitle" content="NLP | Deep Learning | Java">




<title>关于RNN理论和实践的一些总结 | DengBoCong</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.3.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">DengBoCong</a></div>
            <div class="menu navbar-right">
                <div class="search" style="float: left; margin-top: 7px;">
                    <form class="form-search">
                        <input class="input" placeholder="search content..." autocomplete="off" onchange="inputChange(event)" id="pc-search-input"/>
                    </form>
                    <div class="search-btn">
                        <img src="/image/search.png" class="search-btn-img" />
                    </div>
                </div>
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/projects">Projects</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">DengBoCong</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/projects">Projects</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">关于RNN理论和实践的一些总结</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">DengBoCong</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">December 8, 2020&nbsp;&nbsp;17:51:44</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Deep-Learning/">Deep-Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>
</blockquote>
<p>本篇文章主要总结我在学习过程中遇到的RNN、其相关变种，并对相关结构进行说明和结构图展示。内容包括RNN、RecNN、多层、双向、RNNCell等等，同时包括在计算框架（TensorFlow及PyTorch）API层面的一些理解记录。本篇文章不进行深入推导和底层原理介绍，仅做总结记录，感兴趣者可自行根据内容详细查阅资料。</p>
<blockquote>
<p>RNN（递归神经网络）包括Recurrent Neural Network和Recursive Neural Network两种，分别为时间递归神经网络和结构递归神经网络。</p>
</blockquote>
<p>计算框架版本：</p>
<ul>
<li>TensorFlow2.3</li>
<li>PyTorch1.7.0<h1 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h1>在进行后面内容的陈述之前，先来简单结合计算框架说明一下vanilla RNN、LSTM、GRU之间的区别。虽然将vanilla RNN、LSTM、GRU这个三个分开讲进行对比，但是不要忘记它们都是RNN，所以在宏观角度都是如下结构：<br><img src="https://img-blog.csdnimg.cn/20201208151624631.png#pic_center" alt="在这里插入图片描述"><br>而它们区别在于中间的那个隐藏状态计算单元，这里贴出它们的计算单元的细节，从左到右分别是vanilla RNN、LSTM、GRU。<br><img src="https://img-blog.csdnimg.cn/20201208152622203.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>看了隐藏单元之后，你有没有发现LSTM和其他两个的输入多了一个cell state，LSTM的门道就在这，cell state 就是实现LSTM的关键（ps：GRU其实也有分hidden state和cell state，不过在GRU中它们两个是相同的）。细节我不去深究，感兴趣的自行查看论文：</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.2329.pdf">RNN</a></li>
<li><a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf">LSTM</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.1078v3.pdf">GRU</a></li>
</ul>
<p>我这里就简单的结合<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api/stable">TensorFlow</a>和<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">PyTorch</a>说明一下cell state和hidden state，首先看下面两个计算框架的调用（详细参数自行查阅文档，这里只是为了说明state）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow中的LSTM调用</span></span><br><span class="line">whole_seq_output, final_memory_state, final_carry_state =</span><br><span class="line">			tf.keras.layers.LSTM(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)(inputs)</span><br><span class="line"><span class="comment"># Pytorch中的LSTM调用</span></span><br><span class="line">output, (hn, cn) = torch.nn.LSTM(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)(<span class="built_in">input</span>, (h0, c0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow中的GRU调用</span></span><br><span class="line">whole_sequence_output, final_state =</span><br><span class="line">			tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)(inputs)</span><br><span class="line"><span class="comment"># Pytorch中的GRU调用</span></span><br><span class="line">output, hn = torch.nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)(<span class="built_in">input</span>, h0)</span><br></pre></td></tr></table></figure>
<p>以TensorFlow举例（PyTorch默认都返回），当return_state参数设置为True时，将会返回隐藏层状态，即cell_state。在LSTM 的网络结构中，直接根据当前input 数据，得到的输出称为 hidden state，还有一种数据是不仅仅依赖于当前输入数据，而是一种伴随整个网络过程中用来记忆，遗忘，选择并最终影响hidden state结果的东西，称为 cell state。cell state默认是不输出的，它仅对输出 hidden state 产生影响。通常情况，我们不需要访问cell state，但当需要对 cell state 的初始值进行设定时，就需要将其返回。所以在上面的TensorFlow对LSTM的调用中，final_memory_state是最后一个timestep的状态，final_carry_state是最后一个timestep的cell state。既然见到LSTM和GRU，那下面就贴一张它们的状态更新公式图以作记录：<br><img src="https://img-blog.csdnimg.cn/20201208161136131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>后面简要阐述的所有RNN及其变种，都是代指vanilla RNN、LSTM、GRU三个，只不过为了方便描述，以RNN作为总称进行说明。</p>
<blockquote>
<p>TensorFlow中，RNN类是作为如第一张结构图那些的宏观结构，所以它有一个cell参数，你可以根据实际需要传入SimpleRNNCell、LSTMCell和GRUCell（这三个你就可以理解成上面讲的计算单元），它们三个可以单独使用，在一些地方特别管用。</p>
</blockquote>
<blockquote>
<p>PyTorch中大致是一样的，不过RNN类则是标准的RNN实现的，而不是像Tensorflow那样的架构，PyTorch同样有RNNCell、LSTMCell和GRUCell</p>
</blockquote>
<h1 id="标准RNN"><a href="#标准RNN" class="headerlink" title="标准RNN"></a>标准RNN</h1><p>RNN忽略单元细节的具体结构图如下。从图中就能够很清楚的看到，上一时刻的隐藏层是如何影响当前时刻的隐藏层的（注意这里Output的数量画少了，看起来不够形象，应该是 $X=[x_1,x_2,…,x_m]$和 $O=[o_1,o_2,…,o_m]$）。这里的Output是对应时间步的状态，而 $s$ 是隐藏状态，一般在实践中用它来初始化RNN。<br><img src="https://img-blog.csdnimg.cn/20201208112157265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>当然，可以换一种方式画结构图，如下图所示，按照RNN时间线展开。注意了，隐藏层 $s_t$ 不仅取决于 $x_t$ 还取决与 $s_{t-1}$。<br><img src="https://img-blog.csdnimg.cn/2020120811351449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从上面总结公式如下：<br>$$o_t=g(V_{s_t}) \quad\quad (1)$$    $$s_t=f(U_{x_t}+W_{s_{t-1}}) \quad\quad (2)$$<br>式（1）是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。$V$是输出层的权重矩阵，$g$是激活函数。式（2）是隐藏层的计算公式，它是循环层。$U$ 是输入 $x$ 的权重矩阵，$W$ 是上一次的值作为这一次的输入的权重矩阵，$f$ 是激活函数。从宏观意义上来说，循环层和全连接层的区别就是循环层多了一个权重矩阵 $W$。通过循环带入得下式：<br>$$o_t=Vf(U_{x_t}+Wf(U_{x_{t-1}}+Wf(U_{x_{t-2}}+Wf(U_{x_{t-3}}+…))))$$<br>从上面可以看出，循环神经网络的输出值 $o_t$，是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、$x_{t-3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p>
<h1 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h1><p>论文：<a target="_blank" rel="noopener" href="http://deeplearning.cs.cmu.edu/F20/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf">Link</a><br><img src="https://img-blog.csdnimg.cn/20201208144157106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从上图可以看出，双向RNN的隐藏层要保存两个值，一个 $A$ 参与正向计算，另一个值 $A’$ 参与反向计算（注意了，正向计算和反向计算不共享权重），最终的输出值取决于 $A$ 和 $A’$ 的计算方式。其计算方法有很多种，这里结合TensorFlow和PyTorch说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow中，需要使用Bidirectional来实现双向RNN，如下所示</span></span><br><span class="line"><span class="comment"># 其中merge_mode就是A和A&#x27;两者的计算方式：&#123;&#x27;sum&#x27;, &#x27;mul&#x27;, &#x27;concat&#x27;, &#x27;ave&#x27;, None&#125;</span></span><br><span class="line">tf.keras.layers.Bidirectional(</span><br><span class="line">    layer, merge_mode=<span class="string">&#x27;concat&#x27;</span>, weights=<span class="literal">None</span>, backward_layer=<span class="literal">None</span>, **kwargs</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PyTorch则不同，在各RNN的具体实现中，都有一个bidirectional参</span></span><br><span class="line"><span class="comment"># 数来控制是否是双向的，可自行查看PyTorch的API文档，特别说明的是</span></span><br><span class="line"><span class="comment"># PyTorch没有merge_mode，所以双向RNN直接会返回正向和反向的状态，</span></span><br><span class="line"><span class="comment"># 需要你自行进行合并操作</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="Multi-layer-stacked-RNN"><a href="#Multi-layer-stacked-RNN" class="headerlink" title="Multi-layer(stacked) RNN"></a>Multi-layer(stacked) RNN</h1><p>将多个RNN堆叠成多层RNN，每层RNN的输入为上一层RNN的输出，如下图所示。多层 (Multi-layer) RNN 效果很好，但可能会常用到 skip connections 的方式<br><img src="https://img-blog.csdnimg.cn/20201208164736875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h1><p>前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络，如下图所示：<br><img src="https://img-blog.csdnimg.cn/2020120816540656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们把第 $i$ 个隐藏层的值表示为 $s_t^{(i)}$、$s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为：<br>$$o_t=g(V^{(i)}s_t^{(i)}+V^{‘(i)}s_t^{‘(i)})$$    $$s_t^{(i)}=f(U^{(i)}s_t^{(i-1)}+W^{(i)}s_{t-1})$$     $$s_t^{‘(i)}=f(U^{‘(i)}s_t^{‘(i-1)}+W^{‘(i)}s_{t+1}^{‘})$$    $$s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1})$$    $$s_t^{‘(1)}=f(U^{‘(1)}x_t+W^{‘(1)}s_{t+1}^{‘})$$</p>
<h1 id="Recursive-Neural-Network"><a href="#Recursive-Neural-Network" class="headerlink" title="Recursive Neural Network"></a>Recursive Neural Network</h1><p>RNN适用于序列建模，而许多NLP问题需要处理树状结构，因此提出了RecNN的概念。与RNN将前序句子编码成状态向量类似，RecNN将每个树节点编码成状态向量。RecNN中的每棵子树都由一个向量表示，其值由其子节点的向量表示递归确定。<br><img src="https://img-blog.csdnimg.cn/20201208170829765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>RecNN接受的输入为一个有n个单词的句子的语法分析树，每个单词都表示为一个向量，语法分析树表示为一系列的生成式规则。举个例子，The boy saw her duck的分析树如下图：<br><img src="https://img-blog.csdnimg.cn/20201208170959474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对应的生成式规则（无标签+有标签）如下图：<br><img src="https://img-blog.csdnimg.cn/20201208171104105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RCQ18xMjE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>RecNN的输出为句子的内部状态向量（inside state vectors），每一个状态向量都对应一个树节点。具体RecNN细节自行详细查阅资料。</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>普遍来看, 神经网络都会有梯度消失和梯度爆炸的问题，其根源在于现在的神经网络在训练的时候，大多都是基于BP算法，这种误差向后传递的方式，即多元函数求偏导中，链式法则会产生 vanishing，而 RNN 产生梯度消失的根源是权值矩阵复用。</p>
<h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ul>
<li>前向计算每个神经元的输出值</li>
<li>反向计算每个神经元的误差项 $\delta_j$ 值，它是误差函数 $E$ 对神经元 $j$ 的加权输入 $net_j$ 的偏导数</li>
<li>计算每个权重的梯度</li>
<li>最后再用随机梯度下降算法更新权重。</li>
</ul>
<h3 id="RNN的梯度爆炸和消失问题"><a href="#RNN的梯度爆炸和消失问题" class="headerlink" title="RNN的梯度爆炸和消失问题"></a>RNN的梯度爆炸和消失问题</h3><p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>
<ul>
<li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li>
<li>使用 $relu$ 代替 $sigmoid$ 和 $tanh$ 作为激活函数。</li>
<li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li>
</ul>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/">How to use return_state or return_sequences in Keras</a></li>
<li><a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/5f39b91bff11">RNN summarize</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/f08eb58cf16b">从动图中理解 RNN，LSTM 和 GRU</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/135320350">RNN、lstm、gru详解</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/403665b55cd4">用 Recursive Neural Networks 得到分析树</a></li>
<li><a target="_blank" rel="noopener" href="https://zybuluo.com/hanbingtao/note/541458">循环神经网络</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chenjieyouge/p/12556237.html">双向 和 多重 RNN</a></li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>DengBoCong</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://dengbocong.cn/Deep-Learning/f43bcaebe5e9/">http://dengbocong.cn/Deep-Learning/f43bcaebe5e9/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Licensed under the Apache License, Version 2.0 (the "License")</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Stay hungry, Stay foolish.</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        <a href="/tags/RNN/"># RNN</a>
                    
                        <a href="/tags/LSTM/"># LSTM</a>
                    
                        <a href="/tags/GRU/"># GRU</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Deep-Learning/43c556e88dfe/">利器：TTS-Frontend中英Text-to-Phoneme-Converter，附代码</a>
            
            
            <a class="next" rel="next" href="/Paper-Reading/5dff8ecd5ba4/">论文阅读笔记：使用Transformer进行语音合成</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Copyright 2020-2021 DengBoCong. All Rights Reserved. 赣ICP备20002291号</span>
    </div>
</footer>

    </div>
    <div id="u-search">
        <div class="modal">
            <div class="modal-header">
                <div class="container">
                    <form id="u-search-modal-form" class="u-search-modal-form">
                        <button type="submit" class="form-submit-btn">
                            <img src="/image/search.png" class="search-btn-img" />
                        </button>
                        <input placeholder="请输入关键字搜索文章..." class="form-input" onchange="inputChange(event)" id="modal-form-input">
                    </form>
                    <a class="modal-close">x</a>
                </div>
                <div class="search-loading">
                    <div class="search-loading-bar"></div>
                </div>
            </div>
            <div class="modal-body">
            </div>
        </div>
        <div class="modal-overlay"></div>
    </div>
</body>
</html>
