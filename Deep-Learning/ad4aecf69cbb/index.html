<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="stylesheet" href="/css/search.css">
<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/search.js"></script>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #5cb0ff; /*进度条颜色*/
        height: 2px;
    }
    .pace .pace-progress-inner {
        box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>

    <meta name="author" content="DengBoCong">


    <meta name="subtitle" content="NLP | Deep Learning | Java">




<title>如何理解TensorFlow计算图？ | DengBoCong</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.3.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">DengBoCong</a></div>
            <div class="menu navbar-right">
                <div class="search" style="float: left; margin-top: 7px;">
                    <form class="form-search">
                        <input class="input" placeholder="search content..." autocomplete="off" onchange="inputChange(event)" id="pc-search-input"/>
                    </form>
                    <div class="search-btn">
                        <img src="/image/search.png" class="search-btn-img" />
                    </div>
                </div>
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/projects">Projects</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">DengBoCong</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/projects">Projects</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">如何理解TensorFlow计算图？</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">DengBoCong</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">January 16, 2021&nbsp;&nbsp;23:42:11</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Deep-Learning/">Deep-Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>Github：本文代码放在该项目中：<a target="_blank" rel="noopener" href="https://github.com/DengBoCong/nlp-paper">NLP相关Paper笔记和代码复现</a><br>说明：讲解时会对相关文章资料进行思想、结构、优缺点，内容进行提炼和记录，相关引用会标明出处，引用之处如有侵权，烦请告知删除。<br>转载请注明：DengBoCong</p>
</blockquote>
<p>对于TensorFlow或者Pytorch，可能很多小伙伴已经使用它执行了很多模型任务了，但是回过头来仔细想想，对于这些计算框架中的计算图可能还一知半解，没有好好理解研究过，这篇文章就来捋一捋计算图，这毕竟是Tensorflow和Pytorch这样的深度学习计算框架非常重要的概念。</p>
<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>无论是机器学习也好，还是深度学习也好，都是围绕着数学建模以及数学运算进行的，在这样的背景之下，诞生了许多的计算框架，我们所熟知的TensorFlow和Pytorch就是其中的主流，这些计算框架以计算服务为根本，自然需要一个计算模型。如果接触过开发的小伙伴就能体会到，这些计算框架的编程方式有着很大的差异。无论是编译类型的语言还是脚本语言，都是一步一步的计算变量， 从而得到结果，比如<code>result = input1 + input2</code>，当执行完语句后，就会得到<code>result</code>的值。</p>
<p>而TensorFlow和Pytorch则不一样，首先需要通过编程构建一个计算图，然后将数据作为输入，通过这个计算图规定的计算操作进行计算，最后得到计算结果。这种符号式编程有着较多的嵌入和优化，性能也随之提升。同时计算图非常适合用来思考数学表达式，举个例子，比如计算 $e=(a+b)*(b+1)$，在这个式子中存在两个加法和一个乘法的运算，为了更加方便我们讨论，我们引入中间变量来给每个运算的输出表示为一个变量，如下：<br>$$c=a+b$$   $$d=b+1$$   $$e=c∗d$$<br>接下来，我们来构建计算图，我们将所有这些操作放入节点中，并同时计算出计算结果，如下：<br><img src="https://pic4.zhimg.com/80/v2-afe67feb6df30d1fe6e7a18caa288ee7_720w.jpg" alt="在这里插入图片描述"><br>我们可以清晰的看到运算表达式中，各个运算操作以及变量间的依赖和调用关系。 接着我们来求边的偏导数，如下：<br><img src="https://pic3.zhimg.com/80/v2-dd6e59e2939393d595fb0d145014f9be_720w.jpg" alt="在这里插入图片描述"><br>通过链式法则，我们逐节点的计算偏导数，在网络backward时候，需要用链式求导法则求出网络最后输出的梯度，然后再对网络进行优化。类似上图的表达形式就是TensorFlow以及Pytorch的基本计算模型。<strong>总结而言，计算图模型由节点(nodes)和线(edges)组成，节点表示操作符Operator，或者称之为算子，线表示计算间的依赖，实线表示有数据传递依赖，传递的数据即张量，虚线通常可以表示控制依赖，即执行先后顺序。</strong></p>
<p>计算图从本质上来说，是TensorFlow在内存中构建的程序逻辑图，计算图可以被分割成多个块，并且可以并行地运行在多个不同的cpu或gpu上，这被称为并行计算。因此，计算图可以支持大规模的神经网络，如下：<br><img src="https://pic4.zhimg.com/80/v2-da6151da56abe898e72115915e76f603_720w.jpg" alt="在这里插入图片描述"></p>
<h1 id="Tensorflow中的计算图"><a href="#Tensorflow中的计算图" class="headerlink" title="Tensorflow中的计算图"></a>Tensorflow中的计算图</h1><p>TensorFlow中的计算图有三种，分别是静态计算图，动态计算图，以及Autograph，目前TensorFlow2默认采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果（在TensorFlow1中，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图）。对于动态图的好处显而易见，它方便调试程序，让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的，当然，这相对于静态图来讲牺牲了些效率，因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信，而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>
<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码，使用tf.function构建静态图的方式叫做 Autograph。</p>
<ul>
<li>静态计算图：一种比较早先使用静态计算图的方法分两步，第一步定义计算图，第二步在会话中执行计算图，如下展示了TensorFlow1.0和TensorFlow2.0的写法（可以调用tf.global_variables_initializer去初始化变量或者通过tf.control_dependencies去执行计算图中没有包含的节点）：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow1.0</span></span><br><span class="line"><span class="comment">#定义计算图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class="line">    x = tf.placeholder(name=<span class="string">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)  </span><br><span class="line">    y = tf.placeholder(name=<span class="string">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.string_join([x,y],name = <span class="string">&#x27;join&#x27;</span>,separator=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="comment">#执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">&quot;hello&quot;</span>,y:<span class="string">&quot;world&quot;</span>&#125;))</span><br><span class="line">   </span><br><span class="line"><span class="comment"># TensorFlow2.0</span></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">&quot;join&quot;</span>,separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">&quot;hello&quot;</span>,y:<span class="string">&quot;world&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>
<ul>
<li>动态计算图：动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行，因此称之为 Eager Excution。对于上面的操作，我们可以直接如下面代码的第一部分那样直接使用，也可以将使用动态计算图代码的输入和输出关系封装成函数，如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一部分</span></span><br><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class="line">x = tf.constant(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">y = tf.constant(<span class="string">&quot;world&quot;</span>)</span><br><span class="line">z = tf.strings.join([x,y],separator=<span class="string">&quot; &quot;</span>)</span><br><span class="line">tf.print(z) <span class="comment"># hello world</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部分</span></span><br><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span>(<span class="params">x,y</span>):</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line">    tf.print(z) <span class="comment"># hello world</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">result = strjoin(tf.constant(<span class="string">&quot;hello&quot;</span>),tf.constant(<span class="string">&quot;world&quot;</span>))</span><br><span class="line">print(result) <span class="comment"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Autograph：动态计算图运行效率相对较低，可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。不需要使用会话了，一切都像原始的Python语法一样自然。<strong>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率</strong>，如下（这就是为什么我们上面第二部分封装成函数的原因）：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用autograph构建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span>(<span class="params">x,y</span>):</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line">    tf.print(z) <span class="comment"># hello world</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">result = strjoin(tf.constant(<span class="string">&quot;hello&quot;</span>),tf.constant(<span class="string">&quot;world&quot;</span>))</span><br><span class="line">print(result) <span class="comment"># tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br></pre></td></tr></table></figure>
<h2 id="tf-function"><a href="#tf-function" class="headerlink" title="@tf.function"></a>@tf.function</h2><p>需要注意的是不是所有的函数都可以通过tf.function进行加速的，有的任务并不值得将函数转化为计算图形式，比如简单的矩阵乘法，然而，对于大量的计算，如对深度神经网络的优化，这一图转换能给性能带来巨大的提升。我们也把这样的图转化叫作tf.AutoGraph，在Tensorflow 2.0中，会自动的对被@tf.function装饰的函数进行AutoGraph优化。下面我们来看看被tf.function装饰的函数第一次执行时都做了什么：</p>
<ul>
<li>函数被执行并且被跟踪(tracing)，Eager execution处于关闭状态，所有的Tensorflow函数被当做tf.Operation进行图的创建。</li>
<li>AutoGraph被唤醒，去检测Python代码可以转为Tensorflow的逻辑，比如while &gt; tf.while, for &gt; tf.while, if &gt; tf.cond, assert &gt; tf.assert。</li>
<li>通过以上两步，对函数进行建图，为了保证Python代码中每一行的执行顺序，tf.control_dependencies被自动加入到代码中，保证第i行执行完后我们会执行第i+1行。</li>
<li>返回tf.Graph，根据函数名和输入参数，将这个graph存到一个cache中。</li>
<li>对于任何一个该函数的调用，我们会重复利用cache中的计算图进行计算。</li>
</ul>
<p>我们来看一下Tensorflow 2.0中Eager Execution的代码如何转为tf.function的代码，首先来看一段简单的Tensorflow 2.0代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">    a = tf.constant([[<span class="number">10</span>,<span class="number">10</span>],[<span class="number">11.</span>,<span class="number">1.</span>]])</span><br><span class="line">    x = tf.constant([[<span class="number">1.</span>,<span class="number">0.</span>],[<span class="number">0.</span>,<span class="number">1.</span>]])</span><br><span class="line">    b = tf.Variable(<span class="number">12.</span>)</span><br><span class="line">    y = tf.matmul(a, x) + b</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">print(f().numpy())</span><br><span class="line"><span class="comment">#执行结果</span></span><br><span class="line">[[<span class="number">22.</span> <span class="number">22.</span>]</span><br><span class="line"> [<span class="number">23.</span> <span class="number">13.</span>]]</span><br></pre></td></tr></table></figure>
<p>因为Tensorflow 2.0默认是Eager execution，代码的阅读和执行就和普通的Python代码一样，简单易读。首先我们简单的加上@tf.function装饰一下，为了方便调试，我们加入一个print和一个tf.print，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">    a = tf.constant([[<span class="number">10</span>,<span class="number">10</span>],[<span class="number">11.</span>,<span class="number">1.</span>]])</span><br><span class="line">    x = tf.constant([[<span class="number">1.</span>,<span class="number">0.</span>],[<span class="number">0.</span>,<span class="number">1.</span>]])</span><br><span class="line">    b = tf.Variable(<span class="number">12.</span>)</span><br><span class="line">    y = tf.matmul(a, x) + b</span><br><span class="line">    print(<span class="string">&quot;PRINT: &quot;</span>, y)</span><br><span class="line">    tf.print(<span class="string">&quot;TF-PRINT: &quot;</span>, y)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">f()</span><br><span class="line"><span class="comment">#执行结果</span></span><br><span class="line">PRINT:  Tensor(<span class="string">&quot;add:0&quot;</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">ValueError: tf.function-decorated function tried to create variables on non-first call.</span><br></pre></td></tr></table></figure>
<p>这里有个异常，为什么？因为tf.function可能会对一段Python函数进行多次执行来构图，在多次执行的过程中，同样的Variable被创建了多次，产生错误。这其实也是一个很容易混乱的概念，在eager mode下一个Variable是一个Python object，所以会在执行范围外被销毁，但是在tf.function的装饰下，Variable变成了tf.Variable，是在Graph中持续存在的。所以，把一个在eager mode下正常执行的函数转换到Tensorflow图形式，需要一边思考着计算图一边构建程序。</p>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@yaoyaowd/tensorflow-2-0%E4%B8%8A%E6%89%8B6-%E8%A7%A3%E5%89%96tf-function%E7%9A%84%E4%BD%BF%E7%94%A8-b48cef249ca4">Tensorflow 2.0上手6: 解剖tf.function的使用</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33378444">pytorch的计算图</a></li>
<li><a target="_blank" rel="noopener" href="http://www.likuli.com/archives/705/">TensorFlow计算模型——计算图</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/2-2,%E4%B8%89%E7%A7%8D%E8%AE%A1%E7%AE%97%E5%9B%BE.md">eat_tensorflow2_in_30_days</a></li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>DengBoCong</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://dengbocong.cn/Deep-Learning/ad4aecf69cbb/">http://dengbocong.cn/Deep-Learning/ad4aecf69cbb/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Licensed under the Apache License, Version 2.0 (the "License")</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Stay hungry, Stay foolish.</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        <a href="/tags/TensorFlow/"># TensorFlow</a>
                    
                        <a href="/tags/AutoGraph/"># AutoGraph</a>
                    
                        <a href="/tags/%E8%AE%A1%E7%AE%97%E5%9B%BE/"># 计算图</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/Paper-Reading/d9b52c160292/">论文阅读笔记：各种Optimizer梯度下降优化算法回顾和总结</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Copyright 2020-2021 DengBoCong. All Rights Reserved. 赣ICP备20002291号</span>
    </div>
</footer>

    </div>
    <div id="u-search">
        <div class="modal">
            <div class="modal-header">
                <div class="container">
                    <form id="u-search-modal-form" class="u-search-modal-form">
                        <button type="submit" class="form-submit-btn">
                            <img src="/image/search.png" class="search-btn-img" />
                        </button>
                        <input placeholder="请输入关键字搜索文章..." class="form-input" onchange="inputChange(event)" id="modal-form-input">
                    </form>
                    <a class="modal-close">x</a>
                </div>
                <div class="search-loading">
                    <div class="search-loading-bar"></div>
                </div>
            </div>
            <div class="modal-body">
            </div>
        </div>
        <div class="modal-overlay"></div>
    </div>
</body>
</html>
